{"id": "2505.17064", "pdf": "https://arxiv.org/pdf/2505.17064", "abs": "https://arxiv.org/abs/2505.17064", "authors": ["Maria-Teresa De Rosa Palmini", "Eva Cetinic"], "title": "Synthetic History: Evaluating Visual Representations of the Past in Diffusion Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "As Text-to-Image (TTI) diffusion models become increasingly influential in\ncontent creation, growing attention is being directed toward their societal and\ncultural implications. While prior research has primarily examined demographic\nand cultural biases, the ability of these models to accurately represent\nhistorical contexts remains largely underexplored. In this work, we present a\nsystematic and reproducible methodology for evaluating how TTI systems depict\ndifferent historical periods. For this purpose, we introduce the HistVis\ndataset, a curated collection of 30,000 synthetic images generated by three\nstate-of-the-art diffusion models using carefully designed prompts depicting\nuniversal human activities across different historical periods. We evaluate\ngenerated imagery across three key aspects: (1) Implicit Stylistic\nAssociations: examining default visual styles associated with specific eras;\n(2) Historical Consistency: identifying anachronisms such as modern artifacts\nin pre-modern contexts; and (3) Demographic Representation: comparing generated\nracial and gender distributions against historically plausible baselines. Our\nfindings reveal systematic inaccuracies in historically themed generated\nimagery, as TTI models frequently stereotype past eras by incorporating\nunstated stylistic cues, introduce anachronisms, and fail to reflect plausible\ndemographic patterns. By offering a scalable methodology and benchmark for\nassessing historical representation in generated imagery, this work provides an\ninitial step toward building more historically accurate and culturally aligned\nTTI models."}
{"id": "2505.17090", "pdf": "https://arxiv.org/pdf/2505.17090", "abs": "https://arxiv.org/abs/2505.17090", "authors": ["Phoebe Chua", "Cathy Mengying Fang", "Takehiko Ohkawa", "Raja Kushalnagar", "Suranga Nanayakkara", "Pattie Maes"], "title": "EmoSign: A Multimodal Dataset for Understanding Emotions in American Sign Language", "categories": ["cs.CV"], "comment": null, "summary": "Unlike spoken languages where the use of prosodic features to convey emotion\nis well studied, indicators of emotion in sign language remain poorly\nunderstood, creating communication barriers in critical settings. Sign\nlanguages present unique challenges as facial expressions and hand movements\nsimultaneously serve both grammatical and emotional functions. To address this\ngap, we introduce EmoSign, the first sign video dataset containing sentiment\nand emotion labels for 200 American Sign Language (ASL) videos. We also collect\nopen-ended descriptions of emotion cues. Annotations were done by 3 Deaf ASL\nsigners with professional interpretation experience. Alongside the annotations,\nwe include baseline models for sentiment and emotion classification. This\ndataset not only addresses a critical gap in existing sign language research\nbut also establishes a new benchmark for understanding model capabilities in\nmultimodal emotion recognition for sign languages. The dataset is made\navailable at https://huggingface.co/datasets/catfang/emosign."}
{"id": "2505.17097", "pdf": "https://arxiv.org/pdf/2505.17097", "abs": "https://arxiv.org/abs/2505.17097", "authors": ["Yanshu Li", "JianJiang Yang", "Bozheng Li", "Ruixiang Tang"], "title": "CAMA: Enhancing Multimodal In-Context Learning with Context-Aware Modulated Attention", "categories": ["cs.CV", "cs.CL"], "comment": "10 pages, 2 figures, 6 tables", "summary": "Multimodal in-context learning (ICL) enables large vision-language models\n(LVLMs) to efficiently adapt to novel tasks, supporting a wide array of\nreal-world applications. However, multimodal ICL remains unstable, and current\nresearch largely focuses on optimizing sequence configuration while overlooking\nthe internal mechanisms of LVLMs. In this work, we first provide a theoretical\nanalysis of attentional dynamics in multimodal ICL and identify three core\nlimitations of standard attention that ICL impair performance. To address these\nchallenges, we propose Context-Aware Modulated Attention (CAMA), a simple yet\neffective plug-and-play method for directly calibrating LVLM attention logits.\nCAMA is training-free and can be seamlessly applied to various open-source\nLVLMs. We evaluate CAMA on four LVLMs across six benchmarks, demonstrating its\neffectiveness and generality. CAMA opens new opportunities for deeper\nexploration and targeted utilization of LVLM attention dynamics to advance\nmultimodal reasoning."}
{"id": "2505.17127", "pdf": "https://arxiv.org/pdf/2505.17127", "abs": "https://arxiv.org/abs/2505.17127", "authors": ["Michal Golovanevsky", "William Rudman", "Michael Lepori", "Amir Bar", "Ritambhara Singh", "Carsten Eickhoff"], "title": "Pixels Versus Priors: Controlling Knowledge Priors in Vision-Language Models through Visual Counterfacts", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) perform well on tasks such as visual\nquestion answering, but it remains unclear whether their reasoning relies more\non memorized world knowledge or on the visual information present in the input\nimage. To investigate this, we introduce Visual CounterFact, a new dataset of\nvisually-realistic counterfactuals that put world knowledge priors (e.g, red\nstrawberry) into direct conflict with visual input (e.g, blue strawberry).\nUsing Visual CounterFact, we show that model predictions initially reflect\nmemorized priors, but shift toward visual evidence in mid-to-late layers. This\ndynamic reveals a competition between the two modalities, with visual input\nultimately overriding priors during evaluation. To control this behavior, we\npropose Pixels Versus Priors (PvP) steering vectors, a mechanism for\ncontrolling model outputs toward either world knowledge or visual input through\nactivation-level interventions. On average, PvP successfully shifts 92.5% of\ncolor and 74.6% of size predictions from priors to counterfactuals. Together,\nthese findings offer new tools for interpreting and controlling factual\nbehavior in multimodal models."}
{"id": "2505.17132", "pdf": "https://arxiv.org/pdf/2505.17132", "abs": "https://arxiv.org/abs/2505.17132", "authors": ["Tanqiu Jiang", "Jiacheng Liang", "Rongyi Zhu", "Jiawei Zhou", "Fenglong Ma", "Ting Wang"], "title": "Robustifying Vision-Language Models via Dynamic Token Reweighting", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Large vision-language models (VLMs) are highly vulnerable to jailbreak\nattacks that exploit visual-textual interactions to bypass safety guardrails.\nIn this paper, we present DTR, a novel inference-time defense that mitigates\nmultimodal jailbreak attacks through optimizing the model's key-value (KV)\ncaches. Rather than relying on curated safety-specific data or costly\nimage-to-text conversion, we introduce a new formulation of the safety-relevant\ndistributional shift induced by the visual modality. This formulation enables\nDTR to dynamically adjust visual token weights, minimizing the impact of\nadversarial visual inputs while preserving the model's general capabilities and\ninference efficiency. Extensive evaluation across diverse VLMs and attack\nbenchmarks demonstrates that \\sys outperforms existing defenses in both attack\nrobustness and benign task performance, marking the first successful\napplication of KV cache optimization for safety enhancement in multimodal\nfoundation models. The code for replicating DTR is available:\nhttps://anonymous.4open.science/r/DTR-2755 (warning: this paper contains\npotentially harmful content generated by VLMs.)"}
{"id": "2505.17201", "pdf": "https://arxiv.org/pdf/2505.17201", "abs": "https://arxiv.org/abs/2505.17201", "authors": ["Chaim Chai Elchik", "Fatemeh Karimi Nejadasl", "Seyed Sahand Mohammadi Ziabari", "Ali Mohammed Mansoor Alsahag"], "title": "A Framework for Multi-View Multiple Object Tracking using Single-View Multi-Object Trackers on Fish Data", "categories": ["cs.CV"], "comment": null, "summary": "Multi-object tracking (MOT) in computer vision has made significant\nadvancements, yet tracking small fish in underwater environments presents\nunique challenges due to complex 3D motions and data noise. Traditional\nsingle-view MOT models often fall short in these settings. This thesis\naddresses these challenges by adapting state-of-the-art single-view MOT models,\nFairMOT and YOLOv8, for underwater fish detecting and tracking in ecological\nstudies. The core contribution of this research is the development of a\nmulti-view framework that utilizes stereo video inputs to enhance tracking\naccuracy and fish behavior pattern recognition. By integrating and evaluating\nthese models on underwater fish video datasets, the study aims to demonstrate\nsignificant improvements in precision and reliability compared to single-view\napproaches. The proposed framework detects fish entities with a relative\naccuracy of 47% and employs stereo-matching techniques to produce a novel 3D\noutput, providing a more comprehensive understanding of fish movements and\ninteractions"}
{"id": "2505.17223", "pdf": "https://arxiv.org/pdf/2505.17223", "abs": "https://arxiv.org/abs/2505.17223", "authors": ["Siyang Song", "Micol Spitale", "Xiangyu Kong", "Hengde Zhu", "Cheng Luo", "Cristina Palmero", "German Barquero", "Sergio Escalera", "Michel Valstar", "Mohamed Daoudi", "Tobias Baur", "Fabien Ringeval", "Andrew Howes", "Elisabeth Andre", "Hatice Gunes"], "title": "REACT 2025: the Third Multiple Appropriate Facial Reaction Generation Challenge", "categories": ["cs.CV", "68T40"], "comment": null, "summary": "In dyadic interactions, a broad spectrum of human facial reactions might be\nappropriate for responding to each human speaker behaviour. Following the\nsuccessful organisation of the REACT 2023 and REACT 2024 challenges, we are\nproposing the REACT 2025 challenge encouraging the development and benchmarking\nof Machine Learning (ML) models that can be used to generate multiple\nappropriate, diverse, realistic and synchronised human-style facial reactions\nexpressed by human listeners in response to an input stimulus (i.e.,\naudio-visual behaviours expressed by their corresponding speakers). As a key of\nthe challenge, we provide challenge participants with the first natural and\nlarge-scale multi-modal MAFRG dataset (called MARS) recording 137 human-human\ndyadic interactions containing a total of 2856 interaction sessions covering\nfive different topics. In addition, this paper also presents the challenge\nguidelines and the performance of our baselines on the two proposed\nsub-challenges: Offline MAFRG and Online MAFRG, respectively. The challenge\nbaseline code is publicly available at\nhttps://github.com/reactmultimodalchallenge/baseline_react2025"}
{"id": "2505.17235", "pdf": "https://arxiv.org/pdf/2505.17235", "abs": "https://arxiv.org/abs/2505.17235", "authors": ["Omar Moured", "Yufan Chen", "Ruiping Liu", "Simon Reiß", "Philip Torr", "Jiaming Zhang", "Rainer Stiefelhagen"], "title": "CHAOS: Chart Analysis with Outlier Samples", "categories": ["cs.CV", "cs.CL"], "comment": "Data and code are publicly available at:\n  http://huggingface.co/datasets/omoured/CHAOS", "summary": "Charts play a critical role in data analysis and visualization, yet\nreal-world applications often present charts with challenging or noisy\nfeatures. However, \"outlier charts\" pose a substantial challenge even for\nMultimodal Large Language Models (MLLMs), which can struggle to interpret\nperturbed charts. In this work, we introduce CHAOS (CHart Analysis with Outlier\nSamples), a robustness benchmark to systematically evaluate MLLMs against chart\nperturbations. CHAOS encompasses five types of textual and ten types of visual\nperturbations, each presented at three levels of severity (easy, mid, hard)\ninspired by the study result of human evaluation. The benchmark includes 13\nstate-of-the-art MLLMs divided into three groups (i.e., general-, document-,\nand chart-specific models) according to the training scope and data.\nComprehensive analysis involves two downstream tasks (ChartQA and\nChart-to-Text). Extensive experiments and case studies highlight critical\ninsights into robustness of models across chart perturbations, aiming to guide\nfuture research in chart understanding domain. Data and code are publicly\navailable at: http://huggingface.co/datasets/omoured/CHAOS."}
{"id": "2505.17245", "pdf": "https://arxiv.org/pdf/2505.17245", "abs": "https://arxiv.org/abs/2505.17245", "authors": ["Ryota Yagi"], "title": "Extending Dataset Pruning to Object Detection: A Variance-based Approach", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Dataset pruning -- selecting a small yet informative subset of training data\n-- has emerged as a promising strategy for efficient machine learning, offering\nsignificant reductions in computational cost and storage compared to\nalternatives like dataset distillation. While pruning methods have shown strong\nperformance in image classification, their extension to more complex computer\nvision tasks, particularly object detection, remains relatively underexplored.\nIn this paper, we present the first principled extension of classification\npruning techniques to the object detection domain, to the best of our\nknowledge. We identify and address three key challenges that hinder this\ntransition: the Object-Level Attribution Problem, the Scoring Strategy Problem,\nand the Image-Level Aggregation Problem. To overcome these, we propose tailored\nsolutions, including a novel scoring method called Variance-based Prediction\nScore (VPS). VPS leverages both Intersection over Union (IoU) and confidence\nscores to effectively identify informative training samples specific to\ndetection tasks. Extensive experiments on PASCAL VOC and MS COCO demonstrate\nthat our approach consistently outperforms prior dataset pruning methods in\nterms of mean Average Precision (mAP). We also show that annotation count and\nclass distribution shift can influence detection performance, but selecting\ninformative examples is a more critical factor than dataset size or balance.\nOur work bridges dataset pruning and object detection, paving the way for\ndataset pruning in complex vision tasks."}
{"id": "2505.17256", "pdf": "https://arxiv.org/pdf/2505.17256", "abs": "https://arxiv.org/abs/2505.17256", "authors": ["Liang Shi", "Yun Fu"], "title": "ExpertGen: Training-Free Expert Guidance for Controllable Text-to-Face Generation", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in diffusion models have significantly improved text-to-face\ngeneration, but achieving fine-grained control over facial features remains a\nchallenge. Existing methods often require training additional modules to handle\nspecific controls such as identity, attributes, or age, making them inflexible\nand resource-intensive. We propose ExpertGen, a training-free framework that\nleverages pre-trained expert models such as face recognition, facial attribute\nrecognition, and age estimation networks to guide generation with fine control.\nOur approach uses a latent consistency model to ensure realistic and\nin-distribution predictions at each diffusion step, enabling accurate guidance\nsignals to effectively steer the diffusion process. We show qualitatively and\nquantitatively that expert models can guide the generation process with high\nprecision, and multiple experts can collaborate to enable simultaneous control\nover diverse facial aspects. By allowing direct integration of off-the-shelf\nexpert models, our method transforms any such model into a plug-and-play\ncomponent for controllable face generation."}
{"id": "2505.17280", "pdf": "https://arxiv.org/pdf/2505.17280", "abs": "https://arxiv.org/abs/2505.17280", "authors": ["Pushkar Shukla", "Aditya Chinchure", "Emily Diana", "Alexander Tolbert", "Kartik Hosanagar", "Vineeth N Balasubramanian", "Leonid Sigal", "Matthew Turk"], "title": "Mitigate One, Skew Another? Tackling Intersectional Biases in Text-to-Image Models", "categories": ["cs.CV"], "comment": null, "summary": "The biases exhibited by text-to-image (TTI) models are often treated as\nindependent, though in reality, they may be deeply interrelated. Addressing\nbias along one dimension - such as ethnicity or age - can inadvertently affect\nanother, like gender, either mitigating or exacerbating existing disparities.\nUnderstanding these interdependencies is crucial for designing fairer\ngenerative models, yet measuring such effects quantitatively remains a\nchallenge. To address this, we introduce BiasConnect, a novel tool for\nanalyzing and quantifying bias interactions in TTI models. BiasConnect uses\ncounterfactual interventions along different bias axes to reveal the underlying\nstructure of these interactions and estimates the effect of mitigating one bias\naxis on another. These estimates show strong correlation (+0.65) with observed\npost-mitigation outcomes. Building on BiasConnect, we propose InterMit, an\nintersectional bias mitigation algorithm guided by user-defined target\ndistributions and priority weights. InterMit achieves lower bias (0.33 vs.\n0.52) with fewer mitigation steps (2.38 vs. 3.15 average steps), and yields\nsuperior image quality compared to traditional techniques. Although our\nimplementation is training-free, InterMit is modular and can be integrated with\nmany existing debiasing approaches for TTI models, making it a flexible and\nextensible solution."}
{"id": "2505.17311", "pdf": "https://arxiv.org/pdf/2505.17311", "abs": "https://arxiv.org/abs/2505.17311", "authors": ["Harim Kim", "Yuhan Wang", "Minkyu Ahn", "Heeyoul Choi", "Yuyin Zhou", "Charmgil Hong"], "title": "Harnessing EHRs for Diffusion-based Anomaly Detection on Chest X-rays", "categories": ["cs.CV", "cs.LG"], "comment": "MICCAI 2025 early accept", "summary": "Unsupervised anomaly detection (UAD) in medical imaging is crucial for\nidentifying pathological abnormalities without requiring extensive labeled\ndata. However, existing diffusion-based UAD models rely solely on imaging\nfeatures, limiting their ability to distinguish between normal anatomical\nvariations and pathological anomalies. To address this, we propose Diff3M, a\nmulti-modal diffusion-based framework that integrates chest X-rays and\nstructured Electronic Health Records (EHRs) for enhanced anomaly detection.\nSpecifically, we introduce a novel image-EHR cross-attention module to\nincorporate structured clinical context into the image generation process,\nimproving the model's ability to differentiate normal from abnormal features.\nAdditionally, we develop a static masking strategy to enhance the\nreconstruction of normal-like images from anomalies. Extensive evaluations on\nCheXpert and MIMIC-CXR/IV demonstrate that Diff3M achieves state-of-the-art\nperformance, outperforming existing UAD methods in medical imaging. Our code is\navailable at this http URL https://github.com/nth221/Diff3M"}
{"id": "2505.17316", "pdf": "https://arxiv.org/pdf/2505.17316", "abs": "https://arxiv.org/abs/2505.17316", "authors": ["Jiachen Jiang", "Jinxin Zhou", "Bo Peng", "Xia Ning", "Zhihui Zhu"], "title": "Analyzing Fine-Grained Alignment and Enhancing Vision Understanding in Multimodal Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Achieving better alignment between vision embeddings and Large Language\nModels (LLMs) is crucial for enhancing the abilities of Multimodal LLMs\n(MLLMs), particularly for recent models that rely on powerful pretrained vision\nencoders and LLMs. A common approach to connect the pretrained vision encoder\nand LLM is through a projector applied after the vision encoder. However, the\nprojector is often trained to enable the LLM to generate captions, and hence\nthe mechanism by which LLMs understand each vision token remains unclear. In\nthis work, we first investigate the role of the projector in compressing vision\nembeddings and aligning them with word embeddings. We show that the projector\nsignificantly compresses visual information, removing redundant details while\npreserving essential elements necessary for the LLM to understand visual\ncontent. We then examine patch-level alignment -- the alignment between each\nvision patch and its corresponding semantic words -- and propose a\n*multi-semantic alignment hypothesis*. Our analysis indicates that the\nprojector trained by caption loss improves patch-level alignment but only to a\nlimited extent, resulting in weak and coarse alignment. To address this issue,\nwe propose *patch-aligned training* to efficiently enhance patch-level\nalignment. Our experiments show that patch-aligned training (1) achieves\nstronger compression capability and improved patch-level alignment, enabling\nthe MLLM to generate higher-quality captions, (2) improves the MLLM's\nperformance by 16% on referring expression grounding tasks, 4% on\nquestion-answering tasks, and 3% on modern instruction-following benchmarks\nwhen using the same supervised fine-tuning (SFT) setting. The proposed method\ncan be easily extended to other multimodal models."}
{"id": "2505.17317", "pdf": "https://arxiv.org/pdf/2505.17317", "abs": "https://arxiv.org/abs/2505.17317", "authors": ["Alyson East", "Elizabeth G. Campolongo", "Luke Meyers", "S M Rayeed", "Samuel Stevens", "Iuliia Zarubiieva", "Isadora E. Fluck", "Jennifer C. Girón", "Maximiliane Jousse", "Scott Lowe", "Kayla I Perry", "Isabelle Betancourt", "Noah Charney", "Evan Donoso", "Nathan Fox", "Kim J. Landsbergen", "Ekaterina Nepovinnykh", "Michelle Ramirez", "Parkash Singh", "Khum Thapa-Magar", "Matthew Thompson", "Evan Waite", "Tanya Berger-Wolf", "Hilmar Lapp", "Paula Mabee", "Graham Taylor", "Sydne Record"], "title": "Optimizing Image Capture for Computer Vision-Powered Taxonomic Identification and Trait Recognition of Biodiversity Specimens", "categories": ["cs.CV", "q-bio.QM"], "comment": null, "summary": "Biological collections house millions of specimens documenting Earth's\nbiodiversity, with digital images increasingly available through open-access\nplatforms. Most imaging protocols were developed for human visual\ninterpretation without considering computational analysis requirements. This\npaper aims to bridge the gap between current imaging practices and the\npotential for automated analysis by presenting key considerations for creating\nbiological specimen images optimized for computer vision applications. We\nprovide conceptual computer vision topics for context, addressing fundamental\nconcerns including model generalization, data leakage, and comprehensive\nmetadata documentation, and outline practical guidance on specimen imagine, and\ndata storage. These recommendations were synthesized through interdisciplinary\ncollaboration between taxonomists, collection managers, ecologists, and\ncomputer scientists. Through this synthesis, we have identified ten\ninterconnected considerations that form a framework for successfully\nintegrating biological specimen images into computer vision pipelines. The key\nelements include: (1) comprehensive metadata documentation, (2) standardized\nspecimen positioning, (3) consistent size and color calibration, (4) protocols\nfor handling multiple specimens in one image, (5) uniform background selection,\n(6) controlled lighting, (7) appropriate resolution and magnification, (8)\noptimal file formats, (9) robust data archiving strategies, and (10) accessible\ndata sharing practices. By implementing these recommendations, collection\nmanagers, taxonomists, and biodiversity informaticians can generate images that\nsupport automated trait extraction, species identification, and novel\necological and evolutionary analyses at unprecedented scales. Successful\nimplementation lies in thorough documentation of methodological choices."}
{"id": "2505.17328", "pdf": "https://arxiv.org/pdf/2505.17328", "abs": "https://arxiv.org/abs/2505.17328", "authors": ["Dylan Kline"], "title": "Game-invariant Features Through Contrastive and Domain-adversarial Learning", "categories": ["cs.CV"], "comment": null, "summary": "Foundational game-image encoders often overfit to game-specific visual\nstyles, undermining performance on downstream tasks when applied to new games.\nWe present a method that combines contrastive learning and domain-adversarial\ntraining to learn game-invariant visual features. By simultaneously encouraging\nsimilar content to cluster and discouraging game-specific cues via an\nadversarial domain classifier, our approach produces embeddings that generalize\nacross diverse games. Experiments on the Bingsu game-image dataset (10,000\nscreenshots from 10 games) demonstrate that after only a few training epochs,\nour model's features no longer cluster by game, indicating successful\ninvariance and potential for improved cross-game transfer (e.g., glitch\ndetection) with minimal fine-tuning. This capability paves the way for more\ngeneralizable game vision models that require little to no retraining on new\ngames."}
{"id": "2505.17330", "pdf": "https://arxiv.org/pdf/2505.17330", "abs": "https://arxiv.org/abs/2505.17330", "authors": ["Amit Agarwal", "Srikant Panda", "Kulbhushan Pachauri"], "title": "FS-DAG: Few Shot Domain Adapting Graph Networks for Visually Rich Document Understanding", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.IR", "cs.LG", "I.2.7; I.5.4; I.7"], "comment": "Published in the Proceedings of the 31st International Conference on\n  Computational Linguistics (COLING 2025), Industry Track, pages 100-114", "summary": "In this work, we propose Few Shot Domain Adapting Graph (FS-DAG), a scalable\nand efficient model architecture for visually rich document understanding\n(VRDU) in few-shot settings. FS-DAG leverages domain-specific and\nlanguage/vision specific backbones within a modular framework to adapt to\ndiverse document types with minimal data. The model is robust to practical\nchallenges such as handling OCR errors, misspellings, and domain shifts, which\nare critical in real-world deployments. FS-DAG is highly performant with less\nthan 90M parameters, making it well-suited for complex real-world applications\nfor Information Extraction (IE) tasks where computational resources are\nlimited. We demonstrate FS-DAG's capability through extensive experiments for\ninformation extraction task, showing significant improvements in convergence\nspeed and performance compared to state-of-the-art methods. Additionally, this\nwork highlights the ongoing progress in developing smaller, more efficient\nmodels that do not compromise on performance. Code :\nhttps://github.com/oracle-samples/fs-dag"}
{"id": "2505.17333", "pdf": "https://arxiv.org/pdf/2505.17333", "abs": "https://arxiv.org/abs/2505.17333", "authors": ["Xin You", "Minghui Zhang", "Hanxiao Zhang", "Jie Yang", "Nassir Navab"], "title": "Temporal Differential Fields for 4D Motion Modeling via Image-to-Video Synthesis", "categories": ["cs.CV"], "comment": "early accepted by MICCAI", "summary": "Temporal modeling on regular respiration-induced motions is crucial to\nimage-guided clinical applications. Existing methods cannot simulate temporal\nmotions unless high-dose imaging scans including starting and ending frames\nexist simultaneously. However, in the preoperative data acquisition stage, the\nslight movement of patients may result in dynamic backgrounds between the first\nand last frames in a respiratory period. This additional deviation can hardly\nbe removed by image registration, thus affecting the temporal modeling. To\naddress that limitation, we pioneeringly simulate the regular motion process\nvia the image-to-video (I2V) synthesis framework, which animates with the first\nframe to forecast future frames of a given length. Besides, to promote the\ntemporal consistency of animated videos, we devise the Temporal Differential\nDiffusion Model to generate temporal differential fields, which measure the\nrelative differential representations between adjacent frames. The prompt\nattention layer is devised for fine-grained differential fields, and the field\naugmented layer is adopted to better interact these fields with the I2V\nframework, promoting more accurate temporal variation of synthesized videos.\nExtensive results on ACDC cardiac and 4D Lung datasets reveal that our approach\nsimulates 4D videos along the intrinsic motion trajectory, rivaling other\ncompetitive methods on perceptual similarity and temporal consistency. Codes\nwill be available soon."}
{"id": "2505.17338", "pdf": "https://arxiv.org/pdf/2505.17338", "abs": "https://arxiv.org/abs/2505.17338", "authors": ["Zhongpai Gao", "Meng Zheng", "Benjamin Planche", "Anwesa Choudhuri", "Terrence Chen", "Ziyan Wu"], "title": "Render-FM: A Foundation Model for Real-time Photorealistic Volumetric Rendering", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Volumetric rendering of Computed Tomography (CT) scans is crucial for\nvisualizing complex 3D anatomical structures in medical imaging. Current\nhigh-fidelity approaches, especially neural rendering techniques, require\ntime-consuming per-scene optimization, limiting clinical applicability due to\ncomputational demands and poor generalizability. We propose Render-FM, a novel\nfoundation model for direct, real-time volumetric rendering of CT scans.\nRender-FM employs an encoder-decoder architecture that directly regresses 6D\nGaussian Splatting (6DGS) parameters from CT volumes, eliminating per-scan\noptimization through large-scale pre-training on diverse medical data. By\nintegrating robust feature extraction with the expressive power of 6DGS, our\napproach efficiently generates high-quality, real-time interactive 3D\nvisualizations across diverse clinical CT data. Experiments demonstrate that\nRender-FM achieves visual fidelity comparable or superior to specialized\nper-scan methods while drastically reducing preparation time from nearly an\nhour to seconds for a single inference step. This advancement enables seamless\nintegration into real-time surgical planning and diagnostic workflows. The\nproject page is: https://gaozhongpai.github.io/renderfm/."}
{"id": "2505.17343", "pdf": "https://arxiv.org/pdf/2505.17343", "abs": "https://arxiv.org/abs/2505.17343", "authors": ["Dillon Lohr", "Michael J. Proulx", "Mehedi Hasan Raju", "Oleg V. Komogortsev"], "title": "Ocular Authentication: Fusion of Gaze and Periocular Modalities", "categories": ["cs.CV", "cs.HC"], "comment": "Supplementary material is available", "summary": "This paper investigates the feasibility of fusing two eye-centric\nauthentication modalities-eye movements and periocular images-within a\ncalibration-free authentication system. While each modality has independently\nshown promise for user authentication, their combination within a unified\ngaze-estimation pipeline has not been thoroughly explored at scale. In this\nreport, we propose a multimodal authentication system and evaluate it using a\nlarge-scale in-house dataset comprising 9202 subjects with an eye tracking (ET)\nsignal quality equivalent to a consumer-facing virtual reality (VR) device. Our\nresults show that the multimodal approach consistently outperforms both\nunimodal systems across all scenarios, surpassing the FIDO benchmark. The\nintegration of a state-of-the-art machine learning architecture contributed\nsignificantly to the overall authentication performance at scale, driven by the\nmodel's ability to capture authentication representations and the complementary\ndiscriminative characteristics of the fused modalities."}
{"id": "2505.17352", "pdf": "https://arxiv.org/pdf/2505.17352", "abs": "https://arxiv.org/abs/2505.17352", "authors": ["Preeti Lamba", "Kiran Ravish", "Ankita Kushwaha", "Pawan Kumar"], "title": "Alignment and Safety of Diffusion Models via Reinforcement Learning and Reward Modeling: A Survey", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion models have emerged as leading generative models for images and\nother modalities, but aligning their outputs with human preferences and safety\nconstraints remains a critical challenge. This thesis proposal investigates\nmethods to align diffusion models using reinforcement learning (RL) and reward\nmodeling. We survey recent advances in fine-tuning text-to-image diffusion\nmodels with human feedback, including reinforcement learning from human and AI\nfeedback, direct preference optimization, and differentiable reward approaches.\nWe classify these methods based on the type of feedback (human, automated,\nbinary or ranked preferences), the fine-tuning technique (policy gradient,\nreward-weighted likelihood, direct backpropagation, etc.), and their efficiency\nand safety outcomes. We compare key algorithms and frameworks, highlighting how\nthey improve alignment with user intent or safety standards, and discuss\ninter-relationships such as how newer methods build on or diverge from earlier\nones. Based on the survey, we identify five promising research directions for\nthe next two years: (1) multi-objective alignment with combined rewards, (2)\nefficient human feedback usage and active learning, (3) robust safety alignment\nagainst adversarial inputs, (4) continual and online alignment of diffusion\nmodels, and (5) interpretable and trustworthy reward modeling for generative\nimages. Each direction is elaborated with its problem statement, challenges,\nrelated work, and a proposed research plan. The proposal is organized as a\ncomprehensive document with literature review, comparative tables of methods,\nand detailed research plans, aiming to contribute new insights and techniques\nfor safer and value-aligned diffusion-based generative AI."}
{"id": "2505.17353", "pdf": "https://arxiv.org/pdf/2505.17353", "abs": "https://arxiv.org/abs/2505.17353", "authors": ["Minseo Kim", "Axel Levy", "Gordon Wetzstein"], "title": "Dual Ascent Diffusion for Inverse Problems", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV"], "comment": "23 pages, 15 figures, 5 tables", "summary": "Ill-posed inverse problems are fundamental in many domains, ranging from\nastrophysics to medical imaging. Emerging diffusion models provide a powerful\nprior for solving these problems. Existing maximum-a-posteriori (MAP) or\nposterior sampling approaches, however, rely on different computational\napproximations, leading to inaccurate or suboptimal samples. To address this\nissue, we introduce a new approach to solving MAP problems with diffusion model\npriors using a dual ascent optimization framework. Our framework achieves\nbetter image quality as measured by various metrics for image restoration\nproblems, it is more robust to high levels of measurement noise, it is faster,\nand it estimates solutions that represent the observations more faithfully than\nthe state of the art."}
{"id": "2505.17358", "pdf": "https://arxiv.org/pdf/2505.17358", "abs": "https://arxiv.org/abs/2505.17358", "authors": ["Chinmay Talegaonkar", "Nikhil Gandudi Suresh", "Zachary Novack", "Yash Belhe", "Priyanka Nagasamudra", "Nicholas Antipa"], "title": "Repurposing Marigold for Zero-Shot Metric Depth Estimation via Defocus Blur Cues", "categories": ["cs.CV"], "comment": null, "summary": "Recent monocular metric depth estimation (MMDE) methods have made notable\nprogress towards zero-shot generalization. However, they still exhibit a\nsignificant performance drop on out-of-distribution datasets. We address this\nlimitation by injecting defocus blur cues at inference time into Marigold, a\n\\textit{pre-trained} diffusion model for zero-shot, scale-invariant monocular\ndepth estimation (MDE). Our method effectively turns Marigold into a metric\ndepth predictor in a training-free manner. To incorporate defocus cues, we\ncapture two images with a small and a large aperture from the same viewpoint.\nTo recover metric depth, we then optimize the metric depth scaling parameters\nand the noise latents of Marigold at inference time using gradients from a loss\nfunction based on the defocus-blur image formation model. We compare our method\nagainst existing state-of-the-art zero-shot MMDE methods on a self-collected\nreal dataset, showing quantitative and qualitative improvements."}
{"id": "2505.17363", "pdf": "https://arxiv.org/pdf/2505.17363", "abs": "https://arxiv.org/abs/2505.17363", "authors": ["Hassan Wasswa", "Hussein Abbass", "Timothy Lynar"], "title": "Are GNNs Worth the Effort for IoT Botnet Detection? A Comparative Study of VAE-GNN vs. ViT-MLP and VAE-MLP Approaches", "categories": ["cs.CV"], "comment": null, "summary": "Due to the exponential rise in IoT-based botnet attacks, researchers have\nexplored various advanced techniques for both dimensionality reduction and\nattack detection to enhance IoT security. Among these, Variational Autoencoders\n(VAE), Vision Transformers (ViT), and Graph Neural Networks (GNN), including\nGraph Convolutional Networks (GCN) and Graph Attention Networks (GAT), have\ngarnered significant research attention in the domain of attack detection. This\nstudy evaluates the effectiveness of four state-of-the-art deep learning\narchitectures for IoT botnet detection: a VAE encoder with a Multi-Layer\nPerceptron (MLP), a VAE encoder with a GCN, a VAE encoder with a GAT, and a ViT\nencoder with an MLP. The evaluation is conducted on a widely studied IoT\nbenchmark dataset--the N-BaIoT dataset for both binary and multiclass tasks.\nFor the binary classification task, all models achieved over 99.93% in\naccuracy, recall, precision, and F1-score, with no notable differences in\nperformance. In contrast, for the multiclass classification task, GNN-based\nmodels showed significantly lower performance compared to VAE-MLP and ViT-MLP,\nwith accuracies of 86.42%, 89.46%, 99.72%, and 98.38% for VAE-GCN, VAE-GAT,\nVAE-MLP, and ViT-MLP, respectively."}
{"id": "2505.17364", "pdf": "https://arxiv.org/pdf/2505.17364", "abs": "https://arxiv.org/abs/2505.17364", "authors": ["Apar Pokhrel", "Gia Dao"], "title": "Optimizing YOLOv8 for Parking Space Detection: Comparative Analysis of Custom YOLOv8 Architecture", "categories": ["cs.CV"], "comment": "9 pages", "summary": "Parking space occupancy detection is a critical component in the development\nof intelligent parking management systems. Traditional object detection\napproaches, such as YOLOv8, provide fast and accurate vehicle detection across\nparking lots but can struggle with borderline cases, such as partially visible\nvehicles, small vehicles (e.g., motorcycles), and poor lighting conditions. In\nthis work, we perform a comprehensive comparative analysis of customized\nbackbone architectures integrated with YOLOv8. Specifically, we evaluate\nvarious backbones -- ResNet-18, VGG16, EfficientNetV2, Ghost -- on the PKLot\ndataset in terms of detection accuracy and computational efficiency.\nExperimental results highlight each architecture's strengths and trade-offs,\nproviding insight into selecting suitable models for parking occupancy."}
{"id": "2505.17367", "pdf": "https://arxiv.org/pdf/2505.17367", "abs": "https://arxiv.org/abs/2505.17367", "authors": ["Zichuan Yang"], "title": "EVM-Fusion: An Explainable Vision Mamba Architecture with Neural Algorithmic Fusion", "categories": ["cs.CV", "cs.AI"], "comment": "16 pages, 4 figures", "summary": "Medical image classification is critical for clinical decision-making, yet\ndemands for accuracy, interpretability, and generalizability remain\nchallenging. This paper introduces EVM-Fusion, an Explainable Vision Mamba\narchitecture featuring a novel Neural Algorithmic Fusion (NAF) mechanism for\nmulti-organ medical image classification. EVM-Fusion leverages a multipath\ndesign, where DenseNet and U-Net based pathways, enhanced by Vision Mamba (Vim)\nmodules, operate in parallel with a traditional feature pathway. These diverse\nfeatures are dynamically integrated via a two-stage fusion process: cross-modal\nattention followed by the iterative NAF block, which learns an adaptive fusion\nalgorithm. Intrinsic explainability is embedded through path-specific spatial\nattention, Vim {\\Delta}-value maps, traditional feature SE-attention, and\ncross-modal attention weights. Experiments on a diverse 9-class multi-organ\nmedical image dataset demonstrate EVM-Fusion's strong classification\nperformance, achieving 99.75% test accuracy and provide multi-faceted insights\ninto its decision-making process, highlighting its potential for trustworthy AI\nin medical diagnostics."}
{"id": "2505.17392", "pdf": "https://arxiv.org/pdf/2505.17392", "abs": "https://arxiv.org/abs/2505.17392", "authors": ["Leon C. C. K", "Zeng Hui"], "title": "Dual-sensing driving detection model", "categories": ["cs.CV", "cs.AI", "68T07, 68T45, 68U10", "I.2.10; I.4.8; J.7"], "comment": "19 pages", "summary": "In this paper, a novel dual-sensing driver fatigue detection method combining\ncomputer vision and physiological signal analysis is proposed. The system\nexploits the complementary advantages of the two sensing modalities and breaks\nthrough the limitations of existing single-modality methods. We introduce an\ninnovative architecture that combines real-time facial feature analysis with\nphysiological signal processing, combined with advanced fusion strategies, for\nrobust fatigue detection. The system is designed to run efficiently on existing\nhardware while maintaining high accuracy and reliability. Through comprehensive\nexperiments, we demonstrate that our method outperforms traditional methods in\nboth controlled environments and real-world conditions, while maintaining high\naccuracy. The practical applicability of the system has been verified through\nextensive tests in various driving scenarios and shows great potential in\nreducing fatigue-related accidents. This study contributes to the field by\nproviding a more reliable, cost-effective, and humane solution for driver\nfatigue detection."}
{"id": "2505.17395", "pdf": "https://arxiv.org/pdf/2505.17395", "abs": "https://arxiv.org/abs/2505.17395", "authors": ["Gowtham Raj Vuppari", "Navarun Gupta", "Ahmed El-Sayed", "Xingguo Xiong"], "title": "Wildfire Detection Using Vision Transformer with the Wildfire Dataset", "categories": ["cs.CV", "cs.AI"], "comment": "Published at ASEE NE 2025", "summary": "The critical need for sophisticated detection techniques has been highlighted\nby the rising frequency and intensity of wildfires in the US, especially in\nCalifornia. In 2023, wildfires caused 130 deaths nationwide, the highest since\n1990. In January 2025, Los Angeles wildfires which included the Palisades and\nEaton fires burnt approximately 40,000 acres and 12,000 buildings, and caused\nloss of human lives. The devastation underscores the urgent need for effective\ndetection and prevention strategies. Deep learning models, such as Vision\nTransformers (ViTs), can enhance early detection by processing complex image\ndata with high accuracy. However, wildfire detection faces challenges,\nincluding the availability of high-quality, real-time data. Wildfires often\noccur in remote areas with limited sensor coverage, and environmental factors\nlike smoke and cloud cover can hinder detection. Additionally, training deep\nlearning models is computationally expensive, and issues like false\npositives/negatives and scaling remain concerns. Integrating detection systems\nwith real-time alert mechanisms also poses difficulties. In this work, we used\nthe wildfire dataset consisting of 10.74 GB high-resolution images categorized\ninto 'fire' and 'nofire' classes is used for training the ViT model. To prepare\nthe data, images are resized to 224 x 224 pixels, converted into tensor format,\nand normalized using ImageNet statistics."}
{"id": "2505.17412", "pdf": "https://arxiv.org/pdf/2505.17412", "abs": "https://arxiv.org/abs/2505.17412", "authors": ["Shuang Wu", "Youtian Lin", "Feihu Zhang", "Yifei Zeng", "Yikang Yang", "Yajie Bao", "Jiachen Qian", "Siyu Zhu", "Philip Torr", "Xun Cao", "Yao Yao"], "title": "Direct3D-S2: Gigascale 3D Generation Made Easy with Spatial Sparse Attention", "categories": ["cs.CV"], "comment": "Project page: https://nju3dv.github.io/projects/Direct3D-S2/", "summary": "Generating high resolution 3D shapes using volumetric representations such as\nSigned Distance Functions presents substantial computational and memory\nchallenges. We introduce Direct3D S2, a scalable 3D generation framework based\non sparse volumes that achieves superior output quality with dramatically\nreduced training costs. Our key innovation is the Spatial Sparse Attention\nmechanism, which greatly enhances the efficiency of Diffusion Transformer\ncomputations on sparse volumetric data. SSA allows the model to effectively\nprocess large token sets within sparse volumes, significantly reducing\ncomputational overhead and achieving a 3.9x speedup in the forward pass and a\n9.6x speedup in the backward pass. Our framework also includes a variational\nautoencoder that maintains a consistent sparse volumetric format across input,\nlatent, and output stages. Compared to previous methods with heterogeneous\nrepresentations in 3D VAE, this unified design significantly improves training\nefficiency and stability. Our model is trained on public available datasets,\nand experiments demonstrate that Direct3D S2 not only surpasses\nstate-of-the-art methods in generation quality and efficiency, but also enables\ntraining at 1024 resolution using only 8 GPUs, a task typically requiring at\nleast 32 GPUs for volumetric representations at 256 resolution, thus making\ngigascale 3D generation both practical and accessible. Project page:\nhttps://nju3dv.github.io/projects/Direct3D-S2/."}
{"id": "2505.17423", "pdf": "https://arxiv.org/pdf/2505.17423", "abs": "https://arxiv.org/abs/2505.17423", "authors": ["Shenghui Chen", "Po-han Li", "Sandeep Chichali", "Ufuk Topcu"], "title": "VIBE: Video-to-Text Information Bottleneck Evaluation for TL;DR", "categories": ["cs.CV", "cs.HC", "cs.IT", "math.IT"], "comment": null, "summary": "Many decision-making tasks, where both accuracy and efficiency matter, still\nrequire human supervision. For example, tasks like traffic officers reviewing\nhour-long dashcam footage or researchers screening conference videos can\nbenefit from concise summaries that reduce cognitive load and save time. Yet\ncurrent vision-language models (VLMs) often produce verbose, redundant outputs\nthat hinder task performance. Existing video caption evaluation depends on\ncostly human annotations and overlooks the summaries' utility in downstream\ntasks. We address these gaps with Video-to-text Information Bottleneck\nEvaluation (VIBE), an annotation-free method that scores VLM outputs using two\nmetrics: grounding (how well the summary aligns with visual content) and\nutility (how informative it is for the task). VIBE selects from randomly\nsampled VLM outputs by ranking them according to the two scores to support\neffective human decision-making. Human studies on LearningPaper24,\nSUTD-TrafficQA, and LongVideoBench show that summaries selected by VIBE\nconsistently improve performance-boosting task accuracy by up to 61.23% and\nreducing response time by 75.77% compared to naive VLM summaries or raw video."}
{"id": "2505.17425", "pdf": "https://arxiv.org/pdf/2505.17425", "abs": "https://arxiv.org/abs/2505.17425", "authors": ["Wei Jie Yeo", "Rui Mao", "Moloud Abdar", "Erik Cambria", "Ranjan Satapathy"], "title": "Debiasing CLIP: Interpreting and Correcting Bias in Attention Heads", "categories": ["cs.CV", "cs.CL"], "comment": "Under review", "summary": "Multimodal models like CLIP have gained significant attention due to their\nremarkable zero-shot performance across various tasks. However, studies have\nrevealed that CLIP can inadvertently learn spurious associations between target\nvariables and confounding factors. To address this, we introduce\n\\textsc{Locate-Then-Correct} (LTC), a contrastive framework that identifies\nspurious attention heads in Vision Transformers via mechanistic insights and\nmitigates them through targeted ablation. Furthermore, LTC identifies salient,\ntask-relevant attention heads, enabling the integration of discriminative\nfeatures through orthogonal projection to improve classification performance.\nWe evaluate LTC on benchmarks with inherent background and gender biases,\nachieving over a $>50\\%$ gain in worst-group accuracy compared to non-training\npost-hoc baselines. Additionally, we visualize the representation of selected\nheads and find that the presented interpretation corroborates our contrastive\nmechanism for identifying both spurious and salient attention heads. Code\navailable at https://github.com/wj210/CLIP_LTC."}
{"id": "2505.17437", "pdf": "https://arxiv.org/pdf/2505.17437", "abs": "https://arxiv.org/abs/2505.17437", "authors": ["Yuanshao Zhu", "James Jianqiao Yu", "Xiangyu Zhao", "Xiao Han", "Qidong Liu", "Xuetao Wei", "Yuxuan Liang"], "title": "Learning Generalized and Flexible Trajectory Models from Omni-Semantic Supervision", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted as a full paper by KDD'25 - Research Track", "summary": "The widespread adoption of mobile devices and data collection technologies\nhas led to an exponential increase in trajectory data, presenting significant\nchallenges in spatio-temporal data mining, particularly for efficient and\naccurate trajectory retrieval. However, existing methods for trajectory\nretrieval face notable limitations, including inefficiencies in large-scale\ndata, lack of support for condition-based queries, and reliance on trajectory\nsimilarity measures. To address the above challenges, we propose OmniTraj, a\ngeneralized and flexible omni-semantic trajectory retrieval framework that\nintegrates four complementary modalities or semantics -- raw trajectories,\ntopology, road segments, and regions -- into a unified system. Unlike\ntraditional approaches that are limited to computing and processing\ntrajectories as a single modality, OmniTraj designs dedicated encoders for each\nmodality, which are embedded and fused into a shared representation space. This\ndesign enables OmniTraj to support accurate and flexible queries based on any\nindividual modality or combination thereof, overcoming the rigidity of\ntraditional similarity-based methods. Extensive experiments on two real-world\ndatasets demonstrate the effectiveness of OmniTraj in handling large-scale\ndata, providing flexible, multi-modality queries, and supporting downstream\ntasks and applications."}
{"id": "2505.17440", "pdf": "https://arxiv.org/pdf/2505.17440", "abs": "https://arxiv.org/abs/2505.17440", "authors": ["Hefei Mei", "Zirui Wang", "Shen You", "Minjing Dong", "Chang Xu"], "title": "VEAttack: Downstream-agnostic Vision Encoder Attack against Large Vision Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Large Vision-Language Models (LVLMs) have demonstrated remarkable\ncapabilities in multimodal understanding and generation, yet their\nvulnerability to adversarial attacks raises significant robustness concerns.\nWhile existing effective attacks always focus on task-specific white-box\nsettings, these approaches are limited in the context of LVLMs, which are\ndesigned for diverse downstream tasks and require expensive full-model gradient\ncomputations. Motivated by the pivotal role and wide adoption of the vision\nencoder in LVLMs, we propose a simple yet effective Vision Encoder Attack\n(VEAttack), which targets the vision encoder of LVLMs only. Specifically, we\npropose to generate adversarial examples by minimizing the cosine similarity\nbetween the clean and perturbed visual features, without accessing the\nfollowing large language models, task information, and labels. It significantly\nreduces the computational overhead while eliminating the task and label\ndependence of traditional white-box attacks in LVLMs. To make this simple\nattack effective, we propose to perturb images by optimizing image tokens\ninstead of the classification token. We provide both empirical and theoretical\nevidence that VEAttack can easily generalize to various tasks. VEAttack has\nachieved a performance degradation of 94.5% on image caption task and 75.7% on\nvisual question answering task. We also reveal some key observations to provide\ninsights into LVLM attack/defense: 1) hidden layer variations of LLM, 2) token\nattention differential, 3) M\\\"obius band in transfer attack, 4) low sensitivity\nto attack steps. The code is available at\nhttps://github.com/hfmei/VEAttack-LVLM"}
{"id": "2505.17442", "pdf": "https://arxiv.org/pdf/2505.17442", "abs": "https://arxiv.org/abs/2505.17442", "authors": ["Hao Jing", "Anhong Wang", "Yifan Zhang", "Donghan Bu", "Junhui Hou"], "title": "Reflectance Prediction-based Knowledge Distillation for Robust 3D Object Detection in Compressed Point Clouds", "categories": ["cs.CV"], "comment": null, "summary": "Regarding intelligent transportation systems for vehicle networking,\nlow-bitrate transmission via lossy point cloud compression is vital for\nfacilitating real-time collaborative perception among vehicles with restricted\nbandwidth. In existing compression transmission systems, the sender lossily\ncompresses point coordinates and reflectance to generate a transmission code\nstream, which faces transmission burdens from reflectance encoding and limited\ndetection robustness due to information loss. To address these issues, this\npaper proposes a 3D object detection framework with reflectance\nprediction-based knowledge distillation (RPKD). We compress point coordinates\nwhile discarding reflectance during low-bitrate transmission, and feed the\ndecoded non-reflectance compressed point clouds into a student detector. The\ndiscarded reflectance is then reconstructed by a geometry-based reflectance\nprediction (RP) module within the student detector for precise detection. A\nteacher detector with the same structure as student detector is designed for\nperforming reflectance knowledge distillation (RKD) and detection knowledge\ndistillation (DKD) from raw to compressed point clouds. Our RPKD framework\njointly trains detectors on both raw and compressed point clouds to improve the\nstudent detector's robustness. Experimental results on the KITTI dataset and\nWaymo Open Dataset demonstrate that our method can boost detection accuracy for\ncompressed point clouds across multiple code rates. Notably, at a low code rate\nof 2.146 Bpp on the KITTI dataset, our RPKD-PV achieves the highest mAP of\n73.6, outperforming existing detection methods with the PV-RCNN baseline."}
{"id": "2505.17445", "pdf": "https://arxiv.org/pdf/2505.17445", "abs": "https://arxiv.org/abs/2505.17445", "authors": ["Inpyo Song", "Hyemin Hwang", "Jangwon Lee"], "title": "PawPrint: Whose Footprints Are These? Identifying Animal Individuals by Their Footprints", "categories": ["cs.CV"], "comment": "Accepted to ICIP 2025", "summary": "In the United States, as of 2023, pet ownership has reached 66% of households\nand continues to rise annually. This trend underscores the critical need for\neffective pet identification and monitoring methods, particularly as nearly 10\nmillion cats and dogs are reported stolen or lost each year. However,\ntraditional methods for finding lost animals like GPS tags or ID photos have\nlimitations-they can be removed, face signal issues, and depend on someone\nfinding and reporting the pet. To address these limitations, we introduce\nPawPrint and PawPrint+, the first publicly available datasets focused on\nindividual-level footprint identification for dogs and cats. Through\ncomprehensive benchmarking of both modern deep neural networks (e.g., CNN,\nTransformers) and classical local features, we observe varying advantages and\ndrawbacks depending on substrate complexity and data availability. These\ninsights suggest future directions for combining learned global representations\nwith local descriptors to enhance reliability across diverse, real-world\nconditions. As this approach provides a non-invasive alternative to traditional\nID tags, we anticipate promising applications in ethical pet management and\nwildlife conservation efforts."}
{"id": "2505.17449", "pdf": "https://arxiv.org/pdf/2505.17449", "abs": "https://arxiv.org/abs/2505.17449", "authors": ["Inpyo Song", "Jangwon Lee"], "title": "Real-time Traffic Accident Anticipation with Feature Reuse", "categories": ["cs.CV"], "comment": "Accepted to ICIP 2025", "summary": "This paper addresses the problem of anticipating traffic accidents, which\naims to forecast potential accidents before they happen. Real-time anticipation\nis crucial for safe autonomous driving, yet most methods rely on\ncomputationally heavy modules like optical flow and intermediate feature\nextractors, making real-world deployment challenging. In this paper, we thus\nintroduce RARE (Real-time Accident anticipation with Reused Embeddings), a\nlightweight framework that capitalizes on intermediate features from a single\npre-trained object detector. By eliminating additional feature-extraction\npipelines, RARE significantly reduces latency. Furthermore, we introduce a\nnovel Attention Score Ranking Loss, which prioritizes higher attention on\naccident-related objects over non-relevant ones. This loss enhances both\naccuracy and interpretability. RARE demonstrates a 4-8 times speedup over\nexisting approaches on the DAD and CCD benchmarks, achieving a latency of\n13.6ms per frame (73.3 FPS) on an RTX 6000. Moreover, despite its reduced\ncomplexity, it attains state-of-the-art Average Precision and reliably\nanticipates imminent collisions in real time. These results highlight RARE's\npotential for safety-critical applications where timely and explainable\nanticipation is essential."}
{"id": "2505.17457", "pdf": "https://arxiv.org/pdf/2505.17457", "abs": "https://arxiv.org/abs/2505.17457", "authors": ["Jiaxuan Lu", "Junyan Shi", "Yuhui Lin", "Fang Yan", "Yue Gao", "Shaoting Zhang", "Xiaosong Wang"], "title": "Graph Mamba for Efficient Whole Slide Image Understanding", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Whole Slide Images (WSIs) in histopathology present a significant challenge\nfor large-scale medical image analysis due to their high resolution, large\nsize, and complex tile relationships. Existing Multiple Instance Learning (MIL)\nmethods, such as Graph Neural Networks (GNNs) and Transformer-based models,\nface limitations in scalability and computational cost. To bridge this gap, we\npropose the WSI-GMamba framework, which synergistically combines the relational\nmodeling strengths of GNNs with the efficiency of Mamba, the State Space Model\ndesigned for sequence learning. The proposed GMamba block integrates Message\nPassing, Graph Scanning & Flattening, and feature aggregation via a\nBidirectional State Space Model (Bi-SSM), achieving Transformer-level\nperformance with 7* fewer FLOPs. By leveraging the complementary strengths of\nlightweight GNNs and Mamba, the WSI-GMamba framework delivers a scalable\nsolution for large-scale WSI analysis, offering both high accuracy and\ncomputational efficiency for slide-level classification."}
{"id": "2505.17461", "pdf": "https://arxiv.org/pdf/2505.17461", "abs": "https://arxiv.org/abs/2505.17461", "authors": ["Kazuki Hayashi", "Shintaro Ozaki", "Yusuke Sakai", "Hidetaka Kamigaito", "Taro Watanabe"], "title": "Diagnosing Vision Language Models' Perception by Leveraging Human Methods for Color Vision Deficiencies", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Large-scale Vision Language Models (LVLMs) are increasingly being applied to\na wide range of real-world multimodal applications, involving complex visual\nand linguistic reasoning. As these models become more integrated into practical\nuse, they are expected to handle complex aspects of human interaction. Among\nthese, color perception is a fundamental yet highly variable aspect of visual\nunderstanding. It differs across individuals due to biological factors such as\nColor Vision Deficiencies (CVDs), as well as differences in culture and\nlanguage. Despite its importance, perceptual diversity has received limited\nattention. In our study, we evaluate LVLMs' ability to account for individual\nlevel perceptual variation using the Ishihara Test, a widely used method for\ndetecting CVDs. Our results show that LVLMs can explain CVDs in natural\nlanguage, but they cannot simulate how people with CVDs perceive color in image\nbased tasks. These findings highlight the need for multimodal systems that can\naccount for color perceptual diversity and support broader discussions on\nperceptual inclusiveness and fairness in multimodal AI."}
{"id": "2505.17473", "pdf": "https://arxiv.org/pdf/2505.17473", "abs": "https://arxiv.org/abs/2505.17473", "authors": ["Jiangning Zhu", "Yuxing Zhou", "Zheng Wang", "Juntao Yao", "Yima Gu", "Yuhui Yuan", "Shixia Liu"], "title": "OrionBench: A Benchmark for Chart and Human-Recognizable Object Detection in Infographics", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Given the central role of charts in scientific, business, and communication\ncontexts, enhancing the chart understanding capabilities of vision-language\nmodels (VLMs) has become increasingly critical. A key limitation of existing\nVLMs lies in their inaccurate visual grounding of infographic elements,\nincluding charts and human-recognizable objects (HROs) such as icons and\nimages. However, chart understanding often requires identifying relevant\nelements and reasoning over them. To address this limitation, we introduce\nOrionBench, a benchmark designed to support the development of accurate object\ndetection models for charts and HROs in infographics. It contains 26,250 real\nand 78,750 synthetic infographics, with over 6.9 million bounding box\nannotations. These annotations are created by combining the model-in-the-loop\nand programmatic methods. We demonstrate the usefulness of OrionBench through\nthree applications: 1) constructing a Thinking-with-Boxes scheme to boost the\nchart understanding performance of VLMs, 2) comparing existing object detection\nmodels, and 3) applying the developed detection model to document layout and UI\nelement detection."}
{"id": "2505.17475", "pdf": "https://arxiv.org/pdf/2505.17475", "abs": "https://arxiv.org/abs/2505.17475", "authors": ["Uyoung Jeong", "Jonathan Freer", "Seungryul Baek", "Hyung Jin Chang", "Kwang In Kim"], "title": "PoseBH: Prototypical Multi-Dataset Training Beyond Human Pose Estimation", "categories": ["cs.CV"], "comment": "accepted to CVPR 2025", "summary": "We study multi-dataset training (MDT) for pose estimation, where skeletal\nheterogeneity presents a unique challenge that existing methods have yet to\naddress. In traditional domains, \\eg regression and classification, MDT\ntypically relies on dataset merging or multi-head supervision. However, the\ndiversity of skeleton types and limited cross-dataset supervision complicate\nintegration in pose estimation. To address these challenges, we introduce\nPoseBH, a new MDT framework that tackles keypoint heterogeneity and limited\nsupervision through two key techniques. First, we propose nonparametric\nkeypoint prototypes that learn within a unified embedding space, enabling\nseamless integration across skeleton types. Second, we develop a cross-type\nself-supervision mechanism that aligns keypoint predictions with keypoint\nembedding prototypes, providing supervision without relying on teacher-student\nmodels or additional augmentations. PoseBH substantially improves\ngeneralization across whole-body and animal pose datasets, including\nCOCO-WholeBody, AP-10K, and APT-36K, while preserving performance on standard\nhuman pose benchmarks (COCO, MPII, and AIC). Furthermore, our learned keypoint\nembeddings transfer effectively to hand shape estimation (InterHand2.6M) and\nhuman body shape estimation (3DPW). The code for PoseBH is available at:\nhttps://github.com/uyoung-jeong/PoseBH."}
{"id": "2505.17476", "pdf": "https://arxiv.org/pdf/2505.17476", "abs": "https://arxiv.org/abs/2505.17476", "authors": ["Yuchen Zhang", "Yaxiong Wang", "Yujiao Wu", "Lianwei Wu", "Li Zhu"], "title": "The Coherence Trap: When MLLM-Crafted Narratives Exploit Manipulated Visual Contexts", "categories": ["cs.CV"], "comment": null, "summary": "The detection and grounding of multimedia manipulation has emerged as a\ncritical challenge in combating AI-generated disinformation. While existing\nmethods have made progress in recent years, we identify two fundamental\nlimitations in current approaches: (1) Underestimation of MLLM-driven deception\nrisk: prevailing techniques primarily address rule-based text manipulations,\nyet fail to account for sophisticated misinformation synthesized by multimodal\nlarge language models (MLLMs) that can dynamically generate semantically\ncoherent, contextually plausible yet deceptive narratives conditioned on\nmanipulated images; (2) Unrealistic misalignment artifacts: currently focused\nscenarios rely on artificially misaligned content that lacks semantic\ncoherence, rendering them easily detectable. To address these gaps\nholistically, we propose a new adversarial pipeline that leverages MLLMs to\ngenerate high-risk disinformation. Our approach begins with constructing the\nMLLM-Driven Synthetic Multimodal (MDSM) dataset, where images are first altered\nusing state-of-the-art editing techniques and then paired with MLLM-generated\ndeceptive texts that maintain semantic consistency with the visual\nmanipulations. Building upon this foundation, we present the Artifact-aware\nManipulation Diagnosis via MLLM (AMD) framework featuring two key innovations:\nArtifact Pre-perception Encoding strategy and Manipulation-Oriented Reasoning,\nto tame MLLMs for the MDSM problem. Comprehensive experiments validate our\nframework's superior generalization capabilities as a unified architecture for\ndetecting MLLM-powered multimodal deceptions."}
{"id": "2505.17493", "pdf": "https://arxiv.org/pdf/2505.17493", "abs": "https://arxiv.org/abs/2505.17493", "authors": ["Jingde Huang", "Zhangyu Huang", "Chenyu Li", "Jiantong Liu"], "title": "Research on Defect Detection Method of Motor Control Board Based on Image Processing", "categories": ["cs.CV"], "comment": null, "summary": "The motor control board has various defects such as inconsistent color\ndifferences, incorrect plug-in positions, solder short circuits, and more.\nThese defects directly affect the performance and stability of the motor\ncontrol board, thereby having a negative impact on product quality. Therefore,\nstudying the defect detection technology of the motor control board is an\nimportant means to improve the quality control level of the motor control\nboard. Firstly, the processing methods of digital images about the motor\ncontrol board were studied, and the noise suppression methods that affect image\nfeature extraction were analyzed. Secondly, a specific model for defect feature\nextraction and color difference recognition of the tested motor control board\nwas established, and qualified or defective products were determined based on\nfeature thresholds. Thirdly, the search algorithm for defective images was\noptimized. Finally, comparative experiments were conducted on the typical motor\ncontrol board, and the experimental results demonstrate that the accuracy of\nthe motor control board defect detection model-based on image processing\nestablished in this paper reached over 99%. It is suitable for timely image\nprocessing of large quantities of motor control boards on the production line,\nand achieved efficient defect detection. The defect detection method can not\nonly be used for online detection of the motor control board defects, but also\nprovide solutions for the integrated circuit board defect processing for the\nindustry."}
{"id": "2505.17501", "pdf": "https://arxiv.org/pdf/2505.17501", "abs": "https://arxiv.org/abs/2505.17501", "authors": ["Yuehan Jin", "Xiaoqing Liu", "Yiyuan Yang", "Zhiwen Yu", "Tong Zhang", "Kaixiang Yang"], "title": "RoHyDR: Robust Hybrid Diffusion Recovery for Incomplete Multimodal Emotion Recognition", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Multimodal emotion recognition analyzes emotions by combining data from\nmultiple sources. However, real-world noise or sensor failures often cause\nmissing or corrupted data, creating the Incomplete Multimodal Emotion\nRecognition (IMER) challenge. In this paper, we propose Robust Hybrid Diffusion\nRecovery (RoHyDR), a novel framework that performs missing-modality recovery at\nunimodal, multimodal, feature, and semantic levels. For unimodal representation\nrecovery of missing modalities, RoHyDR exploits a diffusion-based generator to\ngenerate distribution-consistent and semantically aligned representations from\nGaussian noise, using available modalities as conditioning. For multimodal\nfusion recovery, we introduce adversarial learning to produce a realistic fused\nmultimodal representation and recover missing semantic content. We further\npropose a multi-stage optimization strategy that enhances training stability\nand efficiency. In contrast to previous work, the hybrid diffusion and\nadversarial learning-based recovery mechanism in RoHyDR allows recovery of\nmissing information in both unimodal representation and multimodal fusion, at\nboth feature and semantic levels, effectively mitigating performance\ndegradation caused by suboptimal optimization. Comprehensive experiments\nconducted on two widely used multimodal emotion recognition benchmarks\ndemonstrate that our proposed method outperforms state-of-the-art IMER methods,\nachieving robust recognition performance under various missing-modality\nscenarios. Our code will be made publicly available upon acceptance."}
{"id": "2505.17509", "pdf": "https://arxiv.org/pdf/2505.17509", "abs": "https://arxiv.org/abs/2505.17509", "authors": ["Shiji Zhao", "Qihui Zhu", "Shukun Xiong", "Shouwei Ruan", "Yize Fan", "Ranjie Duan", "Qing Guo", "Xingxing Wei"], "title": "Enhancing Adversarial Robustness of Vision Language Models via Adversarial Mixture Prompt Tuning", "categories": ["cs.CV"], "comment": null, "summary": "Large pre-trained Vision Language Models (VLMs) have excellent generalization\ncapabilities but are highly susceptible to adversarial examples, presenting\npotential security risks. To improve the robustness of VLMs against adversarial\nexamples, adversarial prompt tuning methods are proposed to align the text\nfeature with the adversarial image feature without changing model parameters.\nHowever, when facing various adversarial attacks, a single learnable text\nprompt has insufficient generalization to align well with all adversarial image\nfeatures, which finally leads to the overfitting phenomenon. To address the\nabove challenge, in this paper, we empirically find that increasing the number\nof learned prompts can bring more robustness improvement than a longer prompt.\nThen we propose an adversarial tuning method named Adversarial Mixture Prompt\nTuning (AMPT) to enhance the generalization towards various adversarial attacks\nfor VLMs. AMPT aims to learn mixture text prompts to obtain more robust text\nfeatures. To further enhance the adaptability, we propose a conditional weight\nrouter based on the input adversarial image to predict the mixture weights of\nmultiple learned prompts, which helps obtain sample-specific aggregated text\nfeatures aligning with different adversarial image features. A series of\nexperiments show that our method can achieve better adversarial robustness than\nstate-of-the-art methods on 11 datasets under different experimental settings."}
{"id": "2505.17529", "pdf": "https://arxiv.org/pdf/2505.17529", "abs": "https://arxiv.org/abs/2505.17529", "authors": ["Yeongjae Cho", "Keonwoo Kim", "Taebaek Hwang", "Sungzoon Cho"], "title": "Do You Keep an Eye on What I Ask? Mitigating Multimodal Hallucination via Attention-Guided Ensemble Decoding", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advancements in Large Vision-Language Models (LVLMs) have\nsignificantly expanded their utility in tasks like image captioning and visual\nquestion answering. However, they still struggle with object hallucination,\nwhere models generate descriptions that inaccurately reflect the visual content\nby including nonexistent objects or misrepresenting existing ones. While\nprevious methods, such as data augmentation and training-free approaches,\nstrive to tackle this issue, they still encounter scalability challenges and\noften depend on additional external modules. In this work, we propose Ensemble\nDecoding (ED), a novel strategy that splits the input image into sub-images and\ncombines logit distributions by assigning weights through the attention map.\nFurthermore, we introduce ED adaptive plausibility constraint to calibrate\nlogit distribution and FastED, a variant designed for speed-critical\napplications. Extensive experiments across hallucination benchmarks demonstrate\nthat our proposed method achieves state-of-the-art performance, validating the\neffectiveness of our approach."}
{"id": "2505.17534", "pdf": "https://arxiv.org/pdf/2505.17534", "abs": "https://arxiv.org/abs/2505.17534", "authors": ["Jingjing Jiang", "Chongjie Si", "Jun Luo", "Hanwang Zhang", "Chao Ma"], "title": "Co-Reinforcement Learning for Unified Multimodal Understanding and Generation", "categories": ["cs.CV", "cs.CL", "cs.MM"], "comment": null, "summary": "This paper presents a pioneering exploration of reinforcement learning (RL)\nvia group relative policy optimization for unified multimodal large language\nmodels (ULMs), aimed at simultaneously reinforcing generation and understanding\ncapabilities. Through systematic pilot studies, we uncover the significant\npotential of ULMs to enable the synergistic co-evolution of dual capabilities\nwithin a shared policy optimization framework. Building on this insight, we\nintroduce \\textbf{CoRL}, a co-reinforcement learning framework comprising a\nunified RL stage for joint optimization and a refined RL stage for\ntask-specific enhancement. With the proposed CoRL, our resulting model,\n\\textbf{ULM-R1}, achieves average improvements of \\textbf{7%} on three\ntext-to-image generation datasets and \\textbf{23%} on nine multimodal\nunderstanding benchmarks. These results demonstrate the effectiveness of CoRL\nand highlight the substantial benefit of reinforcement learning in facilitating\ncross-task synergy and optimization for ULMs."}
{"id": "2505.17540", "pdf": "https://arxiv.org/pdf/2505.17540", "abs": "https://arxiv.org/abs/2505.17540", "authors": ["Mingrui Wu", "Lu Wang", "Pu Zhao", "Fangkai Yang", "Jianjin Zhang", "Jianfeng Liu", "Yuefeng Zhan", "Weihao Han", "Hao Sun", "Jiayi Ji", "Xiaoshuai Sun", "Qingwei Lin", "Weiwei Deng", "Dongmei Zhang", "Feng Sun", "Qi Zhang", "Rongrong Ji"], "title": "RePrompt: Reasoning-Augmented Reprompting for Text-to-Image Generation via Reinforcement Learning", "categories": ["cs.CV", "cs.AI"], "comment": "Code is available at:\n  https://github.com/microsoft/DKI_LLM/tree/main/RePrompt", "summary": "Despite recent progress in text-to-image (T2I) generation, existing models\noften struggle to faithfully capture user intentions from short and\nunder-specified prompts. While prior work has attempted to enhance prompts\nusing large language models (LLMs), these methods frequently generate stylistic\nor unrealistic content due to insufficient grounding in visual semantics and\nreal-world composition. Inspired by recent advances in reasoning for language\nmodel, we propose RePrompt, a novel reprompting framework that introduces\nexplicit reasoning into the prompt enhancement process via reinforcement\nlearning. Instead of relying on handcrafted rules or stylistic rewrites, our\nmethod trains a language model to generate structured, self-reflective prompts\nby optimizing for image-level outcomes. The tailored reward models assesse the\ngenerated images in terms of human preference, semantic alignment, and visual\ncomposition, providing indirect supervision to refine prompt generation. Our\napproach enables end-to-end training without human-annotated data. Experiments\non GenEval and T2I-Compbench show that RePrompt significantly boosts spatial\nlayout fidelity and compositional generalization across diverse T2I backbones,\nestablishing new state-of-the-art results."}
{"id": "2505.17550", "pdf": "https://arxiv.org/pdf/2505.17550", "abs": "https://arxiv.org/abs/2505.17550", "authors": ["Xiaoyu Ye", "Songjie Cheng", "Yongtao Wang", "Yajiao Xiong", "Yishen Li"], "title": "T2VUnlearning: A Concept Erasing Method for Text-to-Video Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in text-to-video (T2V) diffusion models have significantly\nenhanced the quality of generated videos. However, their ability to produce\nexplicit or harmful content raises concerns about misuse and potential rights\nviolations. Inspired by the success of unlearning techniques in erasing\nundesirable concepts from text-to-image (T2I) models, we extend unlearning to\nT2V models and propose a robust and precise unlearning method. Specifically, we\nadopt negatively-guided velocity prediction fine-tuning and enhance it with\nprompt augmentation to ensure robustness against LLM-refined prompts. To\nachieve precise unlearning, we incorporate a localization and a preservation\nregularization to preserve the model's ability to generate non-target concepts.\nExtensive experiments demonstrate that our method effectively erases a specific\nconcept while preserving the model's generation capability for all other\nconcepts, outperforming existing methods. We provide the unlearned models in\n\\href{https://github.com/VDIGPKU/T2VUnlearning.git}{https://github.com/VDIGPKU/T2VUnlearning.git}."}
{"id": "2505.17551", "pdf": "https://arxiv.org/pdf/2505.17551", "abs": "https://arxiv.org/abs/2505.17551", "authors": ["Qiyu Chen", "Huiyuan Luo", "Haiming Yao", "Wei Luo", "Zhen Qu", "Chengkan Lv", "Zhengtao Zhang"], "title": "Center-aware Residual Anomaly Synthesis for Multi-class Industrial Anomaly Detection", "categories": ["cs.CV"], "comment": "Accepted by IEEE Transactions on Industrial Informatics (TII)", "summary": "Anomaly detection plays a vital role in the inspection of industrial images.\nMost existing methods require separate models for each category, resulting in\nmultiplied deployment costs. This highlights the challenge of developing a\nunified model for multi-class anomaly detection. However, the significant\nincrease in inter-class interference leads to severe missed detections.\nFurthermore, the intra-class overlap between normal and abnormal samples,\nparticularly in synthesis-based methods, cannot be ignored and may lead to\nover-detection. To tackle these issues, we propose a novel Center-aware\nResidual Anomaly Synthesis (CRAS) method for multi-class anomaly detection.\nCRAS leverages center-aware residual learning to couple samples from different\ncategories into a unified center, mitigating the effects of inter-class\ninterference. To further reduce intra-class overlap, CRAS introduces\ndistance-guided anomaly synthesis that adaptively adjusts noise variance based\non normal data distribution. Experimental results on diverse datasets and\nreal-world industrial applications demonstrate the superior detection accuracy\nand competitive inference speed of CRAS. The source code and the newly\nconstructed dataset are publicly available at\nhttps://github.com/cqylunlun/CRAS."}
{"id": "2505.17560", "pdf": "https://arxiv.org/pdf/2505.17560", "abs": "https://arxiv.org/abs/2505.17560", "authors": ["Shahin Hakemi", "Naveed Akhtar", "Ghulam Mubashar Hassan", "Ajmal Mian"], "title": "Deeper Diffusion Models Amplify Bias", "categories": ["cs.CV"], "comment": null, "summary": "Despite the impressive performance of generative Diffusion Models (DMs),\ntheir internal working is still not well understood, which is potentially\nproblematic. This paper focuses on exploring the important notion of\nbias-variance tradeoff in diffusion models. Providing a systematic foundation\nfor this exploration, it establishes that at one extreme the diffusion models\nmay amplify the inherent bias in the training data and, on the other, they may\ncompromise the presumed privacy of the training samples. Our exploration aligns\nwith the memorization-generalization understanding of the generative models,\nbut it also expands further along this spectrum beyond ``generalization'',\nrevealing the risk of bias amplification in deeper models. Building on the\ninsights, we also introduce a training-free method to improve output quality in\ntext-to-image and image-to-image generation. By progressively encouraging\ntemporary high variance in the generation process with partial bypassing of the\nmid-block's contribution in the denoising process of DMs, our method\nconsistently improves generative image quality with zero training cost. Our\nclaims are validated both theoretically and empirically."}
{"id": "2505.17561", "pdf": "https://arxiv.org/pdf/2505.17561", "abs": "https://arxiv.org/abs/2505.17561", "authors": ["Kwanyoung Kim", "Sanghyun Kim"], "title": "Model Already Knows the Best Noise: Bayesian Active Noise Selection via Attention in Video Diffusion Model", "categories": ["cs.CV", "cs.AI"], "comment": "19 pages, 10 figures", "summary": "The choice of initial noise significantly affects the quality and prompt\nalignment of video diffusion models, where different noise seeds for the same\nprompt can lead to drastically different generations. While recent methods rely\non externally designed priors such as frequency filters or inter-frame\nsmoothing, they often overlook internal model signals that indicate which noise\nseeds are inherently preferable. To address this, we propose ANSE (Active Noise\nSelection for Generation), a model-aware framework that selects high-quality\nnoise seeds by quantifying attention-based uncertainty. At its core is BANSA\n(Bayesian Active Noise Selection via Attention), an acquisition function that\nmeasures entropy disagreement across multiple stochastic attention samples to\nestimate model confidence and consistency. For efficient inference-time\ndeployment, we introduce a Bernoulli-masked approximation of BANSA that enables\nscore estimation using a single diffusion step and a subset of attention\nlayers. Experiments on CogVideoX-2B and 5B demonstrate that ANSE improves video\nquality and temporal coherence with only an 8% and 13% increase in inference\ntime, respectively, providing a principled and generalizable approach to noise\nselection in video diffusion. See our project page:\nhttps://anse-project.github.io/anse-project/"}
{"id": "2505.17567", "pdf": "https://arxiv.org/pdf/2505.17567", "abs": "https://arxiv.org/abs/2505.17567", "authors": ["Denisa Qosja", "Kilian Barth", "Simon Wagner"], "title": "Enhancing Fourier-based Doppler Resolution with Diffusion Models", "categories": ["cs.CV", "eess.SP"], "comment": "Published at International Radar Symposium (IRS) 2025", "summary": "In radar systems, high resolution in the Doppler dimension is important for\ndetecting slow-moving targets as it allows for more distinct separation between\nthese targets and clutter, or stationary objects. However, achieving sufficient\nresolution is constrained by hardware capabilities and physical factors,\nleading to the development of processing techniques to enhance the resolution\nafter acquisition. In this work, we leverage artificial intelligence to\nincrease the Doppler resolution in range-Doppler maps. Based on a zero-padded\nFFT, a refinement via the generative neural networks of diffusion models is\nachieved. We demonstrate that our method overcomes the limitations of\ntraditional FFT, generating data where closely spaced targets are effectively\nseparated."}
{"id": "2505.17574", "pdf": "https://arxiv.org/pdf/2505.17574", "abs": "https://arxiv.org/abs/2505.17574", "authors": ["Xueji Fang", "Liyuan Ma", "Zhiyang Chen", "Mingyuan Zhou", "Guo-jun Qi"], "title": "InfLVG: Reinforce Inference-Time Consistent Long Video Generation with GRPO", "categories": ["cs.CV"], "comment": "Preprint. Under review", "summary": "Recent advances in text-to-video generation, particularly with autoregressive\nmodels, have enabled the synthesis of high-quality videos depicting individual\nscenes. However, extending these models to generate long, cross-scene videos\nremains a significant challenge. As the context length grows during\nautoregressive decoding, computational costs rise sharply, and the model's\nability to maintain consistency and adhere to evolving textual prompts\ndeteriorates. We introduce InfLVG, an inference-time framework that enables\ncoherent long video generation without requiring additional long-form video\ndata. InfLVG leverages a learnable context selection policy, optimized via\nGroup Relative Policy Optimization (GRPO), to dynamically identify and retain\nthe most semantically relevant context throughout the generation process.\nInstead of accumulating the entire generation history, the policy ranks and\nselects the top-$K$ most contextually relevant tokens, allowing the model to\nmaintain a fixed computational budget while preserving content consistency and\nprompt alignment. To optimize the policy, we design a hybrid reward function\nthat jointly captures semantic alignment, cross-scene consistency, and artifact\nreduction. To benchmark performance, we introduce the Cross-scene Video\nBenchmark (CsVBench) along with an Event Prompt Set (EPS) that simulates\ncomplex multi-scene transitions involving shared subjects and varied\nactions/backgrounds. Experimental results show that InfLVG can extend video\nlength by up to 9$\\times$, achieving strong consistency and semantic fidelity\nacross scenes. Our code is available at https://github.com/MAPLE-AIGC/InfLVG."}
{"id": "2505.17581", "pdf": "https://arxiv.org/pdf/2505.17581", "abs": "https://arxiv.org/abs/2505.17581", "authors": ["Hainuo Wang", "Qiming Hu", "Xiaojie Guo"], "title": "MODEM: A Morton-Order Degradation Estimation Mechanism for Adverse Weather Image Recovery", "categories": ["cs.CV"], "comment": null, "summary": "Restoring images degraded by adverse weather remains a significant challenge\ndue to the highly non-uniform and spatially heterogeneous nature of\nweather-induced artifacts, e.g., fine-grained rain streaks versus widespread\nhaze. Accurately estimating the underlying degradation can intuitively provide\nrestoration models with more targeted and effective guidance, enabling adaptive\nprocessing strategies. To this end, we propose a Morton-Order Degradation\nEstimation Mechanism (MODEM) for adverse weather image restoration. Central to\nMODEM is the Morton-Order 2D-Selective-Scan Module (MOS2D), which integrates\nMorton-coded spatial ordering with selective state-space models to capture\nlong-range dependencies while preserving local structural coherence.\nComplementing MOS2D, we introduce a Dual Degradation Estimation Module (DDEM)\nthat disentangles and estimates both global and local degradation priors. These\npriors dynamically condition the MOS2D modules, facilitating adaptive and\ncontext-aware restoration. Extensive experiments and ablation studies\ndemonstrate that MODEM achieves state-of-the-art results across multiple\nbenchmarks and weather types, highlighting its effectiveness in modeling\ncomplex degradation dynamics. Our code will be released at\nhttps://github.com/hainuo-wang/MODEM.git."}
{"id": "2505.17590", "pdf": "https://arxiv.org/pdf/2505.17590", "abs": "https://arxiv.org/abs/2505.17590", "authors": ["Florian Barthel", "Wieland Morgenstern", "Paul Hinzer", "Anna Hilsmann", "Peter Eisert"], "title": "CGS-GAN: 3D Consistent Gaussian Splatting GANs for High Resolution Human Head Synthesis", "categories": ["cs.CV"], "comment": "Main paper 12 pages, supplementary materials 8 pages", "summary": "Recently, 3D GANs based on 3D Gaussian splatting have been proposed for high\nquality synthesis of human heads. However, existing methods stabilize training\nand enhance rendering quality from steep viewpoints by conditioning the random\nlatent vector on the current camera position. This compromises 3D consistency,\nas we observe significant identity changes when re-synthesizing the 3D head\nwith each camera shift. Conversely, fixing the camera to a single viewpoint\nyields high-quality renderings for that perspective but results in poor\nperformance for novel views. Removing view-conditioning typically destabilizes\nGAN training, often causing the training to collapse. In response to these\nchallenges, we introduce CGS-GAN, a novel 3D Gaussian Splatting GAN framework\nthat enables stable training and high-quality 3D-consistent synthesis of human\nheads without relying on view-conditioning. To ensure training stability, we\nintroduce a multi-view regularization technique that enhances generator\nconvergence with minimal computational overhead. Additionally, we adapt the\nconditional loss used in existing 3D Gaussian splatting GANs and propose a\ngenerator architecture designed to not only stabilize training but also\nfacilitate efficient rendering and straightforward scaling, enabling output\nresolutions up to $2048^2$. To evaluate the capabilities of CGS-GAN, we curate\na new dataset derived from FFHQ. This dataset enables very high resolutions,\nfocuses on larger portions of the human head, reduces view-dependent artifacts\nfor improved 3D consistency, and excludes images where subjects are obscured by\nhands or other objects. As a result, our approach achieves very high rendering\nquality, supported by competitive FID scores, while ensuring consistent 3D\nscene generation. Check our our project page here:\nhttps://fraunhoferhhi.github.io/cgs-gan/"}
{"id": "2505.17614", "pdf": "https://arxiv.org/pdf/2505.17614", "abs": "https://arxiv.org/abs/2505.17614", "authors": ["Sinchee Chin", "Yinuo Ma", "Xiaochen Yang", "Jing-Hao Xue", "Wenming Yang"], "title": "PathoSCOPE: Few-Shot Pathology Detection via Self-Supervised Contrastive Learning and Pathology-Informed Synthetic Embeddings", "categories": ["cs.CV"], "comment": null, "summary": "Unsupervised pathology detection trains models on non-pathological data to\nflag deviations as pathologies, offering strong generalizability for\nidentifying novel diseases and avoiding costly annotations. However, building\nreliable normality models requires vast healthy datasets, as hospitals' data is\ninherently biased toward symptomatic populations, while privacy regulations\nhinder the assembly of representative healthy cohorts. To address this\nlimitation, we propose PathoSCOPE, a few-shot unsupervised pathology detection\nframework that requires only a small set of non-pathological samples (minimum 2\nshots), significantly improving data efficiency. We introduce Global-Local\nContrastive Loss (GLCL), comprised of a Local Contrastive Loss to reduce the\nvariability of non-pathological embeddings and a Global Contrastive Loss to\nenhance the discrimination of pathological regions. We also propose a\nPathology-informed Embedding Generation (PiEG) module that synthesizes\npathological embeddings guided by the global loss, better exploiting the\nlimited non-pathological samples. Evaluated on the BraTS2020 and ChestXray8\ndatasets, PathoSCOPE achieves state-of-the-art performance among unsupervised\nmethods while maintaining computational efficiency (2.48 GFLOPs, 166 FPS)."}
{"id": "2505.17618", "pdf": "https://arxiv.org/pdf/2505.17618", "abs": "https://arxiv.org/abs/2505.17618", "authors": ["Haoran He", "Jiajun Liang", "Xintao Wang", "Pengfei Wan", "Di Zhang", "Kun Gai", "Ling Pan"], "title": "Scaling Image and Video Generation via Test-Time Evolutionary Search", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "37 pages. Project: https://tinnerhrhe.github.io/evosearch", "summary": "As the marginal cost of scaling computation (data and parameters) during\nmodel pre-training continues to increase substantially, test-time scaling (TTS)\nhas emerged as a promising direction for improving generative model performance\nby allocating additional computation at inference time. While TTS has\ndemonstrated significant success across multiple language tasks, there remains\na notable gap in understanding the test-time scaling behaviors of image and\nvideo generative models (diffusion-based or flow-based models). Although recent\nworks have initiated exploration into inference-time strategies for vision\ntasks, these approaches face critical limitations: being constrained to\ntask-specific domains, exhibiting poor scalability, or falling into reward\nover-optimization that sacrifices sample diversity. In this paper, we propose\n\\textbf{Evo}lutionary \\textbf{Search} (EvoSearch), a novel, generalist, and\nefficient TTS method that effectively enhances the scalability of both image\nand video generation across diffusion and flow models, without requiring\nadditional training or model expansion. EvoSearch reformulates test-time\nscaling for diffusion and flow models as an evolutionary search problem,\nleveraging principles from biological evolution to efficiently explore and\nrefine the denoising trajectory. By incorporating carefully designed selection\nand mutation mechanisms tailored to the stochastic differential equation\ndenoising process, EvoSearch iteratively generates higher-quality offspring\nwhile preserving population diversity. Through extensive evaluation across both\ndiffusion and flow architectures for image and video generation tasks, we\ndemonstrate that our method consistently outperforms existing approaches,\nachieves higher diversity, and shows strong generalizability to unseen\nevaluation metrics. Our project is available at the website\nhttps://tinnerhrhe.github.io/evosearch."}
{"id": "2505.17619", "pdf": "https://arxiv.org/pdf/2505.17619", "abs": "https://arxiv.org/abs/2505.17619", "authors": ["Bo Wang", "De-Xing Huang", "Xiao-Hu Zhou", "Mei-Jiang Gui", "Nu-Fang Xiao", "Jian-Long Hao", "Ming-Yuan Liu", "Zeng-Guang Hou"], "title": "CAS-IQA: Teaching Vision-Language Models for Synthetic Angiography Quality Assessment", "categories": ["cs.CV"], "comment": "Under review", "summary": "Synthetic X-ray angiographies generated by modern generative models hold\ngreat potential to reduce the use of contrast agents in vascular interventional\nprocedures. However, low-quality synthetic angiographies can significantly\nincrease procedural risk, underscoring the need for reliable image quality\nassessment (IQA) methods. Existing IQA models, however, fail to leverage\nauxiliary images as references during evaluation and lack fine-grained,\ntask-specific metrics necessary for clinical relevance. To address these\nlimitations, this paper proposes CAS-IQA, a vision-language model (VLM)-based\nframework that predicts fine-grained quality scores by effectively\nincorporating auxiliary information from related images. In the absence of\nangiography datasets, CAS-3K is constructed, comprising 3,565 synthetic\nangiographies along with score annotations. To ensure clinically meaningful\nassessment, three task-specific evaluation metrics are defined. Furthermore, a\nMulti-path featUre fuSion and rouTing (MUST) module is designed to enhance\nimage representations by adaptively fusing and routing visual tokens to\nmetric-specific branches. Extensive experiments on the CAS-3K dataset\ndemonstrate that CAS-IQA significantly outperforms state-of-the-art IQA methods\nby a considerable margin."}
{"id": "2505.17645", "pdf": "https://arxiv.org/pdf/2505.17645", "abs": "https://arxiv.org/abs/2505.17645", "authors": ["Chuhao Zhou", "Jianfei Yang"], "title": "HoloLLM: Multisensory Foundation Model for Language-Grounded Human Sensing and Reasoning", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "comment": "18 pages, 13 figures, 6 tables", "summary": "Embodied agents operating in smart homes must understand human behavior\nthrough diverse sensory inputs and communicate via natural language. While\nVision-Language Models (VLMs) have enabled impressive language-grounded\nperception, their reliance on visual data limits robustness in real-world\nscenarios with occlusions, poor lighting, or privacy constraints. In this\npaper, we introduce HoloLLM, a Multimodal Large Language Model (MLLM) that\nintegrates uncommon but powerful sensing modalities, such as LiDAR, infrared,\nmmWave radar, and WiFi, to enable seamless human perception and reasoning\nacross heterogeneous environments. We address two key challenges: (1) the\nscarcity of aligned modality-text data for rare sensors, and (2) the\nheterogeneity of their physical signal representations. To overcome these, we\ndesign a Universal Modality-Injection Projector (UMIP) that enhances\npre-aligned modality embeddings with fine-grained, text-aligned features from\ntailored encoders via coarse-to-fine cross-attention without introducing\nsignificant alignment overhead. We further introduce a human-VLM collaborative\ndata curation pipeline to generate paired textual annotations for sensing\ndatasets. Extensive experiments on two newly constructed benchmarks show that\nHoloLLM significantly outperforms existing MLLMs, improving language-grounded\nhuman sensing accuracy by up to 30%. This work establishes a new foundation for\nreal-world, language-informed multisensory embodied intelligence."}
{"id": "2505.17649", "pdf": "https://arxiv.org/pdf/2505.17649", "abs": "https://arxiv.org/abs/2505.17649", "authors": ["Junhang Li", "Yu Guo", "Chuhua Xian", "Shengfeng He"], "title": "Instruct2See: Learning to Remove Any Obstructions Across Distributions", "categories": ["cs.CV"], "comment": null, "summary": "Images are often obstructed by various obstacles due to capture limitations,\nhindering the observation of objects of interest. Most existing methods address\nocclusions from specific elements like fences or raindrops, but are constrained\nby the wide range of real-world obstructions, making comprehensive data\ncollection impractical. To overcome these challenges, we propose Instruct2See,\na novel zero-shot framework capable of handling both seen and unseen obstacles.\nThe core idea of our approach is to unify obstruction removal by treating it as\na soft-hard mask restoration problem, where any obstruction can be represented\nusing multi-modal prompts, such as visual semantics and textual instructions,\nprocessed through a cross-attention unit to enhance contextual understanding\nand improve mode control. Additionally, a tunable mask adapter allows for\ndynamic soft masking, enabling real-time adjustment of inaccurate masks.\nExtensive experiments on both in-distribution and out-of-distribution obstacles\nshow that Instruct2See consistently achieves strong performance and\ngeneralization in obstruction removal, regardless of whether the obstacles were\npresent during the training phase. Code and dataset are available at\nhttps://jhscut.github.io/Instruct2See."}
{"id": "2505.17665", "pdf": "https://arxiv.org/pdf/2505.17665", "abs": "https://arxiv.org/abs/2505.17665", "authors": ["Yichun Yu", "Yuqing Lan", "Zhihuan Xing", "Xiaoyi Yang", "Tingyue Tang", "Dan Yu"], "title": "EMRA-proxy: Enhancing Multi-Class Region Semantic Segmentation in Remote Sensing Images with Attention Proxy", "categories": ["cs.CV", "cs.AI"], "comment": "Proceedings of the 20th International Conference on Intelligent\n  Computing (ICIC 2024): Poster Volume I. Tianjin, China, 2024: 538-562", "summary": "High-resolution remote sensing (HRRS) image segmentation is challenging due\nto complex spatial layouts and diverse object appearances. While CNNs excel at\ncapturing local features, they struggle with long-range dependencies, whereas\nTransformers can model global context but often neglect local details and are\ncomputationally expensive.We propose a novel approach, Region-Aware Proxy\nNetwork (RAPNet), which consists of two components: Contextual Region Attention\n(CRA) and Global Class Refinement (GCR). Unlike traditional methods that rely\non grid-based layouts, RAPNet operates at the region level for more flexible\nsegmentation. The CRA module uses a Transformer to capture region-level\ncontextual dependencies, generating a Semantic Region Mask (SRM). The GCR\nmodule learns a global class attention map to refine multi-class information,\ncombining the SRM and attention map for accurate segmentation.Experiments on\nthree public datasets show that RAPNet outperforms state-of-the-art methods,\nachieving superior multi-class segmentation accuracy."}
{"id": "2505.17666", "pdf": "https://arxiv.org/pdf/2505.17666", "abs": "https://arxiv.org/abs/2505.17666", "authors": ["Shuxian Ma", "Zihao Dong", "Runmin Cong", "Sam Kwong", "Xiuli Shao"], "title": "Proto-FG3D: Prototype-based Interpretable Fine-Grained 3D Shape Classification", "categories": ["cs.CV", "I.4.0; I.5.0"], "comment": "11 pages, 2 figures, 5 tablets; Submitted to BMVC2025", "summary": "Deep learning-based multi-view coarse-grained 3D shape classification has\nachieved remarkable success over the past decade, leveraging the powerful\nfeature learning capabilities of CNN-based and ViT-based backbones. However, as\na challenging research area critical for detailed shape understanding,\nfine-grained 3D classification remains understudied due to the limited\ndiscriminative information captured during multi-view feature aggregation,\nparticularly for subtle inter-class variations, class imbalance, and inherent\ninterpretability limitations of parametric model. To address these problems, we\npropose the first prototype-based framework named Proto-FG3D for fine-grained\n3D shape classification, achieving a paradigm shift from parametric softmax to\nnon-parametric prototype learning. Firstly, Proto-FG3D establishes joint\nmulti-view and multi-category representation learning via Prototype\nAssociation. Secondly, prototypes are refined via Online Clustering, improving\nboth the robustness of multi-view feature allocation and inter-subclass\nbalance. Finally, prototype-guided supervised learning is established to\nenhance fine-grained discrimination via prototype-view correlation analysis and\nenables ad-hoc interpretability through transparent case-based reasoning.\nExperiments on FG3D and ModelNet40 show Proto-FG3D surpasses state-of-the-art\nmethods in accuracy, transparent predictions, and ad-hoc interpretability with\nvisualizations, challenging conventional fine-grained 3D recognition\napproaches."}
{"id": "2505.17674", "pdf": "https://arxiv.org/pdf/2505.17674", "abs": "https://arxiv.org/abs/2505.17674", "authors": ["Xuerui Qiu", "Peixi Wu", "Yaozhi Wen", "Shaowei Gu", "Yuqi Pan", "Xinhao Luo", "Bo XU", "Guoqi Li"], "title": "SVL: Spike-based Vision-language Pretraining for Efficient 3D Open-world Understanding", "categories": ["cs.CV"], "comment": null, "summary": "Spiking Neural Networks (SNNs) provide an energy-efficient way to extract 3D\nspatio-temporal features. However, existing SNNs still exhibit a significant\nperformance gap compared to Artificial Neural Networks (ANNs) due to inadequate\npre-training strategies. These limitations manifest as restricted\ngeneralization ability, task specificity, and a lack of multimodal\nunderstanding, particularly in challenging tasks such as multimodal question\nanswering and zero-shot 3D classification. To overcome these challenges, we\npropose a Spike-based Vision-Language (SVL) pretraining framework that empowers\nSNNs with open-world 3D understanding while maintaining spike-driven\nefficiency. SVL introduces two key components: (i) Multi-scale Triple Alignment\n(MTA) for label-free triplet-based contrastive learning across 3D, image, and\ntext modalities, and (ii) Re-parameterizable Vision-Language Integration\n(Rep-VLI) to enable lightweight inference without relying on large text\nencoders. Extensive experiments show that SVL achieves a top-1 accuracy of\n85.4% in zero-shot 3D classification, surpassing advanced ANN models, and\nconsistently outperforms prior SNNs on downstream tasks, including 3D\nclassification (+6.1%), DVS action recognition (+2.1%), 3D detection (+1.1%),\nand 3D segmentation (+2.1%) with remarkable efficiency. Moreover, SVL enables\nSNNs to perform open-world 3D question answering, sometimes outperforming ANNs.\nTo the best of our knowledge, SVL represents the first scalable, generalizable,\nand hardware-friendly paradigm for 3D open-world understanding, effectively\nbridging the gap between SNNs and ANNs in complex open-world understanding\ntasks. Code is available https://github.com/bollossom/SVL."}
{"id": "2505.17677", "pdf": "https://arxiv.org/pdf/2505.17677", "abs": "https://arxiv.org/abs/2505.17677", "authors": ["Ming Hu", "Zhendi Yu", "Feilong Tang", "Kaiwen Chen", "Yulong Li", "Imran Razzak", "Junjun He", "Tolga Birdal", "Kaijing Zhou", "Zongyuan Ge"], "title": "Towards Dynamic 3D Reconstruction of Hand-Instrument Interaction in Ophthalmic Surgery", "categories": ["cs.CV"], "comment": null, "summary": "Accurate 3D reconstruction of hands and instruments is critical for\nvision-based analysis of ophthalmic microsurgery, yet progress has been\nhampered by the lack of realistic, large-scale datasets and reliable annotation\ntools. In this work, we introduce OphNet-3D, the first extensive RGB-D dynamic\n3D reconstruction dataset for ophthalmic surgery, comprising 41 sequences from\n40 surgeons and totaling 7.1 million frames, with fine-grained annotations of\n12 surgical phases, 10 instrument categories, dense MANO hand meshes, and full\n6-DoF instrument poses. To scalably produce high-fidelity labels, we design a\nmulti-stage automatic annotation pipeline that integrates multi-view data\nobservation, data-driven motion prior with cross-view geometric consistency and\nbiomechanical constraints, along with a combination of collision-aware\ninteraction constraints for instrument interactions. Building upon OphNet-3D,\nwe establish two challenging benchmarks-bimanual hand pose estimation and\nhand-instrument interaction reconstruction-and propose two dedicated\narchitectures: H-Net for dual-hand mesh recovery and OH-Net for joint\nreconstruction of two-hand-two-instrument interactions. These models leverage a\nnovel spatial reasoning module with weak-perspective camera modeling and\ncollision-aware center-based representation. Both architectures outperform\nexisting methods by substantial margins, achieving improvements of over 2mm in\nMean Per Joint Position Error (MPJPE) and up to 23% in ADD-S metrics for hand\nand instrument reconstruction, respectively."}
{"id": "2505.17684", "pdf": "https://arxiv.org/pdf/2505.17684", "abs": "https://arxiv.org/abs/2505.17684", "authors": ["Nisha Lakshmana Raichur", "Lucas Heublein", "Christopher Mutschler", "Felix Ott"], "title": "5G-DIL: Domain Incremental Learning with Similarity-Aware Sampling for Dynamic 5G Indoor Localization", "categories": ["cs.CV", "62D05, 62J99, 62P12, 68T37", "G.3; H.3.3; I.2.4; I.4; I.5.1"], "comment": "7 pages, 6 figures", "summary": "Indoor positioning based on 5G data has achieved high accuracy through the\nadoption of recent machine learning (ML) techniques. However, the performance\nof learning-based methods degrades significantly when environmental conditions\nchange, thereby hindering their applicability to new scenarios. Acquiring new\ntraining data for each environmental change and fine-tuning ML models is both\ntime-consuming and resource-intensive. This paper introduces a domain\nincremental learning (DIL) approach for dynamic 5G indoor localization, called\n5G-DIL, enabling rapid adaptation to environmental changes. We present a novel\nsimilarity-aware sampling technique based on the Chebyshev distance, designed\nto efficiently select specific exemplars from the previous environment while\ntraining only on the modified regions of the new environment. This avoids the\nneed to train on the entire region, significantly reducing the time and\nresources required for adaptation without compromising localization accuracy.\nThis approach requires as few as 50 exemplars from adaptation domains,\nsignificantly reducing training time while maintaining high positioning\naccuracy in previous environments. Comparative evaluations against\nstate-of-the-art DIL techniques on a challenging real-world indoor dataset\ndemonstrate the effectiveness of the proposed sample selection method. Our\napproach is adaptable to real-world non-line-of-sight propagation scenarios and\nachieves an MAE positioning error of 0.261 meters, even under dynamic\nenvironmental conditions. Code:\nhttps://gitlab.cc-asp.fraunhofer.de/5g-pos/5g-dil"}
{"id": "2505.17685", "pdf": "https://arxiv.org/pdf/2505.17685", "abs": "https://arxiv.org/abs/2505.17685", "authors": ["Shuang Zeng", "Xinyuan Chang", "Mengwei Xie", "Xinran Liu", "Yifan Bai", "Zheng Pan", "Mu Xu", "Xing Wei"], "title": "FutureSightDrive: Thinking Visually with Spatio-Temporal CoT for Autonomous Driving", "categories": ["cs.CV"], "comment": null, "summary": "Visual language models (VLMs) have attracted increasing interest in\nautonomous driving due to their powerful reasoning capabilities. However,\nexisting VLMs typically utilize discrete text Chain-of-Thought (CoT) tailored\nto the current scenario, which essentially represents highly abstract and\nsymbolic compression of visual information, potentially leading to\nspatio-temporal relationship ambiguity and fine-grained information loss. Is\nautonomous driving better modeled on real-world simulation and imagination than\non pure symbolic logic? In this paper, we propose a spatio-temporal CoT\nreasoning method that enables models to think visually. First, VLM serves as a\nworld model to generate unified image frame for predicting future world states:\nwhere perception results (e.g., lane divider and 3D detection) represent the\nfuture spatial relationships, and ordinary future frame represent the temporal\nevolution relationships. This spatio-temporal CoT then serves as intermediate\nreasoning steps, enabling the VLM to function as an inverse dynamics model for\ntrajectory planning based on current observations and future predictions. To\nimplement visual generation in VLMs, we propose a unified pretraining paradigm\nintegrating visual generation and understanding, along with a progressive\nvisual CoT enhancing autoregressive image generation. Extensive experimental\nresults demonstrate the effectiveness of the proposed method, advancing\nautonomous driving towards visual reasoning."}
{"id": "2505.17690", "pdf": "https://arxiv.org/pdf/2505.17690", "abs": "https://arxiv.org/abs/2505.17690", "authors": ["Yunyao Lu", "Yihang Wu", "Reem Kateb", "Ahmad Chaddad"], "title": "Semi-Supervised Medical Image Segmentation via Dual Networks", "categories": ["cs.CV"], "comment": "Accepted in ISBI2025", "summary": "Traditional supervised medical image segmentation models require large\namounts of labeled data for training; however, obtaining such large-scale\nlabeled datasets in the real world is extremely challenging. Recent\nsemi-supervised segmentation models also suffer from noisy pseudo-label issue\nand limited supervision in feature space. To solve these challenges, we propose\nan innovative semi-supervised 3D medical image segmentation method to reduce\nthe dependency on large, expert-labeled datasets. Furthermore, we introduce a\ndual-network architecture to address the limitations of existing methods in\nusing contextual information and generating reliable pseudo-labels. In\naddition, a self-supervised contrastive learning strategy is used to enhance\nthe representation of the network and reduce prediction uncertainty by\ndistinguishing between reliable and unreliable predictions. Experiments on\nclinical magnetic resonance imaging demonstrate that our approach outperforms\nstate-of-the-art techniques. Our code is available at\nhttps://github.com/AIPMLab/Semi-supervised-Segmentation."}
{"id": "2505.17692", "pdf": "https://arxiv.org/pdf/2505.17692", "abs": "https://arxiv.org/abs/2505.17692", "authors": ["Ziteng Yang", "Jingzehua Xu", "Yanshu Li", "Zepeng Li", "Yeqiang Wang", "Xinghui Li"], "title": "ViP$^2$-CLIP: Visual-Perception Prompting with Unified Alignment for Zero-Shot Anomaly Detection", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Zero-shot anomaly detection (ZSAD) aims to detect anomalies without any\ntarget domain training samples, relying solely on external auxiliary data.\nExisting CLIP-based methods attempt to activate the model's ZSAD potential via\nhandcrafted or static learnable prompts. The former incur high engineering\ncosts and limited semantic coverage, whereas the latter apply identical\ndescriptions across diverse anomaly types, thus fail to adapt to complex\nvariations. Furthermore, since CLIP is originally pretrained on large-scale\nclassification tasks, its anomaly segmentation quality is highly sensitive to\nthe exact wording of class names, severely constraining prompting strategies\nthat depend on class labels. To address these challenges, we introduce\nViP$^{2}$-CLIP. The key insight of ViP$^{2}$-CLIP is a Visual-Perception\nPrompting (ViP-Prompt) mechanism, which fuses global and multi-scale local\nvisual context to adaptively generate fine-grained textual prompts, eliminating\nmanual templates and class-name priors. This design enables our model to focus\non precise abnormal regions, making it particularly valuable when category\nlabels are ambiguous or privacy-constrained. Extensive experiments on 15\nindustrial and medical benchmarks demonstrate that ViP$^{2}$-CLIP achieves\nstate-of-the-art performance and robust cross-domain generalization."}
{"id": "2505.17702", "pdf": "https://arxiv.org/pdf/2505.17702", "abs": "https://arxiv.org/abs/2505.17702", "authors": ["Xueyang Li", "Jiahao Li", "Yu Song", "Yunzhong Lou", "Xiangdong Zhou"], "title": "Seek-CAD: A Self-refined Generative Modeling for 3D Parametric CAD Using Local Inference via DeepSeek", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The advent of Computer-Aided Design (CAD) generative modeling will\nsignificantly transform the design of industrial products. The recent research\nendeavor has extended into the realm of Large Language Models (LLMs). In\ncontrast to fine-tuning methods, training-free approaches typically utilize the\nadvanced closed-source LLMs, thereby offering enhanced flexibility and\nefficiency in the development of AI agents for generating CAD parametric\nmodels. However, the substantial cost and limitations of local deployment of\nthe top-tier closed-source LLMs pose challenges in practical applications. The\nSeek-CAD is the pioneer exploration of locally deployed open-source inference\nLLM DeepSeek-R1 for CAD parametric model generation with a training-free\nmethodology. This study is the first investigation to incorporate both visual\nand Chain-of-Thought (CoT) feedback within the self-refinement mechanism for\ngenerating CAD models. Specifically, the initial generated parametric CAD model\nis rendered into a sequence of step-wise perspective images, which are\nsubsequently processed by a Vision Language Model (VLM) alongside the\ncorresponding CoTs derived from DeepSeek-R1 to assess the CAD model generation.\nThen, the feedback is utilized by DeepSeek-R1 to refine the initial generated\nmodel for the next round of generation. Moreover, we present an innovative 3D\nCAD model dataset structured around the SSR (Sketch, Sketch-based feature, and\nRefinements) triple design paradigm. This dataset encompasses a wide range of\nCAD commands, thereby aligning effectively with industrial application\nrequirements and proving suitable for the generation of LLMs. Extensive\nexperiments validate the effectiveness of Seek-CAD under various metrics."}
{"id": "2505.17721", "pdf": "https://arxiv.org/pdf/2505.17721", "abs": "https://arxiv.org/abs/2505.17721", "authors": ["Dekai Zhu", "Yan Di", "Stefan Gavranovic", "Slobodan Ilic"], "title": "SeaLion: Semantic Part-Aware Latent Point Diffusion Models for 3D Generation", "categories": ["cs.CV"], "comment": null, "summary": "Denoising diffusion probabilistic models have achieved significant success in\npoint cloud generation, enabling numerous downstream applications, such as\ngenerative data augmentation and 3D model editing. However, little attention\nhas been given to generating point clouds with point-wise segmentation labels,\nas well as to developing evaluation metrics for this task. Therefore, in this\npaper, we present SeaLion, a novel diffusion model designed to generate\nhigh-quality and diverse point clouds with fine-grained segmentation labels.\nSpecifically, we introduce the semantic part-aware latent point diffusion\ntechnique, which leverages the intermediate features of the generative models\nto jointly predict the noise for perturbed latent points and associated part\nsegmentation labels during the denoising process, and subsequently decodes the\nlatent points to point clouds conditioned on part segmentation labels. To\neffectively evaluate the quality of generated point clouds, we introduce a\nnovel point cloud pairwise distance calculation method named part-aware Chamfer\ndistance (p-CD). This method enables existing metrics, such as 1-NNA, to\nmeasure both the local structural quality and inter-part coherence of generated\npoint clouds. Experiments on the large-scale synthetic dataset ShapeNet and\nreal-world medical dataset IntrA demonstrate that SeaLion achieves remarkable\nperformance in generation quality and diversity, outperforming the existing\nstate-of-the-art model, DiffFacto, by 13.33% and 6.52% on 1-NNA (p-CD) across\nthe two datasets. Experimental analysis shows that SeaLion can be trained\nsemi-supervised, thereby reducing the demand for labeling efforts. Lastly, we\nvalidate the applicability of SeaLion in generative data augmentation for\ntraining segmentation models and the capability of SeaLion to serve as a tool\nfor part-aware 3D shape editing."}
{"id": "2505.17726", "pdf": "https://arxiv.org/pdf/2505.17726", "abs": "https://arxiv.org/abs/2505.17726", "authors": ["Donghwan Chi", "Hyomin Kim", "Yoonjin Oh", "Yongjin Kim", "Donghoon Lee", "Daejin Jo", "Jongmin Kim", "Junyeob Baek", "Sungjin Ahn", "Sungwoong Kim"], "title": "Slot-MLLM: Object-Centric Visual Tokenization for Multimodal LLM", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recently, multimodal large language models (MLLMs) have emerged as a key\napproach in achieving artificial general intelligence. In particular,\nvision-language MLLMs have been developed to generate not only text but also\nvisual outputs from multimodal inputs. This advancement requires efficient\nimage tokens that LLMs can process effectively both in input and output.\nHowever, existing image tokenization methods for MLLMs typically capture only\nglobal abstract concepts or uniformly segmented image patches, restricting\nMLLMs' capability to effectively understand or generate detailed visual\ncontent, particularly at the object level. To address this limitation, we\npropose an object-centric visual tokenizer based on Slot Attention specifically\nfor MLLMs. In particular, based on the Q-Former encoder, diffusion decoder, and\nresidual vector quantization, our proposed discretized slot tokens can encode\nlocal visual details while maintaining high-level semantics, and also align\nwith textual data to be integrated seamlessly within a unified next-token\nprediction framework of LLMs. The resulting Slot-MLLM demonstrates significant\nperformance improvements over baselines with previous visual tokenizers across\nvarious vision-language tasks that entail local detailed comprehension and\ngeneration. Notably, this work is the first demonstration of the feasibility of\nobject-centric slot attention performed with MLLMs and in-the-wild natural\nimages."}
{"id": "2505.17727", "pdf": "https://arxiv.org/pdf/2505.17727", "abs": "https://arxiv.org/abs/2505.17727", "authors": ["Jiawei Zhou", "Linye Lyu", "Zhuotao Tian", "Cheng Zhuo", "Yu Li"], "title": "SafeMVDrive: Multi-view Safety-Critical Driving Video Synthesis in the Real World Domain", "categories": ["cs.CV"], "comment": null, "summary": "Safety-critical scenarios are rare yet pivotal for evaluating and enhancing\nthe robustness of autonomous driving systems. While existing methods generate\nsafety-critical driving trajectories, simulations, or single-view videos, they\nfall short of meeting the demands of advanced end-to-end autonomous systems\n(E2E AD), which require real-world, multi-view video data. To bridge this gap,\nwe introduce SafeMVDrive, the first framework designed to generate\nhigh-quality, safety-critical, multi-view driving videos grounded in real-world\ndomains. SafeMVDrive strategically integrates a safety-critical trajectory\ngenerator with an advanced multi-view video generator. To tackle the challenges\ninherent in this integration, we first enhance scene understanding ability of\nthe trajectory generator by incorporating visual context -- which is previously\nunavailable to such generator -- and leveraging a GRPO-finetuned\nvision-language model to achieve more realistic and context-aware trajectory\ngeneration. Second, recognizing that existing multi-view video generators\nstruggle to render realistic collision events, we introduce a two-stage,\ncontrollable trajectory generation mechanism that produces collision-evasion\ntrajectories, ensuring both video quality and safety-critical fidelity.\nFinally, we employ a diffusion-based multi-view video generator to synthesize\nhigh-quality safety-critical driving videos from the generated trajectories.\nExperiments conducted on an E2E AD planner demonstrate a significant increase\nin collision rate when tested with our generated data, validating the\neffectiveness of SafeMVDrive in stress-testing planning modules. Our code,\nexamples, and datasets are publicly available at:\nhttps://zhoujiawei3.github.io/SafeMVDrive/."}
{"id": "2505.17732", "pdf": "https://arxiv.org/pdf/2505.17732", "abs": "https://arxiv.org/abs/2505.17732", "authors": ["Ozsel Kilinc", "Cem Tarhan"], "title": "RQR3D: Reparametrizing the regression targets for BEV-based 3D object detection", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Accurate, fast, and reliable 3D perception is essential for autonomous\ndriving. Recently, bird's-eye view (BEV)-based perception approaches have\nemerged as superior alternatives to perspective-based solutions, offering\nenhanced spatial understanding and more natural outputs for planning. Existing\nBEV-based 3D object detection methods, typically adhering to angle-based\nrepresentation, directly estimate the size and orientation of rotated bounding\nboxes. We observe that BEV-based 3D object detection is analogous to aerial\noriented object detection, where angle-based methods are recognized for being\naffected by discontinuities in their loss functions. Drawing inspiration from\nthis domain, we propose Restricted Quadrilateral Representation to define 3D\nregression targets. RQR3D regresses the smallest horizontal bounding box\nencapsulating the oriented box, along with the offsets between the corners of\nthese two boxes, thereby transforming the oriented object detection problem\ninto a keypoint regression task. RQR3D is compatible with any 3D object\ndetection approach. We employ RQR3D within an anchor-free single-stage object\ndetection method and introduce an objectness head to address class imbalance\nproblem. Furthermore, we introduce a simplified radar fusion backbone that\neliminates the need for voxel grouping and processes the BEV-mapped point cloud\nwith standard 2D convolutions, rather than sparse convolutions. Extensive\nevaluations on the nuScenes dataset demonstrate that RQR3D achieves\nstate-of-the-art performance in camera-radar 3D object detection, outperforming\nthe previous best method by +4% in NDS and +2.4% in mAP, and significantly\nreducing the translation and orientation errors, which are crucial for safe\nautonomous driving. These consistent gains highlight the robustness, precision,\nand real-world readiness of our approach."}
{"id": "2505.17768", "pdf": "https://arxiv.org/pdf/2505.17768", "abs": "https://arxiv.org/abs/2505.17768", "authors": ["Dong Zhang", "Lingfeng He", "Rui Yan", "Fei Shen", "Jinhui Tang"], "title": "R-Genie: Reasoning-Guided Generative Image Editing", "categories": ["cs.CV", "F.2.2, I.2.7", "F.2.2; I.2.7"], "comment": "https://dongzhang89.github.io/RGenie.github.io/", "summary": "While recent advances in image editing have enabled impressive visual\nsynthesis capabilities, current methods remain constrained by explicit textual\ninstructions and limited editing operations, lacking deep comprehension of\nimplicit user intentions and contextual reasoning. In this work, we introduce a\nnew image editing paradigm: reasoning-guided generative editing, which\nsynthesizes images based on complex, multi-faceted textual queries accepting\nworld knowledge and intention inference. To facilitate this task, we first\nconstruct a comprehensive dataset featuring over 1,000 image-instruction-edit\ntriples that incorporate rich reasoning contexts and real-world knowledge. We\nthen propose R-Genie: a reasoning-guided generative image editor, which\nsynergizes the generation power of diffusion models with advanced reasoning\ncapabilities of multimodal large language models. R-Genie incorporates a\nreasoning-attention mechanism to bridge linguistic understanding with visual\nsynthesis, enabling it to handle intricate editing requests involving abstract\nuser intentions and contextual reasoning relations. Extensive experimental\nresults validate that R-Genie can equip diffusion models with advanced\nreasoning-based editing capabilities, unlocking new potentials for intelligent\nimage synthesis."}
{"id": "2505.17771", "pdf": "https://arxiv.org/pdf/2505.17771", "abs": "https://arxiv.org/abs/2505.17771", "authors": ["Yanping Fu", "Xinyuan Liu", "Tianyu Li", "Yike Ma", "Yucheng Zhang", "Feng Dai"], "title": "TopoPoint: Enhance Topology Reasoning via Endpoint Detection in Autonomous Driving", "categories": ["cs.CV"], "comment": null, "summary": "Topology reasoning, which unifies perception and structured reasoning, plays\na vital role in understanding intersections for autonomous driving. However,\nits performance heavily relies on the accuracy of lane detection, particularly\nat connected lane endpoints. Existing methods often suffer from lane endpoints\ndeviation, leading to incorrect topology construction. To address this issue,\nwe propose TopoPoint, a novel framework that explicitly detects lane endpoints\nand jointly reasons over endpoints and lanes for robust topology reasoning.\nDuring training, we independently initialize point and lane query, and proposed\nPoint-Lane Merge Self-Attention to enhance global context sharing through\nincorporating geometric distances between points and lanes as an attention mask\n. We further design Point-Lane Graph Convolutional Network to enable mutual\nfeature aggregation between point and lane query. During inference, we\nintroduce Point-Lane Geometry Matching algorithm that computes distances\nbetween detected points and lanes to refine lane endpoints, effectively\nmitigating endpoint deviation. Extensive experiments on the OpenLane-V2\nbenchmark demonstrate that TopoPoint achieves state-of-the-art performance in\ntopology reasoning (48.8 on OLS). Additionally, we propose DET$_p$ to evaluate\nendpoint detection, under which our method significantly outperforms existing\napproaches (52.6 v.s. 45.2 on DET$_p$). The code is released at\nhttps://github.com/Franpin/TopoPoint."}
{"id": "2505.17778", "pdf": "https://arxiv.org/pdf/2505.17778", "abs": "https://arxiv.org/abs/2505.17778", "authors": ["Yu Xie", "Jielei Zhang", "Pengyu Chen", "Ziyue Wang", "Weihang Wang", "Longwen Gao", "Peiyi Li", "Huyang Sun", "Qiang Zhang", "Qian Qiao", "Jiaqing Fan", "Zhouhui Lian"], "title": "TextFlux: An OCR-Free DiT Model for High-Fidelity Multilingual Scene Text Synthesis", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion-based scene text synthesis has progressed rapidly, yet existing\nmethods commonly rely on additional visual conditioning modules and require\nlarge-scale annotated data to support multilingual generation. In this work, we\nrevisit the necessity of complex auxiliary modules and further explore an\napproach that simultaneously ensures glyph accuracy and achieves high-fidelity\nscene integration, by leveraging diffusion models' inherent capabilities for\ncontextual reasoning. To this end, we introduce TextFlux, a DiT-based framework\nthat enables multilingual scene text synthesis. The advantages of TextFlux can\nbe summarized as follows: (1) OCR-free model architecture. TextFlux eliminates\nthe need for OCR encoders (additional visual conditioning modules) that are\nspecifically used to extract visual text-related features. (2) Strong\nmultilingual scalability. TextFlux is effective in low-resource multilingual\nsettings, and achieves strong performance in newly added languages with fewer\nthan 1,000 samples. (3) Streamlined training setup. TextFlux is trained with\nonly 1% of the training data required by competing methods. (4) Controllable\nmulti-line text generation. TextFlux offers flexible multi-line synthesis with\nprecise line-level control, outperforming methods restricted to single-line or\nrigid layouts. Extensive experiments and visualizations demonstrate that\nTextFlux outperforms previous methods in both qualitative and quantitative\nevaluations."}
{"id": "2505.17779", "pdf": "https://arxiv.org/pdf/2505.17779", "abs": "https://arxiv.org/abs/2505.17779", "authors": ["Anjie Le", "Henan Liu", "Yue Wang", "Zhenyu Liu", "Rongkun Zhu", "Taohan Weng", "Jinze Yu", "Boyang Wang", "Yalun Wu", "Kaiwen Yan", "Quanlin Sun", "Meirui Jiang", "Jialun Pei", "Siya Liu", "Haoyun Zheng", "Zhoujun Li", "Alison Noble", "Jacques Souquet", "Xiaoqing Guo", "Manxi Lin", "Hongcheng Guo"], "title": "U2-BENCH: Benchmarking Large Vision-Language Models on Ultrasound Understanding", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Ultrasound is a widely-used imaging modality critical to global healthcare,\nyet its interpretation remains challenging due to its varying image quality on\noperators, noises, and anatomical structures. Although large vision-language\nmodels (LVLMs) have demonstrated impressive multimodal capabilities across\nnatural and medical domains, their performance on ultrasound remains largely\nunexplored. We introduce U2-BENCH, the first comprehensive benchmark to\nevaluate LVLMs on ultrasound understanding across classification, detection,\nregression, and text generation tasks. U2-BENCH aggregates 7,241 cases spanning\n15 anatomical regions and defines 8 clinically inspired tasks, such as\ndiagnosis, view recognition, lesion localization, clinical value estimation,\nand report generation, across 50 ultrasound application scenarios. We evaluate\n20 state-of-the-art LVLMs, both open- and closed-source, general-purpose and\nmedical-specific. Our results reveal strong performance on image-level\nclassification, but persistent challenges in spatial reasoning and clinical\nlanguage generation. U2-BENCH establishes a rigorous and unified testbed to\nassess and accelerate LVLM research in the uniquely multimodal domain of\nmedical ultrasound imaging."}
{"id": "2505.17782", "pdf": "https://arxiv.org/pdf/2505.17782", "abs": "https://arxiv.org/abs/2505.17782", "authors": ["Nikolas Papadopoulos", "Nikolaos Ioannis Bountos", "Maria Sdraka", "Andreas Karavias", "Ioannis Papoutsis"], "title": "Hephaestus Minicubes: A Global, Multi-Modal Dataset for Volcanic Unrest Monitoring", "categories": ["cs.CV"], "comment": null, "summary": "Ground deformation is regarded in volcanology as a key precursor signal\npreceding volcanic eruptions. Satellite-based Interferometric Synthetic\nAperture Radar (InSAR) enables consistent, global-scale deformation tracking;\nhowever, deep learning methods remain largely unexplored in this domain, mainly\ndue to the lack of a curated machine learning dataset. In this work, we build\non the existing Hephaestus dataset, and introduce Hephaestus Minicubes, a\nglobal collection of 38 spatiotemporal datacubes offering high resolution,\nmulti-source and multi-temporal information, covering 44 of the world's most\nactive volcanoes over a 7-year period. Each spatiotemporal datacube integrates\nInSAR products, topographic data, as well as atmospheric variables which are\nknown to introduce signal delays that can mimic ground deformation in InSAR\nimagery. Furthermore, we provide expert annotations detailing the type,\nintensity and spatial extent of deformation events, along with rich text\ndescriptions of the observed scenes. Finally, we present a comprehensive\nbenchmark, demonstrating Hephaestus Minicubes' ability to support volcanic\nunrest monitoring as a multi-modal, multi-temporal classification and semantic\nsegmentation task, establishing strong baselines with state-of-the-art\narchitectures. This work aims to advance machine learning research in volcanic\nmonitoring, contributing to the growing integration of data-driven methods\nwithin Earth science applications."}
{"id": "2505.17783", "pdf": "https://arxiv.org/pdf/2505.17783", "abs": "https://arxiv.org/abs/2505.17783", "authors": ["Dekai Zhu", "Stefan Gavranovic", "Flavien Boussuge", "Benjamin Busam", "Slobodan Ilic"], "title": "Generative Data Augmentation for Object Point Cloud Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Data augmentation is widely used to train deep learning models to address\ndata scarcity. However, traditional data augmentation (TDA) typically relies on\nsimple geometric transformation, such as random rotation and rescaling,\nresulting in minimal data diversity enrichment and limited model performance\nimprovement. State-of-the-art generative models for 3D shape generation rely on\nthe denoising diffusion probabilistic models and manage to generate realistic\nnovel point clouds for 3D content creation and manipulation. Nevertheless, the\ngenerated 3D shapes lack associated point-wise semantic labels, restricting\ntheir usage in enlarging the training data for point cloud segmentation tasks.\nTo bridge the gap between data augmentation techniques and the advanced\ndiffusion models, we extend the state-of-the-art 3D diffusion model, Lion, to a\npart-aware generative model that can generate high-quality point clouds\nconditioned on given segmentation masks. Leveraging the novel generative model,\nwe introduce a 3-step generative data augmentation (GDA) pipeline for point\ncloud segmentation training. Our GDA approach requires only a small amount of\nlabeled samples but enriches the training data with generated variants and\npseudo-labeled samples, which are validated by a novel diffusion-based\npseudo-label filtering method. Extensive experiments on two large-scale\nsynthetic datasets and a real-world medical dataset demonstrate that our GDA\nmethod outperforms TDA approach and related semi-supervised and self-supervised\nmethods."}
{"id": "2505.17796", "pdf": "https://arxiv.org/pdf/2505.17796", "abs": "https://arxiv.org/abs/2505.17796", "authors": ["Yuxin Yang", "Yinan Zhou", "Yuxin Chen", "Ziqi Zhang", "Zongyang Ma", "Chunfeng Yuan", "Bing Li", "Lin Song", "Jun Gao", "Peng Li", "Weiming Hu"], "title": "DetailFusion: A Dual-branch Framework with Detail Enhancement for Composed Image Retrieval", "categories": ["cs.CV", "cs.AI", "cs.IR"], "comment": "20 pages, 6 figures", "summary": "Composed Image Retrieval (CIR) aims to retrieve target images from a gallery\nbased on a reference image and modification text as a combined query. Recent\napproaches focus on balancing global information from two modalities and encode\nthe query into a unified feature for retrieval. However, due to insufficient\nattention to fine-grained details, these coarse fusion methods often struggle\nwith handling subtle visual alterations or intricate textual instructions. In\nthis work, we propose DetailFusion, a novel dual-branch framework that\neffectively coordinates information across global and detailed granularities,\nthereby enabling detail-enhanced CIR. Our approach leverages atomic detail\nvariation priors derived from an image editing dataset, supplemented by a\ndetail-oriented optimization strategy to develop a Detail-oriented Inference\nBranch. Furthermore, we design an Adaptive Feature Compositor that dynamically\nfuses global and detailed features based on fine-grained information of each\nunique multimodal query. Extensive experiments and ablation analyses not only\ndemonstrate that our method achieves state-of-the-art performance on both CIRR\nand FashionIQ datasets but also validate the effectiveness and cross-domain\nadaptability of detail enhancement for CIR."}
{"id": "2505.17807", "pdf": "https://arxiv.org/pdf/2505.17807", "abs": "https://arxiv.org/abs/2505.17807", "authors": ["Ping Li", "Jianan Ni", "Bo Pang"], "title": "Temporal Consistency Constrained Transferable Adversarial Attacks with Background Mixup for Action Recognition", "categories": ["cs.CV"], "comment": "Accepted in IJCAI'25", "summary": "Action recognition models using deep learning are vulnerable to adversarial\nexamples, which are transferable across other models trained on the same data\nmodality. Existing transferable attack methods face two major challenges: 1)\nthey heavily rely on the assumption that the decision boundaries of the\nsurrogate (a.k.a., source) model and the target model are similar, which limits\nthe adversarial transferability; and 2) their decision boundary difference\nmakes the attack direction uncertain, which may result in the gradient\noscillation, weakening the adversarial attack. This motivates us to propose a\nBackground Mixup-induced Temporal Consistency (BMTC) attack method for action\nrecognition. From the input transformation perspective, we design a\nmodel-agnostic background adversarial mixup module to reduce the\nsurrogate-target model dependency. In particular, we randomly sample one video\nfrom each category and make its background frame, while selecting the\nbackground frame with the top attack ability for mixup with the clean frame by\nreinforcement learning. Moreover, to ensure an explicit attack direction, we\nleverage the background category as guidance for updating the gradient of\nadversarial example, and design a temporal gradient consistency loss, which\nstrengthens the stability of the attack direction on subsequent frames.\nEmpirical studies on two video datasets, i.e., UCF101 and Kinetics-400, and one\nimage dataset, i.e., ImageNet, demonstrate that our method significantly boosts\nthe transferability of adversarial examples across several action/image\nrecognition models. Our code is available at\nhttps://github.com/mlvccn/BMTC_TransferAttackVid."}
{"id": "2505.17808", "pdf": "https://arxiv.org/pdf/2505.17808", "abs": "https://arxiv.org/abs/2505.17808", "authors": ["Ramanathan Swaminathan"], "title": "An Attention Infused Deep Learning System with Grad-CAM Visualization for Early Screening of Glaucoma", "categories": ["cs.CV", "cs.AI"], "comment": "6 pages in general IEEE format, 8 figures, 4 tables, pdflatex", "summary": "This research work reveals the eye opening wisdom of the hybrid labyrinthine\ndeep learning models synergy born out of combining a trailblazing convolutional\nneural network with a disruptive Vision Transformer, both intertwined together\nwith a radical Cross Attention module. Here, two high yielding datasets for\nartificial intelligence models in detecting glaucoma, namely ACRIMA and\nDrishti, are utilized."}
{"id": "2505.17812", "pdf": "https://arxiv.org/pdf/2505.17812", "abs": "https://arxiv.org/abs/2505.17812", "authors": ["Boxu Chen", "Ziwei Zheng", "Le Yang", "Zeyu Geng", "Zhengyu Zhao", "Chenhao Lin", "Chao Shen"], "title": "Seeing It or Not? Interpretable Vision-aware Latent Steering to Mitigate Object Hallucinations", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Large Vision-Language Models (LVLMs) have achieved remarkable success but\ncontinue to struggle with object hallucination (OH), generating outputs\ninconsistent with visual inputs. While previous work has proposed methods to\nreduce OH, the visual decision-making mechanisms that lead to hallucinations\nremain poorly understood. In this paper, we propose VaLSe, a Vision-aware\nLatent Steering framework that adopts an interpretation-then-mitigation\nstrategy to address OH in LVLMs. By tackling dual challenges of modeling\ncomplex vision-language interactions and eliminating spurious activation\nartifacts, VaLSe can generate visual contribution maps that trace how specific\nvisual inputs influence individual output tokens. These maps reveal the model's\nvision-aware focus regions, which are then used to perform latent space\nsteering, realigning internal representations toward semantically relevant\ncontent and reducing hallucinated outputs. Extensive experiments demonstrate\nthat VaLSe is a powerful interpretability tool and an effective method for\nenhancing model robustness against OH across multiple benchmarks. Furthermore,\nour analysis uncovers limitations in existing OH evaluation metrics,\nunderscoring the need for more nuanced, interpretable, and visually grounded OH\nbenchmarks in future work. Code is available at:\nhttps://github.com/Ziwei-Zheng/VaLSe."}
{"id": "2505.17821", "pdf": "https://arxiv.org/pdf/2505.17821", "abs": "https://arxiv.org/abs/2505.17821", "authors": ["Shihao Li", "Chenglong Li", "Aihua Zheng", "Jin Tang", "Bin Luo"], "title": "ICPL-ReID: Identity-Conditional Prompt Learning for Multi-Spectral Object Re-Identification", "categories": ["cs.CV"], "comment": "Accepted by IEEE Transactions on Multimedia (TMM)", "summary": "Multi-spectral object re-identification (ReID) brings a new perception\nperspective for smart city and intelligent transportation applications,\neffectively addressing challenges from complex illumination and adverse\nweather. However, complex modal differences between heterogeneous spectra pose\nchallenges to efficiently utilizing complementary and discrepancy of spectra\ninformation. Most existing methods fuse spectral data through intricate modal\ninteraction modules, lacking fine-grained semantic understanding of spectral\ninformation (\\textit{e.g.}, text descriptions, part masks, and object\nkeypoints). To solve this challenge, we propose a novel Identity-Conditional\ntext Prompt Learning framework (ICPL), which exploits the powerful cross-modal\nalignment capability of CLIP, to unify different spectral visual features from\ntext semantics. Specifically, we first propose the online prompt learning using\nlearnable text prompt as the identity-level semantic center to bridge the\nidentity semantics of different spectra in online manner. Then, in lack of\nconcrete text descriptions, we propose the multi-spectral identity-condition\nmodule to use identity prototype as spectral identity condition to constraint\nprompt learning. Meanwhile, we construct the alignment loop mutually optimizing\nthe learnable text prompt and spectral visual encoder to avoid online prompt\nlearning disrupting the pre-trained text-image alignment distribution. In\naddition, to adapt to small-scale multi-spectral data and mitigate style\ndifferences between spectra, we propose multi-spectral adapter that employs a\nlow-rank adaption method to learn spectra-specific features. Comprehensive\nexperiments on 5 benchmarks, including RGBNT201, Market-MM, MSVR310, RGBN300,\nand RGBNT100, demonstrate that the proposed method outperforms the\nstate-of-the-art methods."}
{"id": "2505.17835", "pdf": "https://arxiv.org/pdf/2505.17835", "abs": "https://arxiv.org/abs/2505.17835", "authors": ["Marc Lalonde", "Hamed Ghodrati"], "title": "VLM Models and Automated Grading of Atopic Dermatitis", "categories": ["cs.CV"], "comment": "10 pages", "summary": "The task of grading atopic dermatitis (or AD, a form of eczema) from patient\nimages is difficult even for trained dermatologists. Research on automating\nthis task has progressed in recent years with the development of deep learning\nsolutions; however, the rapid evolution of multimodal models and more\nspecifically vision-language models (VLMs) opens the door to new possibilities\nin terms of explainable assessment of medical images, including dermatology.\nThis report describes experiments carried out to evaluate the ability of seven\nVLMs to assess the severity of AD on a set of test images."}
{"id": "2505.17844", "pdf": "https://arxiv.org/pdf/2505.17844", "abs": "https://arxiv.org/abs/2505.17844", "authors": ["Fabian Deuser", "Philipp Hausenblas", "Hannah Schieber", "Daniel Roth", "Martin Werner", "Norbert Oswald"], "title": "Locality-Sensitive Hashing for Efficient Hard Negative Sampling in Contrastive Learning", "categories": ["cs.CV"], "comment": null, "summary": "Contrastive learning is a representational learning paradigm in which a\nneural network maps data elements to feature vectors. It improves the feature\nspace by forming lots with an anchor and examples that are either positive or\nnegative based on class similarity. Hard negative examples, which are close to\nthe anchor in the feature space but from a different class, improve learning\nperformance. Finding such examples of high quality efficiently in large,\nhigh-dimensional datasets is computationally challenging. In this paper, we\npropose a GPU-friendly Locality-Sensitive Hashing (LSH) scheme that quantizes\nreal-valued feature vectors into binary representations for approximate nearest\nneighbor search. We investigate its theoretical properties and evaluate it on\nseveral datasets from textual and visual domain. Our approach achieves\ncomparable or better performance while requiring significantly less computation\nthan existing hard negative mining strategies."}
{"id": "2505.17867", "pdf": "https://arxiv.org/pdf/2505.17867", "abs": "https://arxiv.org/abs/2505.17867", "authors": ["Konstantinos Spathis", "Nikolaos Kardaris", "Petros Maragos"], "title": "Multi-task Learning For Joint Action and Gesture Recognition", "categories": ["cs.CV"], "comment": null, "summary": "In practical applications, computer vision tasks often need to be addressed\nsimultaneously. Multitask learning typically achieves this by jointly training\na single deep neural network to learn shared representations, providing\nefficiency and improving generalization. Although action and gesture\nrecognition are closely related tasks, since they focus on body and hand\nmovements, current state-of-the-art methods handle them separately. In this\npaper, we show that employing a multi-task learning paradigm for action and\ngesture recognition results in more efficient, robust and generalizable visual\nrepresentations, by leveraging the synergies between these tasks. Extensive\nexperiments on multiple action and gesture datasets demonstrate that handling\nactions and gestures in a single architecture can achieve better performance\nfor both tasks in comparison to their single-task learning variants."}
{"id": "2505.17881", "pdf": "https://arxiv.org/pdf/2505.17881", "abs": "https://arxiv.org/abs/2505.17881", "authors": ["Wenjin Qin", "Hailin Wang", "Hao Shu", "Feng Zhang", "Jianjun Wang", "Xiangyong Cao", "Xi-Le Zhao", "Gemine Vivone"], "title": "Hyperspectral Anomaly Detection Fused Unified Nonconvex Tensor Ring Factors Regularization", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "In recent years, tensor decomposition-based approaches for hyperspectral\nanomaly detection (HAD) have gained significant attention in the field of\nremote sensing. However, existing methods often fail to fully leverage both the\nglobal correlations and local smoothness of the background components in\nhyperspectral images (HSIs), which exist in both the spectral and spatial\ndomains. This limitation results in suboptimal detection performance. To\nmitigate this critical issue, we put forward a novel HAD method named\nHAD-EUNTRFR, which incorporates an enhanced unified nonconvex tensor ring (TR)\nfactors regularization. In the HAD-EUNTRFR framework, the raw HSIs are first\ndecomposed into background and anomaly components. The TR decomposition is then\nemployed to capture the spatial-spectral correlations within the background\ncomponent. Additionally, we introduce a unified and efficient nonconvex\nregularizer, induced by tensor singular value decomposition (TSVD), to\nsimultaneously encode the low-rankness and sparsity of the 3-D gradient TR\nfactors into a unique concise form. The above characterization scheme enables\nthe interpretable gradient TR factors to inherit the low-rankness and\nsmoothness of the original background. To further enhance anomaly detection, we\ndesign a generalized nonconvex regularization term to exploit the group\nsparsity of the anomaly component. To solve the resulting doubly nonconvex\nmodel, we develop a highly efficient optimization algorithm based on the\nalternating direction method of multipliers (ADMM) framework. Experimental\nresults on several benchmark datasets demonstrate that our proposed method\noutperforms existing state-of-the-art (SOTA) approaches in terms of detection\naccuracy."}
{"id": "2505.17884", "pdf": "https://arxiv.org/pdf/2505.17884", "abs": "https://arxiv.org/abs/2505.17884", "authors": ["Nikita Ivanov", "Mark Klimov", "Dmitry Glukhikh", "Tatiana Chernysheva", "Igor Glukhikh"], "title": "Track Anything Annotate: Video annotation and dataset generation of computer vision models", "categories": ["cs.CV"], "comment": "9 pages, 11 figures", "summary": "Modern machine learning methods require significant amounts of labelled data,\nmaking the preparation process time-consuming and resource-intensive. In this\npaper, we propose to consider the process of prototyping a tool for annotating\nand generating training datasets based on video tracking and segmentation. We\nexamine different approaches to solving this problem, from technology selection\nthrough to final implementation. The developed prototype significantly\naccelerates dataset generation compared to manual annotation. All resources are\navailable at https://github.com/lnikioffic/track-anything-annotate"}
{"id": "2505.17893", "pdf": "https://arxiv.org/pdf/2505.17893", "abs": "https://arxiv.org/abs/2505.17893", "authors": ["Shruti Atul Mali", "Zohaib Salahuddin", "Danial Khan", "Yumeng Zhang", "Henry C. Woodruff", "Eduardo Ibor-Crespo", "Ana Jimenez-Pastor", "Luis Marti-Bonmati", "Philippe Lambin"], "title": "Pixels to Prognosis: Harmonized Multi-Region CT-Radiomics and Foundation-Model Signatures Across Multicentre NSCLC Data", "categories": ["cs.CV"], "comment": null, "summary": "Purpose: To evaluate the impact of harmonization and multi-region CT image\nfeature integration on survival prediction in non-small cell lung cancer\n(NSCLC) patients, using handcrafted radiomics, pretrained foundation model (FM)\nfeatures, and clinical data from a multicenter dataset.\n  Methods: We analyzed CT scans and clinical data from 876 NSCLC patients (604\ntraining, 272 test) across five centers. Features were extracted from the whole\nlung, tumor, mediastinal nodes, coronary arteries, and coronary artery calcium\n(CAC). Handcrafted radiomics and FM deep features were harmonized using ComBat,\nreconstruction kernel normalization (RKN), and RKN+ComBat. Regularized Cox\nmodels predicted overall survival; performance was assessed using the\nconcordance index (C-index), 5-year time-dependent area under the curve\n(t-AUC), and hazard ratio (HR). SHapley Additive exPlanations (SHAP) values\nexplained feature contributions. A consensus model used agreement across top\nregion of interest (ROI) models to stratify patient risk.\n  Results: TNM staging showed prognostic utility (C-index = 0.67; HR = 2.70;\nt-AUC = 0.85). The clinical + tumor radiomics model with ComBat achieved a\nC-index of 0.7552 and t-AUC of 0.8820. FM features (50-voxel cubes) combined\nwith clinical data yielded the highest performance (C-index = 0.7616; t-AUC =\n0.8866). An ensemble of all ROIs and FM features reached a C-index of 0.7142\nand t-AUC of 0.7885. The consensus model, covering 78% of valid test cases,\nachieved a t-AUC of 0.92, sensitivity of 97.6%, and specificity of 66.7%.\n  Conclusion: Harmonization and multi-region feature integration improve\nsurvival prediction in multicenter NSCLC data. Combining interpretable\nradiomics, FM features, and consensus modeling enables robust risk\nstratification across imaging centers."}
{"id": "2505.17905", "pdf": "https://arxiv.org/pdf/2505.17905", "abs": "https://arxiv.org/abs/2505.17905", "authors": ["Xie Ting", "Ye Huang", "Zhilin Liu", "Lixin Duan"], "title": "Semantic segmentation with reward", "categories": ["cs.CV"], "comment": "Tech report", "summary": "In real-world scenarios, pixel-level labeling is not always available.\nSometimes, we need a semantic segmentation network, and even a visual encoder\ncan have a high compatibility, and can be trained using various types of\nfeedback beyond traditional labels, such as feedback that indicates the quality\nof the parsing results. To tackle this issue, we proposed RSS (Reward in\nSemantic Segmentation), the first practical application of reward-based\nreinforcement learning on pure semantic segmentation offered in two granular\nlevels (pixel-level and image-level). RSS incorporates various novel\ntechnologies, such as progressive scale rewards (PSR) and pair-wise spatial\ndifference (PSD), to ensure that the reward facilitates the convergence of the\nsemantic segmentation network, especially under image-level rewards.\nExperiments and visualizations on benchmark datasets demonstrate that the\nproposed RSS can successfully ensure the convergence of the semantic\nsegmentation network on two levels of rewards. Additionally, the RSS, which\nutilizes an image-level reward, outperforms existing weakly supervised methods\nthat also rely solely on image-level signals during training."}
{"id": "2505.17910", "pdf": "https://arxiv.org/pdf/2505.17910", "abs": "https://arxiv.org/abs/2505.17910", "authors": ["Bin Wu", "Wei Wang", "Yahui Liu", "Zixiang Li", "Yao Zhao"], "title": "DiffusionReward: Enhancing Blind Face Restoration through Reward Feedback Learning", "categories": ["cs.CV", "cs.AI"], "comment": "22 pages, 13 figures, 5 tables", "summary": "Reward Feedback Learning (ReFL) has recently shown great potential in\naligning model outputs with human preferences across various generative tasks.\nIn this work, we introduce a ReFL framework, named DiffusionReward, to the\nBlind Face Restoration task for the first time. DiffusionReward effectively\novercomes the limitations of diffusion-based methods, which often fail to\ngenerate realistic facial details and exhibit poor identity consistency. The\ncore of our framework is the Face Reward Model (FRM), which is trained using\ncarefully annotated data. It provides feedback signals that play a pivotal role\nin steering the optimization process of the restoration network. In particular,\nour ReFL framework incorporates a gradient flow into the denoising process of\noff-the-shelf face restoration methods to guide the update of model parameters.\nThe guiding gradient is collaboratively determined by three aspects: (i) the\nFRM to ensure the perceptual quality of the restored faces; (ii) a\nregularization term that functions as a safeguard to preserve generative\ndiversity; and (iii) a structural consistency constraint to maintain facial\nfidelity. Furthermore, the FRM undergoes dynamic optimization throughout the\nprocess. It not only ensures that the restoration network stays precisely\naligned with the real face manifold, but also effectively prevents reward\nhacking. Experiments on synthetic and wild datasets demonstrate that our method\noutperforms state-of-the-art methods, significantly improving identity\nconsistency and facial details. The source codes, data, and models are\navailable at: https://github.com/01NeuralNinja/DiffusionReward."}
{"id": "2505.17911", "pdf": "https://arxiv.org/pdf/2505.17911", "abs": "https://arxiv.org/abs/2505.17911", "authors": ["Zheyang Huang", "Jagannath Aryal", "Saeid Nahavandi", "Xuequan Lu", "Chee Peng Lim", "Lei Wei", "Hailing Zhou"], "title": "Object-level Cross-view Geo-localization with Location Enhancement and Multi-Head Cross Attention", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Cross-view geo-localization determines the location of a query image,\ncaptured by a drone or ground-based camera, by matching it to a geo-referenced\nsatellite image. While traditional approaches focus on image-level\nlocalization, many applications, such as search-and-rescue, infrastructure\ninspection, and precision delivery, demand object-level accuracy. This enables\nusers to prompt a specific object with a single click on a drone image to\nretrieve precise geo-tagged information of the object. However, variations in\nviewpoints, timing, and imaging conditions pose significant challenges,\nespecially when identifying visually similar objects in extensive satellite\nimagery. To address these challenges, we propose an Object-level Cross-view\nGeo-localization Network (OCGNet). It integrates user-specified click locations\nusing Gaussian Kernel Transfer (GKT) to preserve location information\nthroughout the network. This cue is dually embedded into the feature encoder\nand feature matching blocks, ensuring robust object-specific localization.\nAdditionally, OCGNet incorporates a Location Enhancement (LE) module and a\nMulti-Head Cross Attention (MHCA) module to adaptively emphasize\nobject-specific features or expand focus to relevant contextual regions when\nnecessary. OCGNet achieves state-of-the-art performance on a public dataset,\nCVOGL. It also demonstrates few-shot learning capabilities, effectively\ngeneralizing from limited examples, making it suitable for diverse applications\n(https://github.com/ZheyangH/OCGNet)."}
{"id": "2505.17921", "pdf": "https://arxiv.org/pdf/2505.17921", "abs": "https://arxiv.org/abs/2505.17921", "authors": ["Carlos Salazar-Ruiz", "Francisco Lopez-Tiro", "Ivan Reyes-Amezcua", "Clement Larose", "Gilberto Ochoa-Ruiz", "Christian Daul"], "title": "Evaluation of Few-Shot Learning Methods for Kidney Stone Type Recognition in Ureteroscopy", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "6 pages, 3 figures, 3 tables, conference, cbms25", "summary": "Determining the type of kidney stones is crucial for prescribing appropriate\ntreatments to prevent recurrence. Currently, various approaches exist to\nidentify the type of kidney stones. However, obtaining results through the\nreference ex vivo identification procedure can take several weeks, while in\nvivo visual recognition requires highly trained specialists. For this reason,\ndeep learning models have been developed to provide urologists with an\nautomated classification of kidney stones during ureteroscopies. Nevertheless,\na common issue with these models is the lack of training data. This\ncontribution presents a deep learning method based on few-shot learning, aimed\nat producing sufficiently discriminative features for identifying kidney stone\ntypes in endoscopic images, even with a very limited number of samples. This\napproach was specifically designed for scenarios where endoscopic images are\nscarce or where uncommon classes are present, enabling classification even with\na limited training dataset. The results demonstrate that Prototypical Networks,\nusing up to 25% of the training data, can achieve performance equal to or\nbetter than traditional deep learning models trained with the complete dataset."}
{"id": "2505.17931", "pdf": "https://arxiv.org/pdf/2505.17931", "abs": "https://arxiv.org/abs/2505.17931", "authors": ["Xingjian Li", "Qifeng Wu", "Colleen Que", "Yiran Ding", "Adithya S. Ubaradka", "Jianhua Xing", "Tianyang Wang", "Min Xu"], "title": "AutoMiSeg: Automatic Medical Image Segmentation via Test-Time Adaptation of Foundation Models", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Medical image segmentation is vital for clinical diagnosis, yet current deep\nlearning methods often demand extensive expert effort, i.e., either through\nannotating large training datasets or providing prompts at inference time for\neach new case. This paper introduces a zero-shot and automatic segmentation\npipeline that combines off-the-shelf vision-language and segmentation\nfoundation models. Given a medical image and a task definition (e.g., \"segment\nthe optic disc in an eye fundus image\"), our method uses a grounding model to\ngenerate an initial bounding box, followed by a visual prompt boosting module\nthat enhance the prompts, which are then processed by a promptable segmentation\nmodel to produce the final mask. To address the challenges of domain gap and\nresult verification, we introduce a test-time adaptation framework featuring a\nset of learnable adaptors that align the medical inputs with foundation model\nrepresentations. Its hyperparameters are optimized via Bayesian Optimization,\nguided by a proxy validation model without requiring ground-truth labels. Our\npipeline offers an annotation-efficient and scalable solution for zero-shot\nmedical image segmentation across diverse tasks. Our pipeline is evaluated on\nseven diverse medical imaging datasets and shows promising results. By proper\ndecomposition and test-time adaptation, our fully automatic pipeline performs\ncompetitively with weakly-prompted interactive foundation models."}
{"id": "2505.17951", "pdf": "https://arxiv.org/pdf/2505.17951", "abs": "https://arxiv.org/abs/2505.17951", "authors": ["Haihong Xiao", "Jianan Zou", "Yuxin Zhou", "Ying He", "Wenxiong Kang"], "title": "SplatCo: Structure-View Collaborative Gaussian Splatting for Detail-Preserving Rendering of Large-Scale Unbounded Scenes", "categories": ["cs.CV"], "comment": null, "summary": "We present SplatCo, a structure-view collaborative Gaussian splatting\nframework for high-fidelity rendering of complex outdoor environments. SplatCo\nbuilds upon two novel components: (1) a cross-structure collaboration module\nthat combines global tri-plane representations, which capture coarse scene\nlayouts, with local context grid features that represent fine surface details.\nThis fusion is achieved through a novel hierarchical compensation strategy,\nensuring both global consistency and local detail preservation; and (2) a\ncross-view assisted training strategy that enhances multi-view consistency by\nsynchronizing gradient updates across viewpoints, applying visibility-aware\ndensification, and pruning overfitted or inaccurate Gaussians based on\nstructural consistency. Through joint optimization of structural representation\nand multi-view coherence, SplatCo effectively reconstructs fine-grained\ngeometric structures and complex textures in large-scale scenes. Comprehensive\nevaluations on 13 diverse large-scale scenes, including Mill19, MatrixCity,\nTanks & Temples, WHU, and custom aerial captures, demonstrate that SplatCo\nconsistently achieves higher reconstruction quality than state-of-the-art\nmethods, with PSNR improvements of 1-2 dB and SSIM gains of 0.1 to 0.2. These\nresults establish a new benchmark for high-fidelity rendering of large-scale\nunbounded scenes. Code and additional information are available at\nhttps://github.com/SCUT-BIP-Lab/SplatCo."}
{"id": "2505.17955", "pdf": "https://arxiv.org/pdf/2505.17955", "abs": "https://arxiv.org/abs/2505.17955", "authors": ["Yujin Jeong", "Arnas Uselis", "Seong Joon Oh", "Anna Rohrbach"], "title": "Diffusion Classifiers Understand Compositionality, but Conditions Apply", "categories": ["cs.CV"], "comment": null, "summary": "Understanding visual scenes is fundamental to human intelligence. While\ndiscriminative models have significantly advanced computer vision, they often\nstruggle with compositional understanding. In contrast, recent generative\ntext-to-image diffusion models excel at synthesizing complex scenes, suggesting\ninherent compositional capabilities. Building on this, zero-shot diffusion\nclassifiers have been proposed to repurpose diffusion models for discriminative\ntasks. While prior work offered promising results in discriminative\ncompositional scenarios, these results remain preliminary due to a small number\nof benchmarks and a relatively shallow analysis of conditions under which the\nmodels succeed. To address this, we present a comprehensive study of the\ndiscriminative capabilities of diffusion classifiers on a wide range of\ncompositional tasks. Specifically, our study covers three diffusion models (SD\n1.5, 2.0, and, for the first time, 3-m) spanning 10 datasets and over 30 tasks.\nFurther, we shed light on the role that target dataset domains play in\nrespective performance; to isolate the domain effects, we introduce a new\ndiagnostic benchmark Self-Bench comprised of images created by diffusion models\nthemselves. Finally, we explore the importance of timestep weighting and\nuncover a relationship between domain gap and timestep sensitivity,\nparticularly for SD3-m. To sum up, diffusion classifiers understand\ncompositionality, but conditions apply! Code and dataset are available at\nhttps://github.com/eugene6923/Diffusion-Classifiers-Compositionality."}
{"id": "2505.17959", "pdf": "https://arxiv.org/pdf/2505.17959", "abs": "https://arxiv.org/abs/2505.17959", "authors": ["Nguyen Duc", "Yan-Ling Lai", "Patrick Madlindl", "Xinyuan Zhu", "Benedikt Schwab", "Olaf Wysocki", "Ludwig Hoegner", "Thomas H. Kolbe"], "title": "Mind the Domain Gap: Measuring the Domain Gap Between Real-World and Synthetic Point Clouds for Automated Driving Development", "categories": ["cs.CV", "cs.LG"], "comment": "Submitted to PFG Journal of Photogrammetry, Remote Sensing and\n  Geoinformation Science", "summary": "Owing to the typical long-tail data distribution issues, simulating\ndomain-gap-free synthetic data is crucial in robotics, photogrammetry, and\ncomputer vision research. The fundamental challenge pertains to credibly\nmeasuring the difference between real and simulated data. Such a measure is\nvital for safety-critical applications, such as automated driving, where\nout-of-domain samples may impact a car's perception and cause fatal accidents.\nPrevious work has commonly focused on simulating data on one scene and\nanalyzing performance on a different, real-world scene, hampering the disjoint\nanalysis of domain gap coming from networks' deficiencies, class definitions,\nand object representation. In this paper, we propose a novel approach to\nmeasuring the domain gap between the real world sensor observations and\nsimulated data representing the same location, enabling comprehensive domain\ngap analysis. To measure such a domain gap, we introduce a novel metric\nDoGSS-PCL and evaluation assessing the geometric and semantic quality of the\nsimulated point cloud. Our experiments corroborate that the introduced approach\ncan be used to measure the domain gap. The tests also reveal that synthetic\nsemantic point clouds may be used for training deep neural networks,\nmaintaining the performance at the 50/50 real-to-synthetic ratio. We strongly\nbelieve that this work will facilitate research on credible data simulation and\nallow for at-scale deployment in automated driving testing and digital\ntwinning."}
{"id": "2505.17972", "pdf": "https://arxiv.org/pdf/2505.17972", "abs": "https://arxiv.org/abs/2505.17972", "authors": ["Kazi Mahmudul Hassan", "Xuyang Zhao", "Hidenori Sugano", "Toshihisa Tanaka"], "title": "MR-EEGWaveNet: Multiresolutional EEGWaveNet for Seizure Detection from Long EEG Recordings", "categories": ["cs.CV", "cs.LG"], "comment": "26 pages, 6 figures, 12 tables", "summary": "Feature engineering for generalized seizure detection models remains a\nsignificant challenge. Recently proposed models show variable performance\ndepending on the training data and remain ineffective at accurately\ndistinguishing artifacts from seizure data. In this study, we propose a novel\nend-to-end model, ''Multiresolutional EEGWaveNet (MR-EEGWaveNet),'' which\nefficiently distinguishes seizure events from background electroencephalogram\n(EEG) and artifacts/noise by capturing both temporal dependencies across\ndifferent time frames and spatial relationships between channels. The model has\nthree modules: convolution, feature extraction, and predictor. The convolution\nmodule extracts features through depth-wise and spatio-temporal convolution.\nThe feature extraction module individually reduces the feature dimension\nextracted from EEG segments and their sub-segments. Subsequently, the extracted\nfeatures are concatenated into a single vector for classification using a fully\nconnected classifier called the predictor module. In addition, an anomaly\nscore-based post-classification processing technique was introduced to reduce\nthe false-positive rates of the model. Experimental results were reported and\nanalyzed using different parameter settings and datasets (Siena (public) and\nJuntendo (private)). The proposed MR-EEGWaveNet significantly outperformed the\nconventional non-multiresolution approach, improving the F1 scores from 0.177\nto 0.336 on Siena and 0.327 to 0.488 on Juntendo, with precision gains of 15.9%\nand 20.62%, respectively."}
{"id": "2505.17973", "pdf": "https://arxiv.org/pdf/2505.17973", "abs": "https://arxiv.org/abs/2505.17973", "authors": ["Simone Gaisbauer", "Prabin Gyawali", "Qilin Zhang", "Olaf Wysocki", "Boris Jutzi"], "title": "To Glue or Not to Glue? Classical vs Learned Image Matching for Mobile Mapping Cameras to Textured Semantic 3D Building Models", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted to MMT, Xiamen, China; ISPRS Annals", "summary": "Feature matching is a necessary step for many computer vision and\nphotogrammetry applications such as image registration, structure-from-motion,\nand visual localization. Classical handcrafted methods such as SIFT feature\ndetection and description combined with nearest neighbour matching and RANSAC\noutlier removal have been state-of-the-art for mobile mapping cameras. With\nrecent advances in deep learning, learnable methods have been introduced and\nproven to have better robustness and performance under complex conditions.\nDespite their growing adoption, a comprehensive comparison between classical\nand learnable feature matching methods for the specific task of semantic 3D\nbuilding camera-to-model matching is still missing. This submission\nsystematically evaluates the effectiveness of different feature-matching\ntechniques in visual localization using textured CityGML LoD2 models. We use\nstandard benchmark datasets (HPatches, MegaDepth-1500) and custom datasets\nconsisting of facade textures and corresponding camera images (terrestrial and\ndrone). For the latter, we evaluate the achievable accuracy of the absolute\npose estimated using a Perspective-n-Point (PnP) algorithm, with geometric\nground truth derived from geo-referenced trajectory data. The results indicate\nthat the learnable feature matching methods vastly outperform traditional\napproaches regarding accuracy and robustness on our challenging custom datasets\nwith zero to 12 RANSAC-inliers and zero to 0.16 area under the curve. We\nbelieve that this work will foster the development of model-based visual\nlocalization methods. Link to the code:\nhttps://github.com/simBauer/To\\_Glue\\_or\\_not\\_to\\_Glue"}
{"id": "2505.17982", "pdf": "https://arxiv.org/pdf/2505.17982", "abs": "https://arxiv.org/abs/2505.17982", "authors": ["Bryan Wong", "Jong Woo Kim", "Huazhu Fu", "Mun Yong Yi"], "title": "Few-Shot Learning from Gigapixel Images via Hierarchical Vision-Language Alignment and Modeling", "categories": ["cs.CV"], "comment": null, "summary": "Vision-language models (VLMs) have recently been integrated into multiple\ninstance learning (MIL) frameworks to address the challenge of few-shot, weakly\nsupervised classification of whole slide images (WSIs). A key trend involves\nleveraging multi-scale information to better represent hierarchical tissue\nstructures. However, existing methods often face two key limitations: (1)\ninsufficient modeling of interactions within the same modalities across scales\n(e.g., 5x and 20x) and (2) inadequate alignment between visual and textual\nmodalities on the same scale. To address these gaps, we propose HiVE-MIL, a\nhierarchical vision-language framework that constructs a unified graph\nconsisting of (1) parent-child links between coarse (5x) and fine (20x)\nvisual/textual nodes to capture hierarchical relationships, and (2)\nheterogeneous intra-scale edges linking visual and textual nodes on the same\nscale. To further enhance semantic consistency, HiVE-MIL incorporates a\ntwo-stage, text-guided dynamic filtering mechanism that removes weakly\ncorrelated patch-text pairs, and introduces a hierarchical contrastive loss to\nalign textual semantics across scales. Extensive experiments on TCGA breast,\nlung, and kidney cancer datasets demonstrate that HiVE-MIL consistently\noutperforms both traditional MIL and recent VLM-based MIL approaches, achieving\ngains of up to 4.1% in macro F1 under 16-shot settings. Our results demonstrate\nthe value of jointly modeling hierarchical structure and multimodal alignment\nfor efficient and scalable learning from limited pathology data. The code is\navailable at https://github.com/bryanwong17/HiVE-MIL"}
{"id": "2505.17992", "pdf": "https://arxiv.org/pdf/2505.17992", "abs": "https://arxiv.org/abs/2505.17992", "authors": ["Fahd Alhamazani", "Yu-Kun Lai", "Paul L. Rosin"], "title": "Canonical Pose Reconstruction from Single Depth Image for 3D Non-rigid Pose Recovery on Limited Datasets", "categories": ["cs.CV"], "comment": null, "summary": "3D reconstruction from 2D inputs, especially for non-rigid objects like\nhumans, presents unique challenges due to the significant range of possible\ndeformations. Traditional methods often struggle with non-rigid shapes, which\nrequire extensive training data to cover the entire deformation space. This\nstudy addresses these limitations by proposing a canonical pose reconstruction\nmodel that transforms single-view depth images of deformable shapes into a\ncanonical form. This alignment facilitates shape reconstruction by enabling the\napplication of rigid object reconstruction techniques, and supports recovering\nthe input pose in voxel representation as part of the reconstruction task,\nutilizing both the original and deformed depth images. Notably, our model\nachieves effective results with only a small dataset of approximately 300\nsamples. Experimental results on animal and human datasets demonstrate that our\nmodel outperforms other state-of-the-art methods."}
{"id": "2505.17994", "pdf": "https://arxiv.org/pdf/2505.17994", "abs": "https://arxiv.org/abs/2505.17994", "authors": ["Zhihua Liu", "Amrutha Saseendran", "Lei Tong", "Xilin He", "Fariba Yousefi", "Nikolay Burlutskiy", "Dino Oglic", "Tom Diethe", "Philip Teare", "Huiyu Zhou", "Chen Jin"], "title": "Segment Anyword: Mask Prompt Inversion for Open-Set Grounded Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Open-set image segmentation poses a significant challenge because existing\nmethods often demand extensive training or fine-tuning and generally struggle\nto segment unified objects consistently across diverse text reference\nexpressions. Motivated by this, we propose Segment Anyword, a novel\ntraining-free visual concept prompt learning approach for open-set language\ngrounded segmentation that relies on token-level cross-attention maps from a\nfrozen diffusion model to produce segmentation surrogates or mask prompts,\nwhich are then refined into targeted object masks. Initial prompts typically\nlack coherence and consistency as the complexity of the image-text increases,\nresulting in suboptimal mask fragments. To tackle this issue, we further\nintroduce a novel linguistic-guided visual prompt regularization that binds and\nclusters visual prompts based on sentence dependency and syntactic structural\ninformation, enabling the extraction of robust, noise-tolerant mask prompts,\nand significant improvements in segmentation accuracy. The proposed approach is\neffective, generalizes across different open-set segmentation tasks, and\nachieves state-of-the-art results of 52.5 (+6.8 relative) mIoU on Pascal\nContext 59, 67.73 (+25.73 relative) cIoU on gRefCOCO, and 67.4 (+1.1 relative\nto fine-tuned methods) mIoU on GranDf, which is the most complex open-set\ngrounded segmentation task in the field."}
{"id": "2505.18010", "pdf": "https://arxiv.org/pdf/2505.18010", "abs": "https://arxiv.org/abs/2505.18010", "authors": ["Jens De Winne", "Siri Willems", "Siri Luthman", "Danilo Babin", "Hiep Luong", "Wim Ceelen"], "title": "Clinical Validation of Deep Learning for Real-Time Tissue Oxygenation Estimation Using Spectral Imaging", "categories": ["cs.CV"], "comment": "Provisionally accepted to the MICCAI 2025 conference", "summary": "Accurate, real-time monitoring of tissue ischemia is crucial to understand\ntissue health and guide surgery. Spectral imaging shows great potential for\ncontactless and intraoperative monitoring of tissue oxygenation. Due to the\ndifficulty of obtaining direct reference oxygenation values, conventional\nmethods are based on linear unmixing techniques. These are prone to assumptions\nand these linear relations may not always hold in practice. In this work, we\npresent deep learning approaches for real-time tissue oxygenation estimation\nusing Monte-Carlo simulated spectra. We train a fully connected neural network\n(FCN) and a convolutional neural network (CNN) for this task and propose a\ndomain-adversarial training approach to bridge the gap between simulated and\nreal clinical spectral data. Results demonstrate that these deep learning\nmodels achieve a higher correlation with capillary lactate measurements, a\nwell-known marker of hypoxia, obtained during spectral imaging in surgery,\ncompared to traditional linear unmixing. Notably, domain-adversarial training\neffectively reduces the domain gap, optimizing performance in real clinical\nsettings."}
{"id": "2505.18015", "pdf": "https://arxiv.org/pdf/2505.18015", "abs": "https://arxiv.org/abs/2505.18015", "authors": ["Shashank Agnihotri", "David Schader", "Jonas Jakubassa", "Nico Sharei", "Simon Kral", "Mehmet Ege Kaçar", "Ruben Weber", "Margret Keuper"], "title": "SemSegBench & DetecBench: Benchmarking Reliability and Generalization Beyond Classification", "categories": ["cs.CV", "cs.LG"], "comment": "First seven listed authors have equal contribution. GitHub:\n  https://github.com/shashankskagnihotri/benchmarking_reliability_generalization.\n  arXiv admin note: text overlap with arXiv:2505.05091", "summary": "Reliability and generalization in deep learning are predominantly studied in\nthe context of image classification. Yet, real-world applications in\nsafety-critical domains involve a broader set of semantic tasks, such as\nsemantic segmentation and object detection, which come with a diverse set of\ndedicated model architectures. To facilitate research towards robust model\ndesign in segmentation and detection, our primary objective is to provide\nbenchmarking tools regarding robustness to distribution shifts and adversarial\nmanipulations. We propose the benchmarking tools SEMSEGBENCH and DETECBENCH,\nalong with the most extensive evaluation to date on the reliability and\ngeneralization of semantic segmentation and object detection models. In\nparticular, we benchmark 76 segmentation models across four datasets and 61\nobject detectors across two datasets, evaluating their performance under\ndiverse adversarial attacks and common corruptions. Our findings reveal\nsystematic weaknesses in state-of-the-art models and uncover key trends based\non architecture, backbone, and model capacity. SEMSEGBENCH and DETECBENCH are\nopen-sourced in our GitHub repository\n(https://github.com/shashankskagnihotri/benchmarking_reliability_generalization)\nalong with our complete set of total 6139 evaluations. We anticipate the\ncollected data to foster and encourage future research towards improved model\nreliability beyond classification."}
{"id": "2505.18021", "pdf": "https://arxiv.org/pdf/2505.18021", "abs": "https://arxiv.org/abs/2505.18021", "authors": ["Yao Sun", "Sining Chen", "Yifan Tian", "Xiao Xiang Zhu"], "title": "Building Floor Number Estimation from Crowdsourced Street-Level Images: Munich Dataset and Baseline Method", "categories": ["cs.CV"], "comment": "Code and data: https://github.com/ya0-sun/Munich-SVI-Floor-Benchmark", "summary": "Accurate information on the number of building floors, or above-ground\nstoreys, is essential for household estimation, utility provision, risk\nassessment, evacuation planning, and energy modeling. Yet large-scale\nfloor-count data are rarely available in cadastral and 3D city databases. This\nstudy proposes an end-to-end deep learning framework that infers floor numbers\ndirectly from unrestricted, crowdsourced street-level imagery, avoiding\nhand-crafted features and generalizing across diverse facade styles. To enable\nbenchmarking, we release the Munich Building Floor Dataset, a public set of\nover 6800 geo-tagged images collected from Mapillary and targeted field\nphotography, each paired with a verified storey label. On this dataset, the\nproposed classification-regression network attains 81.2% exact accuracy and\npredicts 97.9% of buildings within +/-1 floor. The method and dataset together\noffer a scalable route to enrich 3D city models with vertical information and\nlay a foundation for future work in urban informatics, remote sensing, and\ngeographic information science. Source code and data will be released under an\nopen license at https://github.com/ya0-sun/Munich-SVI-Floor-Benchmark."}
{"id": "2505.18022", "pdf": "https://arxiv.org/pdf/2505.18022", "abs": "https://arxiv.org/abs/2505.18022", "authors": ["Liang Yao", "Fan Liu", "Delong Chen", "Chuanyi Zhang", "Yijun Wang", "Ziyun Chen", "Wei Xu", "Shimin Di", "Yuhui Zheng"], "title": "RemoteSAM: Towards Segment Anything for Earth Observation", "categories": ["cs.CV"], "comment": null, "summary": "We aim to develop a robust yet flexible visual foundation model for Earth\nobservation. It should possess strong capabilities in recognizing and\nlocalizing diverse visual targets while providing compatibility with various\ninput-output interfaces required across different task scenarios. Current\nsystems cannot meet these requirements, as they typically utilize task-specific\narchitecture trained on narrow data domains with limited semantic coverage. Our\nstudy addresses these limitations from two aspects: data and modeling. We first\nintroduce an automatic data engine that enjoys significantly better scalability\ncompared to previous human annotation or rule-based approaches. It has enabled\nus to create the largest dataset of its kind to date, comprising 270K\nimage-text-mask triplets covering an unprecedented range of diverse semantic\ncategories and attribute specifications. Based on this data foundation, we\nfurther propose a task unification paradigm that centers around referring\nexpression segmentation. It effectively handles a wide range of vision-centric\nperception tasks, including classification, detection, segmentation, grounding,\netc, using a single model without any task-specific heads. Combining these\ninnovations on data and modeling, we present RemoteSAM, a foundation model that\nestablishes new SoTA on several earth observation perception benchmarks,\noutperforming other foundation models such as Falcon, GeoChat, and LHRS-Bot\nwith significantly higher efficiency. Models and data are publicly available at\nhttps://github.com/1e12Leon/RemoteSAM."}
{"id": "2505.18024", "pdf": "https://arxiv.org/pdf/2505.18024", "abs": "https://arxiv.org/abs/2505.18024", "authors": ["Xiaobao Wei", "Jiawei Liu", "Dongbo Yang", "Junda Cheng", "Changyong Shu", "Wei Wang"], "title": "A Wavelet-based Stereo Matching Framework for Solving Frequency Convergence Inconsistency", "categories": ["cs.CV"], "comment": null, "summary": "We find that the EPE evaluation metrics of RAFT-stereo converge\ninconsistently in the low and high frequency regions, resulting high frequency\ndegradation (e.g., edges and thin objects) during the iterative process. The\nunderlying reason for the limited performance of current iterative methods is\nthat it optimizes all frequency components together without distinguishing\nbetween high and low frequencies. We propose a wavelet-based stereo matching\nframework (Wavelet-Stereo) for solving frequency convergence inconsistency.\nSpecifically, we first explicitly decompose an image into high and low\nfrequency components using discrete wavelet transform. Then, the high-frequency\nand low-frequency components are fed into two different multi-scale frequency\nfeature extractors. Finally, we propose a novel LSTM-based high-frequency\npreservation update operator containing an iterative frequency adapter to\nprovide adaptive refined high-frequency features at different iteration steps\nby fine-tuning the initial high-frequency features. By processing high and low\nfrequency components separately, our framework can simultaneously refine\nhigh-frequency information in edges and low-frequency information in smooth\nregions, which is especially suitable for challenging scenes with fine details\nand textures in the distance. Extensive experiments demonstrate that our\nWavelet-Stereo outperforms the state-of-the-art methods and ranks 1st on both\nthe KITTI 2015 and KITTI 2012 leaderboards for almost all metrics. We will\nprovide code and pre-trained models to encourage further exploration,\napplication, and development of our innovative framework\n(https://github.com/SIA-IDE/Wavelet-Stereo)."}
{"id": "2505.18025", "pdf": "https://arxiv.org/pdf/2505.18025", "abs": "https://arxiv.org/abs/2505.18025", "authors": ["Evangelos Sariyanidi", "Claudio Ferrari", "Federico Nocentini", "Stefano Berretti", "Andrea Cavallaro", "Birkan Tunc"], "title": "3D Face Reconstruction Error Decomposed: A Modular Benchmark for Fair and Fast Method Evaluation", "categories": ["cs.CV"], "comment": "To be published in IEEE International Conference on Automatic Face\n  and Gesture Recognition, 2025", "summary": "Computing the standard benchmark metric for 3D face reconstruction, namely\ngeometric error, requires a number of steps, such as mesh cropping, rigid\nalignment, or point correspondence. Current benchmark tools are monolithic\n(they implement a specific combination of these steps), even though there is no\nconsensus on the best way to measure error. We present a toolkit for a\nModularized 3D Face reconstruction Benchmark (M3DFB), where the fundamental\ncomponents of error computation are segregated and interchangeable, allowing\none to quantify the effect of each. Furthermore, we propose a new component,\nnamely correction, and present a computationally efficient approach that\npenalizes for mesh topology inconsistency. Using this toolkit, we test 16 error\nestimators with 10 reconstruction methods on two real and two synthetic\ndatasets. Critically, the widely used ICP-based estimator provides the worst\nbenchmarking performance, as it significantly alters the true ranking of the\ntop-5 reconstruction methods. Notably, the correlation of ICP with the true\nerror can be as low as 0.41. Moreover, non-rigid alignment leads to significant\nimprovement (correlation larger than 0.90), highlighting the importance of\nannotating 3D landmarks on datasets. Finally, the proposed correction scheme,\ntogether with non-rigid warping, leads to an accuracy on a par with the best\nnon-rigid ICP-based estimators, but runs an order of magnitude faster. Our\nopen-source codebase is designed for researchers to easily compare alternatives\nfor each component, thus helping accelerating progress in benchmarking for 3D\nface reconstruction and, furthermore, supporting the improvement of learned\nreconstruction methods, which depend on accurate error estimation for effective\ntraining."}
{"id": "2505.18035", "pdf": "https://arxiv.org/pdf/2505.18035", "abs": "https://arxiv.org/abs/2505.18035", "authors": ["Naseem Khan", "Tuan Nguyen", "Amine Bermak", "Issa Khalil"], "title": "CAMME: Adaptive Deepfake Image Detection with Multi-Modal Cross-Attention", "categories": ["cs.CV", "F.2.2; I.2.7"], "comment": "20 pages, 8 figures, 12 Tables", "summary": "The proliferation of sophisticated AI-generated deepfakes poses critical\nchallenges for digital media authentication and societal security. While\nexisting detection methods perform well within specific generative domains,\nthey exhibit significant performance degradation when applied to manipulations\nproduced by unseen architectures--a fundamental limitation as generative\ntechnologies rapidly evolve. We propose CAMME (Cross-Attention Multi-Modal\nEmbeddings), a framework that dynamically integrates visual, textual, and\nfrequency-domain features through a multi-head cross-attention mechanism to\nestablish robust cross-domain generalization. Extensive experiments demonstrate\nCAMME's superiority over state-of-the-art methods, yielding improvements of\n12.56% on natural scenes and 13.25% on facial deepfakes. The framework\ndemonstrates exceptional resilience, maintaining (over 91%) accuracy under\nnatural image perturbations and achieving 89.01% and 96.14% accuracy against\nPGD and FGSM adversarial attacks, respectively. Our findings validate that\nintegrating complementary modalities through cross-attention enables more\neffective decision boundary realignment for reliable deepfake detection across\nheterogeneous generative architectures."}
{"id": "2505.18039", "pdf": "https://arxiv.org/pdf/2505.18039", "abs": "https://arxiv.org/abs/2505.18039", "authors": ["Li Zhong", "Ahmed Ghazal", "Jun-Jun Wan", "Frederik Zilly", "Patrick Mackens", "Joachim E. Vollrath", "Bogdan Sorin Coseriu"], "title": "Clip4Retrofit: Enabling Real-Time Image Labeling on Edge Devices via Cross-Architecture CLIP Distillation", "categories": ["cs.CV"], "comment": null, "summary": "Foundation models like CLIP (Contrastive Language-Image Pretraining) have\nrevolutionized vision-language tasks by enabling zero-shot and few-shot\nlearning through cross-modal alignment. However, their computational complexity\nand large memory footprint make them unsuitable for deployment on\nresource-constrained edge devices, such as in-car cameras used for image\ncollection and real-time processing. To address this challenge, we propose\nClip4Retrofit, an efficient model distillation framework that enables real-time\nimage labeling on edge devices. The framework is deployed on the Retrofit\ncamera, a cost-effective edge device retrofitted into thousands of vehicles,\ndespite strict limitations on compute performance and memory. Our approach\ndistills the knowledge of the CLIP model into a lightweight student model,\ncombining EfficientNet-B3 with multi-layer perceptron (MLP) projection heads to\npreserve cross-modal alignment while significantly reducing computational\nrequirements. We demonstrate that our distilled model achieves a balance\nbetween efficiency and performance, making it ideal for deployment in\nreal-world scenarios. Experimental results show that Clip4Retrofit can perform\nreal-time image labeling and object identification on edge devices with limited\nresources, offering a practical solution for applications such as autonomous\ndriving and retrofitting existing systems. This work bridges the gap between\nstate-of-the-art vision-language models and their deployment in\nresource-constrained environments, paving the way for broader adoption of\nfoundation models in edge computing."}
{"id": "2505.18047", "pdf": "https://arxiv.org/pdf/2505.18047", "abs": "https://arxiv.org/abs/2505.18047", "authors": ["Sudarshan Rajagopalan", "Kartik Narayan", "Vishal M. Patel"], "title": "RestoreVAR: Visual Autoregressive Generation for All-in-One Image Restoration", "categories": ["cs.CV", "cs.AI"], "comment": "Project page: https://sudraj2002.github.io/restorevarpage/", "summary": "The use of latent diffusion models (LDMs) such as Stable Diffusion has\nsignificantly improved the perceptual quality of All-in-One image Restoration\n(AiOR) methods, while also enhancing their generalization capabilities.\nHowever, these LDM-based frameworks suffer from slow inference due to their\niterative denoising process, rendering them impractical for time-sensitive\napplications. To address this, we propose RestoreVAR, a novel generative\napproach for AiOR that significantly outperforms LDM-based models in\nrestoration performance while achieving over $\\mathbf{10\\times}$ faster\ninference. RestoreVAR leverages visual autoregressive modeling (VAR), a\nrecently introduced approach which performs scale-space autoregression for\nimage generation. VAR achieves comparable performance to that of\nstate-of-the-art diffusion transformers with drastically reduced computational\ncosts. To optimally exploit these advantages of VAR for AiOR, we propose\narchitectural modifications and improvements, including intricately designed\ncross-attention mechanisms and a latent-space refinement module, tailored for\nthe AiOR task. Extensive experiments show that RestoreVAR achieves\nstate-of-the-art performance among generative AiOR methods, while also\nexhibiting strong generalization capabilities."}
{"id": "2505.18048", "pdf": "https://arxiv.org/pdf/2505.18048", "abs": "https://arxiv.org/abs/2505.18048", "authors": ["Simon Malzard", "Nitish Mital", "Richard Walters", "Victoria Nockles", "Raghuveer Rao", "Celso M. De Melo"], "title": "SHARDeg: A Benchmark for Skeletal Human Action Recognition in Degraded Scenarios", "categories": ["cs.CV"], "comment": "19 pages, 2 images", "summary": "Computer vision (CV) models for detection, prediction or classification tasks\noperate on video data-streams that are often degraded in the real world, due to\ndeployment in real-time or on resource-constrained hardware. It is therefore\ncritical that these models are robust to degraded data, but state of the art\n(SoTA) models are often insufficiently assessed with these real-world\nconstraints in mind. This is exemplified by Skeletal Human Action Recognition\n(SHAR), which is critical in many CV pipelines operating in real-time and at\nthe edge, but robustness to degraded data has previously only been shallowly\nand inconsistently assessed. Here we address this issue for SHAR by providing\nan important first data degradation benchmark on the most detailed and largest\n3D open dataset, NTU-RGB+D-120, and assess the robustness of five leading SHAR\nmodels to three forms of degradation that represent real-world issues. We\ndemonstrate the need for this benchmark by showing that the form of\ndegradation, which has not previously been considered, has a large impact on\nmodel accuracy; at the same effective frame rate, model accuracy can vary by\n>40% depending on degradation type. We also identify that temporal regularity\nof frames in degraded SHAR data is likely a major driver of differences in\nmodel performance, and harness this to improve performance of existing models\nby up to >40%, through employing a simple mitigation approach based on\ninterpolation. Finally, we highlight how our benchmark has helped identify an\nimportant degradation-resistant SHAR model based in Rough Path Theory; the\nLogSigRNN SHAR model outperforms the SoTA DeGCN model in five out of six cases\nat low frame rates by an average accuracy of 6%, despite trailing the SoTA\nmodel by 11-12% on un-degraded data at high frame rates (30 FPS)."}
{"id": "2505.18049", "pdf": "https://arxiv.org/pdf/2505.18049", "abs": "https://arxiv.org/abs/2505.18049", "authors": ["Gaole Dai", "Menghang Dong", "Rongyu Zhang", "Ruichuan An", "Shanghang Zhang", "Tiejun Huang"], "title": "SpikeGen: Generative Framework for Visual Spike Stream Processing", "categories": ["cs.CV"], "comment": null, "summary": "Neuromorphic Visual Systems, such as spike cameras, have attracted\nconsiderable attention due to their ability to capture clear textures under\ndynamic conditions. This capability effectively mitigates issues related to\nmotion and aperture blur. However, in contrast to conventional RGB modalities\nthat provide dense spatial information, these systems generate binary,\nspatially sparse frames as a trade-off for temporally rich visual streams. In\nthis context, generative models emerge as a promising solution to address the\ninherent limitations of sparse data. These models not only facilitate the\nconditional fusion of existing information from both spike and RGB modalities\nbut also enable the conditional generation based on latent priors. In this\nstudy, we introduce a robust generative processing framework named SpikeGen,\ndesigned for visual spike streams captured by spike cameras. We evaluate this\nframework across multiple tasks involving mixed spike-RGB modalities, including\nconditional image/video deblurring, dense frame reconstruction from spike\nstreams, and high-speed scene novel-view synthesis. Supported by comprehensive\nexperimental results, we demonstrate that leveraging the latent space operation\nabilities of generative models allows us to effectively address the sparsity of\nspatial information while fully exploiting the temporal richness of spike\nstreams, thereby promoting a synergistic enhancement of different visual\nmodalities."}
{"id": "2505.18051", "pdf": "https://arxiv.org/pdf/2505.18051", "abs": "https://arxiv.org/abs/2505.18051", "authors": ["Anthony Fuller", "Yousef Yassin", "Junfeng Wen", "Daniel G. Kyrollos", "Tarek Ibrahim", "James R. Green", "Evan Shelhamer"], "title": "LookWhere? Efficient Visual Recognition by Learning Where to Look and What to See from Self-Supervision", "categories": ["cs.CV"], "comment": null, "summary": "Vision transformers are ever larger, more accurate, and more expensive to\ncompute. The expense is even more extreme at high resolution as the number of\ntokens grows quadratically with the image size. We turn to adaptive computation\nto cope with this cost by learning to predict where to compute. Our LookWhere\nmethod divides the computation between a low-resolution selector and a\nhigh-resolution extractor without ever processing the full high-resolution\ninput. We jointly pretrain the selector and extractor without task supervision\nby distillation from a self-supervised teacher, in effect, learning where and\nwhat to compute simultaneously. Unlike prior token reduction methods, which pay\nto save by pruning already-computed tokens, and prior token selection methods,\nwhich require complex and expensive per-task optimization, LookWhere\neconomically and accurately selects and extracts transferrable representations\nof images. We show that LookWhere excels at sparse recognition on\nhigh-resolution inputs (Traffic Signs), maintaining accuracy while reducing\nFLOPs by up to 34x and time by 6x. It also excels at standard recognition tasks\nthat are global (ImageNet classification) or local (ADE20K segmentation),\nimproving accuracy while reducing time by 1.36x."}
{"id": "2505.18052", "pdf": "https://arxiv.org/pdf/2505.18052", "abs": "https://arxiv.org/abs/2505.18052", "authors": ["Zhihua Liu", "Lei Tong", "Xilin He", "Che Liu", "Rossella Arcucci", "Chen Jin", "Huiyu Zhou"], "title": "BOTM: Echocardiography Segmentation via Bi-directional Optimal Token Matching", "categories": ["cs.CV"], "comment": null, "summary": "Existed echocardiography segmentation methods often suffer from anatomical\ninconsistency challenge caused by shape variation, partial observation and\nregion ambiguity with similar intensity across 2D echocardiographic sequences,\nresulting in false positive segmentation with anatomical defeated structures in\nchallenging low signal-to-noise ratio conditions. To provide a strong\nanatomical guarantee across different echocardiographic frames, we propose a\nnovel segmentation framework named BOTM (Bi-directional Optimal Token Matching)\nthat performs echocardiography segmentation and optimal anatomy transportation\nsimultaneously. Given paired echocardiographic images, BOTM learns to match two\nsets of discrete image tokens by finding optimal correspondences from a novel\nanatomical transportation perspective. We further extend the token matching\ninto a bi-directional cross-transport attention proxy to regulate the preserved\nanatomical consistency within the cardiac cyclic deformation in temporal\ndomain. Extensive experimental results show that BOTM can generate stable and\naccurate segmentation outcomes (e.g. -1.917 HD on CAMUS2H LV, +1.9% Dice on\nTED), and provide a better matching interpretation with anatomical consistency\nguarantee."}
{"id": "2505.18053", "pdf": "https://arxiv.org/pdf/2505.18053", "abs": "https://arxiv.org/abs/2505.18053", "authors": ["Zherui Zhang", "Jiaxin Wu", "Changwei Wang", "Rongtao Xu", "Longzhao Huang", "Wenhao Xu", "Wenbo Xu", "Li Guo", "Shibiao Xu"], "title": "FDBPL: Faster Distillation-Based Prompt Learning for Region-Aware Vision-Language Models Adaptation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Prompt learning as a parameter-efficient method that has been widely adopted\nto adapt Vision-Language Models (VLMs) to downstream tasks. While hard-prompt\ndesign requires domain expertise and iterative optimization, soft-prompt\nmethods rely heavily on task-specific hard labels, limiting their\ngeneralization to unseen categories. Recent popular distillation-based prompt\nlearning methods improve generalization by exploiting larger teacher VLMs and\nunsupervised knowledge transfer, yet their repetitive teacher model online\ninference sacrifices the inherent training efficiency advantage of prompt\nlearning. In this paper, we propose {{\\large {\\textbf{F}}}}aster {{\\large\n{\\textbf{D}}}}istillation-{{\\large {\\textbf{B}}}}ased {{\\large\n{\\textbf{P}}}}rompt {{\\large {\\textbf{L}}}}earning (\\textbf{FDBPL}), which\naddresses these issues by sharing soft supervision contexts across multiple\ntraining stages and implementing accelerated I/O. Furthermore, FDBPL introduces\na region-aware prompt learning paradigm with dual positive-negative prompt\nspaces to fully exploit randomly cropped regions that containing multi-level\ninformation. We propose a positive-negative space mutual learning mechanism\nbased on similarity-difference learning, enabling student CLIP models to\nrecognize correct semantics while learning to reject weakly related concepts,\nthereby improving zero-shot performance. Unlike existing distillation-based\nprompt learning methods that sacrifice parameter efficiency for generalization,\nFDBPL maintains dual advantages of parameter efficiency and strong downstream\ngeneralization. Comprehensive evaluations across 11 datasets demonstrate\nsuperior performance in base-to-new generalization, cross-dataset transfer, and\nrobustness tests, achieving $2.2\\times$ faster training speed."}
{"id": "2505.18060", "pdf": "https://arxiv.org/pdf/2505.18060", "abs": "https://arxiv.org/abs/2505.18060", "authors": ["Kaiyan Zhang", "Xinghui Li", "Jingyi Lu", "Kai Han"], "title": "Semantic Correspondence: Unified Benchmarking and a Strong Baseline", "categories": ["cs.CV"], "comment": null, "summary": "Establishing semantic correspondence is a challenging task in computer\nvision, aiming to match keypoints with the same semantic information across\ndifferent images. Benefiting from the rapid development of deep learning,\nremarkable progress has been made over the past decade. However, a\ncomprehensive review and analysis of this task remains absent. In this paper,\nwe present the first extensive survey of semantic correspondence methods. We\nfirst propose a taxonomy to classify existing methods based on the type of\ntheir method designs. These methods are then categorized accordingly, and we\nprovide a detailed analysis of each approach. Furthermore, we aggregate and\nsummarize the results of methods in literature across various benchmarks into a\nunified comparative table, with detailed configurations to highlight\nperformance variations. Additionally, to provide a detailed understanding on\nexisting methods for semantic matching, we thoroughly conduct controlled\nexperiments to analyse the effectiveness of the components of different\nmethods. Finally, we propose a simple yet effective baseline that achieves\nstate-of-the-art performance on multiple benchmarks, providing a solid\nfoundation for future research in this field. We hope this survey serves as a\ncomprehensive reference and consolidated baseline for future development. Code\nis publicly available at: https://github.com/Visual-AI/Semantic-Correspondence."}
{"id": "2505.18078", "pdf": "https://arxiv.org/pdf/2505.18078", "abs": "https://arxiv.org/abs/2505.18078", "authors": ["Junhao Chen", "Mingjin Chen", "Jianjin Xu", "Xiang Li", "Junting Dong", "Mingze Sun", "Puhua Jiang", "Hongxiang Li", "Yuhang Yang", "Hao Zhao", "Xiaoxiao Long", "Ruqi Huang"], "title": "DanceTogether! Identity-Preserving Multi-Person Interactive Video Generation", "categories": ["cs.CV"], "comment": "Our video demos and code are available at https://DanceTog.github.io/", "summary": "Controllable video generation (CVG) has advanced rapidly, yet current systems\nfalter when more than one actor must move, interact, and exchange positions\nunder noisy control signals. We address this gap with DanceTogether, the first\nend-to-end diffusion framework that turns a single reference image plus\nindependent pose-mask streams into long, photorealistic videos while strictly\npreserving every identity. A novel MaskPoseAdapter binds \"who\" and \"how\" at\nevery denoising step by fusing robust tracking masks with semantically rich-but\nnoisy-pose heat-maps, eliminating the identity drift and appearance bleeding\nthat plague frame-wise pipelines. To train and evaluate at scale, we introduce\n(i) PairFS-4K, 26 hours of dual-skater footage with 7,000+ distinct IDs, (ii)\nHumanRob-300, a one-hour humanoid-robot interaction set for rapid cross-domain\ntransfer, and (iii) TogetherVideoBench, a three-track benchmark centered on the\nDanceTogEval-100 test suite covering dance, boxing, wrestling, yoga, and figure\nskating. On TogetherVideoBench, DanceTogether outperforms the prior arts by a\nsignificant margin. Moreover, we show that a one-hour fine-tune yields\nconvincing human-robot videos, underscoring broad generalization to embodied-AI\nand HRI tasks. Extensive ablations confirm that persistent identity-action\nbinding is critical to these gains. Together, our model, datasets, and\nbenchmark lift CVG from single-subject choreography to compositionally\ncontrollable, multi-actor interaction, opening new avenues for digital\nproduction, simulation, and embodied intelligence. Our video demos and code are\navailable at https://DanceTog.github.io/."}
{"id": "2505.18079", "pdf": "https://arxiv.org/pdf/2505.18079", "abs": "https://arxiv.org/abs/2505.18079", "authors": ["Xiaoyi Zhang", "Zhaoyang Jia", "Zongyu Guo", "Jiahao Li", "Bin Li", "Houqiang Li", "Yan Lu"], "title": "Deep Video Discovery: Agentic Search with Tool Use for Long-form Video Understanding", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Under review", "summary": "Long-form video understanding presents significant challenges due to\nextensive temporal-spatial complexity and the difficulty of question answering\nunder such extended contexts. While Large Language Models (LLMs) have\ndemonstrated considerable advancements in video analysis capabilities and long\ncontext handling, they continue to exhibit limitations when processing\ninformation-dense hour-long videos. To overcome such limitations, we propose\nthe Deep Video Discovery agent to leverage an agentic search strategy over\nsegmented video clips. Different from previous video agents manually designing\na rigid workflow, our approach emphasizes the autonomous nature of agents. By\nproviding a set of search-centric tools on multi-granular video database, our\nDVD agent leverages the advanced reasoning capability of LLM to plan on its\ncurrent observation state, strategically selects tools, formulates appropriate\nparameters for actions, and iteratively refines its internal reasoning in light\nof the gathered information. We perform comprehensive evaluation on multiple\nlong video understanding benchmarks that demonstrates the advantage of the\nentire system design. Our DVD agent achieves SOTA performance, significantly\nsurpassing prior works by a large margin on the challenging LVBench dataset.\nComprehensive ablation studies and in-depth tool analyses are also provided,\nyielding insights to further advance intelligent agents tailored for long-form\nvideo understanding tasks. The code will be released later."}
{"id": "2505.18087", "pdf": "https://arxiv.org/pdf/2505.18087", "abs": "https://arxiv.org/abs/2505.18087", "authors": ["Hyungyung Lee", "Geon Choi", "Jung-Oh Lee", "Hangyul Yoon", "Hyuk Gi Hong", "Edward Choi"], "title": "CXReasonBench: A Benchmark for Evaluating Structured Diagnostic Reasoning in Chest X-rays", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent progress in Large Vision-Language Models (LVLMs) has enabled promising\napplications in medical tasks, such as report generation and visual question\nanswering. However, existing benchmarks focus mainly on the final diagnostic\nanswer, offering limited insight into whether models engage in clinically\nmeaningful reasoning. To address this, we present CheXStruct and CXReasonBench,\na structured pipeline and benchmark built on the publicly available\nMIMIC-CXR-JPG dataset. CheXStruct automatically derives a sequence of\nintermediate reasoning steps directly from chest X-rays, such as segmenting\nanatomical regions, deriving anatomical landmarks and diagnostic measurements,\ncomputing diagnostic indices, and applying clinical thresholds. CXReasonBench\nleverages this pipeline to evaluate whether models can perform clinically valid\nreasoning steps and to what extent they can learn from structured guidance,\nenabling fine-grained and transparent assessment of diagnostic reasoning. The\nbenchmark comprises 18,988 QA pairs across 12 diagnostic tasks and 1,200 cases,\neach paired with up to 4 visual inputs, and supports multi-path, multi-stage\nevaluation including visual grounding via anatomical region selection and\ndiagnostic measurements. Even the strongest of 10 evaluated LVLMs struggle with\nstructured reasoning and generalization, often failing to link abstract\nknowledge with anatomically grounded visual interpretation. The code is\navailable at https://github.com/ttumyche/CXReasonBench"}
{"id": "2505.18096", "pdf": "https://arxiv.org/pdf/2505.18096", "abs": "https://arxiv.org/abs/2505.18096", "authors": ["Ziqiao Peng", "Yanbo Fan", "Haoyu Wu", "Xuan Wang", "Hongyan Liu", "Jun He", "Zhaoxin Fan"], "title": "DualTalk: Dual-Speaker Interaction for 3D Talking Head Conversations", "categories": ["cs.CV", "cs.SD", "eess.AS"], "comment": "Accepted by CVPR 2025", "summary": "In face-to-face conversations, individuals need to switch between speaking\nand listening roles seamlessly. Existing 3D talking head generation models\nfocus solely on speaking or listening, neglecting the natural dynamics of\ninteractive conversation, which leads to unnatural interactions and awkward\ntransitions. To address this issue, we propose a new task -- multi-round\ndual-speaker interaction for 3D talking head generation -- which requires\nmodels to handle and generate both speaking and listening behaviors in\ncontinuous conversation. To solve this task, we introduce DualTalk, a novel\nunified framework that integrates the dynamic behaviors of speakers and\nlisteners to simulate realistic and coherent dialogue interactions. This\nframework not only synthesizes lifelike talking heads when speaking but also\ngenerates continuous and vivid non-verbal feedback when listening, effectively\ncapturing the interplay between the roles. We also create a new dataset\nfeaturing 50 hours of multi-round conversations with over 1,000 characters,\nwhere participants continuously switch between speaking and listening roles.\nExtensive experiments demonstrate that our method significantly enhances the\nnaturalness and expressiveness of 3D talking heads in dual-speaker\nconversations. We recommend watching the supplementary video:\nhttps://ziqiaopeng.github.io/dualtalk."}
{"id": "2505.18106", "pdf": "https://arxiv.org/pdf/2505.18106", "abs": "https://arxiv.org/abs/2505.18106", "authors": ["Varun Ajith", "Anindya Pal", "Saumik Bhattacharya", "Sayantari Ghosh"], "title": "F-ANcGAN: An Attention-Enhanced Cycle Consistent Generative Adversarial Architecture for Synthetic Image Generation of Nanoparticles", "categories": ["cs.CV", "cond-mat.mtrl-sci", "cs.LG", "eess.IV"], "comment": "11 pages, 9 figures, 2 tables, conference paper", "summary": "Nanomaterial research is becoming a vital area for energy, medicine, and\nmaterials science, and accurate analysis of the nanoparticle topology is\nessential to determine their properties. Unfortunately, the lack of\nhigh-quality annotated datasets drastically hinders the creation of strong\nsegmentation models for nanoscale imaging. To alleviate this problem, we\nintroduce F-ANcGAN, an attention-enhanced cycle consistent generative\nadversarial system that can be trained using a limited number of data samples\nand generates realistic scanning electron microscopy (SEM) images directly from\nsegmentation maps. Our model uses a Style U-Net generator and a U-Net\nsegmentation network equipped with self-attention to capture structural\nrelationships and applies augmentation methods to increase the variety of the\ndataset. The architecture reached a raw FID score of 17.65 for TiO$_2$ dataset\ngeneration, with a further reduction in FID score to nearly 10.39 by using\nefficient post-processing techniques. By facilitating scalable high-fidelity\nsynthetic dataset generation, our approach can improve the effectiveness of\ndownstream segmentation task training, overcoming severe data shortage issues\nin nanoparticle analysis, thus extending its applications to resource-limited\nfields."}
{"id": "2505.18111", "pdf": "https://arxiv.org/pdf/2505.18111", "abs": "https://arxiv.org/abs/2505.18111", "authors": ["Cheng-Yen Yang", "Hsiang-Wei Huang", "Pyong-Kun Kim", "Chien-Kai Kuo", "Jui-Wei Chang", "Kwang-Ju Kim", "Chung-I Huang", "Jenq-Neng Hwang"], "title": "Adapting SAM 2 for Visual Object Tracking: 1st Place Solution for MMVPR Challenge Multi-Modal Tracking", "categories": ["cs.CV"], "comment": "Accepted by ICPR Multi-Modal Visual Pattern Recognition Workshop", "summary": "We present an effective approach for adapting the Segment Anything Model 2\n(SAM2) to the Visual Object Tracking (VOT) task. Our method leverages the\npowerful pre-trained capabilities of SAM2 and incorporates several key\ntechniques to enhance its performance in VOT applications. By combining SAM2\nwith our proposed optimizations, we achieved a first place AUC score of 89.4 on\nthe 2024 ICPR Multi-modal Object Tracking challenge, demonstrating the\neffectiveness of our approach. This paper details our methodology, the specific\nenhancements made to SAM2, and a comprehensive analysis of our results in the\ncontext of VOT solutions along with the multi-modality aspect of the dataset."}
{"id": "2505.18115", "pdf": "https://arxiv.org/pdf/2505.18115", "abs": "https://arxiv.org/abs/2505.18115", "authors": ["Jacob Hansen", "Wei Lin", "Junmo Kang", "Muhammad Jehanzeb Mirza", "Hongyin Luo", "Rogerio Feris", "Alan Ritter", "James Glass", "Leonid Karlinsky"], "title": "Instructify: Demystifying Metadata to Visual Instruction Tuning Data Conversion", "categories": ["cs.CV"], "comment": null, "summary": "Visual Instruction Tuning (VisIT) data, commonly available as human-assistant\nconversations with images interleaved in the human turns, are currently the\nmost widespread vehicle for aligning strong LLMs to understand visual inputs,\nconverting them to strong LMMs. While many VisIT datasets are available, most\nare constructed using ad-hoc techniques developed independently by different\ngroups. They are often poorly documented, lack reproducible code, and rely on\npaid, closed-source model APIs such as GPT-4, Gemini, or Claude to convert\nimage metadata (labels) into VisIT instructions. This leads to high costs and\nmakes it challenging to scale, enhance quality, or generate VisIT data for new\ndatasets. In this work, we address these challenges and propose an open and\nunified recipe and approach,~\\textbf{\\method}, for converting available\nmetadata to VisIT instructions using open LLMs. Our multi-stage \\method\nfeatures an efficient framework for metadata grouping, quality control, data\nand prompt organization, and conversation sampling. We show that our approach\ncan reproduce or enhance the data quality of available VisIT datasets when\napplied to the same image data and metadata sources, improving GPT-4 generated\nVisIT instructions by ~3\\% on average and up to 12\\% on individual benchmarks\nusing open models, such as Gemma 2 27B and LLaMa 3.1 70B. Additionally, our\napproach enables effective performance scaling - both in quantity and quality -\nby enhancing the resulting LMM performance across a wide range of benchmarks.\nWe also analyze the impact of various factors, including conversation format,\nbase model selection, and resampling strategies. Our code, which supports the\nreproduction of equal or higher-quality VisIT datasets and facilities future\nmetadata-to-VisIT data conversion for niche domains, is released at\nhttps://github.com/jacob-hansen/Instructify."}
{"id": "2505.18129", "pdf": "https://arxiv.org/pdf/2505.18129", "abs": "https://arxiv.org/abs/2505.18129", "authors": ["Yan Ma", "Linge Du", "Xuyang Shen", "Shaoxiang Chen", "Pengfei Li", "Qibing Ren", "Lizhuang Ma", "Yuchao Dai", "Pengfei Liu", "Junjie Yan"], "title": "One RL to See Them All: Visual Triple Unified Reinforcement Learning", "categories": ["cs.CV", "cs.CL"], "comment": "Technical Report", "summary": "Reinforcement learning (RL) has significantly advanced the reasoning\ncapabilities of vision-language models (VLMs). However, the use of RL beyond\nreasoning tasks remains largely unexplored, especially for perceptionintensive\ntasks like object detection and grounding. We propose V-Triune, a Visual Triple\nUnified Reinforcement Learning system that enables VLMs to jointly learn visual\nreasoning and perception tasks within a single training pipeline. V-Triune\ncomprises triple complementary components: Sample-Level Data Formatting (to\nunify diverse task inputs), Verifier-Level Reward Computation (to deliver\ncustom rewards via specialized verifiers) , and Source-Level Metric Monitoring\n(to diagnose problems at the data-source level). We further introduce a novel\nDynamic IoU reward, which provides adaptive, progressive, and definite feedback\nfor perception tasks handled by V-Triune. Our approach is instantiated within\noff-the-shelf RL training framework using open-source 7B and 32B backbone\nmodels. The resulting model, dubbed Orsta (One RL to See Them All),\ndemonstrates consistent improvements across both reasoning and perception\ntasks. This broad capability is significantly shaped by its training on a\ndiverse dataset, constructed around four representative visual reasoning tasks\n(Math, Puzzle, Chart, and Science) and four visual perception tasks (Grounding,\nDetection, Counting, and OCR). Subsequently, Orsta achieves substantial gains\non MEGA-Bench Core, with improvements ranging from +2.1 to an impressive +14.1\nacross its various 7B and 32B model variants, with performance benefits\nextending to a wide range of downstream tasks. These results highlight the\neffectiveness and scalability of our unified RL approach for VLMs. The V-Triune\nsystem, along with the Orsta models, is publicly available at\nhttps://github.com/MiniMax-AI."}
{"id": "2505.18132", "pdf": "https://arxiv.org/pdf/2505.18132", "abs": "https://arxiv.org/abs/2505.18132", "authors": ["Dingqing Ye", "Chao Fan", "Zhanbo Huang", "Chengwen Luo", "Jianqiang Li", "Shiqi Yu", "Xiaoming Liu"], "title": "BiggerGait: Unlocking Gait Recognition with Layer-wise Representations from Large Vision Models", "categories": ["cs.CV"], "comment": null, "summary": "Large vision models (LVM) based gait recognition has achieved impressive\nperformance. However, existing LVM-based approaches may overemphasize gait\npriors while neglecting the intrinsic value of LVM itself, particularly the\nrich, distinct representations across its multi-layers. To adequately unlock\nLVM's potential, this work investigates the impact of layer-wise\nrepresentations on downstream recognition tasks. Our analysis reveals that\nLVM's intermediate layers offer complementary properties across tasks,\nintegrating them yields an impressive improvement even without rich\nwell-designed gait priors. Building on this insight, we propose a simple and\nuniversal baseline for LVM-based gait recognition, termed BiggerGait.\nComprehensive evaluations on CCPG, CAISA-B*, SUSTech1K, and CCGR\\_MINI validate\nthe superiority of BiggerGait across both within- and cross-domain tasks,\nestablishing it as a simple yet practical baseline for gait representation\nlearning. All the models and code will be publicly available."}
{"id": "2505.18137", "pdf": "https://arxiv.org/pdf/2505.18137", "abs": "https://arxiv.org/abs/2505.18137", "authors": ["Amit Kumar Kundu", "Vaishnavi Patil", "Joseph Jaja"], "title": "Boosting Open Set Recognition Performance through Modulated Representation Learning", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "The open set recognition (OSR) problem aims to identify test samples from\nnovel semantic classes that are not part of the training classes, a task that\nis crucial in many practical scenarios. However, existing OSR methods use a\nconstant scaling factor (the temperature) to the logits before applying a loss\nfunction, which hinders the model from exploring both ends of the spectrum in\nrepresentation learning -- from instance-level to semantic-level features. In\nthis paper, we address this problem by enabling temperature-modulated\nrepresentation learning using our novel negative cosine scheduling scheme. Our\nscheduling lets the model form a coarse decision boundary at the beginning of\ntraining by focusing on fewer neighbors, and gradually prioritizes more\nneighbors to smooth out rough edges. This gradual task switching leads to a\nricher and more generalizable representation space. While other OSR methods\nbenefit by including regularization or auxiliary negative samples, such as with\nmix-up, thereby adding a significant computational overhead, our scheme can be\nfolded into any existing OSR method with no overhead. We implement the proposed\nscheme on top of a number of baselines, using both cross-entropy and\ncontrastive loss functions as well as a few other OSR methods, and find that\nour scheme boosts both the OSR performance and the closed set performance in\nmost cases, especially on the tougher semantic shift benchmarks."}
{"id": "2505.18142", "pdf": "https://arxiv.org/pdf/2505.18142", "abs": "https://arxiv.org/abs/2505.18142", "authors": ["Junfeng Wu", "Dongliang Luo", "Weizhi Zhao", "Zhihao Xie", "Yuanhao Wang", "Junyi Li", "Xudong Xie", "Yuliang Liu", "Xiang Bai"], "title": "TokBench: Evaluating Your Visual Tokenizer before Visual Generation", "categories": ["cs.CV", "cs.DB"], "comment": "Benchmark, homepagee: https://wjf5203.github.io/TokBench", "summary": "In this work, we reveal the limitations of visual tokenizers and VAEs in\npreserving fine-grained features, and propose a benchmark to evaluate\nreconstruction performance for two challenging visual contents: text and face.\nImage tokenization has significantly advanced visual generation and multimodal\nmodeling, particularly with autoregressive models due to the modeling\nsimplicity of discrete tokens. Autoregressive models typically rely on image\ntokenizers to compress images into discrete tokens for sequential prediction,\nwhereas diffusion models often operate on continuous latent space to reduce\ncomputational costs. However, both visual compression approaches inevitably\nlose visual information, thereby limiting the upper bound of visual generation\nquality. To evaluate how these compression losses affect text and faces, the\nmost human-sensitive visual elements, we first collect and curate a collection\nof text and faces images from existing datasets, ensuring clarity and\ndiversity. For text reconstruction, we employ OCR models to assess the\nrecognition accuracy of the reconstructed text, and then we measure feature\nsimilarity between original and reconstructed faces thereby quantifying faces\nreconstruction fidelity. Our method is highly lightweight, requiring just 2GB\nmemory and 4 minutes to complete evaluations. With our benchmark, we analyze\nthe reconstruction quality of text and faces at various scales across different\nimage tokenizers and VAEs. Our results demonstrate that modern visual\ntokenizers still struggle to preserve fine-grained features, particularly at\nsmaller scales. Furthermore, we extend this evaluation framework to the video,\nconducting a comprehensive analysis of video tokenizers. Additionally, we find\nthat traditional metrics fail to accurately reflect the reconstruction\nperformance for faces and text, while our proposed metrics serve as an\neffective complement."}
{"id": "2505.18153", "pdf": "https://arxiv.org/pdf/2505.18153", "abs": "https://arxiv.org/abs/2505.18153", "authors": ["Savya Khosla", "Sethuraman TV", "Barnett Lee", "Alexander Schwing", "Derek Hoiem"], "title": "REN: Fast and Efficient Region Encodings from Patch-Based Image Encoders", "categories": ["cs.CV"], "comment": null, "summary": "We introduce the Region Encoder Network (REN), a fast and effective model for\ngenerating region-based image representations using point prompts. Recent\nmethods combine class-agnostic segmenters (e.g., SAM) with patch-based image\nencoders (e.g., DINO) to produce compact and effective region representations,\nbut they suffer from high computational cost due to the segmentation step. REN\nbypasses this bottleneck using a lightweight module that directly generates\nregion tokens, enabling 60x faster token generation with 35x less memory, while\nalso improving token quality. It uses a few cross-attention blocks that take\npoint prompts as queries and features from a patch-based image encoder as keys\nand values to produce region tokens that correspond to the prompted objects. We\ntrain REN with three popular encoders-DINO, DINOv2, and OpenCLIP-and show that\nit can be extended to other encoders without dedicated training. We evaluate\nREN on semantic segmentation and retrieval tasks, where it consistently\noutperforms the original encoders in both performance and compactness, and\nmatches or exceeds SAM-based region methods while being significantly faster.\nNotably, REN achieves state-of-the-art results on the challenging Ego4D VQ2D\nbenchmark and outperforms proprietary LMMs on Visual Haystacks' single-needle\nchallenge. Code and models are available at: https://github.com/savya08/REN."}
{"id": "2505.17042", "pdf": "https://arxiv.org/pdf/2505.17042", "abs": "https://arxiv.org/abs/2505.17042", "authors": ["Abdullah Abdullah", "Seong Tae Kim"], "title": "VLM-KG: Multimodal Radiology Knowledge Graph Generation", "categories": ["cs.CL", "cs.CV", "cs.IR", "cs.LG"], "comment": "10 pages, 2 figures", "summary": "Vision-Language Models (VLMs) have demonstrated remarkable success in natural\nlanguage generation, excelling at instruction following and structured output\ngeneration. Knowledge graphs play a crucial role in radiology, serving as\nvaluable sources of factual information and enhancing various downstream tasks.\nHowever, generating radiology-specific knowledge graphs presents significant\nchallenges due to the specialized language of radiology reports and the limited\navailability of domain-specific data. Existing solutions are predominantly\nunimodal, meaning they generate knowledge graphs only from radiology reports\nwhile excluding radiographic images. Additionally, they struggle with long-form\nradiology data due to limited context length. To address these limitations, we\npropose a novel multimodal VLM-based framework for knowledge graph generation\nin radiology. Our approach outperforms previous methods and introduces the\nfirst multimodal solution for radiology knowledge graph generation."}
{"id": "2505.17061", "pdf": "https://arxiv.org/pdf/2505.17061", "abs": "https://arxiv.org/abs/2505.17061", "authors": ["Xinlong Chen", "Yuanxing Zhang", "Qiang Liu", "Junfei Wu", "Fuzheng Zhang", "Tieniu Tan"], "title": "Mixture of Decoding: An Attention-Inspired Adaptive Decoding Strategy to Mitigate Hallucinations in Large Vision-Language Models", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "Accepted to Findings of ACL 2025", "summary": "Large Vision-Language Models (LVLMs) have exhibited impressive capabilities\nacross various visual tasks, yet they remain hindered by the persistent\nchallenge of hallucinations. To address this critical issue, we propose Mixture\nof Decoding (MoD), a novel approach for hallucination mitigation that\ndynamically adapts decoding strategies by evaluating the correctness of the\nmodel's attention on image tokens. Specifically, MoD measures the consistency\nbetween outputs generated from the original image tokens and those derived from\nthe model's attended image tokens, to distinguish the correctness\naforementioned. If the outputs are consistent, indicating correct attention,\nMoD employs a complementary strategy to amplify critical information.\nConversely, if the outputs are inconsistent, suggesting erroneous attention,\nMoD utilizes a contrastive strategy to suppress misleading information.\nExtensive experiments demonstrate that MoD significantly outperforms existing\ndecoding methods across multiple mainstream benchmarks, effectively mitigating\nhallucinations in LVLMs. The code is available at\nhttps://github.com/xlchen0205/MoD."}
{"id": "2505.17091", "pdf": "https://arxiv.org/pdf/2505.17091", "abs": "https://arxiv.org/abs/2505.17091", "authors": ["Prateek Verma", "Mert Pilanci"], "title": "Large Language Models Implicitly Learn to See and Hear Just By Reading", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.SD", "eess.AS"], "comment": "6 pages, 3 figures, 4 tables. Under Review WASPAA 2025", "summary": "This paper presents a fascinating find: By training an auto-regressive LLM\nmodel on text tokens, the text model inherently develops internally an ability\nto understand images and audio, thereby developing the ability to see and hear\njust by reading. Popular audio and visual LLM models fine-tune text LLM models\nto give text output conditioned on images and audio embeddings. On the other\nhand, our architecture takes in patches of images, audio waveforms or tokens as\ninput. It gives us the embeddings or category labels typical of a\nclassification pipeline. We show the generality of text weights in aiding audio\nclassification for datasets FSD-50K and GTZAN. Further, we show this working\nfor image classification on CIFAR-10 and Fashion-MNIST, as well on image\npatches. This pushes the notion of text-LLMs learning powerful internal\ncircuits that can be utilized by activating necessary connections for various\napplications rather than training models from scratch every single time."}
{"id": "2505.17096", "pdf": "https://arxiv.org/pdf/2505.17096", "abs": "https://arxiv.org/abs/2505.17096", "authors": ["Sirui Li", "Linkai Peng", "Zheyuan Zhang", "Gorkem Durak", "Ulas Bagci"], "title": "TAGS: 3D Tumor-Adaptive Guidance for SAM", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Foundation models (FMs) such as CLIP and SAM have recently shown great\npromise in image segmentation tasks, yet their adaptation to 3D medical\nimaging-particularly for pathology detection and segmentation-remains\nunderexplored. A critical challenge arises from the domain gap between natural\nimages and medical volumes: existing FMs, pre-trained on 2D data, struggle to\ncapture 3D anatomical context, limiting their utility in clinical applications\nlike tumor segmentation. To address this, we propose an adaptation framework\ncalled TAGS: Tumor Adaptive Guidance for SAM, which unlocks 2D FMs for 3D\nmedical tasks through multi-prompt fusion. By preserving most of the\npre-trained weights, our approach enhances SAM's spatial feature extraction\nusing CLIP's semantic insights and anatomy-specific prompts. Extensive\nexperiments on three open-source tumor segmentation datasets prove that our\nmodel surpasses the state-of-the-art medical image segmentation models (+46.88%\nover nnUNet), interactive segmentation frameworks, and other established\nmedical FMs, including SAM-Med2D, SAM-Med3D, SegVol, Universal, 3D-Adapter, and\nSAM-B (at least +13% over them). This highlights the robustness and\nadaptability of our proposed framework across diverse medical segmentation\ntasks."}
{"id": "2505.17098", "pdf": "https://arxiv.org/pdf/2505.17098", "abs": "https://arxiv.org/abs/2505.17098", "authors": ["Yanshu Li", "Tian Yun", "Jianjiang Yang", "Pinyuan Feng", "Jinfa Huang", "Ruixiang Tang"], "title": "TACO: Enhancing Multimodal In-context Learning via Task Mapping-Guided Sequence Configuration", "categories": ["cs.CL", "cs.CV"], "comment": "29 pages, 11 figures, 19 tables. arXiv admin note: substantial text\n  overlap with arXiv:2503.04839", "summary": "Multimodal in-context learning (ICL) has emerged as a key mechanism for\nharnessing the capabilities of large vision-language models (LVLMs). However,\nits effectiveness remains highly sensitive to the quality of input in-context\nsequences, particularly for tasks involving complex reasoning or open-ended\ngeneration. A major limitation is our limited understanding of how LVLMs\nactually exploit these sequences during inference. To bridge this gap, we\nsystematically interpret multimodal ICL through the lens of task mapping, which\nreveals how local and global relationships within and among demonstrations\nguide model reasoning. Building on this insight, we present TACO, a lightweight\ntransformer-based model equipped with task-aware attention that dynamically\nconfigures in-context sequences. By injecting task-mapping signals into the\nautoregressive decoding process, TACO creates a bidirectional synergy between\nsequence construction and task reasoning. Experiments on five LVLMs and nine\ndatasets demonstrate that TACO consistently surpasses baselines across diverse\nICL tasks. These results position task mapping as a valuable perspective for\ninterpreting and improving multimodal ICL."}
{"id": "2505.17114", "pdf": "https://arxiv.org/pdf/2505.17114", "abs": "https://arxiv.org/abs/2505.17114", "authors": ["Subrata Biswas", "Mohammad Nur Hossain Khan", "Bashima Islam"], "title": "RAVEN: Query-Guided Representation Alignment for Question Answering over Audio, Video, Embedded Sensors, and Natural Language", "categories": ["cs.CL", "cs.CV", "cs.LG", "cs.MM"], "comment": null, "summary": "Multimodal question answering (QA) often requires identifying which video,\naudio, or sensor tokens are relevant to the question. Yet modality\ndisagreements are common: off-camera speech, background noise, or motion\noutside the field of view often mislead fusion models that weight all streams\nequally. We present RAVEN, a unified QA architecture whose core is QuART, a\nquery-conditioned cross-modal gating module that assigns scalar relevance\nscores to each token across modalities, enabling the model to amplify\ninformative signals and suppress distractors before fusion. RAVEN is trained\nthrough a three-stage pipeline comprising unimodal pretraining, query-aligned\nfusion, and disagreement-oriented fine-tuning -- each stage targeting a\ndistinct challenge in multi-modal reasoning: representation quality,\ncross-modal relevance, and robustness to modality mismatch. To support training\nand evaluation, we release AVS-QA, a dataset of 300K synchronized\nAudio--Video-Sensor streams paired with automatically generated question-answer\npairs. Experimental results on seven multi-modal QA benchmarks -- including\negocentric and exocentric tasks -- show that RAVEN achieves up to 14.5\\% and\n8.0\\% gains in accuracy compared to state-of-the-art multi-modal large language\nmodels, respectively. Incorporating sensor data provides an additional 16.4\\%\nboost, and the model remains robust under modality corruption, outperforming\nSOTA baselines by 50.23\\%. Our code and dataset are available at\nhttps://github.com/BASHLab/RAVEN."}
{"id": "2505.17163", "pdf": "https://arxiv.org/pdf/2505.17163", "abs": "https://arxiv.org/abs/2505.17163", "authors": ["Mingxin Huang", "Yongxin Shi", "Dezhi Peng", "Songxuan Lai", "Zecheng Xie", "Lianwen Jin"], "title": "OCR-Reasoning Benchmark: Unveiling the True Capabilities of MLLMs in Complex Text-Rich Image Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Recent advancements in multimodal slow-thinking systems have demonstrated\nremarkable performance across diverse visual reasoning tasks. However, their\ncapabilities in text-rich image reasoning tasks remain understudied due to the\nlack of a systematic benchmark. To address this gap, we propose OCR-Reasoning,\na comprehensive benchmark designed to systematically assess Multimodal Large\nLanguage Models on text-rich image reasoning tasks. The benchmark comprises\n1,069 human-annotated examples spanning 6 core reasoning abilities and 18\npractical reasoning tasks in text-rich visual scenarios. Furthermore, unlike\nother text-rich image understanding benchmarks that only annotate the final\nanswers, OCR-Reasoning also annotates the reasoning process simultaneously.\nWith the annotated reasoning process and the final answers, OCR-Reasoning\nevaluates not only the final answers generated by models but also their\nreasoning processes, enabling a holistic analysis of their problem-solving\nabilities. Leveraging this benchmark, we conducted a comprehensive evaluation\nof state-of-the-art MLLMs. Our results demonstrate the limitations of existing\nmethodologies. Notably, even state-of-the-art MLLMs exhibit substantial\ndifficulties, with none achieving accuracy surpassing 50\\% across\nOCR-Reasoning, indicating that the challenges of text-rich image reasoning are\nan urgent issue to be addressed. The benchmark and evaluation scripts are\navailable at https://github.com/SCUT-DLVCLab/OCR-Reasoning."}
{"id": "2505.17167", "pdf": "https://arxiv.org/pdf/2505.17167", "abs": "https://arxiv.org/abs/2505.17167", "authors": ["Ibrahim Ethem Hamamci", "Sezgin Er", "Suprosanna Shit", "Hadrien Reynaud", "Bernhard Kainz", "Bjoern Menze"], "title": "CRG Score: A Distribution-Aware Clinical Metric for Radiology Report Generation", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Evaluating long-context radiology report generation is challenging. NLG\nmetrics fail to capture clinical correctness, while LLM-based metrics often\nlack generalizability. Clinical accuracy metrics are more relevant but are\nsensitive to class imbalance, frequently favoring trivial predictions. We\npropose the CRG Score, a distribution-aware and adaptable metric that evaluates\nonly clinically relevant abnormalities explicitly described in reference\nreports. CRG supports both binary and structured labels (e.g., type, location)\nand can be paired with any LLM for feature extraction. By balancing penalties\nbased on label distribution, it enables fairer, more robust evaluation and\nserves as a clinically aligned reward function."}
{"id": "2505.17202", "pdf": "https://arxiv.org/pdf/2505.17202", "abs": "https://arxiv.org/abs/2505.17202", "authors": ["Arnav Verma", "Kushin Mukherjee", "Christopher Potts", "Elisa Kreiss", "Judith E. Fan"], "title": "CHART-6: Human-Centered Evaluation of Data Visualization Understanding in Vision-Language Models", "categories": ["cs.HC", "cs.CL", "cs.CV"], "comment": null, "summary": "Data visualizations are powerful tools for communicating patterns in\nquantitative data. Yet understanding any data visualization is no small feat --\nsucceeding requires jointly making sense of visual, numerical, and linguistic\ninputs arranged in a conventionalized format one has previously learned to\nparse. Recently developed vision-language models are, in principle, promising\ncandidates for developing computational models of these cognitive operations.\nHowever, it is currently unclear to what degree these models emulate human\nbehavior on tasks that involve reasoning about data visualizations. This gap\nreflects limitations in prior work that has evaluated data visualization\nunderstanding in artificial systems using measures that differ from those\ntypically used to assess these abilities in humans. Here we evaluated eight\nvision-language models on six data visualization literacy assessments designed\nfor humans and compared model responses to those of human participants. We\nfound that these models performed worse than human participants on average, and\nthis performance gap persisted even when using relatively lenient criteria to\nassess model performance. Moreover, while relative performance across items was\nsomewhat correlated between models and humans, all models produced patterns of\nerrors that were reliably distinct from those produced by human participants.\nTaken together, these findings suggest significant opportunities for further\ndevelopment of artificial systems that might serve as useful models of how\nhumans reason about data visualizations. All code and data needed to reproduce\nthese results are available at:\nhttps://osf.io/e25mu/?view_only=399daff5a14d4b16b09473cf19043f18."}
{"id": "2505.17210", "pdf": "https://arxiv.org/pdf/2505.17210", "abs": "https://arxiv.org/abs/2505.17210", "authors": ["Martin Villagrana", "Francisco Lopez-Tiro", "Clement Larose", "Gilberto Ochoa-Ruiz", "Christian Daul"], "title": "Assessing the generalization performance of SAM for ureteroscopy scene understanding", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": "15 pages, 4 figures, 2 tables, conference, MIUA25", "summary": "The segmentation of kidney stones is regarded as a critical preliminary step\nto enable the identification of urinary stone types through machine- or\ndeep-learning-based approaches. In urology, manual segmentation is considered\ntedious and impractical due to the typically large scale of image databases and\nthe continuous generation of new data. In this study, the potential of the\nSegment Anything Model (SAM) -- a state-of-the-art deep learning framework --\nis investigated for the automation of kidney stone segmentation. The\nperformance of SAM is evaluated in comparison to traditional models, including\nU-Net, Residual U-Net, and Attention U-Net, which, despite their efficiency,\nfrequently exhibit limitations in generalizing to unseen datasets. The findings\nhighlight SAM's superior adaptability and efficiency. While SAM achieves\ncomparable performance to U-Net on in-distribution data (Accuracy: 97.68 +\n3.04; Dice: 97.78 + 2.47; IoU: 95.76 + 4.18), it demonstrates significantly\nenhanced generalization capabilities on out-of-distribution data, surpassing\nall U-Net variants by margins of up to 23 percent."}
{"id": "2505.17357", "pdf": "https://arxiv.org/pdf/2505.17357", "abs": "https://arxiv.org/abs/2505.17357", "authors": ["Hassan Wasswa", "Hussein Abbass", "Timothy Lynar"], "title": "Graph Attention Neural Network for Botnet Detection: Evaluating Autoencoder, VAE and PCA-Based Dimension Reduction", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "With the rise of IoT-based botnet attacks, researchers have explored various\nlearning models for detection, including traditional machine learning, deep\nlearning, and hybrid approaches. A key advancement involves deploying attention\nmechanisms to capture long-term dependencies among features, significantly\nimproving detection accuracy. However, most models treat attack instances\nindependently, overlooking inter-instance relationships. Graph Neural Networks\n(GNNs) address this limitation by learning an embedding space via iterative\nmessage passing where similar instances are placed closer based on node\nfeatures and relationships, enhancing classification performance. To further\nimprove detection, attention mechanisms have been embedded within GNNs,\nleveraging both long-range dependencies and inter-instance connections.\nHowever, transforming the high dimensional IoT attack datasets into a graph\nstructured dataset poses challenges, such as large graph structures leading\ncomputational overhead. To mitigate this, this paper proposes a framework that\nfirst reduces dimensionality of the NetFlow-based IoT attack dataset before\ntransforming it into a graph dataset. We evaluate three dimension reduction\ntechniques--Variational Autoencoder (VAE-encoder), classical autoencoder\n(AE-encoder), and Principal Component Analysis (PCA)--and compare their effects\non a Graph Attention neural network (GAT) model for botnet attack detection"}
{"id": "2505.17384", "pdf": "https://arxiv.org/pdf/2505.17384", "abs": "https://arxiv.org/abs/2505.17384", "authors": ["Tianyu Xie", "Shuchen Xue", "Zijin Feng", "Tianyang Hu", "Jiacheng Sun", "Zhenguo Li", "Cheng Zhang"], "title": "Variational Autoencoding Discrete Diffusion with Enhanced Dimensional Correlations Modeling", "categories": ["cs.LG", "cs.CV", "stat.ML"], "comment": "23 pages, 14 figures", "summary": "Discrete diffusion models have recently shown great promise for modeling\ncomplex discrete data, with masked diffusion models (MDMs) offering a\ncompelling trade-off between quality and generation speed. MDMs denoise by\nprogressively unmasking multiple dimensions from an all-masked input, but their\nperformance can degrade when using few denoising steps due to limited modeling\nof inter-dimensional dependencies. In this paper, we propose Variational\nAutoencoding Discrete Diffusion (VADD), a novel framework that enhances\ndiscrete diffusion with latent variable modeling to implicitly capture\ncorrelations among dimensions. By introducing an auxiliary recognition model,\nVADD enables stable training via variational lower bounds maximization and\namortized inference over the training set. Our approach retains the efficiency\nof traditional MDMs while significantly improving sample quality, especially\nwhen the number of denoising steps is small. Empirical results on 2D toy data,\npixel-level image generation, and text generation demonstrate that VADD\nconsistently outperforms MDM baselines."}
{"id": "2505.17402", "pdf": "https://arxiv.org/pdf/2505.17402", "abs": "https://arxiv.org/abs/2505.17402", "authors": ["Mahmoud Chick Zaouali", "Todd Charter", "Homayoun Najjaran"], "title": "From Flight to Insight: Semantic 3D Reconstruction for Aerial Inspection via Gaussian Splatting and Language-Guided Segmentation", "categories": ["cs.GR", "cs.CV", "eess.IV"], "comment": null, "summary": "High-fidelity 3D reconstruction is critical for aerial inspection tasks such\nas infrastructure monitoring, structural assessment, and environmental\nsurveying. While traditional photogrammetry techniques enable geometric\nmodeling, they lack semantic interpretability, limiting their effectiveness for\nautomated inspection workflows. Recent advances in neural rendering and 3D\nGaussian Splatting (3DGS) offer efficient, photorealistic reconstructions but\nsimilarly lack scene-level understanding.\n  In this work, we present a UAV-based pipeline that extends Feature-3DGS for\nlanguage-guided 3D segmentation. We leverage LSeg-based feature fields with\nCLIP embeddings to generate heatmaps in response to language prompts. These are\nthresholded to produce rough segmentations, and the highest-scoring point is\nthen used as a prompt to SAM or SAM2 for refined 2D segmentation on novel view\nrenderings. Our results highlight the strengths and limitations of various\nfeature field backbones (CLIP-LSeg, SAM, SAM2) in capturing meaningful\nstructure in large-scale outdoor environments. We demonstrate that this hybrid\napproach enables flexible, language-driven interaction with photorealistic 3D\nreconstructions, opening new possibilities for semantic aerial inspection and\nscene understanding."}
{"id": "2505.17448", "pdf": "https://arxiv.org/pdf/2505.17448", "abs": "https://arxiv.org/abs/2505.17448", "authors": ["Bhanuka Gamage", "Adnan Labib", "Aisha Joomun", "Chern Hong Lim", "KokSheik Wong"], "title": "Baitradar: A Multi-Model Clickbait Detection Algorithm Using Deep Learning", "categories": ["cs.LG", "cs.CV"], "comment": "Appear in IEEE International Conference on Acoustics, Speech and\n  Signal Processing (ICASSP'21), Toronto, ON, Canada", "summary": "Following the rising popularity of YouTube, there is an emerging problem on\nthis platform called clickbait, which provokes users to click on videos using\nattractive titles and thumbnails. As a result, users ended up watching a video\nthat does not have the content as publicized in the title. This issue is\naddressed in this study by proposing an algorithm called BaitRadar, which uses\na deep learning technique where six inference models are jointly consulted to\nmake the final classification decision. These models focus on different\nattributes of the video, including title, comments, thumbnail, tags, video\nstatistics and audio transcript. The final classification is attained by\ncomputing the average of multiple models to provide a robust and accurate\noutput even in situation where there is missing data. The proposed method is\ntested on 1,400 YouTube videos. On average, a test accuracy of 98% is achieved\nwith an inference time of less than 2s."}
{"id": "2505.17472", "pdf": "https://arxiv.org/pdf/2505.17472", "abs": "https://arxiv.org/abs/2505.17472", "authors": ["Jiangjie Wu", "Lixuan Chen", "Zhenghao Li", "Xin Li", "Saban Ozturk", "Lihui Wang", "Rongpin Wang", "Hongjiang Wei", "Yuyao Zhang"], "title": "SUFFICIENT: A scan-specific unsupervised deep learning framework for high-resolution 3D isotropic fetal brain MRI reconstruction", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "High-quality 3D fetal brain MRI reconstruction from motion-corrupted 2D\nslices is crucial for clinical diagnosis. Reliable slice-to-volume registration\n(SVR)-based motion correction and super-resolution reconstruction (SRR) methods\nare essential. Deep learning (DL) has demonstrated potential in enhancing SVR\nand SRR when compared to conventional methods. However, it requires large-scale\nexternal training datasets, which are difficult to obtain for clinical fetal\nMRI. To address this issue, we propose an unsupervised iterative SVR-SRR\nframework for isotropic HR volume reconstruction. Specifically, SVR is\nformulated as a function mapping a 2D slice and a 3D target volume to a rigid\ntransformation matrix, which aligns the slice to the underlying location in the\ntarget volume. The function is parameterized by a convolutional neural network,\nwhich is trained by minimizing the difference between the volume slicing at the\npredicted position and the input slice. In SRR, a decoding network embedded\nwithin a deep image prior framework is incorporated with a comprehensive image\ndegradation model to produce the high-resolution (HR) volume. The deep image\nprior framework offers a local consistency prior to guide the reconstruction of\nHR volumes. By performing a forward degradation model, the HR volume is\noptimized by minimizing loss between predicted slices and the observed slices.\nComprehensive experiments conducted on large-magnitude motion-corrupted\nsimulation data and clinical data demonstrate the superior performance of the\nproposed framework over state-of-the-art fetal brain reconstruction frameworks."}
{"id": "2505.17484", "pdf": "https://arxiv.org/pdf/2505.17484", "abs": "https://arxiv.org/abs/2505.17484", "authors": ["Hai Jiang", "Qiongting Liu", "Yuanpin Zhou", "Jiawei Pan", "Ting Song", "Yao Lu"], "title": "Anatomy-Guided Multitask Learning for MRI-Based Classification of Placenta Accreta Spectrum and its Subtypes", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Placenta Accreta Spectrum Disorders (PAS) pose significant risks during\npregnancy, frequently leading to postpartum hemorrhage during cesarean\ndeliveries and other severe clinical complications, with bleeding severity\ncorrelating to the degree of placental invasion. Consequently, accurate\nprenatal diagnosis of PAS and its subtypes-placenta accreta (PA), placenta\nincreta (PI), and placenta percreta (PP)-is crucial. However, existing\nguidelines and methodologies predominantly focus on the presence of PAS, with\nlimited research addressing subtype recognition. Additionally, previous\nmulti-class diagnostic efforts have primarily relied on inefficient two-stage\ncascaded binary classification tasks. In this study, we propose a novel\nconvolutional neural network (CNN) architecture designed for efficient\none-stage multiclass diagnosis of PAS and its subtypes, based on 4,140 magnetic\nresonance imaging (MRI) slices. Our model features two branches: the main\nclassification branch utilizes a residual block architecture comprising\nmultiple residual blocks, while the second branch integrates anatomical\nfeatures of the uteroplacental area and the adjacent uterine serous layer to\nenhance the model's attention during classification. Furthermore, we implement\na multitask learning strategy to leverage both branches effectively.\nExperiments conducted on a real clinical dataset demonstrate that our model\nachieves state-of-the-art performance."}
{"id": "2505.17528", "pdf": "https://arxiv.org/pdf/2505.17528", "abs": "https://arxiv.org/abs/2505.17528", "authors": ["Hai Jiang", "Chushan Zheng", "Jiawei Pan", "Yuanpin Zhou", "Qiongting Liu", "Xiang Zhang", "Jun Shen", "Yao Lu"], "title": "DECT-based Space-Squeeze Method for Multi-Class Classification of Metastatic Lymph Nodes in Breast Cancer", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Background: Accurate assessment of metastatic burden in axillary lymph nodes\nis crucial for guiding breast cancer treatment decisions, yet conventional\nimaging modalities struggle to differentiate metastatic burden levels and\ncapture comprehensive lymph node characteristics. This study leverages\ndual-energy computed tomography (DECT) to exploit spectral-spatial information\nfor improved multi-class classification. Purpose: To develop a noninvasive\nDECT-based model classifying sentinel lymph nodes into three categories: no\nmetastasis ($N_0$), low metastatic burden ($N_{+(1-2)}$), and heavy metastatic\nburden ($N_{+(\\geq3)}$), thereby aiding therapeutic planning. Methods: We\npropose a novel space-squeeze method combining two innovations: (1) a\nchannel-wise attention mechanism to compress and recalibrate spectral-spatial\nfeatures across 11 energy levels, and (2) virtual class injection to sharpen\ninter-class boundaries and compact intra-class variations in the representation\nspace. Results: Evaluated on 227 biopsy-confirmed cases, our method achieved an\naverage test AUC of 0.86 (95% CI: 0.80-0.91) across three cross-validation\nfolds, outperforming established CNNs (VGG, ResNet, etc). The channel-wise\nattention and virtual class components individually improved AUC by 5.01% and\n5.87%, respectively, demonstrating complementary benefits. Conclusions: The\nproposed framework enhances diagnostic AUC by effectively integrating DECT's\nspectral-spatial data and mitigating class ambiguity, offering a promising tool\nfor noninvasive metastatic burden assessment in clinical practice."}
{"id": "2505.17544", "pdf": "https://arxiv.org/pdf/2505.17544", "abs": "https://arxiv.org/abs/2505.17544", "authors": ["Ruiqi Xing"], "title": "FreqU-FNet: Frequency-Aware U-Net for Imbalanced Medical Image Segmentation", "categories": ["eess.IV", "cs.CV"], "comment": "15 pages, 1 figure", "summary": "Medical image segmentation faces persistent challenges due to severe class\nimbalance and the frequency-specific distribution of anatomical structures.\nMost conventional CNN-based methods operate in the spatial domain and struggle\nto capture minority class signals, often affected by frequency aliasing and\nlimited spectral selectivity. Transformer-based models, while powerful in\nmodeling global dependencies, tend to overlook critical local details necessary\nfor fine-grained segmentation. To overcome these limitations, we propose\nFreqU-FNet, a novel U-shaped segmentation architecture operating in the\nfrequency domain. Our framework incorporates a Frequency Encoder that leverages\nLow-Pass Frequency Convolution and Daubechies wavelet-based downsampling to\nextract multi-scale spectral features. To reconstruct fine spatial details, we\nintroduce a Spatial Learnable Decoder (SLD) equipped with an adaptive\nmulti-branch upsampling strategy. Furthermore, we design a frequency-aware loss\n(FAL) function to enhance minority class learning. Extensive experiments on\nmultiple medical segmentation benchmarks demonstrate that FreqU-FNet\nconsistently outperforms both CNN and Transformer baselines, particularly in\nhandling under-represented classes, by effectively exploiting discriminative\nfrequency bands."}
{"id": "2505.17555", "pdf": "https://arxiv.org/pdf/2505.17555", "abs": "https://arxiv.org/abs/2505.17555", "authors": ["Yuchen He", "Jianbing Lv", "Liqi Cheng", "Lingyu Meng", "Dazhen Deng", "Yingcai Wu"], "title": "ProTAL: A Drag-and-Link Video Programming Framework for Temporal Action Localization", "categories": ["cs.HC", "cs.CV"], "comment": "Accepted at CHI'25", "summary": "Temporal Action Localization (TAL) aims to detect the start and end\ntimestamps of actions in a video. However, the training of TAL models requires\na substantial amount of manually annotated data. Data programming is an\nefficient method to create training labels with a series of human-defined\nlabeling functions. However, its application in TAL faces difficulties of\ndefining complex actions in the context of temporal video frames. In this\npaper, we propose ProTAL, a drag-and-link video programming framework for TAL.\nProTAL enables users to define \\textbf{key events} by dragging nodes\nrepresenting body parts and objects and linking them to constrain the relations\n(direction, distance, etc.). These definitions are used to generate action\nlabels for large-scale unlabelled videos. A semi-supervised method is then\nemployed to train TAL models with such labels. We demonstrate the effectiveness\nof ProTAL through a usage scenario and a user study, providing insights into\ndesigning video programming framework."}
{"id": "2505.17556", "pdf": "https://arxiv.org/pdf/2505.17556", "abs": "https://arxiv.org/abs/2505.17556", "authors": ["Nikolaos Anastasiou", "Spyros Kondylatos", "Ioannis Papoutsis"], "title": "Wildfire spread forecasting with Deep Learning", "categories": ["cs.LG", "cs.CV", "I.2.7"], "comment": "10 pages, 9 figures", "summary": "Accurate prediction of wildfire spread is crucial for effective risk\nmanagement, emergency response, and strategic resource allocation. In this\nstudy, we present a deep learning (DL)-based framework for forecasting the\nfinal extent of burned areas, using data available at the time of ignition. We\nleverage a spatio-temporal dataset that covers the Mediterranean region from\n2006 to 2022, incorporating remote sensing data, meteorological observations,\nvegetation maps, land cover classifications, anthropogenic factors, topography\ndata, and thermal anomalies. To evaluate the influence of temporal context, we\nconduct an ablation study examining how the inclusion of pre- and post-ignition\ndata affects model performance, benchmarking the temporal-aware DL models\nagainst a baseline trained exclusively on ignition-day inputs. Our results\nindicate that multi-day observational data substantially improve predictive\naccuracy. Particularly, the best-performing model, incorporating a temporal\nwindow of four days before to five days after ignition, improves both the F1\nscore and the Intersection over Union by almost 5% in comparison to the\nbaseline on the test dataset. We publicly release our dataset and models to\nenhance research into data-driven approaches for wildfire modeling and\nresponse."}
{"id": "2505.17582", "pdf": "https://arxiv.org/pdf/2505.17582", "abs": "https://arxiv.org/abs/2505.17582", "authors": ["Masataka Kobayashi", "Shintaro Shiba", "Quan Kong", "Norimasa Kobori", "Tsukasa Shimizu", "Shan Lu", "Takaya Yamazato"], "title": "Distance Estimation in Outdoor Driving Environments Using Phase-only Correlation Method with Event Cameras", "categories": ["eess.IV", "cs.CV", "cs.RO", "I.4.8; I.2.10; I.5.4"], "comment": "6 pages, 7 figures. To appear in IEEE Intelligent Vehicles Symposium\n  (IV) 2025", "summary": "With the growing adoption of autonomous driving, the advancement of sensor\ntechnology is crucial for ensuring safety and reliable operation. Sensor fusion\ntechniques that combine multiple sensors such as LiDAR, radar, and cameras have\nproven effective, but the integration of multiple devices increases both\nhardware complexity and cost. Therefore, developing a single sensor capable of\nperforming multiple roles is highly desirable for cost-efficient and scalable\nautonomous driving systems.\n  Event cameras have emerged as a promising solution due to their unique\ncharacteristics, including high dynamic range, low latency, and high temporal\nresolution. These features enable them to perform well in challenging lighting\nconditions, such as low-light or backlit environments. Moreover, their ability\nto detect fine-grained motion events makes them suitable for applications like\npedestrian detection and vehicle-to-infrastructure communication via visible\nlight.\n  In this study, we present a method for distance estimation using a monocular\nevent camera and a roadside LED bar. By applying a phase-only correlation\ntechnique to the event data, we achieve sub-pixel precision in detecting the\nspatial shift between two light sources. This enables accurate\ntriangulation-based distance estimation without requiring stereo vision. Field\nexperiments conducted in outdoor driving scenarios demonstrated that the\nproposed approach achieves over 90% success rate with less than 0.5-meter error\nfor distances ranging from 20 to 60 meters.\n  Future work includes extending this method to full position estimation by\nleveraging infrastructure such as smart poles equipped with LEDs, enabling\nevent-camera-based vehicles to determine their own position in real time. This\nadvancement could significantly enhance navigation accuracy, route\noptimization, and integration into intelligent transportation systems."}
{"id": "2505.17591", "pdf": "https://arxiv.org/pdf/2505.17591", "abs": "https://arxiv.org/abs/2505.17591", "authors": ["Judith Vilella-Cantos", "Juan José Cabrera", "Luis Payá", "Mónica Ballesta", "David Valiente"], "title": "MinkUNeXt-SI: Improving point cloud-based place recognition including spherical coordinates and LiDAR intensity", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.RO"], "comment": null, "summary": "In autonomous navigation systems, the solution of the place recognition\nproblem is crucial for their safe functioning. But this is not a trivial\nsolution, since it must be accurate regardless of any changes in the scene,\nsuch as seasonal changes and different weather conditions, and it must be\ngeneralizable to other environments. This paper presents our method,\nMinkUNeXt-SI, which, starting from a LiDAR point cloud, preprocesses the input\ndata to obtain its spherical coordinates and intensity values normalized within\na range of 0 to 1 for each point, and it produces a robust place recognition\ndescriptor. To that end, a deep learning approach that combines Minkowski\nconvolutions and a U-net architecture with skip connections is used. The\nresults of MinkUNeXt-SI demonstrate that this method reaches and surpasses\nstate-of-the-art performance while it also generalizes satisfactorily to other\ndatasets. Additionally, we showcase the capture of a custom dataset and its use\nin evaluating our solution, which also achieves outstanding results. Both the\ncode of our solution and the runs of our dataset are publicly available for\nreproducibility purposes."}
{"id": "2505.17613", "pdf": "https://arxiv.org/pdf/2505.17613", "abs": "https://arxiv.org/abs/2505.17613", "authors": ["Jihan Yao", "Yushi Hu", "Yujie Yi", "Bin Han", "Shangbin Feng", "Guang Yang", "Bingbing Wen", "Ranjay Krishna", "Lucy Lu Wang", "Yulia Tsvetkov", "Noah A. Smith", "Banghua Zhu"], "title": "MMMG: a Comprehensive and Reliable Evaluation Suite for Multitask Multimodal Generation", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Automatically evaluating multimodal generation presents a significant\nchallenge, as automated metrics often struggle to align reliably with human\nevaluation, especially for complex tasks that involve multiple modalities. To\naddress this, we present MMMG, a comprehensive and human-aligned benchmark for\nmultimodal generation across 4 modality combinations (image, audio, interleaved\ntext and image, interleaved text and audio), with a focus on tasks that present\nsignificant challenges for generation models, while still enabling reliable\nautomatic evaluation through a combination of models and programs. MMMG\nencompasses 49 tasks (including 29 newly developed ones), each with a carefully\ndesigned evaluation pipeline, and 937 instructions to systematically assess\nreasoning, controllability, and other key capabilities of multimodal generation\nmodels. Extensive validation demonstrates that MMMG is highly aligned with\nhuman evaluation, achieving an average agreement of 94.3%. Benchmarking results\non 24 multimodal generation models reveal that even though the state-of-the-art\nmodel, GPT Image, achieves 78.3% accuracy for image generation, it falls short\non multimodal reasoning and interleaved generation. Furthermore, results\nsuggest considerable headroom for improvement in audio generation, highlighting\nan important direction for future research."}
{"id": "2505.17625", "pdf": "https://arxiv.org/pdf/2505.17625", "abs": "https://arxiv.org/abs/2505.17625", "authors": ["Hayato Aida", "Kosuke Takahashi", "Takahiro Omi"], "title": "Enhancing Large Vision-Language Models with Layout Modality for Table Question Answering on Japanese Annual Securities Reports", "categories": ["cs.CL", "cs.CV", "68T50", "I.2"], "comment": "Accepted at IIAI AAI 2025, the 3rd International Conference on\n  Computational and Data Sciences in Economics and Finance", "summary": "With recent advancements in Large Language Models (LLMs) and growing interest\nin retrieval-augmented generation (RAG), the ability to understand table\nstructures has become increasingly important. This is especially critical in\nfinancial domains such as securities reports, where highly accurate question\nanswering (QA) over tables is required. However, tables exist in various\nformats-including HTML, images, and plain text-making it difficult to preserve\nand extract structural information. Therefore, multimodal LLMs are essential\nfor robust and general-purpose table understanding. Despite their promise,\ncurrent Large Vision-Language Models (LVLMs), which are major representatives\nof multimodal LLMs, still face challenges in accurately understanding\ncharacters and their spatial relationships within documents. In this study, we\npropose a method to enhance LVLM-based table understanding by incorporating\nin-table textual content and layout features. Experimental results demonstrate\nthat these auxiliary modalities significantly improve performance, enabling\nrobust interpretation of complex document layouts without relying on explicitly\nstructured input formats."}
{"id": "2505.17644", "pdf": "https://arxiv.org/pdf/2505.17644", "abs": "https://arxiv.org/abs/2505.17644", "authors": ["Taoran Zheng", "Xing Li", "Yan Yang", "Xiang Gu", "Zongben Xu", "Jian Sun"], "title": "Towards Prospective Medical Image Reconstruction via Knowledge-Informed Dynamic Optimal Transport", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Medical image reconstruction from measurement data is a vital but challenging\ninverse problem. Deep learning approaches have achieved promising results, but\noften requires paired measurement and high-quality images, which is typically\nsimulated through a forward model, i.e., retrospective reconstruction. However,\ntraining on simulated pairs commonly leads to performance degradation on real\nprospective data due to the retrospective-to-prospective gap caused by\nincomplete imaging knowledge in simulation. To address this challenge, this\npaper introduces imaging Knowledge-Informed Dynamic Optimal Transport (KIDOT),\na novel dynamic optimal transport framework with optimality in the sense of\npreserving consistency with imaging physics in transport, that conceptualizes\nreconstruction as finding a dynamic transport path. KIDOT learns from unpaired\ndata by modeling reconstruction as a continuous evolution path from\nmeasurements to images, guided by an imaging knowledge-informed cost function\nand transport equation. This dynamic and knowledge-aware approach enhances\nrobustness and better leverages unpaired data while respecting acquisition\nphysics. Theoretically, we demonstrate that KIDOT naturally generalizes dynamic\noptimal transport, ensuring its mathematical rationale and solution existence.\nExtensive experiments on MRI and CT reconstruction demonstrate KIDOT's superior\nperformance."}
{"id": "2505.17659", "pdf": "https://arxiv.org/pdf/2505.17659", "abs": "https://arxiv.org/abs/2505.17659", "authors": ["Xiaolong Tang", "Meina Kan", "Shiguang Shan", "Xilin Chen"], "title": "Plan-R1: Safe and Feasible Trajectory Planning as Language Modeling", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Safe and feasible trajectory planning is essential for real-world autonomous\ndriving systems. However, existing learning-based planning methods often rely\non expert demonstrations, which not only lack explicit safety awareness but\nalso risk inheriting unsafe behaviors such as speeding from suboptimal human\ndriving data. Inspired by the success of large language models, we propose\nPlan-R1, a novel two-stage trajectory planning framework that formulates\ntrajectory planning as a sequential prediction task, guided by explicit\nplanning principles such as safety, comfort, and traffic rule compliance. In\nthe first stage, we train an autoregressive trajectory predictor via next\nmotion token prediction on expert data. In the second stage, we design\nrule-based rewards (e.g., collision avoidance, speed limits) and fine-tune the\nmodel using Group Relative Policy Optimization (GRPO), a reinforcement learning\nstrategy, to align its predictions with these planning principles. Experiments\non the nuPlan benchmark demonstrate that our Plan-R1 significantly improves\nplanning safety and feasibility, achieving state-of-the-art performance."}
{"id": "2505.17683", "pdf": "https://arxiv.org/pdf/2505.17683", "abs": "https://arxiv.org/abs/2505.17683", "authors": ["Dan Yuan", "Yi Feng", "Ziyun Tang"], "title": "Dual Attention Residual U-Net for Accurate Brain Ultrasound Segmentation in IVH Detection", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "10 pages,6 figures and 3 tables", "summary": "Intraventricular hemorrhage (IVH) is a severe neurological complication among\npremature infants, necessitating early and accurate detection from brain\nultrasound (US) images to improve clinical outcomes. While recent deep learning\nmethods offer promise for computer-aided diagnosis, challenges remain in\ncapturing both local spatial details and global contextual dependencies\ncritical for segmenting brain anatomies. In this work, we propose an enhanced\nResidual U-Net architecture incorporating two complementary attention\nmechanisms: the Convolutional Block Attention Module (CBAM) and a Sparse\nAttention Layer (SAL). The CBAM improves the model's ability to refine spatial\nand channel-wise features, while the SAL introduces a dual-branch design,\nsparse attention filters out low-confidence query-key pairs to suppress noise,\nand dense attention ensures comprehensive information propagation. Extensive\nexperiments on the Brain US dataset demonstrate that our method achieves\nstate-of-the-art segmentation performance, with a Dice score of 89.04% and IoU\nof 81.84% for ventricle region segmentation. These results highlight the\neffectiveness of integrating spatial refinement and attention sparsity for\nrobust brain anatomy detection. Code is available at:\nhttps://github.com/DanYuan001/BrainImgSegment."}
{"id": "2505.17695", "pdf": "https://arxiv.org/pdf/2505.17695", "abs": "https://arxiv.org/abs/2505.17695", "authors": ["Dong-Hee Kim", "Hyunjee Song", "Donghyun Kim"], "title": "SynRES: Towards Referring Expression Segmentation in the Wild via Synthetic Data", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Despite the advances in Referring Expression Segmentation (RES) benchmarks,\ntheir evaluation protocols remain constrained, primarily focusing on either\nsingle targets with short queries (containing minimal attributes) or multiple\ntargets from distinctly different queries on a single domain. This limitation\nsignificantly hinders the assessment of more complex reasoning capabilities in\nRES models. We introduce WildRES, a novel benchmark that incorporates long\nqueries with diverse attributes and non-distinctive queries for multiple\ntargets. This benchmark spans diverse application domains, including autonomous\ndriving environments and robotic manipulation scenarios, thus enabling more\nrigorous evaluation of complex reasoning capabilities in real-world settings.\nOur analysis reveals that current RES models demonstrate substantial\nperformance deterioration when evaluated on WildRES. To address this challenge,\nwe introduce SynRES, an automated pipeline generating densely paired\ncompositional synthetic training data through three innovations: (1) a dense\ncaption-driven synthesis for attribute-rich image-mask-expression triplets, (2)\nreliable semantic alignment mechanisms rectifying caption-pseudo mask\ninconsistencies via Image-Text Aligned Grouping, and (3) domain-aware\naugmentations incorporating mosaic composition and superclass replacement to\nemphasize generalization ability and distinguishing attributes over object\ncategories. Experimental results demonstrate that models trained with SynRES\nachieve state-of-the-art performance, improving gIoU by 2.0% on WildRES-ID and\n3.8% on WildRES-DS. Code and datasets are available at\nhttps://github.com/UTLLab/SynRES."}
{"id": "2505.17748", "pdf": "https://arxiv.org/pdf/2505.17748", "abs": "https://arxiv.org/abs/2505.17748", "authors": ["Kerol Djoumessi", "Philipp Berens"], "title": "Soft-CAM: Making black box models self-explainable for high-stakes decisions", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Convolutional neural networks (CNNs) are widely used for high-stakes\napplications like medicine, often surpassing human performance. However, most\nexplanation methods rely on post-hoc attribution, approximating the\ndecision-making process of already trained black-box models. These methods are\noften sensitive, unreliable, and fail to reflect true model reasoning, limiting\ntheir trustworthiness in critical applications. In this work, we introduce\nSoftCAM, a straightforward yet effective approach that makes standard CNN\narchitectures inherently interpretable. By removing the global average pooling\nlayer and replacing the fully connected classification layer with a\nconvolution-based class evidence layer, SoftCAM preserves spatial information\nand produces explicit class activation maps that form the basis of the model's\npredictions. Evaluated on three medical datasets, SoftCAM maintains\nclassification performance while significantly improving both the qualitative\nand quantitative explanation compared to existing post-hoc methods. Our results\ndemonstrate that CNNs can be inherently interpretable without compromising\nperformance, advancing the development of self-explainable deep learning for\nhigh-stakes decision-making."}
{"id": "2505.17799", "pdf": "https://arxiv.org/pdf/2505.17799", "abs": "https://arxiv.org/abs/2505.17799", "authors": ["Brian B. Moser", "Arundhati S. Shanbhag", "Stanislav Frolov", "Federico Raue", "Joachim Folz", "Andreas Dengel"], "title": "A Coreset Selection of Coreset Selection Literature: Introduction and Recent Advances", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Coreset selection targets the challenge of finding a small, representative\nsubset of a large dataset that preserves essential patterns for effective\nmachine learning. Although several surveys have examined data reduction\nstrategies before, most focus narrowly on either classical geometry-based\nmethods or active learning techniques. In contrast, this survey presents a more\ncomprehensive view by unifying three major lines of coreset research, namely,\ntraining-free, training-oriented, and label-free approaches, into a single\ntaxonomy. We present subfields often overlooked by existing work, including\nsubmodular formulations, bilevel optimization, and recent progress in\npseudo-labeling for unlabeled datasets. Additionally, we examine how pruning\nstrategies influence generalization and neural scaling laws, offering new\ninsights that are absent from prior reviews. Finally, we compare these methods\nunder varying computational, robustness, and performance demands and highlight\nopen challenges, such as robustness, outlier filtering, and adapting coreset\nselection to foundation models, for future research."}
{"id": "2505.17860", "pdf": "https://arxiv.org/pdf/2505.17860", "abs": "https://arxiv.org/abs/2505.17860", "authors": ["Wenning Xu", "Shiyu Fan", "Paul Henderson", "Edmond S. L. Ho"], "title": "Multi-Person Interaction Generation from Two-Person Motion Priors", "categories": ["cs.GR", "cs.CV", "cs.LG", "I.3.7"], "comment": "SIGGRAPH 2025 Conference Papers", "summary": "Generating realistic human motion with high-level controls is a crucial task\nfor social understanding, robotics, and animation. With high-quality MOCAP data\nbecoming more available recently, a wide range of data-driven approaches have\nbeen presented. However, modelling multi-person interactions still remains a\nless explored area. In this paper, we present Graph-driven Interaction\nSampling, a method that can generate realistic and diverse multi-person\ninteractions by leveraging existing two-person motion diffusion models as\nmotion priors. Instead of training a new model specific to multi-person\ninteraction synthesis, our key insight is to spatially and temporally separate\ncomplex multi-person interactions into a graph structure of two-person\ninteractions, which we name the Pairwise Interaction Graph. We thus decompose\nthe generation task into simultaneous single-person motion generation\nconditioned on one other's motion. In addition, to reduce artifacts such as\ninterpenetrations of body parts in generated multi-person interactions, we\nintroduce two graph-dependent guidance terms into the diffusion sampling\nscheme. Unlike previous work, our method can produce various high-quality\nmulti-person interactions without having repetitive individual motions.\nExtensive experiments demonstrate that our approach consistently outperforms\nexisting methods in reducing artifacts when generating a wide range of\ntwo-person and multi-person interactions."}
{"id": "2505.17883", "pdf": "https://arxiv.org/pdf/2505.17883", "abs": "https://arxiv.org/abs/2505.17883", "authors": ["Laines Schmalwasser", "Niklas Penzel", "Joachim Denzler", "Julia Niebling"], "title": "FastCAV: Efficient Computation of Concept Activation Vectors for Explaining Deep Neural Networks", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "Accepted at ICML 2025, 27 pages, 20 figures, 9 tables", "summary": "Concepts such as objects, patterns, and shapes are how humans understand the\nworld. Building on this intuition, concept-based explainability methods aim to\nstudy representations learned by deep neural networks in relation to\nhuman-understandable concepts. Here, Concept Activation Vectors (CAVs) are an\nimportant tool and can identify whether a model learned a concept or not.\nHowever, the computational cost and time requirements of existing CAV\ncomputation pose a significant challenge, particularly in large-scale,\nhigh-dimensional architectures. To address this limitation, we introduce\nFastCAV, a novel approach that accelerates the extraction of CAVs by up to\n63.6x (on average 46.4x). We provide a theoretical foundation for our approach\nand give concrete assumptions under which it is equivalent to established\nSVM-based methods. Our empirical results demonstrate that CAVs calculated with\nFastCAV maintain similar performance while being more efficient and stable. In\ndownstream applications, i.e., concept-based explanation methods, we show that\nFastCAV can act as a replacement leading to equivalent insights. Hence, our\napproach enables previously infeasible investigations of deep models, which we\ndemonstrate by tracking the evolution of concepts during model training."}
{"id": "2505.17908", "pdf": "https://arxiv.org/pdf/2505.17908", "abs": "https://arxiv.org/abs/2505.17908", "authors": ["Litao Guo", "Xinli Xu", "Luozhou Wang", "Jiantao Lin", "Jinsong Zhou", "Zixin Zhang", "Bolan Su", "Ying-Cong Chen"], "title": "ComfyMind: Toward General-Purpose Generation via Tree-Based Planning and Reactive Feedback", "categories": ["cs.AI", "cs.CV"], "comment": "Project page: https://github.com/LitaoGuo/ComfyMind", "summary": "With the rapid advancement of generative models, general-purpose generation\nhas gained increasing attention as a promising approach to unify diverse tasks\nacross modalities within a single system. Despite this progress, existing\nopen-source frameworks often remain fragile and struggle to support complex\nreal-world applications due to the lack of structured workflow planning and\nexecution-level feedback. To address these limitations, we present ComfyMind, a\ncollaborative AI system designed to enable robust and scalable general-purpose\ngeneration, built on the ComfyUI platform. ComfyMind introduces two core\ninnovations: Semantic Workflow Interface (SWI) that abstracts low-level node\ngraphs into callable functional modules described in natural language, enabling\nhigh-level composition and reducing structural errors; Search Tree Planning\nmechanism with localized feedback execution, which models generation as a\nhierarchical decision process and allows adaptive correction at each stage.\nTogether, these components improve the stability and flexibility of complex\ngenerative workflows. We evaluate ComfyMind on three public benchmarks:\nComfyBench, GenEval, and Reason-Edit, which span generation, editing, and\nreasoning tasks. Results show that ComfyMind consistently outperforms existing\nopen-source baselines and achieves performance comparable to GPT-Image-1.\nComfyMind paves a promising path for the development of open-source\ngeneral-purpose generative AI systems. Project page:\nhttps://github.com/LitaoGuo/ComfyMind"}
{"id": "2505.17912", "pdf": "https://arxiv.org/pdf/2505.17912", "abs": "https://arxiv.org/abs/2505.17912", "authors": ["Luohong Wu", "Matthias Seibold", "Nicola A. Cavalcanti", "Giuseppe Loggia", "Lisa Reissner", "Bastian Sigrist", "Jonas Hein", "Lilian Calvet", "Arnd Viehöfer", "Philipp Fürnstahl"], "title": "UltraBoneUDF: Self-supervised Bone Surface Reconstruction from Ultrasound Based on Neural Unsigned Distance Functions", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Background: Bone surface reconstruction plays a critical role in\ncomputer-assisted orthopedic surgery. Compared to traditional imaging\nmodalities such as CT and MRI, ultrasound offers a radiation-free,\ncost-effective, and portable alternative. Continuous bone surface\nreconstruction can be employed for many clinical applications. However, due to\nthe inherent limitations of ultrasound imaging, B-mode ultrasound typically\ncapture only partial bone surfaces. Existing reconstruction methods struggle\nwith such incomplete data, leading to artifacts and increased reconstruction\nerrors. Effective techniques for accurately reconstructing thin and open bone\nsurfaces from real-world 3D ultrasound volumes remain lacking. Methods: We\npropose UltraBoneUDF, a self-supervised framework designed for reconstructing\nopen bone surfaces from ultrasound using neural Unsigned Distance Functions. To\nenhance reconstruction quality, we introduce a novel global feature extractor\nthat effectively fuses ultrasound-specific image characteristics. Additionally,\nwe present a novel loss function based on local tangent plane optimization that\nsubstantially improves surface reconstruction quality. UltraBoneUDF and\nbaseline models are extensively evaluated on four open-source datasets.\nResults: Qualitative results highlight the limitations of the state-of-the-art\nmethods for open bone surface reconstruction and demonstrate the effectiveness\nof UltraBoneUDF. Quantitatively, UltraBoneUDF significantly outperforms\ncompeting methods across all evaluated datasets for both open and closed bone\nsurface reconstruction in terms of mean Chamfer distance error: 1.10 mm on the\nUltraBones100k dataset (39.6\\% improvement compared to the SOTA), 0.23 mm on\nthe OpenBoneCT dataset (69.3\\% improvement), 0.18 mm on the ClosedBoneCT\ndataset (70.2\\% improvement), and 0.05 mm on the Prostate dataset (55.3\\%\nimprovement)."}
{"id": "2505.17915", "pdf": "https://arxiv.org/pdf/2505.17915", "abs": "https://arxiv.org/abs/2505.17915", "authors": ["Lynn Karam", "Yipei Wang", "Veeru Kasivisvanathan", "Mirabela Rusu", "Yipeng Hu", "Shaheer U. Saeed"], "title": "Promptable cancer segmentation using minimal expert-curated data", "categories": ["eess.IV", "cs.CV"], "comment": "Accepted at Medical Image Understanding and Analysis (MIUA) 2025", "summary": "Automated segmentation of cancer on medical images can aid targeted\ndiagnostic and therapeutic procedures. However, its adoption is limited by the\nhigh cost of expert annotations required for training and inter-observer\nvariability in datasets. While weakly-supervised methods mitigate some\nchallenges, using binary histology labels for training as opposed to requiring\nfull segmentation, they require large paired datasets of histology and images,\nwhich are difficult to curate. Similarly, promptable segmentation aims to allow\nsegmentation with no re-training for new tasks at inference, however, existing\nmodels perform poorly on pathological regions, again necessitating large\ndatasets for training. In this work we propose a novel approach for promptable\nsegmentation requiring only 24 fully-segmented images, supplemented by 8\nweakly-labelled images, for training. Curating this minimal data to a high\nstandard is relatively feasible and thus issues with the cost and variability\nof obtaining labels can be mitigated. By leveraging two classifiers, one\nweakly-supervised and one fully-supervised, our method refines segmentation\nthrough a guided search process initiated by a single-point prompt. Our\napproach outperforms existing promptable segmentation methods, and performs\ncomparably with fully-supervised methods, for the task of prostate cancer\nsegmentation, while using substantially less annotated data (up to 100X less).\nThis enables promptable segmentation with very minimal labelled data, such that\nthe labels can be curated to a very high standard."}
{"id": "2505.17966", "pdf": "https://arxiv.org/pdf/2505.17966", "abs": "https://arxiv.org/abs/2505.17966", "authors": ["Frederik Nolte", "Bernhard Schölkopf", "Ingmar Posner"], "title": "Is Single-View Mesh Reconstruction Ready for Robotics?", "categories": ["cs.RO", "cs.CV", "I.4.5; I.4.8; I.2.9; I.2.10"], "comment": "20 pages, 17 figures", "summary": "This paper evaluates single-view mesh reconstruction models for creating\ndigital twin environments in robot manipulation. Recent advances in computer\nvision for 3D reconstruction from single viewpoints present a potential\nbreakthrough for efficiently creating virtual replicas of physical environments\nfor robotics contexts. However, their suitability for physics simulations and\nrobotics applications remains unexplored. We establish benchmarking criteria\nfor 3D reconstruction in robotics contexts, including handling typical inputs,\nproducing collision-free and stable reconstructions, managing occlusions, and\nmeeting computational constraints. Our empirical evaluation using realistic\nrobotics datasets shows that despite success on computer vision benchmarks,\nexisting approaches fail to meet robotics-specific requirements. We\nquantitively examine limitations of single-view reconstruction for practical\nrobotics implementation, in contrast to prior work that focuses on multi-view\napproaches. Our findings highlight critical gaps between computer vision\nadvances and robotics needs, guiding future research at this intersection."}
{"id": "2505.17971", "pdf": "https://arxiv.org/pdf/2505.17971", "abs": "https://arxiv.org/abs/2505.17971", "authors": ["Danial Khan", "Zohaib Salahuddin", "Yumeng Zhang", "Sheng Kuang", "Shruti Atul Mali", "Henry C. Woodruff", "Sina Amirrajab", "Rachel Cavill", "Eduardo Ibor-Crespo", "Ana Jimenez-Pastor", "Adrian Galiana-Bordera", "Paula Jimenez Gomez", "Luis Marti-Bonmati", "Philippe Lambin"], "title": "Explainable Anatomy-Guided AI for Prostate MRI: Foundation Models and In Silico Clinical Trials for Virtual Biopsy-based Risk Assessment", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "We present a fully automated, anatomically guided deep learning pipeline for\nprostate cancer (PCa) risk stratification using routine MRI. The pipeline\nintegrates three key components: an nnU-Net module for segmenting the prostate\ngland and its zones on axial T2-weighted MRI; a classification module based on\nthe UMedPT Swin Transformer foundation model, fine-tuned on 3D patches with\noptional anatomical priors and clinical data; and a VAE-GAN framework for\ngenerating counterfactual heatmaps that localize decision-driving image\nregions. The system was developed using 1,500 PI-CAI cases for segmentation and\n617 biparametric MRIs with metadata from the CHAIMELEON challenge for\nclassification (split into 70% training, 10% validation, and 20% testing).\nSegmentation achieved mean Dice scores of 0.95 (gland), 0.94 (peripheral zone),\nand 0.92 (transition zone). Incorporating gland priors improved AUC from 0.69\nto 0.72, with a three-scale ensemble achieving top performance (AUC = 0.79,\ncomposite score = 0.76), outperforming the 2024 CHAIMELEON challenge winners.\nCounterfactual heatmaps reliably highlighted lesions within segmented regions,\nenhancing model interpretability. In a prospective multi-center in-silico trial\nwith 20 clinicians, AI assistance increased diagnostic accuracy from 0.72 to\n0.77 and Cohen's kappa from 0.43 to 0.53, while reducing review time per case\nby 40%. These results demonstrate that anatomy-aware foundation models with\ncounterfactual explainability can enable accurate, interpretable, and efficient\nPCa risk assessment, supporting their potential use as virtual biopsies in\nclinical practice."}
{"id": "2505.18028", "pdf": "https://arxiv.org/pdf/2505.18028", "abs": "https://arxiv.org/abs/2505.18028", "authors": ["Zizhao Chen", "Yoav Artzi"], "title": "Knot So Simple: A Minimalistic Environment for Spatial Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.RO"], "comment": null, "summary": "We propose KnotGym, an interactive environment for complex, spatial reasoning\nand manipulation. KnotGym includes goal-oriented rope manipulation tasks with\nvarying levels of complexity, all requiring acting from pure image\nobservations. Tasks are defined along a clear and quantifiable axis of\ncomplexity based on the number of knot crossings, creating a natural\ngeneralization test. KnotGym has a simple observation space, allowing for\nscalable development, yet it highlights core challenges in integrating acute\nperception, spatial reasoning, and grounded manipulation. We evaluate methods\nof different classes, including model-based RL, model-predictive control, and\nchain-of-thought reasoning, and illustrate the challenges KnotGym presents.\nKnotGym is available at https://github.com/lil-lab/knotgym."}
{"id": "2505.18032", "pdf": "https://arxiv.org/pdf/2505.18032", "abs": "https://arxiv.org/abs/2505.18032", "authors": ["Maximilian Mueller", "Matthias Hein"], "title": "Mahalanobis++: Improving OOD Detection via Feature Normalization", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Detecting out-of-distribution (OOD) examples is an important task for\ndeploying reliable machine learning models in safety-critial applications.\nWhile post-hoc methods based on the Mahalanobis distance applied to pre-logit\nfeatures are among the most effective for ImageNet-scale OOD detection, their\nperformance varies significantly across models. We connect this inconsistency\nto strong variations in feature norms, indicating severe violations of the\nGaussian assumption underlying the Mahalanobis distance estimation. We show\nthat simple $\\ell_2$-normalization of the features mitigates this problem\neffectively, aligning better with the premise of normally distributed data with\nshared covariance matrix. Extensive experiments on 44 models across diverse\narchitectures and pretraining schemes show that $\\ell_2$-normalization improves\nthe conventional Mahalanobis distance-based approaches significantly and\nconsistently, and outperforms other recently proposed OOD detection methods."}
{"id": "2505.18058", "pdf": "https://arxiv.org/pdf/2505.18058", "abs": "https://arxiv.org/abs/2505.18058", "authors": ["Yumeng Zhang", "Zohaib Salahuddin", "Danial Khan", "Shruti Atul Mali", "Henry C. Woodruff", "Sina Amirrajab", "Eduardo Ibor-Crespo", "Ana Jimenez-Pastor", "Luis Marti-Bonmati", "Philippe Lambin"], "title": "A Foundation Model Framework for Multi-View MRI Classification of Extramural Vascular Invasion and Mesorectal Fascia Invasion in Rectal Cancer", "categories": ["eess.IV", "cs.CV"], "comment": "22 pages, 8 figures", "summary": "Background: Accurate MRI-based identification of extramural vascular invasion\n(EVI) and mesorectal fascia invasion (MFI) is pivotal for risk-stratified\nmanagement of rectal cancer, yet visual assessment is subjective and vulnerable\nto inter-institutional variability. Purpose: To develop and externally evaluate\na multicenter, foundation-model-driven framework that automatically classifies\nEVI and MFI on axial and sagittal T2-weighted MRI. Methods: This retrospective\nstudy used 331 pre-treatment rectal cancer MRI examinations from three European\nhospitals. After TotalSegmentator-guided rectal patch extraction, a\nself-supervised frequency-domain harmonization pipeline was trained to minimize\nscanner-related contrast shifts. Four classifiers were compared: ResNet50,\nSeResNet, the universal biomedical pretrained transformer (UMedPT) with a\nlightweight MLP head, and a logistic-regression variant using frozen UMedPT\nfeatures (UMedPT_LR). Results: UMedPT_LR achieved the best EVI detection when\naxial and sagittal features were fused (AUC = 0.82; sensitivity = 0.75; F1\nscore = 0.73), surpassing the Chaimeleon Grand-Challenge winner (AUC = 0.74).\nThe highest MFI performance was attained by UMedPT on axial harmonized images\n(AUC = 0.77), surpassing the Chaimeleon Grand-Challenge winner (AUC = 0.75).\nFrequency-domain harmonization improved MFI classification but variably\naffected EVI performance. Conventional CNNs (ResNet50, SeResNet)\nunderperformed, especially in F1 score and balanced accuracy. Conclusion: These\nfindings demonstrate that combining foundation model features, harmonization,\nand multi-view fusion significantly enhances diagnostic performance in rectal\nMRI."}
{"id": "2505.18097", "pdf": "https://arxiv.org/pdf/2505.18097", "abs": "https://arxiv.org/abs/2505.18097", "authors": ["Chun Tong Lei", "Zhongliang Guo", "Hon Chung Lee", "Minh Quoc Duong", "Chun Pong Lau"], "title": "Towards more transferable adversarial attack in black-box manner", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Adversarial attacks have become a well-explored domain, frequently serving as\nevaluation baselines for model robustness. Among these, black-box attacks based\non transferability have received significant attention due to their practical\napplicability in real-world scenarios. Traditional black-box methods have\ngenerally focused on improving the optimization framework (e.g., utilizing\nmomentum in MI-FGSM) to enhance transferability, rather than examining the\ndependency on surrogate white-box model architectures. Recent state-of-the-art\napproach DiffPGD has demonstrated enhanced transferability by employing\ndiffusion-based adversarial purification models for adaptive attacks. The\ninductive bias of diffusion-based adversarial purification aligns naturally\nwith the adversarial attack process, where both involving noise addition,\nreducing dependency on surrogate white-box model selection. However, the\ndenoising process of diffusion models incurs substantial computational costs\nthrough chain rule derivation, manifested in excessive VRAM consumption and\nextended runtime. This progression prompts us to question whether introducing\ndiffusion models is necessary. We hypothesize that a model sharing similar\ninductive bias to diffusion-based adversarial purification, combined with an\nappropriate loss function, could achieve comparable or superior transferability\nwhile dramatically reducing computational overhead. In this paper, we propose a\nnovel loss function coupled with a unique surrogate model to validate our\nhypothesis. Our approach leverages the score of the time-dependent classifier\nfrom classifier-guided diffusion models, effectively incorporating natural data\ndistribution knowledge into the adversarial optimization process. Experimental\nresults demonstrate significantly improved transferability across diverse model\narchitectures while maintaining robustness against diffusion-based defenses."}
{"id": "2505.18107", "pdf": "https://arxiv.org/pdf/2505.18107", "abs": "https://arxiv.org/abs/2505.18107", "authors": ["Yichi Zhang", "Zhihao Duan", "Yuning Huang", "Fengqing Zhu"], "title": "Accelerating Learned Image Compression Through Modeling Neural Training Dynamics", "categories": ["eess.IV", "cs.CV"], "comment": "Accepted to TMLR", "summary": "As learned image compression (LIC) methods become increasingly\ncomputationally demanding, enhancing their training efficiency is crucial. This\npaper takes a step forward in accelerating the training of LIC methods by\nmodeling the neural training dynamics. We first propose a Sensitivity-aware\nTrue and Dummy Embedding Training mechanism (STDET) that clusters LIC model\nparameters into few separate modes where parameters are expressed as affine\ntransformations of reference parameters within the same mode. By further\nutilizing the stable intra-mode correlations throughout training and parameter\nsensitivities, we gradually embed non-reference parameters, reducing the number\nof trainable parameters. Additionally, we incorporate a Sampling-then-Moving\nAverage (SMA) technique, interpolating sampled weights from stochastic gradient\ndescent (SGD) training to obtain the moving average weights, ensuring smooth\ntemporal behavior and minimizing training state variances. Overall, our method\nsignificantly reduces training space dimensions and the number of trainable\nparameters without sacrificing model performance, thus accelerating model\nconvergence. We also provide a theoretical analysis on the Noisy quadratic\nmodel, showing that the proposed method achieves a lower training variance than\nstandard SGD. Our approach offers valuable insights for further developing\nefficient training methods for LICs."}
{"id": "2505.18134", "pdf": "https://arxiv.org/pdf/2505.18134", "abs": "https://arxiv.org/abs/2505.18134", "authors": ["Alex L. Zhang", "Thomas L. Griffiths", "Karthik R. Narasimhan", "Ofir Press"], "title": "VideoGameBench: Can Vision-Language Models complete popular video games?", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": "9 pages, 33 pages including supplementary", "summary": "Vision-language models (VLMs) have achieved strong results on coding and math\nbenchmarks that are challenging for humans, yet their ability to perform tasks\nthat come naturally to humans--such as perception, spatial navigation, and\nmemory management--remains understudied. Real video games are crafted to be\nintuitive for humans to learn and master by leveraging innate inductive biases,\nmaking them an ideal testbed for evaluating such capabilities in VLMs. To this\nend, we introduce VideoGameBench, a benchmark consisting of 10 popular video\ngames from the 1990s that VLMs directly interact with in real-time.\nVideoGameBench challenges models to complete entire games with access to only\nraw visual inputs and a high-level description of objectives and controls, a\nsignificant departure from existing setups that rely on game-specific\nscaffolding and auxiliary information. We keep three of the games secret to\nencourage solutions that generalize to unseen environments. Our experiments\nshow that frontier vision-language models struggle to progress beyond the\nbeginning of each game. We find inference latency to be a major limitation of\nfrontier models in the real-time setting; therefore, we introduce\nVideoGameBench Lite, a setting where the game pauses while waiting for the LM's\nnext action. The best performing model, Gemini 2.5 Pro, completes only 0.48% of\nVideoGameBench and 1.6% of VideoGameBench Lite. We hope that the formalization\nof the human skills mentioned above into this benchmark motivates progress in\nthese research directions."}
{"id": "2505.18151", "pdf": "https://arxiv.org/pdf/2505.18151", "abs": "https://arxiv.org/abs/2505.18151", "authors": ["Zizhang Li", "Hong-Xing Yu", "Wei Liu", "Yin Yang", "Charles Herrmann", "Gordon Wetzstein", "Jiajun Wu"], "title": "WonderPlay: Dynamic 3D Scene Generation from a Single Image and Actions", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": "The first two authors contributed equally. Project website:\n  https://kyleleey.github.io/WonderPlay/", "summary": "WonderPlay is a novel framework integrating physics simulation with video\ngeneration for generating action-conditioned dynamic 3D scenes from a single\nimage. While prior works are restricted to rigid body or simple elastic\ndynamics, WonderPlay features a hybrid generative simulator to synthesize a\nwide range of 3D dynamics. The hybrid generative simulator first uses a physics\nsolver to simulate coarse 3D dynamics, which subsequently conditions a video\ngenerator to produce a video with finer, more realistic motion. The generated\nvideo is then used to update the simulated dynamic 3D scene, closing the loop\nbetween the physics solver and the video generator. This approach enables\nintuitive user control to be combined with the accurate dynamics of\nphysics-based simulators and the expressivity of diffusion-based video\ngenerators. Experimental results demonstrate that WonderPlay enables users to\ninteract with various scenes of diverse content, including cloth, sand, snow,\nliquid, smoke, elastic, and rigid bodies -- all using a single image input.\nCode will be made public. Project website:\nhttps://kyleleey.github.io/WonderPlay/"}
