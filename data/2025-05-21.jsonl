{"id": "2505.13468", "pdf": "https://arxiv.org/pdf/2505.13468", "abs": "https://arxiv.org/abs/2505.13468", "authors": ["Wenxuan Zhang", "Peng Hu"], "title": "An Edge AI Solution for Space Object Detection", "categories": ["cs.CV", "astro-ph.IM", "cs.LG", "cs.SY", "eess.SY"], "comment": "Accepted as poster paper at the 2025 IEEE Canadian Conference on\n  Electrical and Computer Engineering (CCECE)", "summary": "Effective Edge AI for space object detection (SOD) tasks that can facilitate\nreal-time collision assessment and avoidance is essential with the increasing\nspace assets in near-Earth orbits. In SOD, low Earth orbit (LEO) satellites\nmust detect other objects with high precision and minimal delay. We explore an\nEdge AI solution based on deep-learning-based vision sensing for SOD tasks and\npropose a deep learning model based on Squeeze-and-Excitation (SE) layers,\nVision Transformers (ViT), and YOLOv9 framework. We evaluate the performance of\nthese models across various realistic SOD scenarios, demonstrating their\nability to detect multiple satellites with high accuracy and very low latency."}
{"id": "2505.13584", "pdf": "https://arxiv.org/pdf/2505.13584", "abs": "https://arxiv.org/abs/2505.13584", "authors": ["Thangarajah Akilan", "Nusrat Jahan", "Wandong Zhang"], "title": "Self-Supervised Learning for Image Segmentation: A Comprehensive Survey", "categories": ["cs.CV"], "comment": "22 pages, 19 figures, to be submitted for a possible IEEE publication", "summary": "Supervised learning demands large amounts of precisely annotated data to\nachieve promising results. Such data curation is labor-intensive and imposes\nsignificant overhead regarding time and costs. Self-supervised learning (SSL)\npartially overcomes these limitations by exploiting vast amounts of unlabeled\ndata and creating surrogate (pretext or proxy) tasks to learn useful\nrepresentations without manual labeling. As a result, SSL has become a powerful\nmachine learning (ML) paradigm for solving several practical downstream\ncomputer vision problems, such as classification, detection, and segmentation.\nImage segmentation is the cornerstone of many high-level visual perception\napplications, including medical imaging, intelligent transportation,\nagriculture, and surveillance. Although there is substantial research potential\nfor developing advanced algorithms for SSL-based semantic segmentation, a\ncomprehensive study of existing methodologies is essential to trace advances\nand guide emerging researchers. This survey thoroughly investigates over 150\nrecent image segmentation articles, particularly focusing on SSL. It provides a\npractical categorization of pretext tasks, downstream tasks, and commonly used\nbenchmark datasets for image segmentation research. It concludes with key\nobservations distilled from a large body of literature and offers future\ndirections to make this research field more accessible and comprehensible for\nreaders."}
{"id": "2505.13633", "pdf": "https://arxiv.org/pdf/2505.13633", "abs": "https://arxiv.org/abs/2505.13633", "authors": ["Wentao Song", "He Huang", "Youqiang Sun", "Fang Qu", "Jiaqi Zhang", "Longhui Fang", "Yuwei Hao", "Chenyang Peng"], "title": "IPENS:Interactive Unsupervised Framework for Rapid Plant Phenotyping Extraction via NeRF-SAM2 Fusion", "categories": ["cs.CV"], "comment": null, "summary": "Advanced plant phenotyping technologies play a crucial role in targeted trait\nimprovement and accelerating intelligent breeding. Due to the species diversity\nof plants, existing methods heavily rely on large-scale high-precision manually\nannotated data. For self-occluded objects at the grain level, unsupervised\nmethods often prove ineffective. This study proposes IPENS, an interactive\nunsupervised multi-target point cloud extraction method. The method utilizes\nradiance field information to lift 2D masks, which are segmented by SAM2\n(Segment Anything Model 2), into 3D space for target point cloud extraction. A\nmulti-target collaborative optimization strategy is designed to effectively\nresolve the single-interaction multi-target segmentation challenge.\nExperimental validation demonstrates that IPENS achieves a grain-level\nsegmentation accuracy (mIoU) of 63.72% on a rice dataset, with strong\nphenotypic estimation capabilities: grain volume prediction yields R2 = 0.7697\n(RMSE = 0.0025), leaf surface area R2 = 0.84 (RMSE = 18.93), and leaf length\nand width predictions achieve R2 = 0.97 and 0.87 (RMSE = 1.49 and 0.21). On a\nwheat dataset,IPENS further improves segmentation accuracy to 89.68% (mIoU),\nwith equally outstanding phenotypic estimation performance: spike volume\nprediction achieves R2 = 0.9956 (RMSE = 0.0055), leaf surface area R2 = 1.00\n(RMSE = 0.67), and leaf length and width predictions reach R2 = 0.99 and 0.92\n(RMSE = 0.23 and 0.15). This method provides a non-invasive, high-quality\nphenotyping extraction solution for rice and wheat. Without requiring annotated\ndata, it rapidly extracts grain-level point clouds within 3 minutes through\nsimple single-round interactions on images for multiple targets, demonstrating\nsignificant potential to accelerate intelligent breeding efficiency."}
{"id": "2505.13669", "pdf": "https://arxiv.org/pdf/2505.13669", "abs": "https://arxiv.org/abs/2505.13669", "authors": ["Barkin Dagda", "Muhammad Awais", "Saber Fallah"], "title": "GeoVLM: Improving Automated Vehicle Geolocalisation Using Vision-Language Matching", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Cross-view geo-localisation identifies coarse geographical position of an\nautomated vehicle by matching a ground-level image to a geo-tagged satellite\nimage from a database. Despite the advancements in Cross-view geo-localisation,\nsignificant challenges still persist such as similar looking scenes which makes\nit challenging to find the correct match as the top match. Existing approaches\nreach high recall rates but they still fail to rank the correct image as the\ntop match. To address this challenge, this paper proposes GeoVLM, a novel\napproach which uses the zero-shot capabilities of vision language models to\nenable cross-view geo-localisation using interpretable cross-view language\ndescriptions. GeoVLM is a trainable reranking approach which improves the best\nmatch accuracy of cross-view geo-localisation. GeoVLM is evaluated on standard\nbenchmark VIGOR and University-1652 and also through real-life driving\nenvironments using Cross-View United Kingdom, a new benchmark dataset\nintroduced in this paper. The results of the paper show that GeoVLM improves\nretrieval performance of cross-view geo-localisation compared to the\nstate-of-the-art methods with the help of explainable natural language\ndescriptions. The code is available at\nhttps://github.com/CAV-Research-Lab/GeoVLM"}
{"id": "2505.13731", "pdf": "https://arxiv.org/pdf/2505.13731", "abs": "https://arxiv.org/abs/2505.13731", "authors": ["Pengyue Jia", "Seongheon Park", "Song Gao", "Xiangyu Zhao", "Yixuan Li"], "title": "GeoRanker: Distance-Aware Ranking for Worldwide Image Geolocalization", "categories": ["cs.CV"], "comment": null, "summary": "Worldwide image geolocalization-the task of predicting GPS coordinates from\nimages taken anywhere on Earth-poses a fundamental challenge due to the vast\ndiversity in visual content across regions. While recent approaches adopt a\ntwo-stage pipeline of retrieving candidates and selecting the best match, they\ntypically rely on simplistic similarity heuristics and point-wise supervision,\nfailing to model spatial relationships among candidates. In this paper, we\npropose GeoRanker, a distance-aware ranking framework that leverages large\nvision-language models to jointly encode query-candidate interactions and\npredict geographic proximity. In addition, we introduce a multi-order distance\nloss that ranks both absolute and relative distances, enabling the model to\nreason over structured spatial relationships. To support this, we curate\nGeoRanking, the first dataset explicitly designed for geographic ranking tasks\nwith multimodal candidate information. GeoRanker achieves state-of-the-art\nresults on two well-established benchmarks (IM2GPS3K and YFCC4K), significantly\noutperforming current best methods."}
{"id": "2505.13741", "pdf": "https://arxiv.org/pdf/2505.13741", "abs": "https://arxiv.org/abs/2505.13741", "authors": ["Gaspard Goupy", "Pierre Tirilly", "Ioan Marius Bilasco"], "title": "Frozen Backpropagation: Relaxing Weight Symmetry in Temporally-Coded Deep Spiking Neural Networks", "categories": ["cs.CV", "cs.NE"], "comment": null, "summary": "Direct training of Spiking Neural Networks (SNNs) on neuromorphic hardware\ncan greatly reduce energy costs compared to GPU-based training. However,\nimplementing Backpropagation (BP) on such hardware is challenging because\nforward and backward passes are typically performed by separate networks with\ndistinct weights. To compute correct gradients, forward and feedback weights\nmust remain symmetric during training, necessitating weight transport between\nthe two networks. This symmetry requirement imposes hardware overhead and\nincreases energy costs. To address this issue, we introduce Frozen\nBackpropagation (fBP), a BP-based training algorithm relaxing weight symmetry\nin settings with separate networks. fBP updates forward weights by computing\ngradients with periodically frozen feedback weights, reducing weight transports\nduring training and minimizing synchronization overhead. To further improve\ntransport efficiency, we propose three partial weight transport schemes of\nvarying computational complexity, where only a subset of weights is transported\nat a time. We evaluate our methods on image recognition tasks and compare them\nto existing approaches addressing the weight symmetry requirement. Our results\nshow that fBP outperforms these methods and achieves accuracy comparable to BP.\nWith partial weight transport, fBP can substantially lower transport costs by\n1,000x with an accuracy drop of only 0.5pp on CIFAR-10 and 1.1pp on CIFAR-100,\nor by up to 10,000x at the expense of moderated accuracy loss. This work\nprovides insights for guiding the design of neuromorphic hardware incorporating\nBP-based on-chip learning."}
{"id": "2505.13746", "pdf": "https://arxiv.org/pdf/2505.13746", "abs": "https://arxiv.org/abs/2505.13746", "authors": ["Satoshi Kondo"], "title": "ReSW-VL: Representation Learning for Surgical Workflow Analysis Using Vision-Language Model", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Surgical phase recognition from video is a technology that automatically\nclassifies the progress of a surgical procedure and has a wide range of\npotential applications, including real-time surgical support, optimization of\nmedical resources, training and skill assessment, and safety improvement.\nRecent advances in surgical phase recognition technology have focused primarily\non Transform-based methods, although methods that extract spatial features from\nindividual frames using a CNN and video features from the resulting time series\nof spatial features using time series modeling have shown high performance.\nHowever, there remains a paucity of research on training methods for CNNs\nemployed for feature extraction or representation learning in surgical phase\nrecognition. In this study, we propose a method for representation learning in\nsurgical workflow analysis using a vision-language model (ReSW-VL). Our\nproposed method involves fine-tuning the image encoder of a CLIP (Convolutional\nLanguage Image Model) vision-language model using prompt learning for surgical\nphase recognition. The experimental results on three surgical phase recognition\ndatasets demonstrate the effectiveness of the proposed method in comparison to\nconventional methods."}
{"id": "2505.13777", "pdf": "https://arxiv.org/pdf/2505.13777", "abs": "https://arxiv.org/abs/2505.13777", "authors": ["Subash Khanal", "Srikumar Sastry", "Aayush Dhakal", "Adeel Ahmad", "Nathan Jacobs"], "title": "Sat2Sound: A Unified Framework for Zero-Shot Soundscape Mapping", "categories": ["cs.CV", "cs.AI", "cs.SD"], "comment": null, "summary": "We present Sat2Sound, a multimodal representation learning framework for\nsoundscape mapping, designed to predict the distribution of sounds at any\nlocation on Earth. Existing methods for this task rely on satellite image and\npaired geotagged audio samples, which often fail to capture the diversity of\nsound sources at a given location. To address this limitation, we enhance\nexisting datasets by leveraging a Vision-Language Model (VLM) to generate\nsemantically rich soundscape descriptions for locations depicted in satellite\nimages. Our approach incorporates contrastive learning across audio, audio\ncaptions, satellite images, and satellite image captions. We hypothesize that\nthere is a fixed set of soundscape concepts shared across modalities. To this\nend, we learn a shared codebook of soundscape concepts and represent each\nsample as a weighted average of these concepts. Sat2Sound achieves\nstate-of-the-art performance in cross-modal retrieval between satellite image\nand audio on two datasets: GeoSound and SoundingEarth. Additionally, building\non Sat2Sound's ability to retrieve detailed soundscape captions, we introduce a\nnovel application: location-based soundscape synthesis, which enables immersive\nacoustic experiences. Our code and models will be publicly available."}
{"id": "2505.13784", "pdf": "https://arxiv.org/pdf/2505.13784", "abs": "https://arxiv.org/abs/2505.13784", "authors": ["Dinh Nam Pham", "Eleftherios Avramidis"], "title": "Transfer Learning from Visual Speech Recognition to Mouthing Recognition in German Sign Language", "categories": ["cs.CV"], "comment": "Accepted at 19th IEEE International Conference on Automatic Face and\n  Gesture Recognition 2025", "summary": "Sign Language Recognition (SLR) systems primarily focus on manual gestures,\nbut non-manual features such as mouth movements, specifically mouthing, provide\nvaluable linguistic information. This work directly classifies mouthing\ninstances to their corresponding words in the spoken language while exploring\nthe potential of transfer learning from Visual Speech Recognition (VSR) to\nmouthing recognition in German Sign Language. We leverage three VSR datasets:\none in English, one in German with unrelated words and one in German containing\nthe same target words as the mouthing dataset, to investigate the impact of\ntask similarity in this setting. Our results demonstrate that multi-task\nlearning improves both mouthing recognition and VSR accuracy as well as model\nrobustness, suggesting that mouthing recognition should be treated as a\ndistinct but related task to VSR. This research contributes to the field of SLR\nby proposing knowledge transfer from VSR to SLR datasets with limited mouthing\nannotations."}
{"id": "2505.13788", "pdf": "https://arxiv.org/pdf/2505.13788", "abs": "https://arxiv.org/abs/2505.13788", "authors": ["Yongshuo Zong", "Qin Zhang", "Dongsheng An", "Zhihua Li", "Xiang Xu", "Linghan Xu", "Zhuowen Tu", "Yifan Xing", "Onkar Dabeer"], "title": "Ground-V: Teaching VLMs to Ground Complex Instructions in Pixels", "categories": ["cs.CV"], "comment": "Accepted to CVPR'25", "summary": "This work presents a simple yet effective workflow for automatically scaling\ninstruction-following data to elicit pixel-level grounding capabilities of VLMs\nunder complex instructions. In particular, we address five critical real-world\nchallenges in text-instruction-based grounding: hallucinated references,\nmulti-object scenarios, reasoning, multi-granularity, and part-level\nreferences. By leveraging knowledge distillation from a pre-trained teacher\nmodel, our approach generates high-quality instruction-response pairs linked to\nexisting pixel-level annotations, minimizing the need for costly human\nannotation. The resulting dataset, Ground-V, captures rich object localization\nknowledge and nuanced pixel-level referring expressions. Experiment results\nshow that models trained on Ground-V exhibit substantial improvements across\ndiverse grounding tasks. Specifically, incorporating Ground-V during training\ndirectly achieves an average accuracy boost of 4.4% for LISA and a 7.9% for\nPSALM across six benchmarks on the gIoU metric. It also sets new\nstate-of-the-art results on standard benchmarks such as RefCOCO/+/g. Notably,\non gRefCOCO, we achieve an N-Acc of 83.3%, exceeding the previous\nstate-of-the-art by more than 20%."}
{"id": "2505.13812", "pdf": "https://arxiv.org/pdf/2505.13812", "abs": "https://arxiv.org/abs/2505.13812", "authors": ["Zhongyu Chen", "Rong Zhao", "Xie Han", "Xindong Guo", "Song Wang", "Zherui Qiao"], "title": "Physics-Driven Local-Whole Elastic Deformation Modeling for Point Cloud Representation Learning", "categories": ["cs.CV"], "comment": null, "summary": "Existing point cloud representation learning tend to learning the geometric\ndistribution of objects through data-driven approaches, emphasizing structural\nfeatures while overlooking the relationship between the local information and\nthe whole structure. Local features reflect the fine-grained variations of an\nobject, while the whole structure is determined by the interaction and\ncombination of these local features, collectively defining the object's shape.\nIn real-world, objects undergo elastic deformation under external forces, and\nthis deformation gradually affects the whole structure through the propagation\nof forces from local regions, thereby altering the object's geometric\nproperties. Inspired by this, we propose a physics-driven self-supervised\nlearning method for point cloud representation, which captures the relationship\nbetween parts and the whole by constructing a local-whole force propagation\nmechanism. Specifically, we employ a dual-task encoder-decoder framework,\nintegrating the geometric modeling capability of implicit fields with\nphysics-driven elastic deformation. The encoder extracts features from the\npoint cloud and its tetrahedral mesh representation, capturing both geometric\nand physical properties. These features are then fed into two decoders: one\nlearns the whole geometric shape of the point cloud through an implicit field,\nwhile the other predicts local deformations using two specifically designed\nphysics information loss functions, modeling the deformation relationship\nbetween local and whole shapes. Experimental results show that our method\noutperforms existing approaches in object classification, few-shot learning,\nand segmentation, demonstrating its effectiveness."}
{"id": "2505.13817", "pdf": "https://arxiv.org/pdf/2505.13817", "abs": "https://arxiv.org/abs/2505.13817", "authors": ["Feng Li", "Kun Xu", "Zhaoyue Wang", "Yunduan Cui", "Mohammad Masum Billah", "Jia Liu"], "title": "InstanceBEV: Unifying Instance and BEV Representation for Global Modeling", "categories": ["cs.CV"], "comment": null, "summary": "Occupancy Grid Maps are widely used in navigation for their ability to\nrepresent 3D space occupancy. However, existing methods that utilize multi-view\ncameras to construct Occupancy Networks for perception modeling suffer from\ncubic growth in data complexity. Adopting a Bird's-Eye View (BEV) perspective\noffers a more practical solution for autonomous driving, as it provides higher\nsemantic density and mitigates complex object occlusions. Nonetheless,\nBEV-based approaches still require extensive engineering optimizations to\nenable efficient large-scale global modeling. To address this challenge, we\npropose InstanceBEV, the first method to introduce instance-level\ndimensionality reduction for BEV, enabling global modeling with transformers\nwithout relying on sparsification or acceleration operators. Different from\nother BEV methods, our approach directly employs transformers to aggregate\nglobal features. Compared to 3D object detection models, our method samples\nglobal feature maps into 3D space. Experiments on OpenOcc-NuScenes dataset show\nthat InstanceBEV achieves state-of-the-art performance while maintaining a\nsimple, efficient framework without requiring additional optimizations."}
{"id": "2505.13839", "pdf": "https://arxiv.org/pdf/2505.13839", "abs": "https://arxiv.org/abs/2505.13839", "authors": ["Zhenyu Bao", "Qing Li", "Guibiao Liao", "Zhongyuan Zhao", "Kanglin Liu"], "title": "MGStream: Motion-aware 3D Gaussian for Streamable Dynamic Scene Reconstruction", "categories": ["cs.CV"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) has gained significant attention in streamable\ndynamic novel view synthesis (DNVS) for its photorealistic rendering capability\nand computational efficiency. Despite much progress in improving rendering\nquality and optimization strategies, 3DGS-based streamable dynamic scene\nreconstruction still suffers from flickering artifacts and storage\ninefficiency, and struggles to model the emerging objects. To tackle this, we\nintroduce MGStream which employs the motion-related 3D Gaussians (3DGs) to\nreconstruct the dynamic and the vanilla 3DGs for the static. The motion-related\n3DGs are implemented according to the motion mask and the clustering-based\nconvex hull algorithm. The rigid deformation is applied to the motion-related\n3DGs for modeling the dynamic, and the attention-based optimization on the\nmotion-related 3DGs enables the reconstruction of the emerging objects. As the\ndeformation and optimization are only conducted on the motion-related 3DGs,\nMGStream avoids flickering artifacts and improves the storage efficiency.\nExtensive experiments on real-world datasets N3DV and MeetRoom demonstrate that\nMGStream surpasses existing streaming 3DGS-based approaches in terms of\nrendering quality, training/storage efficiency and temporal consistency. Our\ncode is available at: https://github.com/pcl3dv/MGStream."}
{"id": "2505.13856", "pdf": "https://arxiv.org/pdf/2505.13856", "abs": "https://arxiv.org/abs/2505.13856", "authors": ["Ruqin Zhou", "San Jiang", "Wanshou Jiang", "Yongsheng Zhang", "Chenguang Dai"], "title": "SuperMapNet for Long-Range and High-Accuracy Vectorized HD Map Construction", "categories": ["cs.CV"], "comment": "13 pages, 9 figures", "summary": "Vectorized HD map is essential for autonomous driving. Remarkable work has\nbeen achieved in recent years, but there are still major issues: (1) in the\ngeneration of the BEV features, single modality-based methods are of limited\nperception capability, while direct concatenation-based multi-modal methods\nfail to capture synergies and disparities between different modalities,\nresulting in limited ranges with feature holes; (2) in the classification and\nlocalization of map elements, only point information is used without the\nconsideration of element infor-mation and neglects the interaction between\npoint information and element information, leading to erroneous shapes and\nelement entanglement with low accuracy. To address above issues, we introduce\nSuperMapNet for long-range and high-accuracy vectorized HD map construction. It\nuses both camera images and LiDAR point clouds as input, and first tightly\ncouple semantic information from camera images and geometric information from\nLiDAR point clouds by a cross-attention based synergy enhancement module and a\nflow-based disparity alignment module for long-range BEV feature generation.\nAnd then, local features from point queries and global features from element\nqueries are tightly coupled by three-level interactions for high-accuracy\nclassification and localization, where Point2Point interaction learns local\ngeometric information between points of the same element and of each point,\nElement2Element interaction learns relation constraints between different\nelements and semantic information of each elements, and Point2Element\ninteraction learns complement element information for its constituent points.\nExperiments on the nuScenes and Argoverse2 datasets demonstrate superior\nperformances, surpassing SOTAs over 14.9/8.8 mAP and 18.5/3.1 mAP under\nhard/easy settings, respectively. The code is made publicly available1."}
{"id": "2505.13860", "pdf": "https://arxiv.org/pdf/2505.13860", "abs": "https://arxiv.org/abs/2505.13860", "authors": ["Tiancheng Jiang", "Henry Wang", "Md Sirajus Salekin", "Parmida Atighehchian", "Shinan Zhang"], "title": "Domain Adaptation of VLM for Soccer Video Understanding", "categories": ["cs.CV", "cs.AI"], "comment": "8 pages, 5 figures, accepted to the 11th IEEE International Workshop\n  on Computer Vision in Sports (CVSports) at CVPR 2025; supplementary appendix\n  included as ancillary PDF", "summary": "Vision Language Models (VLMs) have demonstrated strong performance in\nmulti-modal tasks by effectively aligning visual and textual representations.\nHowever, most video understanding VLM research has been domain-agnostic,\nleaving the understanding of their transfer learning capability to specialized\ndomains under-explored. In this work, we address this by exploring the\nadaptability of open-source VLMs to specific domains, and focusing on soccer as\nan initial case study. Our approach uses large-scale soccer datasets and LLM to\ncreate instruction-following data, and use them to iteratively fine-tune the\ngeneral-domain VLM in a curriculum learning fashion (first teaching the model\nkey soccer concepts to then question answering tasks). The final adapted model,\ntrained using a curated dataset of 20k video clips, exhibits significant\nimprovement in soccer-specific tasks compared to the base model, with a 37.5%\nrelative improvement for the visual question-answering task and an accuracy\nimprovement from 11.8% to 63.5% for the downstream soccer action classification\ntask."}
{"id": "2505.13905", "pdf": "https://arxiv.org/pdf/2505.13905", "abs": "https://arxiv.org/abs/2505.13905", "authors": ["Ruihan Liu", "Xiaoyi Wu", "Xijun Chen", "Liang Hu", "Yunjiang Lou"], "title": "4D-ROLLS: 4D Radar Occupancy Learning via LiDAR Supervision", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "A comprehensive understanding of 3D scenes is essential for autonomous\nvehicles (AVs), and among various perception tasks, occupancy estimation plays\na central role by providing a general representation of drivable and occupied\nspace. However, most existing occupancy estimation methods rely on LiDAR or\ncameras, which perform poorly in degraded environments such as smoke, rain,\nsnow, and fog. In this paper, we propose 4D-ROLLS, the first weakly supervised\noccupancy estimation method for 4D radar using the LiDAR point cloud as the\nsupervisory signal. Specifically, we introduce a method for generating\npseudo-LiDAR labels, including occupancy queries and LiDAR height maps, as\nmulti-stage supervision to train the 4D radar occupancy estimation model. Then\nthe model is aligned with the occupancy map produced by LiDAR, fine-tuning its\naccuracy in occupancy estimation. Extensive comparative experiments validate\nthe exceptional performance of 4D-ROLLS. Its robustness in degraded\nenvironments and effectiveness in cross-dataset training are qualitatively\ndemonstrated. The model is also seamlessly transferred to downstream tasks BEV\nsegmentation and point cloud occupancy prediction, highlighting its potential\nfor broader applications. The lightweight network enables 4D-ROLLS model to\nachieve fast inference speeds at about 30 Hz on a 4060 GPU. The code of\n4D-ROLLS will be made available at https://github.com/CLASS-Lab/4D-ROLLS."}
{"id": "2505.13915", "pdf": "https://arxiv.org/pdf/2505.13915", "abs": "https://arxiv.org/abs/2505.13915", "authors": ["Chu Chen", "Kangning Cui", "Pasquale Cascarano", "Wei Tang", "Elena Loli Piccolomini", "Raymond H. Chan"], "title": "Blind Restoration of High-Resolution Ultrasound Video", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Ultrasound imaging is widely applied in clinical practice, yet ultrasound\nvideos often suffer from low signal-to-noise ratios (SNR) and limited\nresolutions, posing challenges for diagnosis and analysis. Variations in\nequipment and acquisition settings can further exacerbate differences in data\ndistribution and noise levels, reducing the generalizability of pre-trained\nmodels. This work presents a self-supervised ultrasound video super-resolution\nalgorithm called Deep Ultrasound Prior (DUP). DUP employs a video-adaptive\noptimization process of a neural network that enhances the resolution of given\nultrasound videos without requiring paired training data while simultaneously\nremoving noise. Quantitative and visual evaluations demonstrate that DUP\noutperforms existing super-resolution algorithms, leading to substantial\nimprovements for downstream applications."}
{"id": "2505.13923", "pdf": "https://arxiv.org/pdf/2505.13923", "abs": "https://arxiv.org/abs/2505.13923", "authors": ["Chinedu Emmanuel Mbonu", "Kenechukwu Anigbogu", "Doris Asogwa", "Tochukwu Belonwu"], "title": "An Explorative Analysis of SVM Classifier and ResNet50 Architecture on African Food Classification", "categories": ["cs.CV"], "comment": "7 pages, 9 figures", "summary": "Food recognition systems has advanced significantly for Western cuisines, yet\nits application to African foods remains underexplored. This study addresses\nthis gap by evaluating both deep learning and traditional machine learning\nmethods for African food classification. We compared the performance of a\nfine-tuned ResNet50 model with a Support Vector Machine (SVM) classifier. The\ndataset comprises 1,658 images across six selected food categories that are\nknown in Africa. To assess model effectiveness, we utilize five key evaluation\nmetrics: Confusion matrix, F1-score, accuracy, recall and precision. Our\nfindings offer valuable insights into the strengths and limitations of both\napproaches, contributing to the advancement of food recognition for African\ncuisines."}
{"id": "2505.13928", "pdf": "https://arxiv.org/pdf/2505.13928", "abs": "https://arxiv.org/abs/2505.13928", "authors": ["Qifeng Cai", "Hao Liang", "Hejun Dong", "Meiyi Qiang", "Ruichuan An", "Zhaoyang Han", "Zhengzhou Zhu", "Bin Cui", "Wentao Zhang"], "title": "LoVR: A Benchmark for Long Video Retrieval in Multimodal Contexts", "categories": ["cs.CV", "cs.IR"], "comment": null, "summary": "Long videos contain a vast amount of information, making video-text retrieval\nan essential and challenging task in multimodal learning. However, existing\nbenchmarks suffer from limited video duration, low-quality captions, and coarse\nannotation granularity, which hinder the evaluation of advanced video-text\nretrieval methods. To address these limitations, we introduce LoVR, a benchmark\nspecifically designed for long video-text retrieval. LoVR contains 467 long\nvideos and over 40,804 fine-grained clips with high-quality captions. To\novercome the issue of poor machine-generated annotations, we propose an\nefficient caption generation framework that integrates VLM automatic\ngeneration, caption quality scoring, and dynamic refinement. This pipeline\nimproves annotation accuracy while maintaining scalability. Furthermore, we\nintroduce a semantic fusion method to generate coherent full-video captions\nwithout losing important contextual information. Our benchmark introduces\nlonger videos, more detailed captions, and a larger-scale dataset, presenting\nnew challenges for video understanding and retrieval. Extensive experiments on\nvarious advanced embedding models demonstrate that LoVR is a challenging\nbenchmark, revealing the limitations of current approaches and providing\nvaluable insights for future research. We release the code and dataset link at\nhttps://github.com/TechNomad-ds/LoVR-benchmark"}
{"id": "2505.13943", "pdf": "https://arxiv.org/pdf/2505.13943", "abs": "https://arxiv.org/abs/2505.13943", "authors": ["Samee Arif", "Sualeha Farid"], "title": "Every Pixel Tells a Story: End-to-End Urdu Newspaper OCR", "categories": ["cs.CV"], "comment": null, "summary": "This paper introduces a comprehensive end-to-end pipeline for Optical\nCharacter Recognition (OCR) on Urdu newspapers. In our approach, we address the\nunique challenges of complex multi-column layouts, low-resolution archival\nscans, and diverse font styles. Our process decomposes the OCR task into four\nkey modules: (1) article segmentation, (2) image super-resolution, (3) column\nsegmentation, and (4) text recognition. For article segmentation, we fine-tune\nand evaluate YOLOv11x to identify and separate individual articles from\ncluttered layouts. Our model achieves a precision of 0.963 and mAP@50 of 0.975.\nFor super-resolution, we fine-tune and benchmark the SwinIR model (reaching\n32.71 dB PSNR) to enhance the quality of degraded newspaper scans. To do our\ncolumn segmentation, we use YOLOv11x to separate columns in text to further\nenhance performance - this model reaches a precision of 0.970 and mAP@50 of\n0.975. In the text recognition stage, we benchmark a range of LLMs from\ndifferent families, including Gemini, GPT, Llama, and Claude. The lowest WER of\n0.133 is achieved by Gemini-2.5-Pro."}
{"id": "2505.13997", "pdf": "https://arxiv.org/pdf/2505.13997", "abs": "https://arxiv.org/abs/2505.13997", "authors": ["Huaijie Wang", "De Cheng", "Guozhang Li", "Zhipeng Xu", "Lingfeng He", "Jie Li", "Nannan Wang", "Xinbo Gao"], "title": "StPR: Spatiotemporal Preservation and Routing for Exemplar-Free Video Class-Incremental Learning", "categories": ["cs.CV"], "comment": null, "summary": "Video Class-Incremental Learning (VCIL) seeks to develop models that\ncontinuously learn new action categories over time without forgetting\npreviously acquired knowledge. Unlike traditional Class-Incremental Learning\n(CIL), VCIL introduces the added complexity of spatiotemporal structures,\nmaking it particularly challenging to mitigate catastrophic forgetting while\neffectively capturing both frame-shared semantics and temporal dynamics.\nExisting approaches either rely on exemplar rehearsal, raising concerns over\nmemory and privacy, or adapt static image-based methods that neglect temporal\nmodeling. To address these limitations, we propose Spatiotemporal Preservation\nand Routing (StPR), a unified and exemplar-free VCIL framework that explicitly\ndisentangles and preserves spatiotemporal information. First, we introduce\nFrame-Shared Semantics Distillation (FSSD), which identifies semantically\nstable and meaningful channels by jointly considering semantic sensitivity and\nclassification contribution. These important semantic channels are selectively\nregularized to maintain prior knowledge while allowing for adaptation. Second,\nwe design a Temporal Decomposition-based Mixture-of-Experts (TD-MoE), which\ndynamically routes task-specific experts based on their temporal dynamics,\nenabling inference without task ID or stored exemplars. Together, StPR\neffectively leverages spatial semantics and temporal dynamics, achieving a\nunified, exemplar-free VCIL framework. Extensive experiments on UCF101, HMDB51,\nand Kinetics400 show that our method outperforms existing baselines while\noffering improved interpretability and efficiency in VCIL. Code is available in\nthe supplementary materials."}
{"id": "2505.14008", "pdf": "https://arxiv.org/pdf/2505.14008", "abs": "https://arxiv.org/abs/2505.14008", "authors": ["Zhidan Liu", "Chengtang Yao", "Jiaxi Zeng", "Yuwei Wu", "Yunde Jia"], "title": "Multi-Label Stereo Matching for Transparent Scene Depth Estimation", "categories": ["cs.CV"], "comment": null, "summary": "In this paper, we present a multi-label stereo matching method to\nsimultaneously estimate the depth of the transparent objects and the occluded\nbackground in transparent scenes.Unlike previous methods that assume a unimodal\ndistribution along the disparity dimension and formulate the matching as a\nsingle-label regression problem, we propose a multi-label regression\nformulation to estimate multiple depth values at the same pixel in transparent\nscenes. To resolve the multi-label regression problem, we introduce a\npixel-wise multivariate Gaussian representation, where the mean vector encodes\nmultiple depth values at the same pixel, and the covariance matrix determines\nwhether a multi-label representation is necessary for a given pixel. The\nrepresentation is iteratively predicted within a GRU framework. In each\niteration, we first predict the update step for the mean parameters and then\nuse both the update step and the updated mean parameters to estimate the\ncovariance matrix. We also synthesize a dataset containing 10 scenes and 89\nobjects to validate the performance of transparent scene depth estimation. The\nexperiments show that our method greatly improves the performance on\ntransparent surfaces while preserving the background information for scene\nreconstruction. Code is available at https://github.com/BFZD233/TranScene."}
{"id": "2505.14010", "pdf": "https://arxiv.org/pdf/2505.14010", "abs": "https://arxiv.org/abs/2505.14010", "authors": ["Pu Wang", "Pengwen Dai", "Chen Wu", "Yeying Jin", "Dianjie Lu", "Guijuan Zhang", "Youshan Zhang", "Zhuoran Zheng"], "title": "UHD Image Dehazing via anDehazeFormer with Atmospheric-aware KV Cache", "categories": ["cs.CV"], "comment": "Under review", "summary": "In this paper, we propose an efficient visual transformer framework for\nultra-high-definition (UHD) image dehazing that addresses the key challenges of\nslow training speed and high memory consumption for existing methods. Our\napproach introduces two key innovations: 1) an \\textbf{a}daptive\n\\textbf{n}ormalization mechanism inspired by the nGPT architecture that enables\nultra-fast and stable training with a network with a restricted range of\nparameter expressions; and 2) we devise an atmospheric scattering-aware KV\ncaching mechanism that dynamically optimizes feature preservation based on the\nphysical haze formation model. The proposed architecture improves the training\nconvergence speed by \\textbf{5 $\\times$} while reducing memory overhead,\nenabling real-time processing of 50 high-resolution images per second on an\nRTX4090 GPU. Experimental results show that our approach maintains\nstate-of-the-art dehazing quality while significantly improving computational\nefficiency for 4K/8K image restoration tasks. Furthermore, we provide a new\ndehazing image interpretable method with the help of an integrated gradient\nattribution map. Our code can be found here:\nhttps://anonymous.4open.science/r/anDehazeFormer-632E/README.md."}
{"id": "2505.14014", "pdf": "https://arxiv.org/pdf/2505.14014", "abs": "https://arxiv.org/abs/2505.14014", "authors": ["Zelin Zhang", "Tao Zhang", "KediLI", "Xu Zheng"], "title": "EGFormer: Towards Efficient and Generalizable Multimodal Semantic Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Recent efforts have explored multimodal semantic segmentation using various\nbackbone architectures. However, while most methods aim to improve accuracy,\ntheir computational efficiency remains underexplored. To address this, we\npropose EGFormer, an efficient multimodal semantic segmentation framework that\nflexibly integrates an arbitrary number of modalities while significantly\nreducing model parameters and inference time without sacrificing performance.\nOur framework introduces two novel modules. First, the Any-modal Scoring Module\n(ASM) assigns importance scores to each modality independently, enabling\ndynamic ranking based on their feature maps. Second, the Modal Dropping Module\n(MDM) filters out less informative modalities at each stage, selectively\npreserving and aggregating only the most valuable features. This design allows\nthe model to leverage useful information from all available modalities while\ndiscarding redundancy, thus ensuring high segmentation quality. In addition to\nefficiency, we evaluate EGFormer on a synthetic-to-real transfer task to\ndemonstrate its generalizability. Extensive experiments show that EGFormer\nachieves competitive performance with up to 88 percent reduction in parameters\nand 50 percent fewer GFLOPs. Under unsupervised domain adaptation settings, it\nfurther achieves state-of-the-art transfer performance compared to existing\nmethods."}
{"id": "2505.14028", "pdf": "https://arxiv.org/pdf/2505.14028", "abs": "https://arxiv.org/abs/2505.14028", "authors": ["Ye Wang", "Ruiqi Liu", "Jiang Lin", "Fei Liu", "Zili Yi", "Yilin Wang", "Rui Ma"], "title": "OmniStyle: Filtering High Quality Style Transfer Data at Scale", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "In this paper, we introduce OmniStyle-1M, a large-scale paired style transfer\ndataset comprising over one million content-style-stylized image triplets\nacross 1,000 diverse style categories, each enhanced with textual descriptions\nand instruction prompts. We show that OmniStyle-1M can not only enable\nefficient and scalable of style transfer models through supervised training but\nalso facilitate precise control over target stylization. Especially, to ensure\nthe quality of the dataset, we introduce OmniFilter, a comprehensive style\ntransfer quality assessment framework, which filters high-quality triplets\nbased on content preservation, style consistency, and aesthetic appeal.\nBuilding upon this foundation, we propose OmniStyle, a framework based on the\nDiffusion Transformer (DiT) architecture designed for high-quality and\nefficient style transfer. This framework supports both instruction-guided and\nimage-guided style transfer, generating high resolution outputs with\nexceptional detail. Extensive qualitative and quantitative evaluations\ndemonstrate OmniStyle's superior performance compared to existing approaches,\nhighlighting its efficiency and versatility. OmniStyle-1M and its accompanying\nmethodologies provide a significant contribution to advancing high-quality\nstyle transfer, offering a valuable resource for the research community."}
{"id": "2505.14029", "pdf": "https://arxiv.org/pdf/2505.14029", "abs": "https://arxiv.org/abs/2505.14029", "authors": ["Laura-Sophia von Hirschhausen", "Jannes S. Magnusson", "Mykyta Kovalenko", "Fredrik Boye", "Tanay Rawat", "Peter Eisert", "Anna Hilsmann", "Sebastian Pretzsch", "Sebastian Bosse"], "title": "AppleGrowthVision: A large-scale stereo dataset for phenological analysis, fruit detection, and 3D reconstruction in apple orchards", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Deep learning has transformed computer vision for precision agriculture, yet\napple orchard monitoring remains limited by dataset constraints. The lack of\ndiverse, realistic datasets and the difficulty of annotating dense,\nheterogeneous scenes. Existing datasets overlook different growth stages and\nstereo imagery, both essential for realistic 3D modeling of orchards and tasks\nlike fruit localization, yield estimation, and structural analysis. To address\nthese gaps, we present AppleGrowthVision, a large-scale dataset comprising two\nsubsets. The first includes 9,317 high resolution stereo images collected from\na farm in Brandenburg (Germany), covering six agriculturally validated growth\nstages over a full growth cycle. The second subset consists of 1,125 densely\nannotated images from the same farm in Brandenburg and one in Pillnitz\n(Germany), containing a total of 31,084 apple labels. AppleGrowthVision\nprovides stereo-image data with agriculturally validated growth stages,\nenabling precise phenological analysis and 3D reconstructions. Extending\nMinneApple with our data improves YOLOv8 performance by 7.69 % in terms of\nF1-score, while adding it to MinneApple and MAD boosts Faster R-CNN F1-score by\n31.06 %. Additionally, six BBCH stages were predicted with over 95 % accuracy\nusing VGG16, ResNet152, DenseNet201, and MobileNetv2. AppleGrowthVision bridges\nthe gap between agricultural science and computer vision, by enabling the\ndevelopment of robust models for fruit detection, growth modeling, and 3D\nanalysis in precision agriculture. Future work includes improving annotation,\nenhancing 3D reconstruction, and extending multimodal analysis across all\ngrowth stages."}
{"id": "2505.14043", "pdf": "https://arxiv.org/pdf/2505.14043", "abs": "https://arxiv.org/abs/2505.14043", "authors": ["Qianqian Zhang", "WeiJun Wang", "Yunxing Liu", "Li Zhou", "Hao Zhao", "Junshe An", "Zihan Wang"], "title": "Selective Structured State Space for Multispectral-fused Small Target Detection", "categories": ["cs.CV"], "comment": null, "summary": "Target detection in high-resolution remote sensing imagery faces challenges\ndue to the low recognition accuracy of small targets and high computational\ncosts. The computational complexity of the Transformer architecture increases\nquadratically with image resolution, while Convolutional Neural Networks (CNN)\narchitectures are forced to stack deeper convolutional layers to expand their\nreceptive fields, leading to an explosive growth in computational demands. To\naddress these computational constraints, we leverage Mamba's linear complexity\nfor efficiency. However, Mamba's performance declines for small targets,\nprimarily because small targets occupy a limited area in the image and have\nlimited semantic information. Accurate identification of these small targets\nnecessitates not only Mamba's global attention capabilities but also the\nprecise capture of fine local details. To this end, we enhance Mamba by\ndeveloping the Enhanced Small Target Detection (ESTD) module and the\nConvolutional Attention Residual Gate (CARG) module. The ESTD module bolsters\nlocal attention to capture fine-grained details, while the CARG module, built\nupon Mamba, emphasizes spatial and channel-wise information, collectively\nimproving the model's ability to capture distinctive representations of small\ntargets. Additionally, to highlight the semantic representation of small\ntargets, we design a Mask Enhanced Pixel-level Fusion (MEPF) module for\nmultispectral fusion, which enhances target features by effectively fusing\nvisible and infrared multimodal information."}
{"id": "2505.14049", "pdf": "https://arxiv.org/pdf/2505.14049", "abs": "https://arxiv.org/abs/2505.14049", "authors": ["Yibo Gao", "Hangqi Zhou", "Zheyao Gao", "Bomin Wang", "Shangqi Gao", "Sihan Wang", "Xiahai Zhuang"], "title": "Learning Concept-Driven Logical Rules for Interpretable and Generalizable Medical Image Classification", "categories": ["cs.CV"], "comment": "early accepted by MICCAI 2025", "summary": "The pursuit of decision safety in clinical applications highlights the\npotential of concept-based methods in medical imaging. While these models offer\nactive interpretability, they often suffer from concept leakages, where\nunintended information within soft concept representations undermines both\ninterpretability and generalizability. Moreover, most concept-based models\nfocus solely on local explanations (instance-level), neglecting the global\ndecision logic (dataset-level). To address these limitations, we propose\nConcept Rule Learner (CRL), a novel framework to learn Boolean logical rules\nfrom binarized visual concepts. CRL employs logical layers to capture concept\ncorrelations and extract clinically meaningful rules, thereby providing both\nlocal and global interpretability. Experiments on two medical image\nclassification tasks show that CRL achieves competitive performance with\nexisting methods while significantly improving generalizability to\nout-of-distribution data. The code of our work is available at\nhttps://github.com/obiyoag/crl."}
{"id": "2505.14059", "pdf": "https://arxiv.org/pdf/2505.14059", "abs": "https://arxiv.org/abs/2505.14059", "authors": ["Hao Feng", "Shu Wei", "Xiang Fei", "Wei Shi", "Yingdong Han", "Lei Liao", "Jinghui Lu", "Binghong Wu", "Qi Liu", "Chunhui Lin", "Jingqun Tang", "Hao Liu", "Can Huang"], "title": "Dolphin: Document Image Parsing via Heterogeneous Anchor Prompting", "categories": ["cs.CV"], "comment": "Accepted to ACL 2025", "summary": "Document image parsing is challenging due to its complexly intertwined\nelements such as text paragraphs, figures, formulas, and tables. Current\napproaches either assemble specialized expert models or directly generate\npage-level content autoregressively, facing integration overhead, efficiency\nbottlenecks, and layout structure degradation despite their decent performance.\nTo address these limitations, we present \\textit{Dolphin}\n(\\textit{\\textbf{Do}cument Image \\textbf{P}arsing via \\textbf{H}eterogeneous\nAnchor Prompt\\textbf{in}g}), a novel multimodal document image parsing model\nfollowing an analyze-then-parse paradigm. In the first stage, Dolphin generates\na sequence of layout elements in reading order. These heterogeneous elements,\nserving as anchors and coupled with task-specific prompts, are fed back to\nDolphin for parallel content parsing in the second stage. To train Dolphin, we\nconstruct a large-scale dataset of over 30 million samples, covering\nmulti-granularity parsing tasks. Through comprehensive evaluations on both\nprevalent benchmarks and self-constructed ones, Dolphin achieves\nstate-of-the-art performance across diverse page-level and element-level\nsettings, while ensuring superior efficiency through its lightweight\narchitecture and parallel parsing mechanism. The code and pre-trained models\nare publicly available at https://github.com/ByteDance/Dolphin"}
{"id": "2505.14062", "pdf": "https://arxiv.org/pdf/2505.14062", "abs": "https://arxiv.org/abs/2505.14062", "authors": ["Bo Li", "Haoke Xiao", "Lv Tang"], "title": "Scaling Vision Mamba Across Resolutions via Fractal Traversal", "categories": ["cs.CV"], "comment": "Work in progressing", "summary": "Vision Mamba has recently emerged as a promising alternative to\nTransformer-based architectures, offering linear complexity in sequence length\nwhile maintaining strong modeling capacity. However, its adaptation to visual\ninputs is hindered by challenges in 2D-to-1D patch serialization and weak\nscalability across input resolutions. Existing serialization strategies such as\nraster scanning disrupt local spatial continuity and limit the model's ability\nto generalize across scales. In this paper, we propose FractalMamba++, a robust\nvision backbone that leverages fractal-based patch serialization via Hilbert\ncurves to preserve spatial locality and enable seamless resolution\nadaptability. To address long-range dependency fading in high-resolution\ninputs, we further introduce a Cross-State Routing (CSR) mechanism that\nenhances global context propagation through selective state reuse.\nAdditionally, we propose a Positional-Relation Capture (PRC) module to recover\nlocal adjacency disrupted by curve inflection points. Extensive experiments on\nimage classification, semantic segmentation, object detection, and change\ndetection demonstrate that FractalMamba++ consistently outperforms previous\nMamba-based backbones, particularly under high-resolution settings."}
{"id": "2505.14068", "pdf": "https://arxiv.org/pdf/2505.14068", "abs": "https://arxiv.org/abs/2505.14068", "authors": ["Zhenyu Li", "Tianyi Shang", "Pengjie Xu", "Zhaojun Deng"], "title": "Place Recognition: A Comprehensive Review, Current Challenges and Future Directions", "categories": ["cs.CV"], "comment": "35 pages", "summary": "Place recognition is a cornerstone of vehicle navigation and mapping, which\nis pivotal in enabling systems to determine whether a location has been\npreviously visited. This capability is critical for tasks such as loop closure\nin Simultaneous Localization and Mapping (SLAM) and long-term navigation under\nvarying environmental conditions. In this survey, we comprehensively review\nrecent advancements in place recognition, emphasizing three representative\nmethodological paradigms: Convolutional Neural Network (CNN)-based approaches,\nTransformer-based frameworks, and cross-modal strategies. We begin by\nelucidating the significance of place recognition within the broader context of\nautonomous systems. Subsequently, we trace the evolution of CNN-based methods,\nhighlighting their contributions to robust visual descriptor learning and\nscalability in large-scale environments. We then examine the emerging class of\nTransformer-based models, which leverage self-attention mechanisms to capture\nglobal dependencies and offer improved generalization across diverse scenes.\nFurthermore, we discuss cross-modal approaches that integrate heterogeneous\ndata sources such as Lidar, vision, and text description, thereby enhancing\nresilience to viewpoint, illumination, and seasonal variations. We also\nsummarize standard datasets and evaluation metrics widely adopted in the\nliterature. Finally, we identify current research challenges and outline\nprospective directions, including domain adaptation, real-time performance, and\nlifelong learning, to inspire future advancements in this domain. The unified\nframework of leading-edge place recognition methods, i.e., code library, and\nthe results of their experimental evaluations are available at\nhttps://github.com/CV4RA/SOTA-Place-Recognitioner."}
{"id": "2505.14088", "pdf": "https://arxiv.org/pdf/2505.14088", "abs": "https://arxiv.org/abs/2505.14088", "authors": ["Xi Chen", "Shen Yan", "Juelin Zhu", "Chen Chen", "Yu Liu", "Maojun Zhang"], "title": "Generalizable Multispectral Land Cover Classification via Frequency-Aware Mixture of Low-Rank Token Experts", "categories": ["cs.CV"], "comment": null, "summary": "We introduce Land-MoE, a novel approach for multispectral land cover\nclassification (MLCC). Spectral shift, which emerges from disparities in\nsensors and geospatial conditions, poses a significant challenge in this\ndomain. Existing methods predominantly rely on domain adaptation and\ngeneralization strategies, often utilizing small-scale models that exhibit\nlimited performance. In contrast, Land-MoE addresses these issues by\nhierarchically inserting a Frequency-aware Mixture of Low-rank Token Experts,\nto fine-tune Vision Foundation Models (VFMs) in a parameter-efficient manner.\nSpecifically, Land-MoE comprises two key modules: the mixture of low-rank token\nexperts (MoLTE) and frequency-aware filters (FAF). MoLTE leverages\nrank-differentiated tokens to generate diverse feature adjustments for\nindividual instances within multispectral images. By dynamically combining\nlearnable low-rank token experts of varying ranks, it enhances the robustness\nagainst spectral shifts. Meanwhile, FAF conducts frequency-domain modulation on\nthe refined features. This process enables the model to effectively capture\nfrequency band information that is strongly correlated with semantic essence,\nwhile simultaneously suppressing frequency noise irrelevant to the task.\nComprehensive experiments on MLCC tasks involving cross-sensor and\ncross-geospatial setups demonstrate that Land-MoE outperforms existing methods\nby a large margin. Additionally, the proposed approach has also achieved\nstate-of-the-art performance in domain generalization semantic segmentation\ntasks of RGB remote sensing images."}
{"id": "2505.14100", "pdf": "https://arxiv.org/pdf/2505.14100", "abs": "https://arxiv.org/abs/2505.14100", "authors": ["Qianxiong Xu", "Lanyun Zhu", "Xuanyi Liu", "Guosheng Lin", "Cheng Long", "Ziyue Li", "Rui Zhao"], "title": "Unlocking the Power of SAM 2 for Few-Shot Segmentation", "categories": ["cs.CV"], "comment": "This paper is accepted by ICML'25", "summary": "Few-Shot Segmentation (FSS) aims to learn class-agnostic segmentation on few\nclasses to segment arbitrary classes, but at the risk of overfitting. To\naddress this, some methods use the well-learned knowledge of foundation models\n(e.g., SAM) to simplify the learning process. Recently, SAM 2 has extended SAM\nby supporting video segmentation, whose class-agnostic matching ability is\nuseful to FSS. A simple idea is to encode support foreground (FG) features as\nmemory, with which query FG features are matched and fused. Unfortunately, the\nFG objects in different frames of SAM 2's video data are always the same\nidentity, while those in FSS are different identities, i.e., the matching step\nis incompatible. Therefore, we design Pseudo Prompt Generator to encode pseudo\nquery memory, matching with query features in a compatible way. However, the\nmemories can never be as accurate as the real ones, i.e., they are likely to\ncontain incomplete query FG, and some unexpected query background (BG)\nfeatures, leading to wrong segmentation. Hence, we further design Iterative\nMemory Refinement to fuse more query FG features into the memory, and devise a\nSupport-Calibrated Memory Attention to suppress the unexpected query BG\nfeatures in memory. Extensive experiments have been conducted on PASCAL-5$^i$\nand COCO-20$^i$ to validate the effectiveness of our design, e.g., the 1-shot\nmIoU can be 4.2\\% better than the best baseline."}
{"id": "2505.14105", "pdf": "https://arxiv.org/pdf/2505.14105", "abs": "https://arxiv.org/abs/2505.14105", "authors": ["Zsfia Molnr", "Gergely Szab", "Andrs Horvth"], "title": "Unintended Bias in 2D+ Image Segmentation and Its Effect on Attention Asymmetry", "categories": ["cs.CV"], "comment": null, "summary": "Supervised pretrained models have become widely used in deep learning,\nespecially for image segmentation tasks. However, when applied to specialized\ndatasets such as biomedical imaging, pretrained weights often introduce\nunintended biases. These biases cause models to assign different levels of\nimportance to different slices, leading to inconsistencies in feature\nutilization, which can be observed as asymmetries in saliency map\ndistributions. This transfer of color distributions from natural images to\nnon-natural datasets can compromise model performance and reduce the\nreliability of results. In this study, we investigate the effects of these\nbiases and propose strategies to mitigate them. Through a series of\nexperiments, we test both pretrained and randomly initialized models, comparing\ntheir performance and saliency map distributions. Our proposed methods, which\naim to neutralize the bias introduced by pretrained color channel weights,\ndemonstrate promising results, offering a practical approach to improving model\nexplainability while maintaining the benefits of pretrained models. This\npublication presents our findings, providing insights into addressing\npretrained weight biases across various deep learning tasks."}
{"id": "2505.14113", "pdf": "https://arxiv.org/pdf/2505.14113", "abs": "https://arxiv.org/abs/2505.14113", "authors": ["Bruno Viti", "Elias Karabelas", "Martin Holler"], "title": "CONSIGN: Conformal Segmentation Informed by Spatial Groupings via Decomposition", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Most machine learning-based image segmentation models produce pixel-wise\nconfidence scores - typically derived from softmax outputs - that represent the\nmodel's predicted probability for each class label at every pixel. While this\ninformation can be particularly valuable in high-stakes domains such as medical\nimaging, these (uncalibrated) scores are heuristic in nature and do not\nconstitute rigorous quantitative uncertainty estimates. Conformal prediction\n(CP) provides a principled framework for transforming heuristic confidence\nscores into statistically valid uncertainty estimates. However, applying CP\ndirectly to image segmentation ignores the spatial correlations between pixels,\na fundamental characteristic of image data. This can result in overly\nconservative and less interpretable uncertainty estimates. To address this, we\npropose CONSIGN (Conformal Segmentation Informed by Spatial Groupings via\nDecomposition), a CP-based method that incorporates spatial correlations to\nimprove uncertainty quantification in image segmentation. Our method generates\nmeaningful prediction sets that come with user-specified, high-probability\nerror guarantees. It is compatible with any pre-trained segmentation model\ncapable of generating multiple sample outputs - such as those using dropout,\nBayesian modeling, or ensembles. We evaluate CONSIGN against a standard\npixel-wise CP approach across three medical imaging datasets and two COCO\ndataset subsets, using three different pre-trained segmentation models. Results\ndemonstrate that accounting for spatial structure significantly improves\nperformance across multiple metrics and enhances the quality of uncertainty\nestimates."}
{"id": "2505.14124", "pdf": "https://arxiv.org/pdf/2505.14124", "abs": "https://arxiv.org/abs/2505.14124", "authors": ["Hongjun Choi", "Eun Som Jeon", "Ankita Shukla", "Pavan Turaga"], "title": "Intra-class Patch Swap for Self-Distillation", "categories": ["cs.CV"], "comment": "Accepted for publication in Neurocomputing", "summary": "Knowledge distillation (KD) is a valuable technique for compressing large\ndeep learning models into smaller, edge-suitable networks. However,\nconventional KD frameworks rely on pre-trained high-capacity teacher networks,\nwhich introduce significant challenges such as increased memory/storage\nrequirements, additional training costs, and ambiguity in selecting an\nappropriate teacher for a given student model. Although a teacher-free\ndistillation (self-distillation) has emerged as a promising alternative, many\nexisting approaches still rely on architectural modifications or complex\ntraining procedures, which limit their generality and efficiency.\n  To address these limitations, we propose a novel framework based on\nteacher-free distillation that operates using a single student network without\nany auxiliary components, architectural modifications, or additional learnable\nparameters. Our approach is built on a simple yet highly effective\naugmentation, called intra-class patch swap augmentation. This augmentation\nsimulates a teacher-student dynamic within a single model by generating pairs\nof intra-class samples with varying confidence levels, and then applying\ninstance-to-instance distillation to align their predictive distributions. Our\nmethod is conceptually simple, model-agnostic, and easy to implement, requiring\nonly a single augmentation function. Extensive experiments across image\nclassification, semantic segmentation, and object detection show that our\nmethod consistently outperforms both existing self-distillation baselines and\nconventional teacher-based KD approaches. These results suggest that the\nsuccess of self-distillation could hinge on the design of the augmentation\nitself. Our codes are available at\nhttps://github.com/hchoi71/Intra-class-Patch-Swap."}
{"id": "2505.14135", "pdf": "https://arxiv.org/pdf/2505.14135", "abs": "https://arxiv.org/abs/2505.14135", "authors": ["Ruihuang Li", "Caijin Zhou", "Shoujian Zheng", "Jianxiang Lu", "Jiabin Huang", "Comi Chen", "Junshu Tang", "Guangzheng Xu", "Jiale Tao", "Hongmei Wang", "Donghao Li", "Wenqing Yu", "Senbo Wang", "Zhimin Li", "Yetshuan Shi", "Haoyu Yang", "Yukun Wang", "Wenxun Dai", "Jiaqi Li", "Linqing Wang", "Qixun Wang", "Zhiyong Xu", "Yingfang Zhang", "Jiangfeng Xiong", "Weijie Kong", "Chao Zhang", "Hongxin Zhang", "Qiaoling Zheng", "Weiting Guo", "Xinchi Deng", "Yixuan Li", "Renjia Wei", "Yulin Jian", "Duojun Huang", "Xuhua Ren", "Sihuan Lin", "Yifu Sun", "Yuan Zhou", "Joey Wang", "Qin Lin", "Jingmiao Yu", "Jihong Zhang", "Caesar Zhong", "Di Wang", "Yuhong Liu", "Linus", "Jie Jiang", "Longhuang Wu", "Shuai Shao", "Qinglin Lu"], "title": "Hunyuan-Game: Industrial-grade Intelligent Game Creation Model", "categories": ["cs.CV"], "comment": null, "summary": "Intelligent game creation represents a transformative advancement in game\ndevelopment, utilizing generative artificial intelligence to dynamically\ngenerate and enhance game content. Despite notable progress in generative\nmodels, the comprehensive synthesis of high-quality game assets, including both\nimages and videos, remains a challenging frontier. To create high-fidelity game\ncontent that simultaneously aligns with player preferences and significantly\nboosts designer efficiency, we present Hunyuan-Game, an innovative project\ndesigned to revolutionize intelligent game production. Hunyuan-Game encompasses\ntwo primary branches: image generation and video generation. The image\ngeneration component is built upon a vast dataset comprising billions of game\nimages, leading to the development of a group of customized image generation\nmodels tailored for game scenarios: (1) General Text-to-Image Generation. (2)\nGame Visual Effects Generation, involving text-to-effect and reference\nimage-based game visual effect generation. (3) Transparent Image Generation for\ncharacters, scenes, and game visual effects. (4) Game Character Generation\nbased on sketches, black-and-white images, and white models. The video\ngeneration component is built upon a comprehensive dataset of millions of game\nand anime videos, leading to the development of five core algorithmic models,\neach targeting critical pain points in game development and having robust\nadaptation to diverse game video scenarios: (1) Image-to-Video Generation. (2)\n360 A/T Pose Avatar Video Synthesis. (3) Dynamic Illustration Generation. (4)\nGenerative Video Super-Resolution. (5) Interactive Game Video Generation. These\nimage and video generation models not only exhibit high-level aesthetic\nexpression but also deeply integrate domain-specific knowledge, establishing a\nsystematic understanding of diverse game and anime art styles."}
{"id": "2505.14151", "pdf": "https://arxiv.org/pdf/2505.14151", "abs": "https://arxiv.org/abs/2505.14151", "authors": ["Jiaming Li", "Sheng Wang", "Xin Wang", "Yitao Zhu", "Honglin Xiong", "Zixu Zhuang", "Qian Wang"], "title": "ReactDiff: Latent Diffusion for Facial Reaction Generation", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Given the audio-visual clip of the speaker, facial reaction generation aims\nto predict the listener's facial reactions. The challenge lies in capturing the\nrelevance between video and audio while balancing appropriateness, realism, and\ndiversity. While prior works have mostly focused on uni-modal inputs or\nsimplified reaction mappings, recent approaches such as PerFRDiff have explored\nmulti-modal inputs and the one-to-many nature of appropriate reaction mappings.\nIn this work, we propose the Facial Reaction Diffusion (ReactDiff) framework\nthat uniquely integrates a Multi-Modality Transformer with conditional\ndiffusion in the latent space for enhanced reaction generation. Unlike existing\nmethods, ReactDiff leverages intra- and inter-class attention for fine-grained\nmulti-modal interaction, while the latent diffusion process between the encoder\nand decoder enables diverse yet contextually appropriate outputs. Experimental\nresults demonstrate that ReactDiff significantly outperforms existing\napproaches, achieving a facial reaction correlation of 0.26 and diversity score\nof 0.094 while maintaining competitive realism. The code is open-sourced at\n\\href{https://github.com/Hunan-Tiger/ReactDiff}{github}."}
{"id": "2505.14156", "pdf": "https://arxiv.org/pdf/2505.14156", "abs": "https://arxiv.org/abs/2505.14156", "authors": ["Songhao Wu", "Quan Tu", "Hong Liu", "Jia Xu", "Zhongyi Liu", "Guannan Zhang", "Ran Wang", "Xiuying Chen", "Rui Yan"], "title": "Unify Graph Learning with Text: Unleashing LLM Potentials for Session Search", "categories": ["cs.CV", "cs.AI", "cs.IR", "I.2; H.3.3"], "comment": null, "summary": "Session search involves a series of interactive queries and actions to\nfulfill user's complex information need. Current strategies typically\nprioritize sequential modeling for deep semantic understanding, overlooking the\ngraph structure in interactions. While some approaches focus on capturing\nstructural information, they use a generalized representation for documents,\nneglecting the word-level semantic modeling. In this paper, we propose Symbolic\nGraph Ranker (SGR), which aims to take advantage of both text-based and\ngraph-based approaches by leveraging the power of recent Large Language Models\n(LLMs). Concretely, we first introduce a set of symbolic grammar rules to\nconvert session graph into text. This allows integrating session history,\ninteraction process, and task instruction seamlessly as inputs for the LLM.\nMoreover, given the natural discrepancy between LLMs pre-trained on textual\ncorpora, and the symbolic language we produce using our graph-to-text grammar,\nour objective is to enhance LLMs' ability to capture graph structures within a\ntextual format. To achieve this, we introduce a set of self-supervised symbolic\nlearning tasks including link prediction, node content generation, and\ngenerative contrastive learning, to enable LLMs to capture the topological\ninformation from coarse-grained to fine-grained. Experiment results and\ncomprehensive analysis on two benchmark datasets, AOL and Tiangong-ST, confirm\nthe superiority of our approach. Our paradigm also offers a novel and effective\nmethodology that bridges the gap between traditional search strategies and\nmodern LLMs."}
{"id": "2505.14159", "pdf": "https://arxiv.org/pdf/2505.14159", "abs": "https://arxiv.org/abs/2505.14159", "authors": ["Junjie Li", "Jiawei Wang", "Miyu Li", "Yu Liu", "Yumei Wang", "Haitao Xu"], "title": "M3Depth: Wavelet-Enhanced Depth Estimation on Mars via Mutual Boosting of Dual-Modal Data", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Depth estimation plays a great potential role in obstacle avoidance and\nnavigation for further Mars exploration missions. Compared to traditional\nstereo matching, learning-based stereo depth estimation provides a data-driven\napproach to infer dense and precise depth maps from stereo image pairs.\nHowever, these methods always suffer performance degradation in environments\nwith sparse textures and lacking geometric constraints, such as the\nunstructured terrain of Mars. To address these challenges, we propose M3Depth,\na depth estimation model tailored for Mars rovers. Considering the sparse and\nsmooth texture of Martian terrain, which is primarily composed of low-frequency\nfeatures, our model incorporates a convolutional kernel based on wavelet\ntransform that effectively captures low-frequency response and expands the\nreceptive field. Additionally, we introduce a consistency loss that explicitly\nmodels the complementary relationship between depth map and surface normal map,\nutilizing the surface normal as a geometric constraint to enhance the accuracy\nof depth estimation. Besides, a pixel-wise refinement module with mutual\nboosting mechanism is designed to iteratively refine both depth and surface\nnormal predictions. Experimental results on synthetic Mars datasets with depth\nannotations show that M3Depth achieves a significant 16% improvement in depth\nestimation accuracy compared to other state-of-the-art methods in depth\nestimation. Furthermore, the model demonstrates strong applicability in\nreal-world Martian scenarios, offering a promising solution for future Mars\nexploration missions."}
{"id": "2505.14167", "pdf": "https://arxiv.org/pdf/2505.14167", "abs": "https://arxiv.org/abs/2505.14167", "authors": ["Changgu Chen", "Xiaoyan Yang", "Junwei Shu", "Changbo Wang", "Yang Li"], "title": "LMP: Leveraging Motion Prior in Zero-Shot Video Generation with Diffusion Transformer", "categories": ["cs.CV"], "comment": null, "summary": "In recent years, large-scale pre-trained diffusion transformer models have\nmade significant progress in video generation. While current DiT models can\nproduce high-definition, high-frame-rate, and highly diverse videos, there is a\nlack of fine-grained control over the video content. Controlling the motion of\nsubjects in videos using only prompts is challenging, especially when it comes\nto describing complex movements. Further, existing methods fail to control the\nmotion in image-to-video generation, as the subject in the reference image\noften differs from the subject in the reference video in terms of initial\nposition, size, and shape. To address this, we propose the Leveraging Motion\nPrior (LMP) framework for zero-shot video generation. Our framework harnesses\nthe powerful generative capabilities of pre-trained diffusion transformers to\nenable motion in the generated videos to reference user-provided motion videos\nin both text-to-video and image-to-video generation. To this end, we first\nintroduce a foreground-background disentangle module to distinguish between\nmoving subjects and backgrounds in the reference video, preventing interference\nin the target video generation. A reweighted motion transfer module is designed\nto allow the target video to reference the motion from the reference video. To\navoid interference from the subject in the reference video, we propose an\nappearance separation module to suppress the appearance of the reference\nsubject in the target video. We annotate the DAVIS dataset with detailed\nprompts for our experiments and design evaluation metrics to validate the\neffectiveness of our method. Extensive experiments demonstrate that our\napproach achieves state-of-the-art performance in generation quality,\nprompt-video consistency, and control capability. Our homepage is available at\nhttps://vpx-ecnu.github.io/LMP-Website/"}
{"id": "2505.14197", "pdf": "https://arxiv.org/pdf/2505.14197", "abs": "https://arxiv.org/abs/2505.14197", "authors": ["Xinshen Zhang", "Zhen Ye", "Xu Zheng"], "title": "Towards Omnidirectional Reasoning with 360-R1: A Dataset, Benchmark, and GRPO-based Method", "categories": ["cs.CV"], "comment": null, "summary": "Omnidirectional images (ODIs), with their 360{\\deg} field of view, provide\nunparalleled spatial awareness for immersive applications like augmented\nreality and embodied AI. However, the capability of existing multi-modal large\nlanguage models (MLLMs) to comprehend and reason about such panoramic scenes\nremains underexplored. This paper addresses this gap by introducing OmniVQA,\nthe first dataset and conducting the first benchmark for omnidirectional visual\nquestion answering. Our evaluation of state-of-the-art MLLMs reveals\nsignificant limitations in handling omnidirectional visual question answering,\nhighlighting persistent challenges in object localization, feature extraction,\nand hallucination suppression within panoramic contexts. These results\nunderscore the disconnect between current MLLM capabilities and the demands of\nomnidirectional visual understanding, which calls for dedicated architectural\nor training innovations tailored to 360{\\deg} imagery. Building on the OmniVQA\ndataset and benchmark, we further introduce a rule-based reinforcement learning\nmethod, 360-R1, based on Qwen2.5-VL-Instruct. Concretely, we modify the group\nrelative policy optimization (GRPO) by proposing three novel reward functions:\n(1) reasoning process similarity reward, (2) answer semantic accuracy reward,\nand (3) structured format compliance reward. Extensive experiments on our\nOmniVQA demonstrate the superiority of our proposed method in omnidirectional\nspace (+6% improvement)."}
{"id": "2505.14204", "pdf": "https://arxiv.org/pdf/2505.14204", "abs": "https://arxiv.org/abs/2505.14204", "authors": ["Yang Hu", "Runchen Wang", "Stephen Chong Zhao", "Xuhui Zhan", "Do Hun Kim", "Mark Wallace", "David A. Tovar"], "title": "Beginning with You: Perceptual-Initialization Improves Vision-Language Representation and Alignment", "categories": ["cs.CV", "q-bio.NC"], "comment": "10 pages, 5 figures, 2 tables", "summary": "We introduce Perceptual-Initialization (PI), a paradigm shift in visual\nrepresentation learning that incorporates human perceptual structure during the\ninitialization phase rather than as a downstream fine-tuning step. By\nintegrating human-derived triplet embeddings from the NIGHTS dataset to\ninitialize a CLIP vision encoder, followed by self-supervised learning on\nYFCC15M, our approach demonstrates significant zero-shot performance\nimprovements, without any task-specific fine-tuning, across 29 zero shot\nclassification and 2 retrieval benchmarks. On ImageNet-1K, zero-shot gains\nemerge after approximately 15 epochs of pretraining. Benefits are observed\nacross datasets of various scales, with improvements manifesting at different\nstages of the pretraining process depending on dataset characteristics. Our\napproach consistently enhances zero-shot top-1 accuracy, top-5 accuracy, and\nretrieval recall (e.g., R@1, R@5) across these diverse evaluation tasks,\nwithout requiring any adaptation to target domains. These findings challenge\nthe conventional wisdom of using human-perceptual data primarily for\nfine-tuning and demonstrate that embedding human perceptual structure during\nearly representation learning yields more capable and vision-language aligned\nsystems that generalize immediately to unseen tasks. Our work shows that\n\"beginning with you\", starting with human perception, provides a stronger\nfoundation for general-purpose vision-language intelligence."}
{"id": "2505.14218", "pdf": "https://arxiv.org/pdf/2505.14218", "abs": "https://arxiv.org/abs/2505.14218", "authors": ["Jie Li", "Shengwei Tian", "Long Yu", "Xin Ning"], "title": "Flexible-weighted Chamfer Distance: Enhanced Objective Function for Point Cloud Completion", "categories": ["cs.CV"], "comment": null, "summary": "Chamfer Distance (CD) comprises two components that can evaluate the global\ndistribution and local performance of generated point clouds, making it widely\nutilized as a similarity measure between generated and target point clouds in\npoint cloud completion tasks. Additionally, CD's computational efficiency has\nled to its frequent application as an objective function for guiding point\ncloud generation. However, using CD directly as an objective function with\nfixed equal weights for its two components can often result in seemingly high\noverall performance (i.e., low CD score), while failing to achieve a good\nglobal distribution. This is typically reflected in high Earth Mover's Distance\n(EMD) and Decomposed Chamfer Distance (DCD) scores, alongside poor human\nassessments. To address this issue, we propose a Flexible-Weighted Chamfer\nDistance (FCD) to guide point cloud generation. FCD assigns a higher weight to\nthe global distribution component of CD and incorporates a flexible weighting\nstrategy to adjust the balance between the two components, aiming to improve\nglobal distribution while maintaining robust overall performance. Experimental\nresults on two state-of-the-art networks demonstrate that our method achieves\nsuperior results across multiple evaluation metrics, including CD, EMD, DCD,\nand F-Score, as well as in human evaluations."}
{"id": "2505.14227", "pdf": "https://arxiv.org/pdf/2505.14227", "abs": "https://arxiv.org/abs/2505.14227", "authors": ["Luyang Jiang", "Jianing An", "Jie Luo", "Wenjun Wu", "Lei Huang"], "title": "VoQA: Visual-only Question Answering", "categories": ["cs.CV", "cs.AI"], "comment": "18 pages", "summary": "We propose Visual-only Question Answering (VoQA), a novel multimodal task in\nwhich questions are visually embedded within images, without any accompanying\ntextual input. This requires models to locate, recognize, and reason over\nvisually embedded textual questions, posing challenges for existing large\nvision-language models (LVLMs), which show notable performance drops even with\ncarefully designed prompts. To bridge this gap, we introduce Guided Response\nTriggering Supervised Fine-tuning (GRT-SFT), a structured fine-tuning strategy\nthat guides the model to perform step-by-step reasoning purely based on visual\ninput, significantly improving model performance. Our work enhances models'\ncapacity for human-like visual understanding in complex multimodal scenarios,\nwhere information, including language, is perceived visually."}
{"id": "2505.14231", "pdf": "https://arxiv.org/pdf/2505.14231", "abs": "https://arxiv.org/abs/2505.14231", "authors": ["Sule Bai", "Mingxing Li", "Yong Liu", "Jing Tang", "Haoji Zhang", "Lei Sun", "Xiangxiang Chu", "Yansong Tang"], "title": "UniVG-R1: Reasoning Guided Universal Visual Grounding with Reinforcement Learning", "categories": ["cs.CV"], "comment": null, "summary": "Traditional visual grounding methods primarily focus on single-image\nscenarios with simple textual references. However, extending these methods to\nreal-world scenarios that involve implicit and complex instructions,\nparticularly in conjunction with multiple images, poses significant challenges,\nwhich is mainly due to the lack of advanced reasoning ability across diverse\nmulti-modal contexts. In this work, we aim to address the more practical\nuniversal grounding task, and propose UniVG-R1, a reasoning guided multimodal\nlarge language model (MLLM) for universal visual grounding, which enhances\nreasoning capabilities through reinforcement learning (RL) combined with\ncold-start data. Specifically, we first construct a high-quality\nChain-of-Thought (CoT) grounding dataset, annotated with detailed reasoning\nchains, to guide the model towards correct reasoning paths via supervised\nfine-tuning. Subsequently, we perform rule-based reinforcement learning to\nencourage the model to identify correct reasoning chains, thereby incentivizing\nits reasoning capabilities. In addition, we identify a difficulty bias arising\nfrom the prevalence of easy samples as RL training progresses, and we propose a\ndifficulty-aware weight adjustment strategy to further strengthen the\nperformance. Experimental results demonstrate the effectiveness of UniVG-R1,\nwhich achieves state-of-the-art performance on MIG-Bench with a 9.1%\nimprovement over the previous method. Furthermore, our model exhibits strong\ngeneralizability, achieving an average improvement of 23.4% in zero-shot\nperformance across four image and video reasoning grounding benchmarks. The\nproject page can be accessed at https://amap-ml.github.io/UniVG-R1-page/."}
{"id": "2505.14239", "pdf": "https://arxiv.org/pdf/2505.14239", "abs": "https://arxiv.org/abs/2505.14239", "authors": ["Bin-Bin Gao", "Xiaochen Chen", "Zhongyi Huang", "Congchong Nie", "Jun Liu", "Jinxiang Lai", "Guannan Jiang", "Xi Wang", "Chengjie Wang"], "title": "Decoupling Classifier for Boosting Few-shot Object Detection and Instance Segmentation", "categories": ["cs.CV"], "comment": "Accepted by NeurIPS 2022", "summary": "This paper focus on few-shot object detection~(FSOD) and instance\nsegmentation~(FSIS), which requires a model to quickly adapt to novel classes\nwith a few labeled instances. The existing methods severely suffer from bias\nclassification because of the missing label issue which naturally exists in an\ninstance-level few-shot scenario and is first formally proposed by us. Our\nanalysis suggests that the standard classification head of most FSOD or FSIS\nmodels needs to be decoupled to mitigate the bias classification. Therefore, we\npropose an embarrassingly simple but effective method that decouples the\nstandard classifier into two heads. Then, these two individual heads are\ncapable of independently addressing clear positive samples and noisy negative\nsamples which are caused by the missing label. In this way, the model can\neffectively learn novel classes while mitigating the effects of noisy negative\nsamples. Without bells and whistles, our model without any additional\ncomputation cost and parameters consistently outperforms its baseline and\nstate-of-the-art by a large margin on PASCAL VOC and MS-COCO benchmarks for\nFSOD and FSIS tasks. The Code is available at\nhttps://csgaobb.github.io/Projects/DCFS."}
{"id": "2505.14246", "pdf": "https://arxiv.org/pdf/2505.14246", "abs": "https://arxiv.org/abs/2505.14246", "authors": ["Ziyu Liu", "Yuhang Zang", "Yushan Zou", "Zijian Liang", "Xiaoyi Dong", "Yuhang Cao", "Haodong Duan", "Dahua Lin", "Jiaqi Wang"], "title": "Visual Agentic Reinforcement Fine-Tuning", "categories": ["cs.CV", "cs.AI"], "comment": "project url:\n  https://github.com/Liuziyu77/Visual-RFT/tree/main/Visual-ARFT", "summary": "A key trend in Large Reasoning Models (e.g., OpenAI's o3) is the native\nagentic ability to use external tools such as web browsers for searching and\nwriting/executing code for image manipulation to think with images. In the\nopen-source research community, while significant progress has been made in\nlanguage-only agentic abilities such as function calling and tool integration,\nthe development of multi-modal agentic capabilities that involve truly thinking\nwith images, and their corresponding benchmarks, are still less explored. This\nwork highlights the effectiveness of Visual Agentic Reinforcement Fine-Tuning\n(Visual-ARFT) for enabling flexible and adaptive reasoning abilities for Large\nVision-Language Models (LVLMs). With Visual-ARFT, open-source LVLMs gain the\nability to browse websites for real-time information updates and write code to\nmanipulate and analyze input images through cropping, rotation, and other image\nprocessing techniques. We also present a Multi-modal Agentic Tool Bench (MAT)\nwith two settings (MAT-Search and MAT-Coding) designed to evaluate LVLMs'\nagentic search and coding abilities. Our experimental results demonstrate that\nVisual-ARFT outperforms its baseline by +18.6% F1 / +13.0% EM on MAT-Coding and\n+10.3% F1 / +8.7% EM on MAT-Search, ultimately surpassing GPT-4o. Visual-ARFT\nalso achieves +29.3 F1% / +25.9% EM gains on existing multi-hop QA benchmarks\nsuch as 2Wiki and HotpotQA, demonstrating strong generalization capabilities.\nOur findings suggest that Visual-ARFT offers a promising path toward building\nrobust and generalizable multimodal agents."}
{"id": "2505.14254", "pdf": "https://arxiv.org/pdf/2505.14254", "abs": "https://arxiv.org/abs/2505.14254", "authors": ["Yuanyuan Chang", "Yinghua Yao", "Tao Qin", "Mengmeng Wang", "Ivor Tsang", "Guang Dai"], "title": "Instructing Text-to-Image Diffusion Models via Classifier-Guided Semantic Optimization", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-image diffusion models have emerged as powerful tools for\nhigh-quality image generation and editing. Many existing approaches rely on\ntext prompts as editing guidance. However, these methods are constrained by the\nneed for manual prompt crafting, which can be time-consuming, introduce\nirrelevant details, and significantly limit editing performance. In this work,\nwe propose optimizing semantic embeddings guided by attribute classifiers to\nsteer text-to-image models toward desired edits, without relying on text\nprompts or requiring any training or fine-tuning of the diffusion model. We\nutilize classifiers to learn precise semantic embeddings at the dataset level.\nThe learned embeddings are theoretically justified as the optimal\nrepresentation of attribute semantics, enabling disentangled and accurate\nedits. Experiments further demonstrate that our method achieves high levels of\ndisentanglement and strong generalization across different domains of data."}
{"id": "2505.14257", "pdf": "https://arxiv.org/pdf/2505.14257", "abs": "https://arxiv.org/abs/2505.14257", "authors": ["Jianfei Zhao", "Feng Zhang", "Xin Sun", "Chong Feng"], "title": "Aligning Attention Distribution to Information Flow for Hallucination Mitigation in Large Vision-Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Due to the unidirectional masking mechanism, Decoder-Only models propagate\ninformation from left to right. LVLMs (Large Vision-Language Models) follow the\nsame architecture, with visual information gradually integrated into semantic\nrepresentations during forward propagation. Through systematic analysis, we\nobserve that over 80\\% of the visual information is absorbed into the semantic\nrepresentations. However, the model's attention still predominantly focuses on\nthe visual representations. This misalignment between the attention\ndistribution and the actual information flow undermines the model's visual\nunderstanding ability and contributes to hallucinations. To address this issue,\nwe enhance the model's visual understanding by leveraging the core information\nembedded in semantic representations. Specifically, we identify attention heads\nthat focus on core semantic representations based on their attention\ndistributions. Then, through a two-stage optimization paradigm, we propagate\nthe advantages of these attention heads across the entire model, aligning the\nattention distribution with the actual information flow. We evaluate our method\non three image captioning benchmarks using five different LVLMs, demonstrating\nits effectiveness in significantly reducing hallucinations. Further experiments\nreveal a trade-off between reduced hallucinations and richer details. Notably,\nour method allows for manual adjustment of the model's conservativeness,\nenabling flexible control to meet diverse real-world requirements. Code will be\nreleased once accepted."}
{"id": "2505.14260", "pdf": "https://arxiv.org/pdf/2505.14260", "abs": "https://arxiv.org/abs/2505.14260", "authors": ["Luxi Lin", "Zhihang Lin", "Zhanpeng Zeng", "Rongrong Ji"], "title": "Speculative Decoding Reimagined for Multimodal Large Language Models", "categories": ["cs.CV", "cs.AI"], "comment": "12 pages", "summary": "This paper introduces Multimodal Speculative Decoding (MSD) to accelerate\nMultimodal Large Language Models (MLLMs) inference. Speculative decoding has\nbeen shown to accelerate Large Language Models (LLMs) without sacrificing\naccuracy. However, current speculative decoding methods for MLLMs fail to\nachieve the same speedup as they do for LLMs. To address this, we reimagine\nspeculative decoding specifically for MLLMs. Our analysis of MLLM\ncharacteristics reveals two key design principles for MSD: (1) Text and visual\ntokens have fundamentally different characteristics and need to be processed\nseparately during drafting. (2) Both language modeling ability and visual\nperception capability are crucial for the draft model. For the first principle,\nMSD decouples text and visual tokens in the draft model, allowing each to be\nhandled based on its own characteristics. For the second principle, MSD uses a\ntwo-stage training strategy: In stage one, the draft model is trained on\ntext-only instruction-tuning datasets to improve its language modeling ability.\nIn stage two, MSD gradually introduces multimodal data to enhance the visual\nperception capability of the draft model. Experiments show that MSD boosts\ninference speed by up to $2.29\\times$ for LLaVA-1.5-7B and up to $2.46\\times$\nfor LLaVA-1.5-13B on multimodal benchmarks, demonstrating its effectiveness.\nOur code is available at https://github.com/Lyn-Lucy/MSD."}
{"id": "2505.14270", "pdf": "https://arxiv.org/pdf/2505.14270", "abs": "https://arxiv.org/abs/2505.14270", "authors": ["Yoorhim Cho", "Hongyeob Kim", "Semin Kim", "Youjia Zhang", "Yunseok Choi", "Sungeun Hong"], "title": "RA-Touch: Retrieval-Augmented Touch Understanding with Enriched Visual Data", "categories": ["cs.CV"], "comment": null, "summary": "Visuo-tactile perception aims to understand an object's tactile properties,\nsuch as texture, softness, and rigidity. However, the field remains\nunderexplored because collecting tactile data is costly and labor-intensive. We\nobserve that visually distinct objects can exhibit similar surface textures or\nmaterial properties. For example, a leather sofa and a leather jacket have\ndifferent appearances but share similar tactile properties. This implies that\ntactile understanding can be guided by material cues in visual data, even\nwithout direct tactile supervision. In this paper, we introduce RA-Touch, a\nretrieval-augmented framework that improves visuo-tactile perception by\nleveraging visual data enriched with tactile semantics. We carefully recaption\na large-scale visual dataset with tactile-focused descriptions, enabling the\nmodel to access tactile semantics typically absent from conventional visual\ndatasets. A key challenge remains in effectively utilizing these tactile-aware\nexternal descriptions. RA-Touch addresses this by retrieving visual-textual\nrepresentations aligned with tactile inputs and integrating them to focus on\nrelevant textural and material properties. By outperforming prior methods on\nthe TVL benchmark, our method demonstrates the potential of retrieval-based\nvisual reuse for tactile understanding. Code is available at\nhttps://aim-skku.github.io/RA-Touch"}
{"id": "2505.14296", "pdf": "https://arxiv.org/pdf/2505.14296", "abs": "https://arxiv.org/abs/2505.14296", "authors": ["Abdul-Kazeem Shamba"], "title": "Towards Generating Realistic Underwater Images", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "This paper explores the use of contrastive learning and generative\nadversarial networks for generating realistic underwater images from synthetic\nimages with uniform lighting. We investigate the performance of image\ntranslation models for generating realistic underwater images using the VAROS\ndataset. Two key evaluation metrics, Fr\\'echet Inception Distance (FID) and\nStructural Similarity Index Measure (SSIM), provide insights into the\ntrade-offs between perceptual quality and structural preservation. For paired\nimage translation, pix2pix achieves the best FID scores due to its paired\nsupervision and PatchGAN discriminator, while the autoencoder model attains the\nhighest SSIM, suggesting better structural fidelity despite producing blurrier\noutputs. Among unpaired methods, CycleGAN achieves a competitive FID score by\nleveraging cycle-consistency loss, whereas CUT, which replaces\ncycle-consistency with contrastive learning, attains higher SSIM, indicating\nimproved spatial similarity retention. Notably, incorporating depth information\ninto CUT results in the lowest overall FID score, demonstrating that depth cues\nenhance realism. However, the slight decrease in SSIM suggests that depth-aware\nlearning may introduce structural variations."}
{"id": "2505.14298", "pdf": "https://arxiv.org/pdf/2505.14298", "abs": "https://arxiv.org/abs/2505.14298", "authors": ["Fulong Yao", "Wenju Zhou", "Huosheng Hu"], "title": "A Review of Vision-Based Assistive Systems for Visually Impaired People: Technologies, Applications, and Future Directions", "categories": ["cs.CV"], "comment": null, "summary": "Visually impaired individuals rely heavily on accurate and timely information\nabout obstacles and their surrounding environments to achieve independent\nliving. In recent years, significant progress has been made in the development\nof assistive technologies, particularly vision-based systems, that enhance\nmobility and facilitate interaction with the external world in both indoor and\noutdoor settings. This paper presents a comprehensive review of recent advances\nin assistive systems designed for the visually impaired, with a focus on\nstate-of-the-art technologies in obstacle detection, navigation, and user\ninteraction. In addition, emerging trends and future directions in visual\nguidance systems are discussed."}
{"id": "2505.14318", "pdf": "https://arxiv.org/pdf/2505.14318", "abs": "https://arxiv.org/abs/2505.14318", "authors": ["Wenjun Hou", "Yi Cheng", "Kaishuai Xu", "Heng Li", "Yan Hu", "Wenjie Li", "Jiang Liu"], "title": "RADAR: Enhancing Radiology Report Generation with Supplementary Knowledge Injection", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\nvarious domains, including radiology report generation. Previous approaches\nhave attempted to utilize multimodal LLMs for this task, enhancing their\nperformance through the integration of domain-specific knowledge retrieval.\nHowever, these approaches often overlook the knowledge already embedded within\nthe LLMs, leading to redundant information integration and inefficient\nutilization of learned representations. To address this limitation, we propose\nRADAR, a framework for enhancing radiology report generation with supplementary\nknowledge injection. RADAR improves report generation by systematically\nleveraging both the internal knowledge of an LLM and externally retrieved\ninformation. Specifically, it first extracts the model's acquired knowledge\nthat aligns with expert image-based classification outputs. It then retrieves\nrelevant supplementary knowledge to further enrich this information. Finally,\nby aggregating both sources, RADAR generates more accurate and informative\nradiology reports. Extensive experiments on MIMIC-CXR, CheXpert-Plus, and IU\nX-ray demonstrate that our model outperforms state-of-the-art LLMs in both\nlanguage quality and clinical accuracy"}
{"id": "2505.14319", "pdf": "https://arxiv.org/pdf/2505.14319", "abs": "https://arxiv.org/abs/2505.14319", "authors": ["Weihao Xia", "Chenliang Zhou", "Cengiz Oztireli"], "title": "RETRO: REthinking Tactile Representation Learning with Material PriOrs", "categories": ["cs.CV", "cs.MM"], "comment": "Code: https://github.com/weihaox/RETRO", "summary": "Tactile perception is profoundly influenced by the surface properties of\nobjects in contact. However, despite their crucial role in shaping tactile\nexperiences, these material characteristics have been largely neglected in\nexisting tactile representation learning methods. Most approaches primarily\nfocus on aligning tactile data with visual or textual information, overlooking\nthe richness of tactile feedback that comes from understanding the materials'\ninherent properties. In this work, we address this gap by revisiting the\ntactile representation learning framework and incorporating material-aware\npriors into the learning process. These priors, which represent pre-learned\ncharacteristics specific to different materials, allow tactile models to better\ncapture and generalize the nuances of surface texture. Our method enables more\naccurate, contextually rich tactile feedback across diverse materials and\ntextures, improving performance in real-world applications such as robotics,\nhaptic feedback systems, and material editing."}
{"id": "2505.14320", "pdf": "https://arxiv.org/pdf/2505.14320", "abs": "https://arxiv.org/abs/2505.14320", "authors": ["Maria Cuellar", "Hon Kiu", "To", "Arush Mehrotra"], "title": "Accuracy and Fairness of Facial Recognition Technology in Low-Quality Police Images: An Experiment With Synthetic Faces", "categories": ["cs.CV", "stat.AP"], "comment": null, "summary": "Facial recognition technology (FRT) is increasingly used in criminal\ninvestigations, yet most evaluations of its accuracy rely on high-quality\nimages, unlike those often encountered by law enforcement. This study examines\nhow five common forms of image degradation--contrast, brightness, motion blur,\npose shift, and resolution--affect FRT accuracy and fairness across demographic\ngroups. Using synthetic faces generated by StyleGAN3 and labeled with FairFace,\nwe simulate degraded images and evaluate performance using Deepface with\nArcFace loss in 1:n identification tasks. We perform an experiment and find\nthat false positive rates peak near baseline image quality, while false\nnegatives increase as degradation intensifies--especially with blur and low\nresolution. Error rates are consistently higher for women and Black\nindividuals, with Black females most affected. These disparities raise concerns\nabout fairness and reliability when FRT is used in real-world investigative\ncontexts. Nevertheless, even under the most challenging conditions and for the\nmost affected subgroups, FRT accuracy remains substantially higher than that of\nmany traditional forensic methods. This suggests that, if appropriately\nvalidated and regulated, FRT should be considered a valuable investigative\ntool. However, algorithmic accuracy alone is not sufficient: we must also\nevaluate how FRT is used in practice, including user-driven data manipulation.\nSuch cases underscore the need for transparency and oversight in FRT deployment\nto ensure both fairness and forensic validity."}
{"id": "2505.14321", "pdf": "https://arxiv.org/pdf/2505.14321", "abs": "https://arxiv.org/abs/2505.14321", "authors": ["Bo Feng", "Zhengfeng Lai", "Shiyu Li", "Zizhen Wang", "Simon Wang", "Ping Huang", "Meng Cao"], "title": "Breaking Down Video LLM Benchmarks: Knowledge, Spatial Perception, or True Temporal Understanding?", "categories": ["cs.CV"], "comment": null, "summary": "Existing video understanding benchmarks often conflate knowledge-based and\npurely image-based questions, rather than clearly isolating a model's temporal\nreasoning ability, which is the key aspect that distinguishes video\nunderstanding from other modalities. We identify two major limitations that\nobscure whether higher scores truly indicate stronger understanding of the\ndynamic content in videos: (1) strong language priors, where models can answer\nquestions without watching the video; and (2) shuffling invariance, where\nmodels maintain similar performance on certain questions even when video frames\nare temporally shuffled. To alleviate these issues, we propose VBenchComp, an\nautomated pipeline that categorizes questions into different domains:\nLLM-Answerable, Semantic, and Temporal. Specifically, LLM-Answerable questions\ncan be answered without viewing the video; Semantic questions remain answerable\neven when the video frames are shuffled; and Temporal questions require\nunderstanding the correct temporal order of frames. The rest of the questions\nare labeled as Others. This can enable fine-grained evaluation of different\ncapabilities of a video LLM. Our analysis reveals nuanced model weaknesses that\nare hidden by traditional overall scores, and we offer insights and\nrecommendations for designing future benchmarks that more accurately assess\nvideo LLMs."}
{"id": "2505.14330", "pdf": "https://arxiv.org/pdf/2505.14330", "abs": "https://arxiv.org/abs/2505.14330", "authors": ["Rajat Kanti Bhattacharjee", "Meghali Nandi", "Amrit Jha", "Gunajit Kalita", "Ferdous Ahmed Barbhuiya"], "title": "Handloom Design Generation Using Generative Networks", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "This paper proposes deep learning techniques of generating designs for\nclothing, focused on handloom fabric and discusses the associated challenges\nalong with its application. The capability of generative neural network models\nin understanding artistic designs and synthesizing those is not yet explored\nwell. In this work, multiple methods are employed incorporating the current\nstate of the art generative models and style transfer algorithms to study and\nobserve their performance for the task. The results are then evaluated through\nuser score. This work also provides a new dataset NeuralLoom for the task of\nthe design generation."}
{"id": "2505.14333", "pdf": "https://arxiv.org/pdf/2505.14333", "abs": "https://arxiv.org/abs/2505.14333", "authors": ["Inder Pal Singh", "Enjie Ghorbel", "Anis Kacem", "Djamila Aouada"], "title": "Domain Adaptation for Multi-label Image Classification: a Discriminator-free Approach", "categories": ["cs.CV"], "comment": "The paper is under consideration at Computer Vision and Image\n  Understanding. arXiv admin note: text overlap with arXiv:2301.10611", "summary": "This paper introduces a discriminator-free adversarial-based approach termed\nDDA-MLIC for Unsupervised Domain Adaptation (UDA) in the context of Multi-Label\nImage Classification (MLIC). While recent efforts have explored\nadversarial-based UDA methods for MLIC, they typically include an additional\ndiscriminator subnet. Nevertheless, decoupling the classification and the\ndiscrimination tasks may harm their task-specific discriminative power. Herein,\nwe address this challenge by presenting a novel adversarial critic directly\nderived from the task-specific classifier. Specifically, we employ a\ntwo-component Gaussian Mixture Model (GMM) to model both source and target\npredictions, distinguishing between two distinct clusters. Instead of using the\ntraditional Expectation Maximization (EM) algorithm, our approach utilizes a\nDeep Neural Network (DNN) to estimate the parameters of each GMM component.\nSubsequently, the source and target GMM parameters are leveraged to formulate\nan adversarial loss using the Fr\\'echet distance. The proposed framework is\ntherefore not only fully differentiable but is also cost-effective as it avoids\nthe expensive iterative process usually induced by the standard EM method. The\nproposed method is evaluated on several multi-label image datasets covering\nthree different types of domain shift. The obtained results demonstrate that\nDDA-MLIC outperforms existing state-of-the-art methods in terms of precision\nwhile requiring a lower number of parameters. The code is made publicly\navailable at github.com/cvi2snt/DDA-MLIC."}
{"id": "2505.14340", "pdf": "https://arxiv.org/pdf/2505.14340", "abs": "https://arxiv.org/abs/2505.14340", "authors": ["Seunghyuk Cho", "Zhenyue Qin", "Yang Liu", "Youngbin Choi", "Seungbeom Lee", "Dongwoo Kim"], "title": "Plane Geometry Problem Solving with Multi-modal Reasoning: A Survey", "categories": ["cs.CV", "cs.LG"], "comment": "18 pages", "summary": "Plane geometry problem solving (PGPS) has recently gained significant\nattention as a benchmark to assess the multi-modal reasoning capabilities of\nlarge vision-language models. Despite the growing interest in PGPS, the\nresearch community still lacks a comprehensive overview that systematically\nsynthesizes recent work in PGPS. To fill this gap, we present a survey of\nexisting PGPS studies. We first categorize PGPS methods into an encoder-decoder\nframework and summarize the corresponding output formats used by their encoders\nand decoders. Subsequently, we classify and analyze these encoders and decoders\naccording to their architectural designs. Finally, we outline major challenges\nand promising directions for future research. In particular, we discuss the\nhallucination issues arising during the encoding phase within encoder-decoder\narchitectures, as well as the problem of data leakage in current PGPS\nbenchmarks."}
{"id": "2505.14341", "pdf": "https://arxiv.org/pdf/2505.14341", "abs": "https://arxiv.org/abs/2505.14341", "authors": ["Sifan Li", "Ming Tao", "Hao Zhao", "Ling Shao", "Hao Tang"], "title": "Replace in Translation: Boost Concept Alignment in Counterfactual Text-to-Image", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Text-to-Image (T2I) has been prevalent in recent years, with most common\ncondition tasks having been optimized nicely. Besides, counterfactual\nText-to-Image is obstructing us from a more versatile AIGC experience. For\nthose scenes that are impossible to happen in real world and anti-physics, we\nshould spare no efforts in increasing the factual feel, which means\nsynthesizing images that people think very likely to be happening, and concept\nalignment, which means all the required objects should be in the same frame. In\nthis paper, we focus on concept alignment. As controllable T2I models have\nachieved satisfactory performance for real applications, we utilize this\ntechnology to replace the objects in a synthesized image in latent space\nstep-by-step to change the image from a common scene to a counterfactual scene\nto meet the prompt. We propose a strategy to instruct this replacing process,\nwhich is called as Explicit Logical Narrative Prompt (ELNP), by using the newly\nSoTA language model DeepSeek to generate the instructions. Furthermore, to\nevaluate models' performance in counterfactual T2I, we design a metric to\ncalculate how many required concepts in the prompt can be covered averagely in\nthe synthesized images. The extensive experiments and qualitative comparisons\ndemonstrate that our strategy can boost the concept alignment in counterfactual\nT2I."}
{"id": "2505.14346", "pdf": "https://arxiv.org/pdf/2505.14346", "abs": "https://arxiv.org/abs/2505.14346", "authors": ["Mingfang Zhang", "Ryo Yonetani", "Yifei Huang", "Liangyang Ouyang", "Ruicong Liu", "Yoichi Sato"], "title": "Egocentric Action-aware Inertial Localization in Point Clouds", "categories": ["cs.CV"], "comment": null, "summary": "This paper presents a novel inertial localization framework named Egocentric\nAction-aware Inertial Localization (EAIL), which leverages egocentric action\ncues from head-mounted IMU signals to localize the target individual within a\n3D point cloud. Human inertial localization is challenging due to IMU sensor\nnoise that causes trajectory drift over time. The diversity of human actions\nfurther complicates IMU signal processing by introducing various motion\npatterns. Nevertheless, we observe that some actions observed through the\nhead-mounted IMU correlate with spatial environmental structures (e.g., bending\ndown to look inside an oven, washing dishes next to a sink), thereby serving as\nspatial anchors to compensate for the localization drift. The proposed EAIL\nframework learns such correlations via hierarchical multi-modal alignment. By\nassuming that the 3D point cloud of the environment is available, it\ncontrastively learns modality encoders that align short-term egocentric action\ncues in IMU signals with local environmental features in the point cloud. These\nencoders are then used in reasoning the IMU data and the point cloud over time\nand space to perform inertial localization. Interestingly, these encoders can\nfurther be utilized to recognize the corresponding sequence of actions as a\nby-product. Extensive experiments demonstrate the effectiveness of the proposed\nframework over state-of-the-art inertial localization and inertial action\nrecognition baselines."}
{"id": "2505.14357", "pdf": "https://arxiv.org/pdf/2505.14357", "abs": "https://arxiv.org/abs/2505.14357", "authors": ["Siqiao Huang", "Jialong Wu", "Qixing Zhou", "Shangchen Miao", "Mingsheng Long"], "title": "Vid2World: Crafting Video Diffusion Models to Interactive World Models", "categories": ["cs.CV", "cs.LG"], "comment": "Project page: http://knightnemo.github.io/vid2world/", "summary": "World models, which predict transitions based on history observation and\naction sequences, have shown great promise in improving data efficiency for\nsequential decision making. However, existing world models often require\nextensive domain-specific training and still produce low-fidelity, coarse\npredictions, limiting their applicability in complex environments. In contrast,\nvideo diffusion models trained on large, internet-scale datasets have\ndemonstrated impressive capabilities in generating high-quality videos that\ncapture diverse real-world dynamics. In this work, we present Vid2World, a\ngeneral approach for leveraging and transferring pre-trained video diffusion\nmodels into interactive world models. To bridge the gap, Vid2World performs\ncasualization of a pre-trained video diffusion model by crafting its\narchitecture and training objective to enable autoregressive generation.\nFurthermore, it introduces a causal action guidance mechanism to enhance action\ncontrollability in the resulting interactive world model. Extensive experiments\nin robot manipulation and game simulation domains show that our method offers a\nscalable and effective approach for repurposing highly capable video diffusion\nmodels to interactive world models."}
{"id": "2505.14359", "pdf": "https://arxiv.org/pdf/2505.14359", "abs": "https://arxiv.org/abs/2505.14359", "authors": ["Ruoxin Chen", "Junwei Xi", "Zhiyuan Yan", "Ke-Yue Zhang", "Shuang Wu", "Jingyi Xie", "Xu Chen", "Lei Xu", "Isabel Guan", "Taiping Yao", "Shouhong Ding"], "title": "Dual Data Alignment Makes AI-Generated Image Detector Easier Generalizable", "categories": ["cs.CV"], "comment": "12 Pages, 9 figures", "summary": "Existing detectors are often trained on biased datasets, leading to the\npossibility of overfitting on non-causal image attributes that are spuriously\ncorrelated with real/synthetic labels. While these biased features enhance\nperformance on the training data, they result in substantial performance\ndegradation when applied to unbiased datasets. One common solution is to\nperform dataset alignment through generative reconstruction, matching the\nsemantic content between real and synthetic images. However, we revisit this\napproach and show that pixel-level alignment alone is insufficient. The\nreconstructed images still suffer from frequency-level misalignment, which can\nperpetuate spurious correlations. To illustrate, we observe that reconstruction\nmodels tend to restore the high-frequency details lost in real images (possibly\ndue to JPEG compression), inadvertently creating a frequency-level\nmisalignment, where synthetic images appear to have richer high-frequency\ncontent than real ones. This misalignment leads to models associating\nhigh-frequency features with synthetic labels, further reinforcing biased cues.\nTo resolve this, we propose Dual Data Alignment (DDA), which aligns both the\npixel and frequency domains. Moreover, we introduce two new test sets:\nDDA-COCO, containing DDA-aligned synthetic images for testing detector\nperformance on the most aligned dataset, and EvalGEN, featuring the latest\ngenerative models for assessing detectors under new generative architectures\nsuch as visual auto-regressive generators. Finally, our extensive evaluations\ndemonstrate that a detector trained exclusively on DDA-aligned MSCOCO could\nimprove across 8 diverse benchmarks by a non-trivial margin, showing a +7.2% on\nin-the-wild benchmarks, highlighting the improved generalizability of unbiased\ndetectors."}
{"id": "2505.14361", "pdf": "https://arxiv.org/pdf/2505.14361", "abs": "https://arxiv.org/abs/2505.14361", "authors": ["Xingxing Weng", "Chao Pang", "Gui-Song Xia"], "title": "Vision-Language Modeling Meets Remote Sensing: Models, Datasets and Perspectives", "categories": ["cs.CV"], "comment": "Accepted by IEEE Geoscience and Remote Sensing Magazine", "summary": "Vision-language modeling (VLM) aims to bridge the information gap between\nimages and natural language. Under the new paradigm of first pre-training on\nmassive image-text pairs and then fine-tuning on task-specific data, VLM in the\nremote sensing domain has made significant progress. The resulting models\nbenefit from the absorption of extensive general knowledge and demonstrate\nstrong performance across a variety of remote sensing data analysis tasks.\nMoreover, they are capable of interacting with users in a conversational\nmanner. In this paper, we aim to provide the remote sensing community with a\ntimely and comprehensive review of the developments in VLM using the two-stage\nparadigm. Specifically, we first cover a taxonomy of VLM in remote sensing:\ncontrastive learning, visual instruction tuning, and text-conditioned image\ngeneration. For each category, we detail the commonly used network architecture\nand pre-training objectives. Second, we conduct a thorough review of existing\nworks, examining foundation models and task-specific adaptation methods in\ncontrastive-based VLM, architectural upgrades, training strategies and model\ncapabilities in instruction-based VLM, as well as generative foundation models\nwith their representative downstream applications. Third, we summarize datasets\nused for VLM pre-training, fine-tuning, and evaluation, with an analysis of\ntheir construction methodologies (including image sources and caption\ngeneration) and key properties, such as scale and task adaptability. Finally,\nwe conclude this survey with insights and discussions on future research\ndirections: cross-modal representation alignment, vague requirement\ncomprehension, explanation-driven model reliability, continually scalable model\ncapabilities, and large-scale datasets featuring richer modalities and greater\nchallenges."}
{"id": "2505.14362", "pdf": "https://arxiv.org/pdf/2505.14362", "abs": "https://arxiv.org/abs/2505.14362", "authors": ["Ziwei Zheng", "Michael Yang", "Jack Hong", "Chenxiao Zhao", "Guohai Xu", "Le Yang", "Chao Shen", "Xing Yu"], "title": "DeepEyes: Incentivizing \"Thinking with Images\" via Reinforcement Learning", "categories": ["cs.CV"], "comment": null, "summary": "Large Vision-Language Models (VLMs) have shown strong capabilities in\nmultimodal understanding and reasoning, yet they are primarily constrained by\ntext-based reasoning processes. However, achieving seamless integration of\nvisual and textual reasoning which mirrors human cognitive processes remains a\nsignificant challenge. In particular, effectively incorporating advanced visual\ninput processing into reasoning mechanisms is still an open question. Thus, in\nthis paper, we explore the interleaved multimodal reasoning paradigm and\nintroduce DeepEyes, a model with \"thinking with images\" capabilities\nincentivized through end-to-end reinforcement learning without the need for\ncold-start SFT. Notably, this ability emerges natively within the model itself,\nleveraging its inherent grounding ability as a tool instead of depending on\nseparate specialized models. Specifically, we propose a tool-use-oriented data\nselection mechanism and a reward strategy to encourage successful tool-assisted\nreasoning trajectories. DeepEyes achieves significant performance gains on\nfine-grained perception and reasoning benchmarks and also demonstrates\nimprovement in grounding, hallucination, and mathematical reasoning tasks.\nInterestingly, we observe the distinct evolution of tool-calling behavior from\ninitial exploration to efficient and accurate exploitation, and diverse\nthinking patterns that closely mirror human visual reasoning processes. Code is\navailable at https://github.com/Visual-Agent/DeepEyes."}
{"id": "2505.14404", "pdf": "https://arxiv.org/pdf/2505.14404", "abs": "https://arxiv.org/abs/2505.14404", "authors": ["Xuecheng Wu", "Jiaxing Liu", "Danlei Huang", "Xiaoyu Li", "Yifan Wang", "Chen Chen", "Liya Ma", "Xuezhi Cao", "Junxiao Xue"], "title": "ViC-Bench: Benchmarking Visual-Interleaved Chain-of-Thought Capability in MLLMs with Free-Style Intermediate State Representations", "categories": ["cs.CV"], "comment": null, "summary": "Visual-Interleaved Chain-of-Thought (VI-CoT) enables MLLMs to continually\nupdate their understanding and decisions based on step-wise intermediate visual\nstates (IVS), much like a human would, which demonstrates impressive success in\nvarious tasks, thereby leading to emerged advancements in related benchmarks.\nDespite promising progress, current benchmarks provide models with relatively\nfixed IVS, rather than free-style IVS, whch might forcibly distort the original\nthinking trajectories, failing to evaluate their intrinsic reasoning\ncapabilities. More importantly, existing benchmarks neglect to systematically\nexplore the impact factors that IVS would impart to untamed reasoning\nperformance. To tackle above gaps, we introduce a specialized benchmark termed\nViC-Bench, consisting of four representive tasks: maze navigation, jigsaw\npuzzle, embodied long-horizon planning, and complex counting, where each task\nhas dedicated free-style IVS generation pipeline supporting function calls. To\nsystematically examine VI-CoT capability, we propose a thorough evaluation\nsuite incorporating a progressive three-stage strategy with targeted new\nmetrics. Besides, we establish Incremental Prompting Information Injection\n(IPII) strategy to ablatively explore the prompting factors for VI-CoT. We\nextensively conduct evaluations for 18 advanced MLLMs, revealing key insights\ninto their VI-CoT capability. Our proposed benchmark is publicly open at\nHuggingface."}
{"id": "2505.14405", "pdf": "https://arxiv.org/pdf/2505.14405", "abs": "https://arxiv.org/abs/2505.14405", "authors": ["Jiafeng Liang", "Shixin Jiang", "Xuan Dong", "Ning Wang", "Zheng Chu", "Hui Su", "Jinlan Fu", "Ming Liu", "See-Kiong Ng", "Bing Qin"], "title": "Investigating and Enhancing the Robustness of Large Multimodal Models Against Temporal Inconsistency", "categories": ["cs.CV"], "comment": null, "summary": "Large Multimodal Models (LMMs) have recently demonstrated impressive\nperformance on general video comprehension benchmarks. Nevertheless, for\nbroader applications, the robustness of their temporal analysis capability\nneeds to be thoroughly investigated yet predominantly ignored. Motivated by\nthis, we propose a novel temporal robustness benchmark (TemRobBench), which\nintroduces temporal inconsistency perturbations separately at the visual and\ntextual modalities to assess the robustness of models. We evaluate 16\nmainstream LMMs and find that they exhibit over-reliance on prior knowledge and\ntextual context in adversarial environments, while ignoring the actual temporal\ndynamics in the video. To mitigate this issue, we design panoramic direct\npreference optimization (PanoDPO), which encourages LMMs to incorporate both\nvisual and linguistic feature preferences simultaneously. Experimental results\nshow that PanoDPO can effectively enhance the model's robustness and\nreliability in temporal analysis."}
{"id": "2505.14414", "pdf": "https://arxiv.org/pdf/2505.14414", "abs": "https://arxiv.org/abs/2505.14414", "authors": ["Chengtang Yao", "Lidong Yu", "Zhidan Liu", "Jiaxi Zeng", "Yuwei Wu", "Yunde Jia"], "title": "Diving into the Fusion of Monocular Priors for Generalized Stereo Matching", "categories": ["cs.CV"], "comment": "Code:\n  https://github.com/YaoChengTang/Diving-into-the-Fusion-of-Monocular-Priors-for-Generalized-Stereo-Matching", "summary": "The matching formulation makes it naturally hard for the stereo matching to\nhandle ill-posed regions like occlusions and non-Lambertian surfaces. Fusing\nmonocular priors has been proven helpful for ill-posed matching, but the biased\nmonocular prior learned from small stereo datasets constrains the\ngeneralization. Recently, stereo matching has progressed by leveraging the\nunbiased monocular prior from the vision foundation model (VFM) to improve the\ngeneralization in ill-posed regions. We dive into the fusion process and\nobserve three main problems limiting the fusion of the VFM monocular prior. The\nfirst problem is the misalignment between affine-invariant relative monocular\ndepth and absolute depth of disparity. Besides, when we use the monocular\nfeature in an iterative update structure, the over-confidence in the disparity\nupdate leads to local optima results. A direct fusion of a monocular depth map\ncould alleviate the local optima problem, but noisy disparity results computed\nat the first several iterations will misguide the fusion. In this paper, we\npropose a binary local ordering map to guide the fusion, which converts the\ndepth map into a binary relative format, unifying the relative and absolute\ndepth representation. The computed local ordering map is also used to re-weight\nthe initial disparity update, resolving the local optima and noisy problem. In\naddition, we formulate the final direct fusion of monocular depth to the\ndisparity as a registration problem, where a pixel-wise linear regression\nmodule can globally and adaptively align them. Our method fully exploits the\nmonocular prior to support stereo matching results effectively and efficiently.\nWe significantly improve the performance from the experiments when generalizing\nfrom SceneFlow to Middlebury and Booster datasets while barely reducing the\nefficiency."}
{"id": "2505.14454", "pdf": "https://arxiv.org/pdf/2505.14454", "abs": "https://arxiv.org/abs/2505.14454", "authors": ["Xuyang Liu", "Yiyu Wang", "Junpeng Ma", "Linfeng Zhang"], "title": "Video Compression Commander: Plug-and-Play Inference Acceleration for Video Large Language Models", "categories": ["cs.CV"], "comment": "Our code is available at https://github.com/xuyang-liu16/VidCom2", "summary": "Video large language models (VideoLLM) excel at video understanding, but face\nefficiency challenges due to the quadratic complexity of abundant visual\ntokens. Our systematic analysis of token compression methods for VideoLLMs\nreveals two critical issues: (i) overlooking distinctive visual signals across\nframes, leading to information loss; (ii) suffering from implementation\nconstraints, causing incompatibility with modern architectures or efficient\noperators. To address these challenges, we distill three design principles for\nVideoLLM token compression and propose a plug-and-play inference acceleration\nframework \"Video Compression Commander\" (VidCom2). By quantifying each frame's\nuniqueness, VidCom2 adaptively adjusts compression intensity across frames,\neffectively preserving essential information while reducing redundancy in video\nsequences. Extensive experiments across various VideoLLMs and benchmarks\ndemonstrate the superior performance and efficiency of our VidCom2. With only\n25% visual tokens, VidCom2 achieves 99.6% of the original performance on\nLLaVA-OV while reducing 70.8% of the LLM generation latency. Notably, our Frame\nCompression Adjustment strategy is compatible with other token compression\nmethods to further improve their performance. Our code is available at\nhttps://github.com/xuyang-liu16/VidCom2."}
{"id": "2505.14460", "pdf": "https://arxiv.org/pdf/2505.14460", "abs": "https://arxiv.org/abs/2505.14460", "authors": ["Tianhe Wu", "Jian Zou", "Jie Liang", "Lei Zhang", "Kede Ma"], "title": "VisualQuality-R1: Reasoning-Induced Image Quality Assessment via Reinforcement Learning to Rank", "categories": ["cs.CV"], "comment": null, "summary": "DeepSeek-R1 has demonstrated remarkable effectiveness in incentivizing\nreasoning and generalization capabilities of large language models (LLMs)\nthrough reinforcement learning. Nevertheless, the potential of\nreasoning-induced computational modeling has not been thoroughly explored in\nthe context of image quality assessment (IQA), a task critically dependent on\nvisual reasoning. In this paper, we introduce VisualQuality-R1, a\nreasoning-induced no-reference IQA (NR-IQA) model, and we train it with\nreinforcement learning to rank, a learning algorithm tailored to the\nintrinsically relative nature of visual quality. Specifically, for a pair of\nimages, we employ group relative policy optimization to generate multiple\nquality scores for each image. These estimates are then used to compute\ncomparative probabilities of one image having higher quality than the other\nunder the Thurstone model. Rewards for each quality estimate are defined using\ncontinuous fidelity measures rather than discretized binary labels. Extensive\nexperiments show that the proposed VisualQuality-R1 consistently outperforms\ndiscriminative deep learning-based NR-IQA models as well as a recent\nreasoning-induced quality regression method. Moreover, VisualQuality-R1 is\ncapable of generating contextually rich, human-aligned quality descriptions,\nand supports multi-dataset training without requiring perceptual scale\nrealignment. These features make VisualQuality-R1 especially well-suited for\nreliably measuring progress in a wide range of image processing tasks like\nsuper-resolution and image generation."}
{"id": "2505.14462", "pdf": "https://arxiv.org/pdf/2505.14462", "abs": "https://arxiv.org/abs/2505.14462", "authors": ["Jiaang Li", "Yifei Yuan", "Wenyan Li", "Mohammad Aliannejadi", "Daniel Hershcovich", "Anders Sgaard", "Ivan Vuli", "Wenxuan Zhang", "Paul Pu Liang", "Yang Deng", "Serge Belongie"], "title": "RAVENEA: A Benchmark for Multimodal Retrieval-Augmented Visual Culture Understanding", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "As vision-language models (VLMs) become increasingly integrated into daily\nlife, the need for accurate visual culture understanding is becoming critical.\nYet, these models frequently fall short in interpreting cultural nuances\neffectively. Prior work has demonstrated the effectiveness of\nretrieval-augmented generation (RAG) in enhancing cultural understanding in\ntext-only settings, while its application in multimodal scenarios remains\nunderexplored. To bridge this gap, we introduce RAVENEA (Retrieval-Augmented\nVisual culturE uNdErstAnding), a new benchmark designed to advance visual\nculture understanding through retrieval, focusing on two tasks: culture-focused\nvisual question answering (cVQA) and culture-informed image captioning (cIC).\nRAVENEA extends existing datasets by integrating over 10,000 Wikipedia\ndocuments curated and ranked by human annotators. With RAVENEA, we train and\nevaluate seven multimodal retrievers for each image query, and measure the\ndownstream impact of retrieval-augmented inputs across fourteen\nstate-of-the-art VLMs. Our results show that lightweight VLMs, when augmented\nwith culture-aware retrieval, outperform their non-augmented counterparts (by\nat least 3.2% absolute on cVQA and 6.2% absolute on cIC). This highlights the\nvalue of retrieval-augmented methods and culturally inclusive benchmarks for\nmultimodal understanding."}
{"id": "2505.14476", "pdf": "https://arxiv.org/pdf/2505.14476", "abs": "https://arxiv.org/abs/2505.14476", "authors": ["Farshad Sangari Abiz", "Reshad Hosseini", "Babak N. Araabi"], "title": "Enhancing Interpretability of Sparse Latent Representations with Class Information", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Variational Autoencoders (VAEs) are powerful generative models for learning\nlatent representations. Standard VAEs generate dispersed and unstructured\nlatent spaces by utilizing all dimensions, which limits their interpretability,\nespecially in high-dimensional spaces. To address this challenge, Variational\nSparse Coding (VSC) introduces a spike-and-slab prior distribution, resulting\nin sparse latent representations for each input. These sparse representations,\ncharacterized by a limited number of active dimensions, are inherently more\ninterpretable. Despite this advantage, VSC falls short in providing structured\ninterpretations across samples within the same class. Intuitively, samples from\nthe same class are expected to share similar attributes while allowing for\nvariations in those attributes. This expectation should manifest as consistent\npatterns of active dimensions in their latent representations, but VSC does not\nenforce such consistency.\n  In this paper, we propose a novel approach to enhance the latent space\ninterpretability by ensuring that the active dimensions in the latent space are\nconsistent across samples within the same class. To achieve this, we introduce\na new loss function that encourages samples from the same class to share\nsimilar active dimensions. This alignment creates a more structured and\ninterpretable latent space, where each shared dimension corresponds to a\nhigh-level concept, or \"factor.\" Unlike existing disentanglement-based methods\nthat primarily focus on global factors shared across all classes, our method\ncaptures both global and class-specific factors, thereby enhancing the utility\nand interpretability of latent representations."}
{"id": "2505.14511", "pdf": "https://arxiv.org/pdf/2505.14511", "abs": "https://arxiv.org/abs/2505.14511", "authors": ["Guillaume Vray", "Devavrat Tomar", "Xufeng Gao", "Jean-Philippe Thiran", "Evan Shelhamer", "Behzad Bozorgtabar"], "title": "ReservoirTTA: Prolonged Test-time Adaptation for Evolving and Recurring Domains", "categories": ["cs.CV"], "comment": null, "summary": "This paper introduces ReservoirTTA, a novel plug-in framework designed for\nprolonged test-time adaptation (TTA) in scenarios where the test domain\ncontinuously shifts over time, including cases where domains recur or evolve\ngradually. At its core, ReservoirTTA maintains a reservoir of\ndomain-specialized models -- an adaptive test-time model ensemble -- that both\ndetects new domains via online clustering over style features of incoming\nsamples and routes each sample to the appropriate specialized model, and\nthereby enables domain-specific adaptation. This multi-model strategy overcomes\nkey limitations of single model adaptation, such as catastrophic forgetting,\ninter-domain interference, and error accumulation, ensuring robust and stable\nperformance on sustained non-stationary test distributions. Our theoretical\nanalysis reveals key components that bound parameter variance and prevent model\ncollapse, while our plug-in TTA module mitigates catastrophic forgetting of\npreviously encountered domains. Extensive experiments on the classification\ncorruption benchmarks, including ImageNet-C and CIFAR-10/100-C, as well as the\nCityscapes$\\rightarrow$ACDC semantic segmentation task, covering recurring and\ncontinuously evolving domain shifts, demonstrate that ReservoirTTA\nsignificantly improves adaptation accuracy and maintains stable performance\nacross prolonged, recurring shifts, outperforming state-of-the-art methods."}
{"id": "2505.14521", "pdf": "https://arxiv.org/pdf/2505.14521", "abs": "https://arxiv.org/abs/2505.14521", "authors": ["Zhihao Li", "Yufei Wang", "Heliang Zheng", "Yihao Luo", "Bihan Wen"], "title": "SparC: Sparse Representation and Construction for High-Resolution 3D Shapes Modeling", "categories": ["cs.CV"], "comment": "Homepage: https://lizhihao6.github.io/SparC", "summary": "High-fidelity 3D object synthesis remains significantly more challenging than\n2D image generation due to the unstructured nature of mesh data and the cubic\ncomplexity of dense volumetric grids. Existing two-stage pipelines-compressing\nmeshes with a VAE (using either 2D or 3D supervision), followed by latent\ndiffusion sampling-often suffer from severe detail loss caused by inefficient\nrepresentations and modality mismatches introduced in VAE. We introduce SparC,\na unified framework that combines a sparse deformable marching cubes\nrepresentation SparseCubes with a novel encoder SparConv-VAE. SparseCubes\nconverts raw meshes into high-resolution ($1024^3$) surfaces with arbitrary\ntopology by scattering signed distance and deformation fields onto a sparse\ncube, allowing differentiable optimization. SparConv-VAE is the first\nmodality-consistent variational autoencoder built entirely upon sparse\nconvolutional networks, enabling efficient and near-lossless 3D reconstruction\nsuitable for high-resolution generative modeling through latent diffusion.\nSparC achieves state-of-the-art reconstruction fidelity on challenging inputs,\nincluding open surfaces, disconnected components, and intricate geometry. It\npreserves fine-grained shape details, reduces training and inference cost, and\nintegrates naturally with latent diffusion models for scalable, high-resolution\n3D generation."}
{"id": "2505.14527", "pdf": "https://arxiv.org/pdf/2505.14527", "abs": "https://arxiv.org/abs/2505.14527", "authors": ["Nitish Shukla", "Arun Ross"], "title": "diffDemorph: Extending Reference-Free Demorphing to Unseen Faces", "categories": ["cs.CV"], "comment": null, "summary": "A face morph is created by combining two (or more) face images corresponding\nto two (or more) identities to produce a composite that successfully matches\nthe constituent identities. Reference-free (RF) demorphing reverses this\nprocess using only the morph image, without the need for additional reference\nimages. Previous RF demorphing methods were overly constrained, as they rely on\nassumptions about the distributions of training and testing morphs such as the\nmorphing technique used, face style, and images used to create the morph. In\nthis paper, we introduce a novel diffusion-based approach that effectively\ndisentangles component images from a composite morph image with high visual\nfidelity. Our method is the first to generalize across morph techniques and\nface styles, beating the current state of the art by $\\geq 59.46\\%$ under a\ncommon training protocol across all datasets tested. We train our method on\nmorphs created using synthetically generated face images and test on real\nmorphs, thereby enhancing the practicality of the technique. Experiments on six\ndatasets and two face matchers establish the utility and efficacy of our\nmethod."}
{"id": "2505.14537", "pdf": "https://arxiv.org/pdf/2505.14537", "abs": "https://arxiv.org/abs/2505.14537", "authors": ["Yuxuan Wang", "Xuanyu Yi", "Qingshan Xu", "Yuan Zhou", "Long Chen", "Hanwang Zhang"], "title": "Personalize Your Gaussian: Consistent 3D Scene Personalization from a Single Image", "categories": ["cs.CV"], "comment": "9 pages", "summary": "Personalizing 3D scenes from a single reference image enables intuitive\nuser-guided editing, which requires achieving both multi-view consistency\nacross perspectives and referential consistency with the input image. However,\nthese goals are particularly challenging due to the viewpoint bias caused by\nthe limited perspective provided in a single image. Lacking the mechanisms to\neffectively expand reference information beyond the original view, existing\nmethods of image-conditioned 3DGS personalization often suffer from this\nviewpoint bias and struggle to produce consistent results. Therefore, in this\npaper, we present Consistent Personalization for 3D Gaussian Splatting (CP-GS),\na framework that progressively propagates the single-view reference appearance\nto novel perspectives. In particular, CP-GS integrates pre-trained image-to-3D\ngeneration and iterative LoRA fine-tuning to extract and extend the reference\nappearance, and finally produces faithful multi-view guidance images and the\npersonalized 3DGS outputs through a view-consistent generation process guided\nby geometric cues. Extensive experiments on real-world scenes show that our\nCP-GS effectively mitigates the viewpoint bias, achieving high-quality\npersonalization that significantly outperforms existing methods. The code will\nbe released at https://github.com/Yuxuan-W/CP-GS."}
{"id": "2505.14556", "pdf": "https://arxiv.org/pdf/2505.14556", "abs": "https://arxiv.org/abs/2505.14556", "authors": ["Marlne Careil", "Yohann Benchetrit", "Jean-Rmi King"], "title": "Dynadiff: Single-stage Decoding of Images from Continuously Evolving fMRI", "categories": ["cs.CV"], "comment": null, "summary": "Brain-to-image decoding has been recently propelled by the progress in\ngenerative AI models and the availability of large ultra-high field functional\nMagnetic Resonance Imaging (fMRI). However, current approaches depend on\ncomplicated multi-stage pipelines and preprocessing steps that typically\ncollapse the temporal dimension of brain recordings, thereby limiting\ntime-resolved brain decoders. Here, we introduce Dynadiff (Dynamic Neural\nActivity Diffusion for Image Reconstruction), a new single-stage diffusion\nmodel designed for reconstructing images from dynamically evolving fMRI\nrecordings. Our approach offers three main contributions. First, Dynadiff\nsimplifies training as compared to existing approaches. Second, our model\noutperforms state-of-the-art models on time-resolved fMRI signals, especially\non high-level semantic image reconstruction metrics, while remaining\ncompetitive on preprocessed fMRI data that collapse time. Third, this approach\nallows a precise characterization of the evolution of image representations in\nbrain activity. Overall, this work lays the foundation for time-resolved\nbrain-to-image decoding."}
{"id": "2505.14583", "pdf": "https://arxiv.org/pdf/2505.14583", "abs": "https://arxiv.org/abs/2505.14583", "authors": ["Abhimanyu Talwar", "Julien Laasri"], "title": "Instance Segmentation for Point Sets", "categories": ["cs.CV", "cs.LG", "68T45", "I.2.10"], "comment": "6 pages, 11 figures, paper dated 2019", "summary": "Recently proposed neural network architectures like PointNet [QSMG16] and\nPointNet++ [QYSG17] have made it possible to apply Deep Learning to 3D point\nsets. The feature representations of shapes learned by these two networks\nenabled training classifiers for Semantic Segmentation, and more recently for\nInstance Segmentation via the Similarity Group Proposal Network (SGPN)\n[WYHN17]. One area of improvement which has been highlighted by SGPN's authors,\npertains to use of memory intensive similarity matrices which occupy memory\nquadratic in the number of points. In this report, we attempt to tackle this\nissue through use of two sampling based methods, which compute Instance\nSegmentation on a sub-sampled Point Set, and then extrapolate labels to the\ncomplete set using the nearest neigbhour approach. While both approaches\nperform equally well on large sub-samples, the random-based strategy gives the\nmost improvements in terms of speed and memory usage."}
{"id": "2505.14621", "pdf": "https://arxiv.org/pdf/2505.14621", "abs": "https://arxiv.org/abs/2505.14621", "authors": ["Abhimanyu Talwar", "Julien Laasri"], "title": "3D Reconstruction from Sketches", "categories": ["cs.CV", "cs.LG", "68T45", "I.2.10"], "comment": "6 pages, 8 figures, paper dated December 12, 2018", "summary": "We consider the problem of reconstructing a 3D scene from multiple sketches.\nWe propose a pipeline which involves (1) stitching together multiple sketches\nthrough use of correspondence points, (2) converting the stitched sketch into a\nrealistic image using a CycleGAN, and (3) estimating that image's depth-map\nusing a pre-trained convolutional neural network based architecture called\nMegaDepth. Our contribution includes constructing a dataset of image-sketch\npairs, the images for which are from the Zurich Building Database, and sketches\nhave been generated by us. We use this dataset to train a CycleGAN for our\npipeline's second step. We end up with a stitching process that does not\ngeneralize well to real drawings, but the rest of the pipeline that creates a\n3D reconstruction from a single sketch performs quite well on a wide variety of\ndrawings."}
{"id": "2505.14634", "pdf": "https://arxiv.org/pdf/2505.14634", "abs": "https://arxiv.org/abs/2505.14634", "authors": ["Gokul Bhusal", "Yifei Lou", "Cristina Garcia-Cardona", "Ekaterina Merkurjev"], "title": "A General Framework for Group Sparsity in Hyperspectral Unmixing Using Endmember Bundles", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Due to low spatial resolution, hyperspectral data often consists of mixtures\nof contributions from multiple materials. This limitation motivates the task of\nhyperspectral unmixing (HU), a fundamental problem in hyperspectral imaging. HU\naims to identify the spectral signatures (\\textit{endmembers}) of the materials\npresent in an observed scene, along with their relative proportions\n(\\textit{fractional abundance}) in each pixel. A major challenge lies in the\nclass variability in materials, which hinders accurate representation by a\nsingle spectral signature, as assumed in the conventional linear mixing model.\nMoreover, To address this issue, we propose using group sparsity after\nrepresenting each material with a set of spectral signatures, known as\nendmember bundles, where each group corresponds to a specific material. In\nparticular, we develop a bundle-based framework that can enforce either\ninter-group sparsity or sparsity within and across groups (SWAG) on the\nabundance coefficients. Furthermore, our framework offers the flexibility to\nincorporate a variety of sparsity-promoting penalties, among which the\ntransformed $\\ell_1$ (TL1) penalty is a novel regularization in the HU\nliterature. Extensive experiments conducted on both synthetic and real\nhyperspectral data demonstrate the effectiveness and superiority of the\nproposed approaches."}
{"id": "2505.14638", "pdf": "https://arxiv.org/pdf/2505.14638", "abs": "https://arxiv.org/abs/2505.14638", "authors": ["Tomer Gafni", "Asaf Karnieli", "Yair Hanani"], "title": "Dual Precision Quantization for Efficient and Accurate Deep Neural Networks Inference", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": "Accepted at eLVM Workshop, CVPR, 2025", "summary": "Deep neural networks have achieved state-of-the-art results in a wide range\nof applications, from natural language processing and computer vision to speech\nrecognition. However, as tasks become increasingly complex, model sizes\ncontinue to grow, posing challenges in latency and memory efficiency. To meet\nthese constraints, post-training quantization has emerged as a promising\nsolution. In this paper, we propose a novel hardware-efficient quantization and\ninference scheme that exploits hardware advantages with minimal accuracy\ndegradation. Specifically, we introduce a W4A8 scheme, where weights are\nquantized and stored using 4-bit integer precision, and inference computations\nare performed using 8-bit floating-point arithmetic, demonstrating significant\nspeedups and improved memory utilization compared to 16-bit operations,\napplicable on various modern accelerators. To mitigate accuracy loss, we\ndevelop a novel quantization algorithm, dubbed Dual Precision Quantization\n(DPQ), that leverages the unique structure of our scheme without introducing\nadditional inference overhead. Experimental results demonstrate improved\nperformance (i.e., increased throughput) while maintaining tolerable accuracy\ndegradation relative to the full-precision model."}
{"id": "2505.14640", "pdf": "https://arxiv.org/pdf/2505.14640", "abs": "https://arxiv.org/abs/2505.14640", "authors": ["Wentao Ma", "Weiming Ren", "Yiming Jia", "Zhuofeng Li", "Ping Nie", "Ge Zhang", "Wenhu Chen"], "title": "VideoEval-Pro: Robust and Realistic Long Video Understanding Evaluation", "categories": ["cs.CV"], "comment": "Dataset: https://huggingface.co/datasets/TIGER-Lab/VideoEval-Pro,\n  Project Webpage: https://tiger-ai-lab.github.io/VideoEval-Pro", "summary": "Large multimodal models (LMMs) have recently emerged as a powerful tool for\nlong video understanding (LVU), prompting the development of standardized LVU\nbenchmarks to evaluate their performance. However, our investigation reveals a\nrather sober lesson for existing LVU benchmarks. First, most existing\nbenchmarks rely heavily on multiple-choice questions (MCQs), whose evaluation\nresults are inflated due to the possibility of guessing the correct answer;\nSecond, a significant portion of questions in these benchmarks have strong\npriors to allow models to answer directly without even reading the input video.\nFor example, Gemini-1.5-Pro can achieve over 50\\% accuracy given a random frame\nfrom a long video on Video-MME. We also observe that increasing the number of\nframes does not necessarily lead to improvement on existing benchmarks, which\nis counterintuitive. As a result, the validity and robustness of current LVU\nbenchmarks are undermined, impeding a faithful assessment of LMMs' long-video\nunderstanding capability. To tackle this problem, we propose VideoEval-Pro, a\nrealistic LVU benchmark containing questions with open-ended short-answer,\nwhich truly require understanding the entire video. VideoEval-Pro assesses both\nsegment-level and full-video understanding through perception and reasoning\ntasks. By evaluating 21 proprietary and open-source video LMMs, we conclude the\nfollowing findings: (1) video LMMs show drastic performance ($>$25\\%) drops on\nopen-ended questions compared with MCQs; (2) surprisingly, higher MCQ scores do\nnot lead to higher open-ended scores on VideoEval-Pro; (3) compared to other\nMCQ benchmarks, VideoEval-Pro benefits more from increasing the number of input\nframes. Our results show that VideoEval-Pro offers a more realistic and\nreliable measure of long video understanding, providing a clearer view of\nprogress in this domain."}
{"id": "2505.14646", "pdf": "https://arxiv.org/pdf/2505.14646", "abs": "https://arxiv.org/abs/2505.14646", "authors": ["Anna C. Doris", "Md Ferdous Alam", "Amin Heyrani Nobari", "Faez Ahmed"], "title": "CAD-Coder: An Open-Source Vision-Language Model for Computer-Aided Design Code Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Efficient creation of accurate and editable 3D CAD models is critical in\nengineering design, significantly impacting cost and time-to-market in product\ninnovation. Current manual workflows remain highly time-consuming and demand\nextensive user expertise. While recent developments in AI-driven CAD generation\nshow promise, existing models are limited by incomplete representations of CAD\noperations, inability to generalize to real-world images, and low output\naccuracy. This paper introduces CAD-Coder, an open-source Vision-Language Model\n(VLM) explicitly fine-tuned to generate editable CAD code (CadQuery Python)\ndirectly from visual input. Leveraging a novel dataset that we\ncreated--GenCAD-Code, consisting of over 163k CAD-model image and code\npairs--CAD-Coder outperforms state-of-the-art VLM baselines such as GPT-4.5 and\nQwen2.5-VL-72B, achieving a 100% valid syntax rate and the highest accuracy in\n3D solid similarity. Notably, our VLM demonstrates some signs of\ngeneralizability, successfully generating CAD code from real-world images and\nexecuting CAD operations unseen during fine-tuning. The performance and\nadaptability of CAD-Coder highlights the potential of VLMs fine-tuned on code\nto streamline CAD workflows for engineers and designers. CAD-Coder is publicly\navailable at: https://github.com/anniedoris/CAD-Coder."}
{"id": "2505.14654", "pdf": "https://arxiv.org/pdf/2505.14654", "abs": "https://arxiv.org/abs/2505.14654", "authors": ["Zikai Liao", "Yi Ouyang", "Yi-Lun Lee", "Chen-Ping Yu", "Yi-Hsuan Tsai", "Zhaozheng Yin"], "title": "Beyond Words: Multimodal LLM Knows When to Speak", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Project page: https://github.com/lzk901372/MM-When2Speak", "summary": "While large language model (LLM)-based chatbots have demonstrated strong\ncapabilities in generating coherent and contextually relevant responses, they\noften struggle with understanding when to speak, particularly in delivering\nbrief, timely reactions during ongoing conversations. This limitation arises\nlargely from their reliance on text input, lacking the rich contextual cues in\nreal-world human dialogue. In this work, we focus on real-time prediction of\nresponse types, with an emphasis on short, reactive utterances that depend on\nsubtle, multimodal signals across vision, audio, and text. To support this, we\nintroduce a new multimodal dataset constructed from real-world conversational\nvideos, containing temporally aligned visual, auditory, and textual streams.\nThis dataset enables fine-grained modeling of response timing in dyadic\ninteractions. Building on this dataset, we propose MM-When2Speak, a multimodal\nLLM-based model that adaptively integrates visual, auditory, and textual\ncontext to predict when a response should occur, and what type of response is\nappropriate. Experiments show that MM-When2Speak significantly outperforms\nstate-of-the-art unimodal and LLM-based baselines, achieving up to a 4x\nimprovement in response timing accuracy over leading commercial LLMs. These\nresults underscore the importance of multimodal inputs for producing timely,\nnatural, and engaging conversational AI."}
{"id": "2505.14664", "pdf": "https://arxiv.org/pdf/2505.14664", "abs": "https://arxiv.org/abs/2505.14664", "authors": ["Yilin Ye", "Junchao Huang", "Xingchen Zeng", "Jiazhi Xia", "Wei Zeng"], "title": "AKRMap: Adaptive Kernel Regression for Trustworthy Visualization of Cross-Modal Embeddings", "categories": ["cs.CV", "cs.AI", "cs.HC", "cs.LG"], "comment": null, "summary": "Cross-modal embeddings form the foundation for multi-modal models. However,\nvisualization methods for interpreting cross-modal embeddings have been\nprimarily confined to traditional dimensionality reduction (DR) techniques like\nPCA and t-SNE. These DR methods primarily focus on feature distributions within\na single modality, whilst failing to incorporate metrics (e.g., CLIPScore)\nacross multiple modalities.This paper introduces AKRMap, a new DR technique\ndesigned to visualize cross-modal embeddings metric with enhanced accuracy by\nlearning kernel regression of the metric landscape in the projection space.\nSpecifically, AKRMap constructs a supervised projection network guided by a\npost-projection kernel regression loss, and employs adaptive generalized\nkernels that can be jointly optimized with the projection. This approach\nenables AKRMap to efficiently generate visualizations that capture complex\nmetric distributions, while also supporting interactive features such as zoom\nand overlay for deeper exploration. Quantitative experiments demonstrate that\nAKRMap outperforms existing DR methods in generating more accurate and\ntrustworthy visualizations. We further showcase the effectiveness of AKRMap in\nvisualizing and comparing cross-modal embeddings for text-to-image models. Code\nand demo are available at https://github.com/yilinye/AKRMap."}
{"id": "2505.14671", "pdf": "https://arxiv.org/pdf/2505.14671", "abs": "https://arxiv.org/abs/2505.14671", "authors": ["Ruichuan An", "Sihan Yang", "Renrui Zhang", "Zijun Shen", "Ming Lu", "Gaole Dai", "Hao Liang", "Ziyu Guo", "Shilin Yan", "Yulin Luo", "Bocheng Zou", "Chaoqun Yang", "Wentao Zhang"], "title": "UniCTokens: Boosting Personalized Understanding and Generation via Unified Concept Tokens", "categories": ["cs.CV"], "comment": null, "summary": "Personalized models have demonstrated remarkable success in understanding and\ngenerating concepts provided by users. However, existing methods use separate\nconcept tokens for understanding and generation, treating these tasks in\nisolation. This may result in limitations for generating images with complex\nprompts. For example, given the concept $\\langle bo\\rangle$, generating\n\"$\\langle bo\\rangle$ wearing its hat\" without additional textual descriptions\nof its hat. We call this kind of generation personalized knowledge-driven\ngeneration. To address the limitation, we present UniCTokens, a novel framework\nthat effectively integrates personalized information into a unified vision\nlanguage model (VLM) for understanding and generation. UniCTokens trains a set\nof unified concept tokens to leverage complementary semantics, boosting two\npersonalized tasks. Moreover, we propose a progressive training strategy with\nthree stages: understanding warm-up, bootstrapping generation from\nunderstanding, and deepening understanding from generation to enhance mutual\nbenefits between both tasks. To quantitatively evaluate the unified VLM\npersonalization, we present UnifyBench, the first benchmark for assessing\nconcept understanding, concept generation, and knowledge-driven generation.\nExperimental results on UnifyBench indicate that UniCTokens shows competitive\nperformance compared to leading methods in concept understanding, concept\ngeneration, and achieving state-of-the-art results in personalized\nknowledge-driven generation. Our research demonstrates that enhanced\nunderstanding improves generation, and the generation process can yield\nvaluable insights into understanding. Our code and dataset will be released at:\n\\href{https://github.com/arctanxarc/UniCTokens}{https://github.com/arctanxarc/UniCTokens}."}
{"id": "2505.14673", "pdf": "https://arxiv.org/pdf/2505.14673", "abs": "https://arxiv.org/abs/2505.14673", "authors": ["Yu Tong", "Zihao Pan", "Shuai Yang", "Kaiyang Zhou"], "title": "Training-Free Watermarking for Autoregressive Image Generation", "categories": ["cs.CV", "cs.AI", "cs.CR"], "comment": null, "summary": "Invisible image watermarking can protect image ownership and prevent\nmalicious misuse of visual generative models. However, existing generative\nwatermarking methods are mainly designed for diffusion models while\nwatermarking for autoregressive image generation models remains largely\nunderexplored. We propose IndexMark, a training-free watermarking framework for\nautoregressive image generation models. IndexMark is inspired by the redundancy\nproperty of the codebook: replacing autoregressively generated indices with\nsimilar indices produces negligible visual differences. The core component in\nIndexMark is a simple yet effective match-then-replace method, which carefully\nselects watermark tokens from the codebook based on token similarity, and\npromotes the use of watermark tokens through token replacement, thereby\nembedding the watermark without affecting the image quality. Watermark\nverification is achieved by calculating the proportion of watermark tokens in\ngenerated images, with precision further improved by an Index Encoder.\nFurthermore, we introduce an auxiliary validation scheme to enhance robustness\nagainst cropping attacks. Experiments demonstrate that IndexMark achieves\nstate-of-the-art performance in terms of image quality and verification\naccuracy, and exhibits robustness against various perturbations, including\ncropping, noises, Gaussian blur, random erasing, color jittering, and JPEG\ncompression."}
{"id": "2505.14677", "pdf": "https://arxiv.org/pdf/2505.14677", "abs": "https://arxiv.org/abs/2505.14677", "authors": ["Jiaer Xia", "Yuhang Zang", "Peng Gao", "Yixuan Li", "Kaiyang Zhou"], "title": "Visionary-R1: Mitigating Shortcuts in Visual Reasoning with Reinforcement Learning", "categories": ["cs.CV"], "comment": null, "summary": "Learning general-purpose reasoning capabilities has long been a challenging\nproblem in AI. Recent research in large language models (LLMs), such as\nDeepSeek-R1, has shown that reinforcement learning techniques like GRPO can\nenable pre-trained LLMs to develop reasoning capabilities using simple\nquestion-answer pairs. In this paper, we aim to train visual language models\n(VLMs) to perform reasoning on image data through reinforcement learning and\nvisual question-answer pairs, without any explicit chain-of-thought (CoT)\nsupervision. Our findings indicate that simply applying reinforcement learning\nto a VLM -- by prompting the model to produce a reasoning chain before\nproviding an answer -- can lead the model to develop shortcuts from easy\nquestions, thereby reducing its ability to generalize across unseen data\ndistributions. We argue that the key to mitigating shortcut learning is to\nencourage the model to interpret images prior to reasoning. Therefore, we train\nthe model to adhere to a caption-reason-answer output format: initially\ngenerating a detailed caption for an image, followed by constructing an\nextensive reasoning chain. When trained on 273K CoT-free visual question-answer\npairs and using only reinforcement learning, our model, named Visionary-R1,\noutperforms strong multimodal models, such as GPT-4o, Claude3.5-Sonnet, and\nGemini-1.5-Pro, on multiple visual reasoning benchmarks."}
{"id": "2505.14682", "pdf": "https://arxiv.org/pdf/2505.14682", "abs": "https://arxiv.org/abs/2505.14682", "authors": ["Rui Tian", "Mingfei Gao", "Mingze Xu", "Jiaming Hu", "Jiasen Lu", "Zuxuan Wu", "Yinfei Yang", "Afshin Dehghan"], "title": "UniGen: Enhanced Training & Test-Time Strategies for Unified Multimodal Understanding and Generation", "categories": ["cs.CV"], "comment": "Technical report", "summary": "We introduce UniGen, a unified multimodal large language model (MLLM) capable\nof image understanding and generation. We study the full training pipeline of\nUniGen from a data-centric perspective, including multi-stage pre-training,\nsupervised fine-tuning, and direct preference optimization. More importantly,\nwe propose a new Chain-of-Thought Verification (CoT-V) strategy for test-time\nscaling, which significantly boosts UniGen's image generation quality using a\nsimple Best-of-N test-time strategy. Specifically, CoT-V enables UniGen to act\nas both image generator and verifier at test time, assessing the semantic\nalignment between a text prompt and its generated image in a step-by-step CoT\nmanner. Trained entirely on open-source datasets across all stages, UniGen\nachieves state-of-the-art performance on a range of image understanding and\ngeneration benchmarks, with a final score of 0.78 on GenEval and 85.19 on\nDPG-Bench. Through extensive ablation studies, our work provides actionable\ninsights and addresses key challenges in the full life cycle of building\nunified MLLMs, contributing meaningful directions to the future research."}
{"id": "2505.14683", "pdf": "https://arxiv.org/pdf/2505.14683", "abs": "https://arxiv.org/abs/2505.14683", "authors": ["Chaorui Deng", "Deyao Zhu", "Kunchang Li", "Chenhui Gou", "Feng Li", "Zeyu Wang", "Shu Zhong", "Weihao Yu", "Xiaonan Nie", "Ziang Song", "Guang Shi", "Haoqi Fan"], "title": "Emerging Properties in Unified Multimodal Pretraining", "categories": ["cs.CV"], "comment": "37 pages, 17 figures", "summary": "Unifying multimodal understanding and generation has shown impressive\ncapabilities in cutting-edge proprietary systems. In this work, we introduce\nBAGEL, an open0source foundational model that natively supports multimodal\nunderstanding and generation. BAGEL is a unified, decoder0only model pretrained\non trillions of tokens curated from large0scale interleaved text, image, video,\nand web data. When scaled with such diverse multimodal interleaved data, BAGEL\nexhibits emerging capabilities in complex multimodal reasoning. As a result, it\nsignificantly outperforms open-source unified models in both multimodal\ngeneration and understanding across standard benchmarks, while exhibiting\nadvanced multimodal reasoning abilities such as free-form image manipulation,\nfuture frame prediction, 3D manipulation, and world navigation. In the hope of\nfacilitating further opportunities for multimodal research, we share the key\nfindings, pretraining details, data creation protocal, and release our code and\ncheckpoints to the community. The project page is at https://bagel-ai.org/"}
{"id": "2505.14687", "pdf": "https://arxiv.org/pdf/2505.14687", "abs": "https://arxiv.org/abs/2505.14687", "authors": ["Sucheng Ren", "Qihang Yu", "Ju He", "Alan Yuille", "Liang-Chieh Chen"], "title": "Grouping First, Attending Smartly: Training-Free Acceleration for Diffusion Transformers", "categories": ["cs.CV"], "comment": "Project website at oliverrensu.github.io/project/GRAT", "summary": "Diffusion-based Transformers have demonstrated impressive generative\ncapabilities, but their high computational costs hinder practical deployment,\nfor example, generating an $8192\\times 8192$ image can take over an hour on an\nA100 GPU. In this work, we propose GRAT (\\textbf{GR}ouping first,\n\\textbf{AT}tending smartly), a training-free attention acceleration strategy\nfor fast image and video generation without compromising output quality. The\nkey insight is to exploit the inherent sparsity in learned attention maps\n(which tend to be locally focused) in pretrained Diffusion Transformers and\nleverage better GPU parallelism. Specifically, GRAT first partitions contiguous\ntokens into non-overlapping groups, aligning both with GPU execution patterns\nand the local attention structures learned in pretrained generative\nTransformers. It then accelerates attention by having all query tokens within\nthe same group share a common set of attendable key and value tokens. These key\nand value tokens are further restricted to structured regions, such as\nsurrounding blocks or criss-cross regions, significantly reducing computational\noverhead (e.g., attaining a \\textbf{35.8$\\times$} speedup over full attention\nwhen generating $8192\\times 8192$ images) while preserving essential attention\npatterns and long-range context. We validate GRAT on pretrained Flux and\nHunyuanVideo for image and video generation, respectively. In both cases, GRAT\nachieves substantially faster inference without any fine-tuning, while\nmaintaining the performance of full attention. We hope GRAT will inspire future\nresearch on accelerating Diffusion Transformers for scalable visual generation."}
{"id": "2505.06699", "pdf": "https://arxiv.org/pdf/2505.06699", "abs": "https://arxiv.org/abs/2505.06699", "authors": ["Xiyuan Wei", "Ming Lin", "Fanjiang Ye", "Fengguang Song", "Liangliang Cao", "My T. Thai", "Tianbao Yang"], "title": "Model Steering: Learning with a Reference Model Improves Generalization Bounds and Scaling Laws", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "comment": "18 pages, 6 figures", "summary": "This paper formalizes an emerging learning paradigm that uses a trained model\nas a reference to guide and enhance the training of a target model through\nstrategic data selection or weighting, named $\\textbf{model steering}$. While\nad-hoc methods have been used in various contexts, including the training of\nlarge foundation models, its underlying principles remain insufficiently\nunderstood, leading to sub-optimal performance. In this work, we propose a\ntheory-driven framework for model steering called $\\textbf{DRRho risk\nminimization}$, which is rooted in Distributionally Robust Optimization (DRO).\nThrough a generalization analysis, we provide theoretical insights into why\nthis approach improves generalization and data efficiency compared to training\nwithout a reference model. To the best of our knowledge, this is the first time\nsuch theoretical insights are provided for the new learning paradigm, which\nsignificantly enhance our understanding and practice of model steering.\nBuilding on these insights and the connection between contrastive learning and\nDRO, we introduce a novel method for Contrastive Language-Image Pretraining\n(CLIP) with a reference model, termed DRRho-CLIP. Extensive experiments\nvalidate the theoretical insights, reveal a superior scaling law compared to\nCLIP without a reference model, and demonstrate its strength over existing\nheuristic approaches."}
{"id": "2505.13462", "pdf": "https://arxiv.org/pdf/2505.13462", "abs": "https://arxiv.org/abs/2505.13462", "authors": ["Thien Nguyen", "William Guicquero"], "title": "End-to-end fully-binarized network design: from Generic Learned Thermometer to Block Pruning", "categories": ["cs.LG", "cs.AR", "cs.CV", "eess.IV", "stat.ML"], "comment": "Accepted to IEEE AICAS 2025", "summary": "Existing works on Binary Neural Network (BNN) mainly focus on model's weights\nand activations while discarding considerations on the input raw data. This\narticle introduces Generic Learned Thermometer (GLT), an encoding technique to\nimprove input data representation for BNN, relying on learning non linear\nquantization thresholds. This technique consists in multiple data binarizations\nwhich can advantageously replace a conventional Analog to Digital Conversion\n(ADC) that uses natural binary coding. Additionally, we jointly propose a\ncompact topology with light-weight grouped convolutions being trained thanks to\nblock pruning and Knowledge Distillation (KD), aiming at reducing furthermore\nthe model size so as its computational complexity. We show that GLT brings\nversatility to the BNN by intrinsically performing global tone mapping,\nenabling significant accuracy gains in practice (demonstrated by simulations on\nthe STL-10 and VWW datasets). Moreover, when combining GLT with our proposed\nblock-pruning technique, we successfully achieve lightweight (under 1Mb),\nfully-binarized models with limited accuracy degradation while being suitable\nfor in-sensor always-on inference use cases."}
{"id": "2505.13507", "pdf": "https://arxiv.org/pdf/2505.13507", "abs": "https://arxiv.org/abs/2505.13507", "authors": ["Haoyang Chen"], "title": "Open Set Domain Adaptation with Vision-language models via Gradient-aware Separation", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Open-Set Domain Adaptation (OSDA) confronts the dual challenge of aligning\nknown-class distributions across domains while identifying\ntarget-domain-specific unknown categories. Current approaches often fail to\nleverage semantic relationships between modalities and struggle with error\naccumulation in unknown sample detection. We propose to harness Contrastive\nLanguage-Image Pretraining (CLIP) to address these limitations through two key\ninnovations: 1) Prompt-driven cross-domain alignment: Learnable textual prompts\nconditioned on domain discrepancy metrics dynamically adapt CLIP's text\nencoder, enabling semantic consistency between source and target domains\nwithout explicit unknown-class supervision. 2) Gradient-aware open-set\nseparation: A gradient analysis module quantifies domain shift by comparing the\nL2-norm of gradients from the learned prompts, where known/unknown samples\nexhibit statistically distinct gradient behaviors. Evaluations on Office-Home\nshow that our method consistently outperforms CLIP baseline and standard\nbaseline. Ablation studies confirm the gradient norm's critical role."}
{"id": "2505.13539", "pdf": "https://arxiv.org/pdf/2505.13539", "abs": "https://arxiv.org/abs/2505.13539", "authors": ["Rodrigo Fritz", "Pablo Surez-Serrato", "Victor Mijangos", "Anayanzi D. Martinez-Hernandez", "Eduardo Ivan Velazquez Richards"], "title": "EuLearn: A 3D database for learning Euler characteristics", "categories": ["cs.CG", "cs.CV", "cs.LG", "math.DG", "math.GT"], "comment": "35 pages, many figures. Datasets and source code publicly available\n  at https://huggingface.co/datasets/appliedgeometry/EuLearn and\n  https://github.com/appliedgeometry/EuLearn_db", "summary": "We present EuLearn, the first surface datasets equitably representing a\ndiversity of topological types. We designed our embedded surfaces of uniformly\nvarying genera relying on random knots, thus allowing our surfaces to knot with\nthemselves. EuLearn contributes new topological datasets of meshes, point\nclouds, and scalar fields in 3D. We aim to facilitate the training of machine\nlearning systems that can discern topological features. We experimented with\nspecific emblematic 3D neural network architectures, finding that their vanilla\nimplementations perform poorly on genus classification. To enhance performance,\nwe developed a novel, non-Euclidean, statistical sampling method adapted to\ngraph and manifold data. We also introduce adjacency-informed adaptations of\nPointNet and Transformer architectures that rely on our non-Euclidean sampling\nstrategy. Our results demonstrate that incorporating topological information\ninto deep learning workflows significantly improves performance on these\notherwise challenging EuLearn datasets."}
{"id": "2505.13542", "pdf": "https://arxiv.org/pdf/2505.13542", "abs": "https://arxiv.org/abs/2505.13542", "authors": ["Karthik Sivakoti"], "title": "GANCompress: GAN-Enhanced Neural Image Compression with Binary Spherical Quantization", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "The exponential growth of visual data in digital communications has\nintensified the need for efficient compression techniques that balance\nrate-distortion performance with computational feasibility. While recent neural\ncompression approaches have shown promise, they still struggle with fundamental\nchallenges: preserving perceptual quality at high compression ratios,\ncomputational efficiency, and adaptability to diverse visual content. This\npaper introduces GANCompress, a novel neural compression framework that\nsynergistically combines Binary Spherical Quantization (BSQ) with Generative\nAdversarial Networks (GANs) to address these challenges. Our approach employs a\ntransformer-based autoencoder with an enhanced BSQ bottleneck that projects\nlatent representations onto a hypersphere, enabling efficient discretization\nwith bounded quantization error. This is followed by a specialized GAN\narchitecture incorporating frequency-domain attention and color consistency\noptimization. Experimental results demonstrate that GANCompress achieves\nsubstantial improvement in compression efficiency -- reducing file sizes by up\nto 100x with minimal visual distortion. Our method outperforms traditional\ncodecs like H.264 by 12-15% in perceptual metrics while maintaining comparable\nPSNR/SSIM values, with 2.4x faster encoding and decoding speeds. On standard\nbenchmarks including ImageNet-1k and COCO2017, GANCompress sets a new\nstate-of-the-art, reducing FID from 0.72 to 0.41 (43% improvement) compared to\nprevious methods while maintaining higher throughput. This work presents a\nsignificant advancement in neural compression technology with promising\napplications for real-time visual communication systems."}
{"id": "2505.13563", "pdf": "https://arxiv.org/pdf/2505.13563", "abs": "https://arxiv.org/abs/2505.13563", "authors": ["Xiaohui Wang", "Peng Ye", "Chenyu Huang", "Shenghe Zheng", "Bo Zhang", "Wanli Ouyang", "Tao Chen"], "title": "Breaking the Compression Ceiling: Data-Free Pipeline for Ultra-Efficient Delta Compression", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "With the rise of the fine-tuned--pretrained paradigm, storing numerous\nfine-tuned models for multi-tasking creates significant storage overhead. Delta\ncompression alleviates this by storing only the pretrained model and the highly\ncompressed delta weights (the differences between fine-tuned and pretrained\nmodel weights). However, existing methods fail to maintain both high\ncompression and performance, and often rely on data. To address these\nchallenges, we propose UltraDelta, the first data-free delta compression\npipeline that achieves both ultra-high compression and strong performance.\nUltraDelta is designed to minimize redundancy, maximize information, and\nstabilize performance across inter-layer, intra-layer, and global dimensions,\nusing three key components: (1) Variance-Based Mixed Sparsity Allocation\nassigns sparsity based on variance, giving lower sparsity to high-variance\nlayers to preserve inter-layer information. (2) Distribution-Aware Compression\napplies uniform quantization and then groups parameters by value, followed by\ngroup-wise pruning, to better preserve intra-layer distribution. (3)\nTrace-Norm-Guided Rescaling uses the trace norm of delta weights to estimate a\nglobal rescaling factor, improving model stability under higher compression.\nExtensive experiments across (a) large language models (fine-tuned on LLaMA-2\n7B and 13B) with up to 133x, (b) general NLP models (RoBERTa-base, T5-base)\nwith up to 800x, (c) vision models (ViT-B/32, ViT-L/14) with up to 400x, and\n(d) multi-modal models (BEiT-3) with 40x compression ratio, demonstrate that\nUltraDelta consistently outperforms existing methods, especially under\nultra-high compression."}
{"id": "2505.13579", "pdf": "https://arxiv.org/pdf/2505.13579", "abs": "https://arxiv.org/abs/2505.13579", "authors": ["Yipeng Sun", "Linda-Sophie Schneider", "Chengze Ye", "Mingxuan Gu", "Siyuan Mei", "Siming Bayer", "Andreas Maier"], "title": "Learning Wavelet-Sparse FDK for 3D Cone-Beam CT Reconstruction", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": "Accepted by Fully3D 2025", "summary": "Cone-Beam Computed Tomography (CBCT) is essential in medical imaging, and the\nFeldkamp-Davis-Kress (FDK) algorithm is a popular choice for reconstruction due\nto its efficiency. However, FDK is susceptible to noise and artifacts. While\nrecent deep learning methods offer improved image quality, they often increase\ncomputational complexity and lack the interpretability of traditional methods.\nIn this paper, we introduce an enhanced FDK-based neural network that maintains\nthe classical algorithm's interpretability by selectively integrating trainable\nelements into the cosine weighting and filtering stages. Recognizing the\nchallenge of a large parameter space inherent in 3D CBCT data, we leverage\nwavelet transformations to create sparse representations of the cosine weights\nand filters. This strategic sparsification reduces the parameter count by\n$93.75\\%$ without compromising performance, accelerates convergence, and\nimportantly, maintains the inference computational cost equivalent to the\nclassical FDK algorithm. Our method not only ensures volumetric consistency and\nboosts robustness to noise, but is also designed for straightforward\nintegration into existing CT reconstruction pipelines. This presents a\npragmatic enhancement that can benefit clinical applications, particularly in\nenvironments with computational limitations."}
{"id": "2505.13617", "pdf": "https://arxiv.org/pdf/2505.13617", "abs": "https://arxiv.org/abs/2505.13617", "authors": ["Christopher Ick", "Gordon Wichern", "Yoshiki Masuyama", "Franois Germain", "Jonathan Le Roux"], "title": "Direction-Aware Neural Acoustic Fields for Few-Shot Interpolation of Ambisonic Impulse Responses", "categories": ["eess.AS", "cs.AI", "cs.CV", "cs.LG", "cs.SD"], "comment": "Accepted at Interspeech 2025", "summary": "The characteristics of a sound field are intrinsically linked to the\ngeometric and spatial properties of the environment surrounding a sound source\nand a listener. The physics of sound propagation is captured in a time-domain\nsignal known as a room impulse response (RIR). Prior work using neural fields\n(NFs) has allowed learning spatially-continuous representations of RIRs from\nfinite RIR measurements. However, previous NF-based methods have focused on\nmonaural omnidirectional or at most binaural listeners, which does not\nprecisely capture the directional characteristics of a real sound field at a\nsingle point. We propose a direction-aware neural field (DANF) that more\nexplicitly incorporates the directional information by Ambisonic-format RIRs.\nWhile DANF inherently captures spatial relations between sources and listeners,\nwe further propose a direction-aware loss. In addition, we investigate the\nability of DANF to adapt to new rooms in various ways including low-rank\nadaptation."}
{"id": "2505.13643", "pdf": "https://arxiv.org/pdf/2505.13643", "abs": "https://arxiv.org/abs/2505.13643", "authors": ["Rakibul Hasan Rajib", "Md Akil Raihan Iftee", "Mir Sazzat Hossain", "A. K. M. Mahbubur Rahman", "Sajib Mistry", "M Ashraful Amin", "Amin Ahsan Ali"], "title": "FedCTTA: A Collaborative Approach to Continual Test-Time Adaptation in Federated Learning", "categories": ["cs.LG", "cs.CV"], "comment": "8 pages, 5 figures, Accepted In IJCNN 2025", "summary": "Federated Learning (FL) enables collaborative model training across\ndistributed clients without sharing raw data, making it ideal for\nprivacy-sensitive applications. However, FL models often suffer performance\ndegradation due to distribution shifts between training and deployment.\nTest-Time Adaptation (TTA) offers a promising solution by allowing models to\nadapt using only test samples. However, existing TTA methods in FL face\nchallenges such as computational overhead, privacy risks from feature sharing,\nand scalability concerns due to memory constraints. To address these\nlimitations, we propose Federated Continual Test-Time Adaptation (FedCTTA), a\nprivacy-preserving and computationally efficient framework for federated\nadaptation. Unlike prior methods that rely on sharing local feature statistics,\nFedCTTA avoids direct feature exchange by leveraging similarity-aware\naggregation based on model output distributions over randomly generated noise\nsamples. This approach ensures adaptive knowledge sharing while preserving data\nprivacy. Furthermore, FedCTTA minimizes the entropy at each client for\ncontinual adaptation, enhancing the model's confidence in evolving target\ndistributions. Our method eliminates the need for server-side training during\nadaptation and maintains a constant memory footprint, making it scalable even\nas the number of clients or training rounds increases. Extensive experiments\nshow that FedCTTA surpasses existing methods across diverse temporal and\nspatial heterogeneity scenarios."}
{"id": "2505.13740", "pdf": "https://arxiv.org/pdf/2505.13740", "abs": "https://arxiv.org/abs/2505.13740", "authors": ["Chenning Yu", "Sicun Gao"], "title": "Improving Compositional Generation with Diffusion Models Using Lift Scores", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "We introduce a novel resampling criterion using lift scores, for improving\ncompositional generation in diffusion models. By leveraging the lift scores, we\nevaluate whether generated samples align with each single condition and then\ncompose the results to determine whether the composed prompt is satisfied. Our\nkey insight is that lift scores can be efficiently approximated using only the\noriginal diffusion model, requiring no additional training or external modules.\nWe develop an optimized variant that achieves relatively lower computational\noverhead during inference while maintaining effectiveness. Through extensive\nexperiments, we demonstrate that lift scores significantly improved the\ncondition alignment for compositional generation across 2D synthetic data,\nCLEVR position tasks, and text-to-image synthesis. Our code is available at\nhttp://github.com/rainorangelemon/complift."}
{"id": "2505.13813", "pdf": "https://arxiv.org/pdf/2505.13813", "abs": "https://arxiv.org/abs/2505.13813", "authors": ["Matthew Raffel", "Lizhong Chen"], "title": "FlashKAT: Understanding and Addressing Performance Bottlenecks in the Kolmogorov-Arnold Transformer", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "The Kolmogorov-Arnold Network (KAN) has been gaining popularity as an\nalternative to the multi-layer perceptron (MLP) with its increased\nexpressiveness and interpretability. However, the KAN can be orders of\nmagnitude slower due to its increased computational cost and training\ninstability, limiting its applicability to larger-scale tasks. Recently, the\nKolmogorov-Arnold Transformer (KAT) has been proposed, which can achieve FLOPs\nsimilar to the traditional Transformer with MLPs by leveraging Group-Rational\nKAN (GR-KAN). Unfortunately, despite the comparable FLOPs, our\ncharacterizations reveal that the KAT is still 123x slower in training speeds,\nindicating that there are other performance bottlenecks beyond FLOPs. In this\npaper, we conduct a series of experiments to understand the root cause of the\nslowdown in KAT. We uncover that the slowdown can be isolated to memory stalls\nand, more specifically, in the backward pass of GR-KAN caused by inefficient\ngradient accumulation. To address this memory bottleneck, we propose FlashKAT,\nwhich builds on our restructured kernel that minimizes gradient accumulation\nwith atomic adds and accesses to slow memory. Evaluations demonstrate that\nFlashKAT can achieve a training speedup of 86.5x compared with the\nstate-of-the-art KAT, while reducing rounding errors in the coefficient\ngradients. Our code is available at https://github.com/OSU-STARLAB/FlashKAT."}
{"id": "2505.13841", "pdf": "https://arxiv.org/pdf/2505.13841", "abs": "https://arxiv.org/abs/2505.13841", "authors": ["Yixuan Gao", "Xiongkuo Min", "Guangtao Zhai"], "title": "Exploring Image Quality Assessment from a New Perspective: Pupil Size", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "This paper explores how the image quality assessment (IQA) task affects the\ncognitive processes of people from the perspective of pupil size and studies\nthe relationship between pupil size and image quality. Specifically, we first\ninvited subjects to participate in a subjective experiment, which includes two\ntasks: free observation and IQA. In the free observation task, subjects did not\nneed to perform any action, and they only needed to observe images as they\nusually do with an album. In the IQA task, subjects were required to score\nimages according to their overall impression of image quality. Then, by\nanalyzing the difference in pupil size between the two tasks, we find that\npeople may activate the visual attention mechanism when evaluating image\nquality. Meanwhile, we also find that the change in pupil size is closely\nrelated to image quality in the IQA task. For future research on IQA, this\nresearch can not only provide a theoretical basis for the objective IQA method\nand promote the development of more effective objective IQA methods, but also\nprovide a new subjective IQA method for collecting the authentic subjective\nimpression of image quality."}
{"id": "2505.13875", "pdf": "https://arxiv.org/pdf/2505.13875", "abs": "https://arxiv.org/abs/2505.13875", "authors": ["Lanlan Kang", "Jian Wang", "Jian QIn", "Yiqin Liang", "Yongjun He"], "title": "Automated Quality Evaluation of Cervical Cytopathology Whole Slide Images Based on Content Analysis", "categories": ["eess.IV", "cs.CV"], "comment": "12 pages, 10 figures", "summary": "The ThinPrep Cytologic Test (TCT) is the most widely used method for cervical\ncancer screening, and the sample quality directly impacts the accuracy of the\ndiagnosis. Traditional manual evaluation methods rely on the observation of\npathologist under microscopes. These methods exhibit high subjectivity, high\ncost, long duration, and low reliability. With the development of\ncomputer-aided diagnosis (CAD), an automated quality assessment system that\nperforms at the level of a professional pathologist is necessary. To address\nthis need, we propose a fully automated quality assessment method for Cervical\nCytopathology Whole Slide Images (WSIs) based on The Bethesda System (TBS)\ndiagnostic standards, artificial intelligence algorithms, and the\ncharacteristics of clinical data. The method analysis the context of WSIs to\nquantify quality evaluation metrics which are focused by TBS such as staining\nquality, cell counts and cell mass proportion through multiple models including\nobject detection, classification and segmentation. Subsequently, the XGBoost\nmodel is used to mine the attention paid by pathologists to different quality\nevaluation metrics when evaluating samples, thereby obtaining a comprehensive\nWSI sample score calculation model. Experimental results on 100 WSIs\ndemonstrate that the proposed evaluation method has significant advantages in\nterms of speed and consistency."}
{"id": "2505.13906", "pdf": "https://arxiv.org/pdf/2505.13906", "abs": "https://arxiv.org/abs/2505.13906", "authors": ["Soyabul Islam Lincoln", "Mirza Mohd Shahriar Maswood"], "title": "XDementNET: An Explainable Attention Based Deep Convolutional Network to Detect Alzheimer Progression from MRI data", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "20 pages, 12 figures,", "summary": "A common neurodegenerative disease, Alzheimer's disease requires a precise\ndiagnosis and efficient treatment, particularly in light of escalating\nhealthcare expenses and the expanding use of artificial intelligence in medical\ndiagnostics. Many recent studies shows that the combination of brain Magnetic\nResonance Imaging (MRI) and deep neural networks have achieved promising\nresults for diagnosing AD. Using deep convolutional neural networks, this paper\nintroduces a novel deep learning architecture that incorporates multiresidual\nblocks, specialized spatial attention blocks, grouped query attention, and\nmulti-head attention. The study assessed the model's performance on four\npublicly accessible datasets and concentrated on identifying binary and\nmulticlass issues across various categories. This paper also takes into account\nof the explainability of AD's progression and compared with state-of-the-art\nmethods namely Gradient Class Activation Mapping (GradCAM), Score-CAM, Faster\nScore-CAM, and XGRADCAM. Our methodology consistently outperforms current\napproaches, achieving 99.66\\% accuracy in 4-class classification, 99.63\\% in\n3-class classification, and 100\\% in binary classification using Kaggle\ndatasets. For Open Access Series of Imaging Studies (OASIS) datasets the\naccuracies are 99.92\\%, 99.90\\%, and 99.95\\% respectively. The Alzheimer's\nDisease Neuroimaging Initiative-1 (ADNI-1) dataset was used for experiments in\nthree planes (axial, sagittal, and coronal) and a combination of all planes.\nThe study achieved accuracies of 99.08\\% for axis, 99.85\\% for sagittal, 99.5\\%\nfor coronal, and 99.17\\% for all axis, and 97.79\\% and 8.60\\% respectively for\nADNI-2. The network's ability to retrieve important information from MRI images\nis demonstrated by its excellent accuracy in categorizing AD stages."}
{"id": "2505.13911", "pdf": "https://arxiv.org/pdf/2505.13911", "abs": "https://arxiv.org/abs/2505.13911", "authors": ["Ruijie Zhao", "Zuopeng Tan", "Xiao Xue", "Longfei Zhao", "Bing Li", "Zicheng Liao", "Ying Ming", "Jiaru Wang", "Ran Xiao", "Sirong Piao", "Rui Zhao", "Qiqi Xu", "Wei Song"], "title": "Bronchovascular Tree-Guided Weakly Supervised Learning Method for Pulmonary Segment Segmentation", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Pulmonary segment segmentation is crucial for cancer localization and\nsurgical planning. However, the pixel-wise annotation of pulmonary segments is\nlaborious, as the boundaries between segments are indistinguishable in medical\nimages. To this end, we propose a weakly supervised learning (WSL) method,\ntermed Anatomy-Hierarchy Supervised Learning (AHSL), which consults the precise\nclinical anatomical definition of pulmonary segments to perform pulmonary\nsegment segmentation. Since pulmonary segments reside within the lobes and are\ndetermined by the bronchovascular tree, i.e., artery, airway and vein, the\ndesign of the loss function is founded on two principles. First, segment-level\nlabels are utilized to directly supervise the output of the pulmonary segments,\nensuring that they accurately encompass the appropriate bronchovascular tree.\nSecond, lobe-level supervision indirectly oversees the pulmonary segment,\nensuring their inclusion within the corresponding lobe. Besides, we introduce a\ntwo-stage segmentation strategy that incorporates bronchovascular priori\ninformation. Furthermore, a consistency loss is proposed to enhance the\nsmoothness of segment boundaries, along with an evaluation metric designed to\nmeasure the smoothness of pulmonary segment boundaries. Visual inspection and\nevaluation metrics from experiments conducted on a private dataset demonstrate\nthe effectiveness of our method."}
{"id": "2505.13973", "pdf": "https://arxiv.org/pdf/2505.13973", "abs": "https://arxiv.org/abs/2505.13973", "authors": ["Wenhui Zhu", "Xuanzhao Dong", "Xin Li", "Peijie Qiu", "Xiwen Chen", "Abolfazl Razi", "Aris Sotiras", "Yi Su", "Yalin Wang"], "title": "Toward Effective Reinforcement Learning Fine-Tuning for Medical VQA in Vision-Language Models", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Recently, reinforcement learning (RL)-based tuning has shifted the trajectory\nof Multimodal Large Language Models (MLLMs), particularly following the\nintroduction of Group Relative Policy Optimization (GRPO). However, directly\napplying it to medical tasks remains challenging for achieving clinically\ngrounded model behavior. Motivated by the need to align model response with\nclinical expectations, we investigate four critical dimensions that affect the\neffectiveness of RL-based tuning in medical visual question answering (VQA):\nbase model initialization strategy, the role of medical semantic alignment, the\nimpact of length-based rewards on long-chain reasoning, and the influence of\nbias. We conduct extensive experiments to analyze these factors for medical\nMLLMs, providing new insights into how models are domain-specifically\nfine-tuned. Additionally, our results also demonstrate that GRPO-based RL\ntuning consistently outperforms standard supervised fine-tuning (SFT) in both\naccuracy and reasoning quality."}
{"id": "2505.14017", "pdf": "https://arxiv.org/pdf/2505.14017", "abs": "https://arxiv.org/abs/2505.14017", "authors": ["Jesper Duemose Nielsen", "Karthik Gopinath", "Andrew Hoopes", "Adrian Dalca", "Colin Magdamo", "Steven Arnold", "Sudeshna Das", "Axel Thielscher", "Juan Eugenio Iglesias", "Oula Puonti"], "title": "End-to-end Cortical Surface Reconstruction from Clinical Magnetic Resonance Images", "categories": ["eess.IV", "cs.CV"], "comment": "11 pages, 4 figures", "summary": "Surface-based cortical analysis is valuable for a variety of neuroimaging\ntasks, such as spatial normalization, parcellation, and gray matter (GM)\nthickness estimation. However, most tools for estimating cortical surfaces work\nexclusively on scans with at least 1 mm isotropic resolution and are tuned to a\nspecific magnetic resonance (MR) contrast, often T1-weighted (T1w). This\nprecludes application using most clinical MR scans, which are very\nheterogeneous in terms of contrast and resolution. Here, we use synthetic\ndomain-randomized data to train the first neural network for explicit\nestimation of cortical surfaces from scans of any contrast and resolution,\nwithout retraining. Our method deforms a template mesh to the white matter (WM)\nsurface, which guarantees topological correctness. This mesh is further\ndeformed to estimate the GM surface. We compare our method to\nrecon-all-clinical (RAC), an implicit surface reconstruction method which is\ncurrently the only other tool capable of processing heterogeneous clinical MR\nscans, on ADNI and a large clinical dataset (n=1,332). We show a approximately\n50 % reduction in cortical thickness error (from 0.50 to 0.24 mm) with respect\nto RAC and better recovery of the aging-related cortical thinning patterns\ndetected by FreeSurfer on high-resolution T1w scans. Our method enables fast\nand accurate surface reconstruction of clinical scans, allowing studies (1)\nwith sample sizes far beyond what is feasible in a research setting, and (2) of\nclinical populations that are difficult to enroll in research studies. The code\nis publicly available at https://github.com/simnibs/brainnet."}
{"id": "2505.14021", "pdf": "https://arxiv.org/pdf/2505.14021", "abs": "https://arxiv.org/abs/2505.14021", "authors": ["Soichiro Kumano", "Hiroshi Kera", "Toshihiko Yamasaki"], "title": "Adversarial Training from Mean Field Perspective", "categories": ["cs.LG", "cs.CV", "stat.ML"], "comment": "NeurIPS23", "summary": "Although adversarial training is known to be effective against adversarial\nexamples, training dynamics are not well understood. In this study, we present\nthe first theoretical analysis of adversarial training in random deep neural\nnetworks without any assumptions on data distributions. We introduce a new\ntheoretical framework based on mean field theory, which addresses the\nlimitations of existing mean field-based approaches. Based on this framework,\nwe derive (empirically tight) upper bounds of $\\ell_q$ norm-based adversarial\nloss with $\\ell_p$ norm-based adversarial examples for various values of $p$\nand $q$. Moreover, we prove that networks without shortcuts are generally not\nadversarially trainable and that adversarial training reduces network capacity.\nWe also show that network width alleviates these issues. Furthermore, we\npresent the various impacts of the input and output dimensions on the upper\nbounds and time evolution of the weight variance."}
{"id": "2505.14022", "pdf": "https://arxiv.org/pdf/2505.14022", "abs": "https://arxiv.org/abs/2505.14022", "authors": ["Chenghuan Huang", "Zhigeng Xu", "Chong Sun", "Chen Li", "Ziyang Ma"], "title": "Towards Efficient Multi-Scale Deformable Attention on NPU", "categories": ["cs.PF", "cs.CV"], "comment": "10 pages, 8 figures", "summary": "Multi-scale deformable attention (MSDA) is a flexible and powerful feature\nextraction mechanism for visual tasks, but its random-access grid sampling\nstrategy poses significant optimization challenges, especially on\ndomain-specific accelerators such as NPUs. In this work, we present a co-design\napproach that systematically rethinks memory access and computation strategies\nfor MSDA on the Ascend NPU architecture. With this co-design approach, our\nimplementation supports both efficient forward and backward computation, is\nfully adapted for training workloads, and incorporates a suite of\nhardware-aware optimizations. Extensive experiments show that our solution\nachieves up to $5.9\\times$ (forward), $8.9\\times$ (backward), and $7.3\\times$\n(end-to-end training) speedup over the grid sample-based baseline, and\n$1.9\\times$, $2.4\\times$, and $2.0\\times$ acceleration over the latest vendor\nlibrary, respectively."}
{"id": "2505.14042", "pdf": "https://arxiv.org/pdf/2505.14042", "abs": "https://arxiv.org/abs/2505.14042", "authors": ["Soichiro Kumano", "Hiroshi Kera", "Toshihiko Yamasaki"], "title": "Adversarially Pretrained Transformers may be Universally Robust In-Context Learners", "categories": ["cs.LG", "cs.CV", "stat.ML"], "comment": null, "summary": "Adversarial training is one of the most effective adversarial defenses, but\nit incurs a high computational cost. In this study, we show that transformers\nadversarially pretrained on diverse tasks can serve as robust foundation models\nand eliminate the need for adversarial training in downstream tasks.\nSpecifically, we theoretically demonstrate that through in-context learning, a\nsingle adversarially pretrained transformer can robustly generalize to multiple\nunseen tasks without any additional training, i.e., without any parameter\nupdates. This robustness stems from the model's focus on robust features and\nits resistance to attacks that exploit non-predictive features. Besides these\npositive findings, we also identify several limitations. Under certain\nconditions (though unrealistic), no universally robust single-layer\ntransformers exist. Moreover, robust transformers exhibit an\naccuracy--robustness trade-off and require a large number of in-context\ndemonstrations. The code is available at\nhttps://github.com/s-kumano/universally-robust-in-context-learner."}
{"id": "2505.14064", "pdf": "https://arxiv.org/pdf/2505.14064", "abs": "https://arxiv.org/abs/2505.14064", "authors": ["Cosmin I. Bercea", "Jun Li", "Philipp Raffler", "Evamaria O. Riedel", "Lena Schmitzer", "Angela Kurz", "Felix Bitzer", "Paula Romller", "Julian Canisius", "Mirjam L. Beyrle", "Che Liu", "Wenjia Bai", "Bernhard Kainz", "Julia A. Schnabel", "Benedikt Wiestler"], "title": "NOVA: A Benchmark for Anomaly Localization and Clinical Reasoning in Brain MRI", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "In many real-world applications, deployed models encounter inputs that differ\nfrom the data seen during training. Out-of-distribution detection identifies\nwhether an input stems from an unseen distribution, while open-world\nrecognition flags such inputs to ensure the system remains robust as\never-emerging, previously $unknown$ categories appear and must be addressed\nwithout retraining. Foundation and vision-language models are pre-trained on\nlarge and diverse datasets with the expectation of broad generalization across\ndomains, including medical imaging. However, benchmarking these models on test\nsets with only a few common outlier types silently collapses the evaluation\nback to a closed-set problem, masking failures on rare or truly novel\nconditions encountered in clinical use.\n  We therefore present $NOVA$, a challenging, real-life $evaluation-only$\nbenchmark of $\\sim$900 brain MRI scans that span 281 rare pathologies and\nheterogeneous acquisition protocols. Each case includes rich clinical\nnarratives and double-blinded expert bounding-box annotations. Together, these\nenable joint assessment of anomaly localisation, visual captioning, and\ndiagnostic reasoning. Because NOVA is never used for training, it serves as an\n$extreme$ stress-test of out-of-distribution generalisation: models must bridge\na distribution gap both in sample appearance and in semantic space. Baseline\nresults with leading vision-language models (GPT-4o, Gemini 2.0 Flash, and\nQwen2.5-VL-72B) reveal substantial performance drops across all tasks,\nestablishing NOVA as a rigorous testbed for advancing models that can detect,\nlocalize, and reason about truly unknown anomalies."}
{"id": "2505.14071", "pdf": "https://arxiv.org/pdf/2505.14071", "abs": "https://arxiv.org/abs/2505.14071", "authors": ["Woody Haosheng Gan", "Deqing Fu", "Julian Asilis", "Ollie Liu", "Dani Yogatama", "Vatsal Sharan", "Robin Jia", "Willie Neiswanger"], "title": "Textual Steering Vectors Can Improve Visual Understanding in Multimodal Large Language Models", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": null, "summary": "Steering methods have emerged as effective and targeted tools for guiding\nlarge language models' (LLMs) behavior without modifying their parameters.\nMultimodal large language models (MLLMs), however, do not currently enjoy the\nsame suite of techniques, due in part to their recency and architectural\ndiversity. Inspired by this gap, we investigate whether MLLMs can be steered\nusing vectors derived from their text-only LLM backbone, via sparse\nautoencoders (SAEs), mean shift, and linear probing. We find that text-derived\nsteering consistently enhances multimodal accuracy across diverse MLLM\narchitectures and visual tasks. In particular, mean shift boosts spatial\nrelationship accuracy on CV-Bench by up to +7.3% and counting accuracy by up to\n+3.3%, outperforming prompting and exhibiting strong generalization to\nout-of-distribution datasets. These results highlight textual steering vectors\nas a powerful, efficient mechanism for enhancing grounding in MLLMs with\nminimal additional data collection and computational overhead."}
{"id": "2505.14087", "pdf": "https://arxiv.org/pdf/2505.14087", "abs": "https://arxiv.org/abs/2505.14087", "authors": ["Ziyi Chang", "He Wang", "George Alex Koulieris", "Hubert P. H. Shum"], "title": "Large-Scale Multi-Character Interaction Synthesis", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Generating large-scale multi-character interactions is a challenging and\nimportant task in character animation. Multi-character interactions involve not\nonly natural interactive motions but also characters coordinated with each\nother for transition. For example, a dance scenario involves characters dancing\nwith partners and also characters coordinated to new partners based on spatial\nand temporal observations. We term such transitions as coordinated interactions\nand decompose them into interaction synthesis and transition planning. Previous\nmethods of single-character animation do not consider interactions that are\ncritical for multiple characters. Deep-learning-based interaction synthesis\nusually focuses on two characters and does not consider transition planning.\nOptimization-based interaction synthesis relies on manually designing objective\nfunctions that may not generalize well. While crowd simulation involves more\ncharacters, their interactions are sparse and passive. We identify two\nchallenges to multi-character interaction synthesis, including the lack of data\nand the planning of transitions among close and dense interactions. Existing\ndatasets either do not have multiple characters or do not have close and dense\ninteractions. The planning of transitions for multi-character close and dense\ninteractions needs both spatial and temporal considerations. We propose a\nconditional generative pipeline comprising a coordinatable multi-character\ninteraction space for interaction synthesis and a transition planning network\nfor coordinations. Our experiments demonstrate the effectiveness of our\nproposed pipeline for multicharacter interaction synthesis and the applications\nfacilitated by our method show the scalability and transferability."}
{"id": "2505.14177", "pdf": "https://arxiv.org/pdf/2505.14177", "abs": "https://arxiv.org/abs/2505.14177", "authors": ["Marien Renaud", "Valentin De Bortoli", "Arthur Leclaire", "Nicolas Papadakis"], "title": "From stability of Langevin diffusion to convergence of proximal MCMC for non-log-concave sampling", "categories": ["stat.ML", "cs.CV", "cs.LG"], "comment": null, "summary": "We consider the problem of sampling distributions stemming from non-convex\npotentials with Unadjusted Langevin Algorithm (ULA). We prove the stability of\nthe discrete-time ULA to drift approximations under the assumption that the\npotential is strongly convex at infinity. In many context, e.g. imaging inverse\nproblems, potentials are non-convex and non-smooth. Proximal Stochastic\nGradient Langevin Algorithm (PSGLA) is a popular algorithm to handle such\npotentials. It combines the forward-backward optimization algorithm with a ULA\nstep. Our main stability result combined with properties of the Moreau envelope\nallows us to derive the first proof of convergence of the PSGLA for non-convex\npotentials. We empirically validate our methodology on synthetic data and in\nthe context of imaging inverse problems. In particular, we observe that PSGLA\nexhibits faster convergence rates than Stochastic Gradient Langevin Algorithm\nfor posterior sampling while preserving its restoration properties."}
{"id": "2505.14180", "pdf": "https://arxiv.org/pdf/2505.14180", "abs": "https://arxiv.org/abs/2505.14180", "authors": ["Songhao Wu", "Quan Tu", "Mingjie Zhong", "Hong Liu", "Jia Xu", "Jinjie Gu", "Rui Yan"], "title": "Bridge the Gap between Past and Future: Siamese Model Optimization for Context-Aware Document Ranking", "categories": ["cs.IR", "cs.CV", "H.3.3"], "comment": null, "summary": "In the realm of information retrieval, users often engage in multi-turn\ninteractions with search engines to acquire information, leading to the\nformation of sequences of user feedback behaviors. Leveraging the session\ncontext has proven to be beneficial for inferring user search intent and\ndocument ranking. A multitude of approaches have been proposed to exploit\nin-session context for improved document ranking. Despite these advances, the\nlimitation of historical session data for capturing evolving user intent\nremains a challenge. In this work, we explore the integration of future\ncontextual information into the session context to enhance document ranking. We\npresent the siamese model optimization framework, comprising a\nhistory-conditioned model and a future-aware model. The former processes only\nthe historical behavior sequence, while the latter integrates both historical\nand anticipated future behaviors. Both models are trained collaboratively using\nthe supervised labels and pseudo labels predicted by the other. The\nhistory-conditioned model, referred to as ForeRanker, progressively learns\nfuture-relevant information to enhance ranking, while it singly uses historical\nsession at inference time. To mitigate inconsistencies during training, we\nintroduce the peer knowledge distillation method with a dynamic gating\nmechanism, allowing models to selectively incorporate contextual information.\nExperimental results on benchmark datasets demonstrate the effectiveness of our\nForeRanker, showcasing its superior performance compared to existing methods."}
{"id": "2505.14336", "pdf": "https://arxiv.org/pdf/2505.14336", "abs": "https://arxiv.org/abs/2505.14336", "authors": ["Umberto Cappellazzo", "Minsu Kim", "Stavros Petridis", "Daniele Falavigna", "Alessio Brutti"], "title": "Scaling and Enhancing LLM-based AVSR: A Sparse Mixture of Projectors Approach", "categories": ["eess.AS", "cs.CV", "cs.MM", "cs.SD"], "comment": null, "summary": "Audio-Visual Speech Recognition (AVSR) enhances robustness in noisy\nenvironments by integrating visual cues. While recent advances integrate Large\nLanguage Models (LLMs) into AVSR, their high computational cost hinders\ndeployment in resource-constrained settings. To address this, we propose\nLlama-SMoP, an efficient Multimodal LLM that employs a Sparse Mixture of\nProjectors (SMoP) module to scale model capacity without increasing inference\ncosts. By incorporating sparsely-gated mixture-of-experts (MoE) projectors,\nLlama-SMoP enables the use of smaller LLMs while maintaining strong\nperformance. We explore three SMoP configurations and show that Llama-SMoP DEDR\n(Disjoint-Experts, Disjoint-Routers), which uses modality-specific routers and\nexperts, achieves superior performance on ASR, VSR, and AVSR tasks. Ablation\nstudies confirm its effectiveness in expert activation, scalability, and noise\nrobustness."}
{"id": "2505.14541", "pdf": "https://arxiv.org/pdf/2505.14541", "abs": "https://arxiv.org/abs/2505.14541", "authors": ["Chuanbo Tang", "Zhuoyuan Li", "Yifan Bian", "Li Li", "Dong Liu"], "title": "Neural Video Compression with Context Modulation", "categories": ["eess.IV", "cs.CV"], "comment": "11 pages, 8 figures, accepted by CVPR 2025", "summary": "Efficient video coding is highly dependent on exploiting the temporal\nredundancy, which is usually achieved by extracting and leveraging the temporal\ncontext in the emerging conditional coding-based neural video codec (NVC).\nAlthough the latest NVC has achieved remarkable progress in improving the\ncompression performance, the inherent temporal context propagation mechanism\nlacks the ability to sufficiently leverage the reference information, limiting\nfurther improvement. In this paper, we address the limitation by modulating the\ntemporal context with the reference frame in two steps. Specifically, we first\npropose the flow orientation to mine the inter-correlation between the\nreference frame and prediction frame for generating the additional oriented\ntemporal context. Moreover, we introduce the context compensation to leverage\nthe oriented context to modulate the propagated temporal context generated from\nthe propagated reference feature. Through the synergy mechanism and decoupling\nloss supervision, the irrelevant propagated information can be effectively\neliminated to ensure better context modeling. Experimental results demonstrate\nthat our codec achieves on average 22.7% bitrate reduction over the advanced\ntraditional video codec H.266/VVC, and offers an average 10.1% bitrate saving\nover the previous state-of-the-art NVC DCVC-FM. The code is available at\nhttps://github.com/Austin4USTC/DCMVC."}
{"id": "2505.14560", "pdf": "https://arxiv.org/pdf/2505.14560", "abs": "https://arxiv.org/abs/2505.14560", "authors": ["Yuan Gao", "Wenhan Guo", "Yu Sun"], "title": "Neural Inverse Scattering with Score-based Regularization", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Inverse scattering is a fundamental challenge in many imaging applications,\nranging from microscopy to remote sensing. Solving this problem often requires\njointly estimating two unknowns -- the image and the scattering field inside\nthe object -- necessitating effective image prior to regularize the inference.\nIn this paper, we propose a regularized neural field (NF) approach which\nintegrates the denoising score function used in score-based generative models.\nThe neural field formulation offers convenient flexibility to performing joint\nestimation, while the denoising score function imposes the rich structural\nprior of images. Our results on three high-contrast simulated objects show that\nthe proposed approach yields a better imaging quality compared to the\nstate-of-the-art NF approach, where regularization is based on total variation."}
{"id": "2505.14572", "pdf": "https://arxiv.org/pdf/2505.14572", "abs": "https://arxiv.org/abs/2505.14572", "authors": ["Jayroop Ramesh", "Valentin Bacher", "Mark C. Eid", "Hoda Kalabizadeh", "Christian Rupprecht", "Ana IL Namburete", "Pak-Hei Yeung", "Madeleine K. Wyburd", "Nicola K. Dinsdale"], "title": "Automated Fetal Biometry Assessment with Deep Ensembles using Sparse-Sampling of 2D Intrapartum Ultrasound Images", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": "Top 5 in MICCAI IUGC 2024: Intrapartum Ultrasound Grand Challenge &\n  Runners up in Classification!", "summary": "The International Society of Ultrasound advocates Intrapartum Ultrasound (US)\nImaging in Obstetrics and Gynecology (ISUOG) to monitor labour progression\nthrough changes in fetal head position. Two reliable ultrasound-derived\nparameters that are used to predict outcomes of instrumental vaginal delivery\nare the angle of progression (AoP) and head-symphysis distance (HSD). In this\nwork, as part of the Intrapartum Ultrasounds Grand Challenge (IUGC) 2024, we\npropose an automated fetal biometry measurement pipeline to reduce intra- and\ninter-observer variability and improve measurement reliability. Our pipeline\nconsists of three key tasks: (i) classification of standard planes (SP) from US\nvideos, (ii) segmentation of fetal head and pubic symphysis from the detected\nSPs, and (iii) computation of the AoP and HSD from the segmented regions. We\nperform sparse sampling to mitigate class imbalances and reduce spurious\ncorrelations in task (i), and utilize ensemble-based deep learning methods for\ntask (i) and (ii) to enhance generalizability under different US acquisition\nsettings. Finally, to promote robustness in task iii) with respect to the\nstructural fidelity of measurements, we retain the largest connected components\nand apply ellipse fitting to the segmentations. Our solution achieved ACC:\n0.9452, F1: 0.9225, AUC: 0.983, MCC: 0.8361, DSC: 0.918, HD: 19.73, ASD: 5.71,\n$\\Delta_{AoP}$: 8.90 and $\\Delta_{HSD}$: 14.35 across an unseen hold-out set of\n4 patients and 224 US frames. The results from the proposed automated pipeline\ncan improve the understanding of labour arrest causes and guide the development\nof clinical risk stratification tools for efficient and effective prenatal\ncare."}
{"id": "2505.14629", "pdf": "https://arxiv.org/pdf/2505.14629", "abs": "https://arxiv.org/abs/2505.14629", "authors": ["Fnu Mohbat", "Mohammed J Zaki"], "title": "KERL: Knowledge-Enhanced Personalized Recipe Recommendation using Large Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": "Accepted at ACL 2025", "summary": "Recent advances in large language models (LLMs) and the abundance of food\ndata have resulted in studies to improve food understanding using LLMs. Despite\nseveral recommendation systems utilizing LLMs and Knowledge Graphs (KGs), there\nhas been limited research on integrating food related KGs with LLMs. We\nintroduce KERL, a unified system that leverages food KGs and LLMs to provide\npersonalized food recommendations and generates recipes with associated\nmicro-nutritional information. Given a natural language question, KERL extracts\nentities, retrieves subgraphs from the KG, which are then fed into the LLM as\ncontext to select the recipes that satisfy the constraints. Next, our system\ngenerates the cooking steps and nutritional information for each recipe. To\nevaluate our approach, we also develop a benchmark dataset by curating recipe\nrelated questions, combined with constraints and personal preferences. Through\nextensive experiments, we show that our proposed KG-augmented LLM significantly\noutperforms existing approaches, offering a complete and coherent solution for\nfood recommendation, recipe generation, and nutritional analysis. Our code and\nbenchmark datasets are publicly available at\nhttps://github.com/mohbattharani/KERL."}
{"id": "2505.14660", "pdf": "https://arxiv.org/pdf/2505.14660", "abs": "https://arxiv.org/abs/2505.14660", "authors": ["Ronald Seoh", "Dan Goldwasser"], "title": "EmoGist: Efficient In-Context Learning for Visual Emotion Understanding", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "In this paper, we introduce EmoGist, a training-free, in-context learning\nmethod for performing visual emotion classification with LVLMs. The key\nintuition of our approach is that context-dependent definition of emotion\nlabels could allow more accurate predictions of emotions, as the ways in which\nemotions manifest within images are highly context dependent and nuanced.\nEmoGist pre-generates multiple explanations of emotion labels, by analyzing the\nclusters of example images belonging to each category. At test time, we\nretrieve a version of explanation based on embedding similarity, and feed it to\na fast VLM for classification. Through our experiments, we show that EmoGist\nallows up to 13 points improvement in micro F1 scores with the multi-label\nMemotion dataset, and up to 8 points in macro F1 in the multi-class FI dataset."}
{"id": "2505.14681", "pdf": "https://arxiv.org/pdf/2505.14681", "abs": "https://arxiv.org/abs/2505.14681", "authors": ["Mengru Wang", "Xingyu Chen", "Yue Wang", "Zhiwei He", "Jiahao Xu", "Tian Liang", "Qiuzhi Liu", "Yunzhi Yao", "Wenxuan Wang", "Ruotian Ma", "Haitao Mi", "Ningyu Zhang", "Zhaopeng Tu", "Xiaolong Li", "Dong Yu"], "title": "Two Experts Are All You Need for Steering Thinking: Reinforcing Cognitive Effort in MoE Reasoning Models Without Additional Training", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.IR", "cs.LG"], "comment": "Work in progress", "summary": "Mixture-of-Experts (MoE) architectures within Large Reasoning Models (LRMs)\nhave achieved impressive reasoning capabilities by selectively activating\nexperts to facilitate structured cognitive processes. Despite notable advances,\nexisting reasoning models often suffer from cognitive inefficiencies like\noverthinking and underthinking. To address these limitations, we introduce a\nnovel inference-time steering methodology called Reinforcing Cognitive Experts\n(RICE), designed to improve reasoning performance without additional training\nor complex heuristics. Leveraging normalized Pointwise Mutual Information\n(nPMI), we systematically identify specialized experts, termed ''cognitive\nexperts'' that orchestrate meta-level reasoning operations characterized by\ntokens like ''<think>''. Empirical evaluations with leading MoE-based LRMs\n(DeepSeek-R1 and Qwen3-235B) on rigorous quantitative and scientific reasoning\nbenchmarks demonstrate noticeable and consistent improvements in reasoning\naccuracy, cognitive efficiency, and cross-domain generalization. Crucially, our\nlightweight approach substantially outperforms prevalent reasoning-steering\ntechniques, such as prompt design and decoding constraints, while preserving\nthe model's general instruction-following skills. These results highlight\nreinforcing cognitive experts as a promising, practical, and interpretable\ndirection to enhance cognitive efficiency within advanced reasoning models."}
