{"id": "2506.03162", "pdf": "https://arxiv.org/pdf/2506.03162", "abs": "https://arxiv.org/abs/2506.03162", "authors": ["Damith Chamalke Senadeera", "Xiaoyun Yang", "Dimitrios Kollias", "Gregory Slabaugh"], "title": "Dual Branch VideoMamba with Gated Class Token Fusion for Violence Detection", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "The rapid proliferation of surveillance cameras has increased the demand for\nautomated violence detection. While CNNs and Transformers have shown success in\nextracting spatio-temporal features, they struggle with long-term dependencies\nand computational efficiency. We propose Dual Branch VideoMamba with Gated\nClass Token Fusion (GCTF), an efficient architecture combining a dual-branch\ndesign and a state-space model (SSM) backbone where one branch captures spatial\nfeatures, while the other focuses on temporal dynamics, with continuous fusion\nvia a gating mechanism. We also present a new benchmark by merging RWF-2000,\nRLVS, and VioPeru datasets in video violence detection, ensuring strict\nseparation between training and testing sets. Our model achieves\nstate-of-the-art performance on this benchmark offering an optimal balance\nbetween accuracy and computational efficiency, demonstrating the promise of\nSSMs for scalable, real-time surveillance violence detection."}
{"id": "2506.03168", "pdf": "https://arxiv.org/pdf/2506.03168", "abs": "https://arxiv.org/abs/2506.03168", "authors": ["Dawen Jiang", "Zhishu Shen", "Qiushi Zheng", "Tiehua Zhang", "Wei Xiang", "Jiong Jin"], "title": "Farm-LightSeek: An Edge-centric Multimodal Agricultural IoT Data Analytics Framework with Lightweight LLMs", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted by IEEE Internet of Things Magazine", "summary": "Amid the challenges posed by global population growth and climate change,\ntraditional agricultural Internet of Things (IoT) systems is currently\nundergoing a significant digital transformation to facilitate efficient big\ndata processing. While smart agriculture utilizes artificial intelligence (AI)\ntechnologies to enable precise control, it still encounters significant\nchallenges, including excessive reliance on agricultural expert knowledge,\ndifficulties in fusing multimodal data, poor adaptability to dynamic\nenvironments, and bottlenecks in real-time decision-making at the edge. Large\nlanguage models (LLMs), with their exceptional capabilities in knowledge\nacquisition and semantic understanding, provide a promising solution to address\nthese challenges. To this end, we propose Farm-LightSeek, an edge-centric\nmultimodal agricultural IoT data analytics framework that integrates LLMs with\nedge computing. This framework collects real-time farmland multi-source data\n(images, weather, geographic information) via sensors, performs cross-modal\nreasoning and disease detection at edge nodes, conducts low-latency management\ndecisions, and enables cloud collaboration for model updates. The main\ninnovations of Farm-LightSeek include: (1) an agricultural\n\"perception-decision-action\" closed-loop architecture; (2) cross-modal adaptive\nmonitoring; and (3)a lightweight LLM deployment strategy balancing performance\nand efficiency. Experiments conducted on two real-world datasets demonstrate\nthat Farm-LightSeek consistently achieves reliable performance in\nmission-critical tasks, even under the limitations of edge computing resources.\nThis work advances intelligent real-time agricultural solutions and highlights\nthe potential for deeper integration of agricultural IoT with LLMs."}
{"id": "2506.03169", "pdf": "https://arxiv.org/pdf/2506.03169", "abs": "https://arxiv.org/abs/2506.03169", "authors": ["Arindam Chaudhuri"], "title": "Improvement of human health lifespan with hybrid group pose estimation methods", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Human beings rely heavily on estimation of poses in order to access their\nbody movements. Human pose estimation methods take advantage of computer vision\nadvances in order to track human body movements in real life applications. This\ncomes from videos which are recorded through available devices. These\npara-digms provide potential to make human movement measurement more accessible\nto users. The consumers of pose estimation movements believe that human poses\ncontent tend to supplement available videos. This has increased pose estimation\nsoftware usage to estimate human poses. In order to address this problem, we\ndevelop hybrid-ensemble-based group pose estimation method to improve human\nhealth. This proposed hybrid-ensemble-based group pose estimation method aims\nto detect multi-person poses using modified group pose estimation and modified\nreal time pose estimation. This ensemble allows fusion of performance of stated\nmethods in real time. The input poses from images are fed into individual\nmeth-ods. The pose transformation method helps to identify relevant features\nfor en-semble to perform training effectively. After this, customized\npre-trained hybrid ensemble is trained on public benchmarked datasets which is\nbeing evaluated through test datasets. The effectiveness and viability of\nproposed method is estab-lished based on comparative analysis of group pose\nestimation methods and ex-periments conducted on benchmarked datasets. It\nprovides best optimized results in real-time pose estimation. It makes pose\nestimation method more robust to oc-clusion and improves dense regression\naccuracy. These results have affirmed po-tential application of this method in\nseveral real-time situations with improvement in human health life span"}
{"id": "2506.03170", "pdf": "https://arxiv.org/pdf/2506.03170", "abs": "https://arxiv.org/abs/2506.03170", "authors": ["Murthy L", "Subarna Tripathi"], "title": "PALADIN : Robust Neural Fingerprinting for Text-to-Image Diffusion Models", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "The risk of misusing text-to-image generative models for malicious uses,\nespecially due to the open-source development of such models, has become a\nserious concern. As a risk mitigation strategy, attributing generative models\nwith neural fingerprinting is emerging as a popular technique. There has been a\nplethora of recent work that aim for addressing neural fingerprinting. A\ntrade-off between the attribution accuracy and generation quality of such\nmodels has been studied extensively. None of the existing methods yet achieved\n$100\\%$ attribution accuracy. However, any model with less than \\emph{perfect}\naccuracy is practically non-deployable. In this work, we propose an accurate\nmethod to incorporate neural fingerprinting for text-to-image diffusion models\nleveraging the concepts of cyclic error correcting codes from the literature of\ncoding theory."}
{"id": "2506.03171", "pdf": "https://arxiv.org/pdf/2506.03171", "abs": "https://arxiv.org/abs/2506.03171", "authors": ["Ghulam Mujtaba", "Eun-Seok Ryu"], "title": "EdgeVidSum: Real-Time Personalized Video Summarization at the Edge", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "EdgeVidSum is a lightweight method that generates personalized, fast-forward\nsummaries of long-form videos directly on edge devices. The proposed approach\nenables real-time video summarization while safeguarding user privacy through\nlocal data processing using innovative thumbnail-based techniques and efficient\nneural architectures. Unlike conventional methods that process entire videos\nframe by frame, the proposed method uses thumbnail containers to significantly\nreduce computational complexity without sacrificing semantic relevance. The\nframework employs a hierarchical analysis approach, where a lightweight 2D CNN\nmodel identifies user-preferred content from thumbnails and generates\ntimestamps to create fast-forward summaries. Our interactive demo highlights\nthe system's ability to create tailored video summaries for long-form videos,\nsuch as movies, sports events, and TV shows, based on individual user\npreferences. The entire computation occurs seamlessly on resource-constrained\ndevices like Jetson Nano, demonstrating how EdgeVidSum addresses the critical\nchallenges of computational efficiency, personalization, and privacy in modern\nvideo consumption environments."}
{"id": "2506.03173", "pdf": "https://arxiv.org/pdf/2506.03173", "abs": "https://arxiv.org/abs/2506.03173", "authors": ["Xiaoyi Liu", "Hao Tang"], "title": "FOLIAGE: Towards Physical Intelligence World Models Via Unbounded Surface Evolution", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Physical intelligence -- anticipating and shaping the world from partial,\nmultisensory observations -- is critical for next-generation world models. We\npropose FOLIAGE, a physics-informed multimodal world model for unbounded\naccretive surface growth. In its Action-Perception loop, a unified context\nencoder maps images, mesh connectivity, and point clouds to a shared latent\nstate. A physics-aware predictor, conditioned on physical control actions,\nadvances this latent state in time to align with the target latent of the\nsurface, yielding a Modality-Agnostic Growth Embedding (MAGE) that interfaces\nwith critic heads for downstream objectives. FOLIAGE's Accretive Graph Network\n(AGN) captures dynamic connectivity through Age Positional Encoding and\nEnergy-Gated Message-Passing. Geometry-Correspondence Fusion and Cross-Patch\nMasking enhance MAGE's expressiveness, while Hierarchical Pooling balances\nglobal context with local dynamics. We create SURF-GARDEN, a world model\nlearning platform comprising a Counterfactual Physics Simulator, a Multimodal\nCorrespondence Extractor, and Evolution Tracing, which generates 7,200 diverse\nsurface-growth sequences. SURF-BENCH, our physical-intelligence evaluation\nsuite, evaluates six core tasks -- topology recognition, inverse material\nestimation, growth-stage classification, latent roll-out, cross-modal\nretrieval, and dense correspondence -- and four stress tests -- sensor dropout,\nzero-shot modality transfer, long-horizon prediction, and physics ablation --\nto probe resilience. FOLIAGE outperforms specialized baselines while remaining\nrobust across dynamic environments, establishing a new world-model based,\nmultimodal pathway to physical intelligence."}
{"id": "2506.03174", "pdf": "https://arxiv.org/pdf/2506.03174", "abs": "https://arxiv.org/abs/2506.03174", "authors": ["Koki Matsuishi", "Kosuke Ukita", "Tsuyoshi Okita"], "title": "Multimodal Foundation Model for Cross-Modal Retrieval and Activity Recognition Tasks", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "25 pages, 8 figures", "summary": "In recent years, the widespread adoption of wearable devices has highlighted\nthe growing importance of behavior analysis using IMU. While applications span\ndiverse fields such as healthcare and robotics, recent studies have\nincreasingly focused on multimodal analysis, in addition to unimodal analysis.\nSeveral studies have proposed multimodal foundation models that incorporate\nfirst-person video and text data; however, these models still fall short in\nproviding a detailed analysis of full-body human activity. To address this\nlimitation, we propose Activity Understanding and Representations Alignment -\nMultimodal Foundation Model (AURA-MFM), a foundational model integrating four\nmodalities: third-person video, motion capture, IMU, and text. By incorporating\nthird-person video and motion capture data, the model enables a detailed and\nmultidimensional understanding of human activity, which first-person\nperspectives alone fail to capture. Additionally, a Transformer-based IMU\nencoder is employed to enhance the model's overall performance. Experimental\nevaluations on retrieval and activity recognition tasks demonstrate that our\nmodel surpasses existing methods. Notably, in the zero-shot classification for\naction recognition, our method achieved significantly higher performance, with\nan F1-score of 0.6226 and an accuracy of 0.7320, whereas the existing method\nrecorded an F1-score of 0.0747 and an accuracy of 0.1961."}
{"id": "2506.03179", "pdf": "https://arxiv.org/pdf/2506.03179", "abs": "https://arxiv.org/abs/2506.03179", "authors": ["Qi Li", "Runpeng Yu", "Xinchao Wang"], "title": "Vid-SME: Membership Inference Attacks against Large Video Understanding Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Multimodal large language models (MLLMs) demonstrate remarkable capabilities\nin handling complex multimodal tasks and are increasingly adopted in video\nunderstanding applications. However, their rapid advancement raises serious\ndata privacy concerns, particularly given the potential inclusion of sensitive\nvideo content, such as personal recordings and surveillance footage, in their\ntraining datasets. Determining improperly used videos during training remains a\ncritical and unresolved challenge. Despite considerable progress on membership\ninference attacks (MIAs) for text and image data in MLLMs, existing methods\nfail to generalize effectively to the video domain. These methods suffer from\npoor scalability as more frames are sampled and generally achieve negligible\ntrue positive rates at low false positive rates (TPR@Low FPR), mainly due to\ntheir failure to capture the inherent temporal variations of video frames and\nto account for model behavior differences as the number of frames varies. To\naddress these challenges, we introduce Vid-SME, the first membership inference\nmethod tailored for video data used in video understanding LLMs (VULLMs).\nVid-SME leverages the confidence of model output and integrates adaptive\nparameterization to compute Sharma-Mittal entropy (SME) for video inputs. By\nleveraging the SME difference between natural and temporally-reversed video\nframes, Vid-SME derives robust membership scores to determine whether a given\nvideo is part of the model's training set. Experiments on various self-trained\nand open-sourced VULLMs demonstrate the strong effectiveness of Vid-SME."}
{"id": "2506.03182", "pdf": "https://arxiv.org/pdf/2506.03182", "abs": "https://arxiv.org/abs/2506.03182", "authors": ["Shivani Chiranjeevi", "Hossein Zaremehrjerdi", "Zi K. Deng", "Talukder Z. Jubery", "Ari Grele", "Arti Singh", "Asheesh K Singh", "Soumik Sarkar", "Nirav Merchant", "Harold F. Greeney", "Baskar Ganapathysubramanian", "Chinmay Hegde"], "title": "TerraIncognita: A Dynamic Benchmark for Species Discovery Using Frontier Models", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "The rapid global loss of biodiversity, particularly among insects, represents\nan urgent ecological crisis. Current methods for insect species discovery are\nmanual, slow, and severely constrained by taxonomic expertise, hindering timely\nconservation actions. We introduce TerraIncognita, a dynamic benchmark designed\nto evaluate state-of-the-art multimodal models for the challenging problem of\nidentifying unknown, potentially undescribed insect species from image data.\nOur benchmark dataset combines a mix of expertly annotated images of insect\nspecies likely known to frontier AI models, and images of rare and poorly known\nspecies, for which few/no publicly available images exist. These images were\ncollected from underexplored biodiversity hotspots, realistically mimicking\nopen-world discovery scenarios faced by ecologists. The benchmark assesses\nmodels' proficiency in hierarchical taxonomic classification, their capability\nto detect and abstain from out-of-distribution (OOD) samples representing novel\nspecies, and their ability to generate explanations aligned with expert\ntaxonomic knowledge. Notably, top-performing models achieve over 90\\% F1 at the\nOrder level on known species, but drop below 2\\% at the Species level,\nhighlighting the sharp difficulty gradient from coarse to fine taxonomic\nprediction (Order $\\rightarrow$ Family $\\rightarrow$ Genus $\\rightarrow$\nSpecies). TerraIncognita will be updated regularly, and by committing to\nquarterly dataset expansions (of both known and novel species), will provide an\nevolving platform for longitudinal benchmarking of frontier AI methods. All\nTerraIncognita data, results, and future updates are available\n\\href{https://baskargroup.github.io/TerraIncognita/}{here}."}
{"id": "2506.03184", "pdf": "https://arxiv.org/pdf/2506.03184", "abs": "https://arxiv.org/abs/2506.03184", "authors": ["Mahe Zabin", "Ho-Jin Choi", "Md. Monirul Islam", "Jia Uddin"], "title": "Impact of Tuning Parameters in Deep Convolutional Neural Network Using a Crack Image Dataset", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "8 pages, 2 figures, published at Proceedings of the 15th KIPS\n  International Conference on Ubiquitous Information Technologies and\n  Applications (CUTE 2021), Jeju, Repubilc of Korea", "summary": "The performance of a classifier depends on the tuning of its parame ters. In\nthis paper, we have experimented the impact of various tuning parameters on the\nperformance of a deep convolutional neural network (DCNN). In the ex perimental\nevaluation, we have considered a DCNN classifier that consists of 2\nconvolutional layers (CL), 2 pooling layers (PL), 1 dropout, and a dense layer.\nTo observe the impact of pooling, activation function, and optimizer tuning pa\nrameters, we utilized a crack image dataset having two classes: negative and\npos itive. The experimental results demonstrate that with the maxpooling, the\nDCNN demonstrates its better performance for adam optimizer and tanh activation\nfunc tion."}
{"id": "2506.03189", "pdf": "https://arxiv.org/pdf/2506.03189", "abs": "https://arxiv.org/abs/2506.03189", "authors": ["Ghada Sokar", "Gintare Karolina Dziugaite", "Anurag Arnab", "Ahmet Iscen", "Pablo Samuel Castro", "Cordelia Schmid"], "title": "Continual Learning in Vision-Language Models via Aligned Model Merging", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Continual learning is conventionally tackled through sequential fine-tuning,\na process that, while enabling adaptation, inherently favors plasticity over\nthe stability needed to retain prior knowledge. While existing approaches\nattempt to mitigate catastrophic forgetting, a bias towards recent tasks\npersists as they build upon this sequential nature. In this work we present a\nnew perspective based on model merging to maintain stability while still\nretaining plasticity. Rather than just sequentially updating the model weights,\nwe propose merging newly trained task parameters with previously learned ones,\npromoting a better balance. To maximize the effectiveness of the merging\nprocess, we propose a simple mechanism that promotes learning aligned weights\nwith previous ones, thereby avoiding interference when merging. We evaluate\nthis approach on large Vision-Language Models (VLMs), and demonstrate its\neffectiveness in reducing forgetting, increasing robustness to various task\norders and similarities, and improving generalization."}
{"id": "2506.03190", "pdf": "https://arxiv.org/pdf/2506.03190", "abs": "https://arxiv.org/abs/2506.03190", "authors": ["Jiaming Yi", "Ruirui Pan", "Jishen Yang", "Xiulong Yang"], "title": "MINT: Memory-Infused Prompt Tuning at Test-time for CLIP", "categories": ["cs.CV", "cs.AI"], "comment": "14 pages, 3 figures", "summary": "Improving the generalization ability of Vision-Language Pre-trained Models\n(VLMs) under test-time data distribution shifts remains a critical challenge.\nThe existing Test-Time Adaptation (TTA) methods fall short in fully leveraging\nthe model's internal knowledge, particularly in dynamically adapting to complex\nand hierarchical visual semantic information. In this paper, we propose\nMemory-Infused Prompt Tuning (MINT), a novel framework to address this issue.\nInspired by human associative memory theory, MINT introduces a Memory Prompt\nBank (MPB), which stores learnable key-value prompt pairs that work as a memory\nof previously seen samples. During the test time, relevant prompt pairs in the\nMPB are retrieved by the hierarchical visual features of test images to\ndynamically assemble Associative Prompts. The associative prompts are then\ninjected into the image encoder for fine-grained, customized visual contextual\nguidance. MINT also utilizes learnable text prompts. MINT thus enables rapid,\nprecise VLM adaptation at test time by leveraging this MPB-acquired memory,\nwithout source data or retraining. The code is available at\nhttps://github.com/Jamieyi2004/MINT."}
{"id": "2506.03191", "pdf": "https://arxiv.org/pdf/2506.03191", "abs": "https://arxiv.org/abs/2506.03191", "authors": ["Muhammad Islam", "Tao Huang", "Euijoon Ahn", "Usman Naseem"], "title": "Multimodal Generative AI with Autoregressive LLMs for Human Motion Understanding and Generation: A Way Forward", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "This paper presents an in-depth survey on the use of multimodal Generative\nArtificial Intelligence (GenAI) and autoregressive Large Language Models (LLMs)\nfor human motion understanding and generation, offering insights into emerging\nmethods, architectures, and their potential to advance realistic and versatile\nmotion synthesis. Focusing exclusively on text and motion modalities, this\nresearch investigates how textual descriptions can guide the generation of\ncomplex, human-like motion sequences. The paper explores various generative\napproaches, including autoregressive models, diffusion models, Generative\nAdversarial Networks (GANs), Variational Autoencoders (VAEs), and\ntransformer-based models, by analyzing their strengths and limitations in terms\nof motion quality, computational efficiency, and adaptability. It highlights\nrecent advances in text-conditioned motion generation, where textual inputs are\nused to control and refine motion outputs with greater precision. The\nintegration of LLMs further enhances these models by enabling semantic\nalignment between instructions and motion, improving coherence and contextual\nrelevance. This systematic survey underscores the transformative potential of\ntext-to-motion GenAI and LLM architectures in applications such as healthcare,\nhumanoids, gaming, animation, and assistive technologies, while addressing\nongoing challenges in generating efficient and realistic human motion."}
{"id": "2506.03193", "pdf": "https://arxiv.org/pdf/2506.03193", "abs": "https://arxiv.org/abs/2506.03193", "authors": ["Ekram Alam", "Abu Sufian", "Paramartha Dutta", "Marco Leo"], "title": "Human Fall Detection using Transfer Learning-based 3D CNN", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Unintentional or accidental falls are one of the significant health issues in\nsenior persons. The population of senior persons is increasing steadily. So,\nthere is a need for an automated fall detection monitoring system. This paper\nintroduces a vision-based fall detection system using a pre-trained 3D CNN.\nUnlike 2D CNN, 3D CNN extracts not only spatial but also temporal features. The\nproposed model leverages the original learned weights of a 3D CNN model\npre-trained on the Sports1M dataset to extract the spatio-temporal features.\nOnly the SVM classifier was trained, which saves the time required to train the\n3D CNN. Stratified shuffle five split cross-validation has been used to split\nthe dataset into training and testing data. Extracted features from the\nproposed 3D CNN model were fed to an SVM classifier to classify the activity as\nfall or ADL. Two datasets, GMDCSA and CAUCAFall, were utilized to conduct the\nexperiment. The source code for this work can be accessed via the following\nlink: https://github.com/ekramalam/HFD_3DCNN."}
{"id": "2506.03194", "pdf": "https://arxiv.org/pdf/2506.03194", "abs": "https://arxiv.org/abs/2506.03194", "authors": ["Rynaa Grover", "Jayant Sravan Tamarapalli", "Sahiti Yerramilli", "Nilay Pande"], "title": "HueManity: Probing Fine-Grained Visual Perception in MLLMs", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) excel at high-level visual\nreasoning, but their performance on nuanced perceptual tasks remains\nsurprisingly limited. We present HueManity, a benchmark designed to assess\nvisual perception in MLLMs. The dataset comprises 83,850 images featuring\ntwo-character alphanumeric strings embedded in Ishihara test style dot\npatterns, challenging models on precise pattern recognition. Our evaluation of\nnine state-of-the-art MLLMs on HueManity demonstrates a significant performance\ndeficit compared to human and traditional computer vision baselines. The\nbest-performing MLLM achieved a 33.6% accuracy on the numeric `easy' task and a\nstriking 3% on the alphanumeric `hard' task. In contrast, human participants\nachieved near-perfect scores (100% and 95.6%), and a fine-tuned ResNet50 model\nreached accuracies of 96.5% and 94.5%. These results highlight a critical gap\nin the visual capabilities of current MLLMs. Our analysis further explores\npotential architectural and training-paradigm factors contributing to this\nperceptual gap in MLLMs. We open-source HueManity dataset and code to foster\nfurther research in improving perceptual robustness of MLLMs."}
{"id": "2506.03195", "pdf": "https://arxiv.org/pdf/2506.03195", "abs": "https://arxiv.org/abs/2506.03195", "authors": ["Yunqi Hong", "Sohyun An", "Andrew Bai", "Neil Y. C. Lin", "Cho-Jui Hsieh"], "title": "Unlabeled Data Improves Fine-Grained Image Zero-shot Classification with Multimodal LLMs", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Despite Multimodal Large Language Models (MLLMs) showing promising results on\ngeneral zero-shot image classification tasks, fine-grained image classification\nremains challenging. It demands precise attention to subtle visual details to\ndistinguish between visually similar subcategories--details that MLLMs may\neasily overlook without explicit guidance. To address this, we introduce\nAutoSEP, an iterative self-supervised prompt learning framework designed to\nenhance MLLM fine-grained classification capabilities in a fully unsupervised\nmanner. Our core idea is to leverage unlabeled data to learn a description\nprompt that guides MLLMs in identifying crucial discriminative features within\nan image, and boosts classification accuracy. We developed an automatic\nself-enhancing prompt learning framework called AutoSEP to iteratively improve\nthe description prompt using unlabeled data, based on instance-level\nclassification scoring function. AutoSEP only requires black-box access to\nMLLMs, eliminating the need for any training or fine-tuning. We evaluate our\napproach on multiple fine-grained classification datasets. It consistently\noutperforms other unsupervised baselines, demonstrating the effectiveness of\nour self-supervised optimization framework. Notably, AutoSEP on average\nimproves 13 percent over standard zero-shot classification and 5 percent over\nthe best-performing baselines. Code is available at:\nhttps://github.com/yq-hong/AutoSEP"}
{"id": "2506.03197", "pdf": "https://arxiv.org/pdf/2506.03197", "abs": "https://arxiv.org/abs/2506.03197", "authors": ["Baode Wang", "Biao Wu", "Weizhen Li", "Meng Fang", "Yanjie Liang", "Zuming Huang", "Haozhe Wang", "Jun Huang", "Ling Chen", "Wei Chu", "Yuan Qi"], "title": "Infinity Parser: Layout Aware Reinforcement Learning for Scanned Document Parsing", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "16 pages, 12 figures", "summary": "Automated parsing of scanned documents into richly structured,\nmachine-readable formats remains a critical bottleneck in Document AI, as\ntraditional multi-stage pipelines suffer from error propagation and limited\nadaptability to diverse layouts. We introduce layoutRL, an end-to-end\nreinforcement learning framework that trains models to be explicitly\nlayout-aware by optimizing a composite reward of normalized edit distance,\nparagraph count accuracy, and reading order preservation. Leveraging our newly\nreleased dataset, Infinity-Doc-55K, which combines 55K high-fidelity synthetic\nscanned document parsing data with expert-filtered real-world documents, we\ninstantiate layoutRL in a vision-language-model-based parser called\nInfinity-Parser. Evaluated on English and Chinese benchmarks for OCR, table and\nformula extraction, and reading order detection, Infinity-Parser achieves new\nstate-of-the-art performance in both accuracy and structural fidelity,\noutpacing specialist pipelines and general-purpose vision-language models. We\nwill publicly release our code and dataset to accelerate progress in robust\ndocument understanding."}
{"id": "2506.03198", "pdf": "https://arxiv.org/pdf/2506.03198", "abs": "https://arxiv.org/abs/2506.03198", "authors": ["Hao Yin", "Lijun Gu", "Paritosh Parmar", "Lin Xu", "Tianxiao Guo", "Weiwei Fu", "Yang Zhang", "Tianyou Zheng"], "title": "FLEX: A Large-Scale Multi-Modal Multi-Action Dataset for Fitness Action Quality Assessment", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "With the increasing awareness of health and the growing desire for aesthetic\nphysique, fitness has become a prevailing trend. However, the potential risks\nassociated with fitness training, especially with weight-loaded fitness\nactions, cannot be overlooked. Action Quality Assessment (AQA), a technology\nthat quantifies the quality of human action and provides feedback, holds the\npotential to assist fitness enthusiasts of varying skill levels in achieving\nbetter training outcomes. Nevertheless, current AQA methodologies and datasets\nare limited to single-view competitive sports scenarios and RGB modality and\nlack professional assessment and guidance of fitness actions. To address this\ngap, we propose the FLEX dataset, the first multi-modal, multi-action,\nlarge-scale dataset that incorporates surface electromyography (sEMG) signals\ninto AQA. FLEX utilizes high-precision MoCap to collect 20 different\nweight-loaded actions performed by 38 subjects across 3 different skill levels\nfor 10 repetitions each, containing 5 different views of the RGB video, 3D\npose, sEMG, and physiological information. Additionally, FLEX incorporates\nknowledge graphs into AQA, constructing annotation rules in the form of penalty\nfunctions that map weight-loaded actions, action keysteps, error types, and\nfeedback. We conducted various baseline methodologies on FLEX, demonstrating\nthat multimodal data, multiview data, and fine-grained annotations\nsignificantly enhance model performance. FLEX not only advances AQA\nmethodologies and datasets towards multi-modal and multi-action scenarios but\nalso fosters the integration of artificial intelligence within the fitness\ndomain. Dataset and code are available at\nhttps://haoyin116.github.io/FLEX_Dataset."}
{"id": "2506.03211", "pdf": "https://arxiv.org/pdf/2506.03211", "abs": "https://arxiv.org/abs/2506.03211", "authors": ["Wanting Yang", "Zehui Xiong", "Qianqian Yang", "Ping Zhang", "Merouane Debbah", "Rahim Tafazolli"], "title": "Channel-adaptive Cross-modal Generative Semantic Communication for Point Cloud Transmission", "categories": ["cs.CV", "cs.NI"], "comment": null, "summary": "With the rapid development of autonomous driving and extended reality,\nefficient transmission of point clouds (PCs) has become increasingly important.\nIn this context, we propose a novel channel-adaptive cross-modal generative\nsemantic communication (SemCom) for PC transmission, called GenSeC-PC.\nGenSeC-PC employs a semantic encoder that fuses images and point clouds, where\nimages serve as non-transmitted side information. Meanwhile, the decoder is\nbuilt upon the backbone of PointDif. Such a cross-modal design not only ensures\nhigh compression efficiency but also delivers superior reconstruction\nperformance compared to PointDif. Moreover, to ensure robust transmission and\nreduce system complexity, we design a streamlined and asymmetric\nchannel-adaptive joint semantic-channel coding architecture, where only the\nencoder needs the feedback of average signal-to-noise ratio (SNR) and available\nbandwidth. In addition, rectified denoising diffusion implicit models is\nemployed to accelerate the decoding process to the millisecond level, enabling\nreal-time PC communication. Unlike existing methods, GenSeC-PC leverages\ngenerative priors to ensure reliable reconstruction even from noisy or\nincomplete source PCs. More importantly, it supports fully analog transmission,\nimproving compression efficiency by eliminating the need for error-free side\ninformation transmission common in prior SemCom approaches. Simulation results\nconfirm the effectiveness of cross-modal semantic extraction and dual-metric\nguided fine-tuning, highlighting the framework's robustness across diverse\nconditions, including low SNR, bandwidth limitations, varying numbers of 2D\nimages, and previously unseen objects."}
{"id": "2506.03213", "pdf": "https://arxiv.org/pdf/2506.03213", "abs": "https://arxiv.org/abs/2506.03213", "authors": ["Abdullah Al Mamun", "Miaohua Zhang", "David Ahmedt-Aristizabal", "Zeeshan Hayder", "Mohammad Awrangjeb"], "title": "ConMamba: Contrastive Vision Mamba for Plant Disease Detection", "categories": ["cs.CV"], "comment": null, "summary": "Plant Disease Detection (PDD) is a key aspect of precision agriculture.\nHowever, existing deep learning methods often rely on extensively annotated\ndatasets, which are time-consuming and costly to generate. Self-supervised\nLearning (SSL) offers a promising alternative by exploiting the abundance of\nunlabeled data. However, most existing SSL approaches suffer from high\ncomputational costs due to convolutional neural networks or transformer-based\narchitectures. Additionally, they struggle to capture long-range dependencies\nin visual representation and rely on static loss functions that fail to align\nlocal and global features effectively. To address these challenges, we propose\nConMamba, a novel SSL framework specially designed for PDD. ConMamba integrates\nthe Vision Mamba Encoder (VME), which employs a bidirectional State Space Model\n(SSM) to capture long-range dependencies efficiently. Furthermore, we introduce\na dual-level contrastive loss with dynamic weight adjustment to optimize\nlocal-global feature alignment. Experimental results on three benchmark\ndatasets demonstrate that ConMamba significantly outperforms state-of-the-art\nmethods across multiple evaluation metrics. This provides an efficient and\nrobust solution for PDD."}
{"id": "2506.03224", "pdf": "https://arxiv.org/pdf/2506.03224", "abs": "https://arxiv.org/abs/2506.03224", "authors": ["Jinwei Zeng", "Yu Liu", "Guozhen Zhang", "Jingtao Ding", "Yuming Lin", "Jian Yuan", "Yong Li"], "title": "OpenCarbon: A Contrastive Learning-based Cross-Modality Neural Approach for High-Resolution Carbon Emission Prediction Using Open Data", "categories": ["cs.CV", "cs.AI", "physics.soc-ph"], "comment": "Accepted by IJCAI 2025", "summary": "Accurately estimating high-resolution carbon emissions is crucial for\neffective emission governance and mitigation planning. While conventional\nmethods for precise carbon accounting are hindered by substantial data\ncollection efforts, the rise of open data and advanced learning techniques\noffers a promising solution. Once an open data-based prediction model is\ndeveloped and trained, it can easily infer emissions for new areas based on\navailable open data. To address this, we incorporate two modalities of open\ndata, satellite images and point-of-interest (POI) data, to predict\nhigh-resolution urban carbon emissions, with satellite images providing\nmacroscopic and static and POI data offering fine-grained and relatively\ndynamic functionality information. However, estimating high-resolution carbon\nemissions presents two significant challenges: the intertwined and implicit\neffects of various functionalities on carbon emissions, and the complex spatial\ncontiguity correlations that give rise to the agglomeration effect. Our model,\nOpenCarbon, features two major designs that target the challenges: a\ncross-modality information extraction and fusion module to extract\ncomplementary functionality information from two modules and model their\ninteractions, and a neighborhood-informed aggregation module to capture the\nspatial contiguity correlations. Extensive experiments demonstrate our model's\nsuperiority, with a significant performance gain of 26.6\\% on R2. Further\ngeneralizability tests and case studies also show OpenCarbon's capacity to\ncapture the intrinsic relation between urban functionalities and carbon\nemissions, validating its potential to empower efficient carbon governance and\ntargeted carbon mitigation planning. Codes and data are available:\nhttps://github.com/JinweiZzz/OpenCarbon."}
{"id": "2506.03229", "pdf": "https://arxiv.org/pdf/2506.03229", "abs": "https://arxiv.org/abs/2506.03229", "authors": ["Qian-Wei Wang", "Yuqiu Xie", "Letian Zhang", "Zimo Liu", "Shu-Tao Xia"], "title": "Pre-trained Vision-Language Models Assisted Noisy Partial Label Learning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "In the context of noisy partial label learning (NPLL), each training sample\nis associated with a set of candidate labels annotated by multiple noisy\nannotators. With the emergence of high-performance pre-trained vision-language\nmodels (VLMs) such as CLIP, LLaVa and GPT-4V, the direction of using these\nmodels to replace time-consuming manual annotation workflows and achieve\n\"manual-annotation-free\" training for downstream tasks has become a highly\npromising research avenue. This paper focuses on learning from noisy partial\nlabels annotated by pre-trained VLMs and proposes an innovative collaborative\nconsistency regularization (Co-Reg) method. Unlike the symmetric noise\nprimarily addressed in traditional noisy label learning, the noise generated by\npre-trained models is instance-dependent, embodying the underlying patterns of\nthe pre-trained models themselves, which significantly increases the learning\ndifficulty for the model. To address this, we simultaneously train two neural\nnetworks that implement collaborative purification of training labels through a\n\"Co-Pseudo-Labeling\" mechanism, while enforcing consistency regularization\nconstraints in both the label space and feature representation space. Our\nmethod can also leverage few-shot manually annotated valid labels to further\nenhance its performances. Comparative experiments with different denoising and\ndisambiguation algorithms, annotation manners, and pre-trained model\napplication schemes fully validate the effectiveness of the proposed method,\nwhile revealing the broad prospects of integrating weakly-supervised learning\ntechniques into the knowledge distillation process of pre-trained models."}
{"id": "2506.03275", "pdf": "https://arxiv.org/pdf/2506.03275", "abs": "https://arxiv.org/abs/2506.03275", "authors": ["Austin Silveria", "Soham V. Govande", "Daniel Y. Fu"], "title": "Chipmunk: Training-Free Acceleration of Diffusion Transformers with Dynamic Column-Sparse Deltas", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages, 4 figures", "summary": "Diffusion Transformers (DiTs) have achieved state-of-the-art performance in\nhigh-quality image and video generation but incur substantial compute cost at\ninference. A common observation is that DiT latent noise vectors change slowly\nacross inference steps, which suggests that the DiT compute may be redundant\nacross steps. In this paper, we aim to speed up inference by reducing this\nredundancy, without additional training. We first study how activations change\nbetween steps in two state-of-the-art open-source DiTs. We find that just 5-25%\nof the values in attention and MLP explain 70-90% of the change in activations\nacross steps. This finding motivates our approach, Chipmunk, which uses dynamic\nsparsity at inference time to recompute only the fastest-changing intermediate\nactivations, while caching the rest. Dynamic sparsity introduces two systems\nchallenges: (1) sparse attention and MLP operations tend to underutilize GPU\ntensor cores; and (2) computing dynamic sparsity patterns at runtime and\ncaching activations both introduce overhead. To address these challenges,\nChipmunk first uses a voxel-based reordering of input tokens to introduce\ncolumn-wise sparsity. We implement column-sparse kernels utilizing efficient\nsparse gathers from global to shared GPU memory, achieving a 9.3x speedup at\n93% sparsity compared to highly-optimized dense baselines. Second, Chipmunk\noverlaps the computation of sparsity patterns and cache updates with other\nparts of the computation (e.g., second layer of the MLP) to hide the extra\nlatency. Chipmunk achieves up to 2.16x speedup on HunyuanVideo and 1.41x on\nFLUX.1-dev without compromising generation quality. Furthermore, we show that\nChipmunk can be stacked on top of full step caching, achieving a 3.72x speedup\non HunyuanVideo, a 2.67x speedup on WAN2.1, and a 2.25x speedup on FLUX.1-dev\nwith minimal quality impact."}
{"id": "2506.03290", "pdf": "https://arxiv.org/pdf/2506.03290", "abs": "https://arxiv.org/abs/2506.03290", "authors": ["Leyla Mirvakhabova", "Hong Cai", "Jisoo Jeong", "Hanno Ackermann", "Farhad Zanjani", "Fatih Porikli"], "title": "Learning Optical Flow Field via Neural Ordinary Differential Equation", "categories": ["cs.CV"], "comment": "CVPRW 2025", "summary": "Recent works on optical flow estimation use neural networks to predict the\nflow field that maps positions of one image to positions of the other. These\nnetworks consist of a feature extractor, a correlation volume, and finally\nseveral refinement steps. These refinement steps mimic the iterative\nrefinements performed by classical optimization algorithms and are usually\nimplemented by neural layers (e.g., GRU) which are recurrently executed for a\nfixed and pre-determined number of steps. However, relying on a fixed number of\nsteps may result in suboptimal performance because it is not tailored to the\ninput data. In this paper, we introduce a novel approach for predicting the\nderivative of the flow using a continuous model, namely neural ordinary\ndifferential equations (ODE). One key advantage of this approach is its\ncapacity to model an equilibrium process, dynamically adjusting the number of\ncompute steps based on the data at hand. By following a particular neural\narchitecture, ODE solver, and associated hyperparameters, our proposed model\ncan replicate the exact same updates as recurrent cells used in existing works,\noffering greater generality. Through extensive experimental analysis on optical\nflow benchmarks, we demonstrate that our approach achieves an impressive\nimprovement over baseline and existing models, all while requiring only a\nsingle refinement step."}
{"id": "2506.03335", "pdf": "https://arxiv.org/pdf/2506.03335", "abs": "https://arxiv.org/abs/2506.03335", "authors": ["Dheeraj Khanna", "Jerrin Bright", "Yuhao Chen", "John S. Zelek"], "title": "SportMamba: Adaptive Non-Linear Multi-Object Tracking with State Space Models for Team Sports", "categories": ["cs.CV"], "comment": "Paper accepted at CVSports IEEE/CVF Conference on Computer Vision and\n  Pattern Recognition Workshops (CVPRW'25). The paper has 8 pages, including 6\n  Figures and 5 Tables", "summary": "Multi-object tracking (MOT) in team sports is particularly challenging due to\nthe fast-paced motion and frequent occlusions resulting in motion blur and\nidentity switches, respectively. Predicting player positions in such scenarios\nis particularly difficult due to the observed highly non-linear motion\npatterns. Current methods are heavily reliant on object detection and\nappearance-based tracking, which struggle to perform in complex team sports\nscenarios, where appearance cues are ambiguous and motion patterns do not\nnecessarily follow a linear pattern. To address these challenges, we introduce\nSportMamba, an adaptive hybrid MOT technique specifically designed for tracking\nin dynamic team sports. The technical contribution of SportMamba is twofold.\nFirst, we introduce a mamba-attention mechanism that models non-linear motion\nby implicitly focusing on relevant embedding dependencies. Second, we propose a\nheight-adaptive spatial association metric to reduce ID switches caused by\npartial occlusions by accounting for scale variations due to depth changes.\nAdditionally, we extend the detection search space with adaptive buffers to\nimprove associations in fast-motion scenarios. Our proposed technique,\nSportMamba, demonstrates state-of-the-art performance on various metrics in the\nSportsMOT dataset, which is characterized by complex motion and severe\nocclusion. Furthermore, we demonstrate its generalization capability through\nzero-shot transfer to VIP-HTD, an ice hockey dataset."}
{"id": "2506.03340", "pdf": "https://arxiv.org/pdf/2506.03340", "abs": "https://arxiv.org/abs/2506.03340", "authors": ["Zihui Xue", "Mi Luo", "Kristen Grauman"], "title": "Seeing the Arrow of Time in Large Multimodal Models", "categories": ["cs.CV"], "comment": "Project website: https://vision.cs.utexas.edu/projects/SeeAoT", "summary": "The Arrow of Time (AoT)-time's irreversible flow shaping physical events-is\nfundamental to video comprehension, yet remains a significant challenge for\nmodern large multimodal models (LMMs). Current LMMs struggle to perceive and\nutilize temporal directionality in video when responding to language queries,\nobstructing deeper temporal understanding. We tackle this deficiency by first\nproviding a critical analysis of existing benchmarks and models. We then\nintroduce ArrowRL, a reinforcement learning (RL)-based training strategy with\nan innovative reverse reward that instills AoT awareness by encouraging\ndivergent video interpretations between forward and reversed visual frames. For\nrigorous evaluation, we additionally develop AoTBench, a new multi-faceted\nbenchmark probing temporally challenging questions. Experiments show ArrowRL\ngreatly advances temporal perception: it not only achieves substantial\nimprovements on our challenging AoTBench but also demonstrably boosts\nperformance on standard video question answering (VQA) benchmarks (with peak\naccuracy gains reaching over 20% and 10% respectively). This validates\nArrowRL's effectiveness and highlights the critical need for dedicated AoT\nunderstanding in LMMs."}
{"id": "2506.03345", "pdf": "https://arxiv.org/pdf/2506.03345", "abs": "https://arxiv.org/abs/2506.03345", "authors": ["Chien-Fu", "Huang", "Katherine Sieg", "Leonid Karlinksy", "Nash Flores", "Rebekah Sheraw", "Xin Zhang"], "title": "Semiconductor SEM Image Defect Classification Using Supervised and Semi-Supervised Learning with Vision Transformers", "categories": ["cs.CV"], "comment": "Published at 36th Annual SEMI Advanced Semiconductor Manufacturing\n  Conference (ASMC) 2025", "summary": "Controlling defects in semiconductor processes is important for maintaining\nyield, improving production cost, and preventing time-dependent critical\ncomponent failures. Electron beam-based imaging has been used as a tool to\nsurvey wafers in the line and inspect for defects. However, manual\nclassification of images for these nano-scale defects is limited by time, labor\nconstraints, and human biases. In recent years, deep learning computer vision\nalgorithms have shown to be effective solutions for image-based inspection\napplications in industry. This work proposes application of vision transformer\n(ViT) neural networks for automatic defect classification (ADC) of scanning\nelectron microscope (SEM) images of wafer defects. We evaluated our proposed\nmethods on 300mm wafer semiconductor defect data from our fab in IBM Albany. We\nstudied 11 defect types from over 7400 total images and investigated the\npotential of transfer learning of DinoV2 and semi-supervised learning for\nimproved classification accuracy and efficient computation. We were able to\nachieve classification accuracies of over 90% with less than 15 images per\ndefect class. Our work demonstrates the potential to apply the proposed\nframework for a platform agnostic in-house classification tool with faster\nturnaround time and flexibility."}
{"id": "2506.03371", "pdf": "https://arxiv.org/pdf/2506.03371", "abs": "https://arxiv.org/abs/2506.03371", "authors": ["Xiaonan Wang", "Bo Shao", "Hansaem Kim"], "title": "Toward Reliable VLM: A Fine-Grained Benchmark and Framework for Exposure, Bias, and Inference in Korean Street Views", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in vision-language models (VLMs) have enabled accurate\nimage-based geolocation, raising serious concerns about location privacy risks\nin everyday social media posts. However, current benchmarks remain\ncoarse-grained, linguistically biased, and lack multimodal and privacy-aware\nevaluations. To address these gaps, we present KoreaGEO Bench, the first\nfine-grained, multimodal geolocation benchmark for Korean street views. Our\ndataset comprises 1,080 high-resolution images sampled across four urban\nclusters and nine place types, enriched with multi-contextual annotations and\ntwo styles of Korean captions simulating real-world privacy exposure. We\nintroduce a three-path evaluation protocol to assess ten mainstream VLMs under\nvarying input modalities and analyze their accuracy, spatial bias, and\nreasoning behavior. Results reveal modality-driven shifts in localization\nprecision and highlight structural prediction biases toward core cities."}
{"id": "2506.03373", "pdf": "https://arxiv.org/pdf/2506.03373", "abs": "https://arxiv.org/abs/2506.03373", "authors": ["Muhammad Shaban", "Yuzhou Chang", "Huaying Qiu", "Yao Yu Yeo", "Andrew H. Song", "Guillaume Jaume", "Yuchen Wang", "Luca L. Weishaupt", "Tong Ding", "Anurag Vaidya", "Abdallah Lamane", "Daniel Shao", "Mohammed Zidane", "Yunhao Bai", "Paige McCallum", "Shuli Luo", "Wenrui Wu", "Yang Wang", "Precious Cramer", "Chi Ngai Chan", "Pierre Stephan", "Johanna Schaffenrath", "Jia Le Lee", "Hendrik A. Michel", "Caiwei Tian", "Cristina Almagro-Perez", "Sophia J. Wagner", "Sharifa Sahai", "Ming Y. Lu", "Richard J. Chen", "Andrew Zhang", "Mark Edward M. Gonzales", "Ahmad Makky", "Jia-Ying Joey Lee", "Hao Cheng", "Nourhan El Ahmar", "Sayed Matar", "Maximilian Haist", "Darci Phillips", "Yuqi Tan", "Garry P. Nolan", "W. Richard Burack", "Jacob D. Estes", "Jonathan T. C. Liu", "Toni K Choueiri", "Neeraj Agarwal", "Marc Barry", "Scott J. Rodig", "Long Phi Le", "Georg Gerber", "Christian M. Schrch", "Fabian J. Theis", "Youn H Kim", "Joe Yeong", "Sabina Signoretti", "Brooke E. Howitt", "Lit-Hsin Loo", "Qin Ma", "Sizun Jiang", "Faisal Mahmood"], "title": "A Foundation Model for Spatial Proteomics", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Foundation models have begun to transform image analysis by acting as\npretrained generalist backbones that can be adapted to many tasks even when\npost-training data are limited, yet their impact on spatial proteomics, imaging\nthat maps proteins at single-cell resolution, remains limited. Here, we\nintroduce KRONOS, a foundation model built for spatial proteomics. KRONOS was\ntrained in a self-supervised manner on over 47 million image patches covering\n175 protein markers, 16 tissue types, and 8 fluorescence-based imaging\nplatforms. We introduce key architectural adaptations to address the\nhigh-dimensional, multi-channel, and heterogeneous nature of multiplex imaging.\nWe demonstrate that KRONOS learns biologically meaningful representations\nacross multiple scales, ranging from cellular and microenvironment to tissue\nlevels, enabling it to address diverse downstream tasks, including cell\nphenotyping, region classification, and patient stratification. Evaluated\nacross 11 independent cohorts, KRONOS achieves state-of-the-art performance\nacross cell phenotyping, treatment response prediction, and retrieval tasks,\nand is highly data-efficient. KRONOS also introduces the paradigm of\nsegmentation-free patch-level processing for efficient and scalable spatial\nproteomics analysis, allowing cross-institutional comparisons, and as an image\nreverse search engine for spatial patterns. Together, these results position\nKRONOS as a flexible and scalable tool for spatial proteomics. The model is\npublicly accessible at https://github.com/mahmoodlab/KRONOS."}
{"id": "2506.03388", "pdf": "https://arxiv.org/pdf/2506.03388", "abs": "https://arxiv.org/abs/2506.03388", "authors": ["Pengyu Chen", "Xiao Huang", "Teng Fei", "Sicheng Wang"], "title": "Cross-Modal Urban Sensing: Evaluating Sound-Vision Alignment Across Street-Level and Aerial Imagery", "categories": ["cs.CV"], "comment": null, "summary": "Environmental soundscapes convey substantial ecological and social\ninformation regarding urban environments; however, their potential remains\nlargely untapped in large-scale geographic analysis. In this study, we\ninvestigate the extent to which urban sounds correspond with visual scenes by\ncomparing various visual representation strategies in capturing acoustic\nsemantics. We employ a multimodal approach that integrates geo-referenced sound\nrecordings with both street-level and remote sensing imagery across three major\nglobal cities: London, New York, and Tokyo. Utilizing the AST model for audio,\nalong with CLIP and RemoteCLIP for imagery, as well as CLIPSeg and Seg-Earth OV\nfor semantic segmentation, we extract embeddings and class-level features to\nevaluate cross-modal similarity. The results indicate that street view\nembeddings demonstrate stronger alignment with environmental sounds compared to\nsegmentation outputs, whereas remote sensing segmentation is more effective in\ninterpreting ecological categories through a Biophony--Geophony--Anthrophony\n(BGA) framework. These findings imply that embedding-based models offer\nsuperior semantic alignment, while segmentation-based methods provide\ninterpretable links between visual structure and acoustic ecology. This work\nadvances the burgeoning field of multimodal urban sensing by offering novel\nperspectives for incorporating sound into geospatial analysis."}
{"id": "2506.03394", "pdf": "https://arxiv.org/pdf/2506.03394", "abs": "https://arxiv.org/abs/2506.03394", "authors": ["Shafqaat Ahmad"], "title": "Temporal Vegetation Index-Based Unsupervised Crop Stress Detection via Eigenvector-Guided Contrastive Learning", "categories": ["cs.CV"], "comment": null, "summary": "Early detection of crop stress is vital for minimizing yield loss and\nenabling timely intervention in precision agriculture. Traditional approaches\nusing NDRE often detect stress only after visible symptoms appear or require\nlabeled datasets, limiting scalability. This study introduces EigenCL, a novel\nunsupervised contrastive learning framework guided by temporal NDRE dynamics\nand biologically grounded eigen decomposition. Using over 10,000 Sentinel-2\nNDRE image patches from drought-affected Iowa cornfields, we constructed\nfive-point NDRE time series per patch and derived an RBF similarity matrix. The\nprincipal eigenvector explaining 76% of the variance and strongly correlated (r\n= 0.95) with raw NDRE values was used to define stress-aware similarity for\ncontrastive embedding learning. Unlike existing methods that rely on visual\naugmentations, EigenCL pulls embeddings together based on biologically similar\nstress trajectories and pushes apart divergent ones. The learned embeddings\nformed physiologically meaningful clusters, achieving superior clustering\nmetrics (Silhouette: 0.748, DBI: 0.35) and enabling 76% early stress detection\nup to 12 days before conventional NDRE thresholds. Downstream classification\nyielded 95% k-NN and 91% logistic regression accuracy. Validation on an\nindependent 2023 Nebraska dataset confirmed generalizability without\nretraining. EigenCL offers a label-free, scalable approach for early stress\ndetection that aligns with underlying plant physiology and is suitable for\nreal-world deployment in data-scarce agricultural environments."}
{"id": "2506.03433", "pdf": "https://arxiv.org/pdf/2506.03433", "abs": "https://arxiv.org/abs/2506.03433", "authors": ["Yifan Li", "Xin Li", "Tianqin Li", "Wenbin He", "Yu Kong", "Liu Ren"], "title": "ViT-Split: Unleashing the Power of Vision Foundation Models via Efficient Splitting Heads", "categories": ["cs.CV"], "comment": "The project is available:\n  https://jackyfl.github.io/vitsplit.github.io/", "summary": "Vision foundation models (VFMs) have demonstrated remarkable performance\nacross a wide range of downstream tasks. While several VFM adapters have shown\npromising results by leveraging the prior knowledge of VFMs, we identify two\ninefficiencies in these approaches. First, the interaction between\nconvolutional neural network (CNN) and VFM backbone triggers early layer\ngradient backpropagation. Second, existing methods require tuning all\ncomponents, adding complexity. Besides, these adapters alter VFM features,\nunderutilizing the prior knowledge. To tackle these challenges, we propose a\nnew approach called ViT-Split, based on a key observation: the layers of\nseveral VFMs, like DINOv2, can be divided into two distinct components: an\nextractor for learning low-level features and an adapter for learning\ntask-specific features. Leveraging this insight, we eliminate the CNN branch\nand introduce two heads, task head and prior head, to the frozen VFM. The task\nhead is designed to learn task-specific features, mitigating the early gradient\npropagation issue. The prior head is used to leverage the multi-scale prior\nfeatures from the frozen VFM, reducing tuning parameters and overfitting.\nExtensive experiments on various tasks (e.g., segmentation, detection, depth\nestimation, and visual question answering) validate the effectiveness and\nefficiency of ViT-Split. Specifically, ViT-Split reduces training time up to\n$4\\times$ while achieving comparable or even better results on ADE20K, compared\nto other VFM adapters."}
{"id": "2506.03440", "pdf": "https://arxiv.org/pdf/2506.03440", "abs": "https://arxiv.org/abs/2506.03440", "authors": ["Tanqiu Qiao", "Ruochen Li", "Frederick W. B. Li", "Yoshiki Kubotani", "Shigeo Morishima", "Hubert P. H. Shum"], "title": "Geometric Visual Fusion Graph Neural Networks for Multi-Person Human-Object Interaction Recognition in Videos", "categories": ["cs.CV"], "comment": "Accepted by Expert Systems with Applications (ESWA)", "summary": "Human-Object Interaction (HOI) recognition in videos requires understanding\nboth visual patterns and geometric relationships as they evolve over time.\nVisual and geometric features offer complementary strengths. Visual features\ncapture appearance context, while geometric features provide structural\npatterns. Effectively fusing these multimodal features without compromising\ntheir unique characteristics remains challenging. We observe that establishing\nrobust, entity-specific representations before modeling interactions helps\npreserve the strengths of each modality. Therefore, we hypothesize that a\nbottom-up approach is crucial for effective multimodal fusion. Following this\ninsight, we propose the Geometric Visual Fusion Graph Neural Network\n(GeoVis-GNN), which uses dual-attention feature fusion combined with\ninterdependent entity graph learning. It progressively builds from\nentity-specific representations toward high-level interaction understanding. To\nadvance HOI recognition to real-world scenarios, we introduce the Concurrent\nPartial Interaction Dataset (MPHOI-120). It captures dynamic multi-person\ninteractions involving concurrent actions and partial engagement. This dataset\nhelps address challenges like complex human-object dynamics and mutual\nocclusions. Extensive experiments demonstrate the effectiveness of our method\nacross various HOI scenarios. These scenarios include two-person interactions,\nsingle-person activities, bimanual manipulations, and complex concurrent\npartial interactions. Our method achieves state-of-the-art performance."}
{"id": "2506.03448", "pdf": "https://arxiv.org/pdf/2506.03448", "abs": "https://arxiv.org/abs/2506.03448", "authors": ["Bimsara Pathiraja", "Maitreya Patel", "Shivam Singh", "Yezhou Yang", "Chitta Baral"], "title": "RefEdit: A Benchmark and Method for Improving Instruction-based Image Editing Model on Referring Expressions", "categories": ["cs.CV"], "comment": "Project page: \\url{http://refedit.vercel.app}", "summary": "Despite recent advances in inversion and instruction-based image editing,\nexisting approaches primarily excel at editing single, prominent objects but\nsignificantly struggle when applied to complex scenes containing multiple\nentities. To quantify this gap, we first introduce RefEdit-Bench, a rigorous\nreal-world benchmark rooted in RefCOCO, where even baselines trained on\nmillions of samples perform poorly. To overcome this limitation, we introduce\nRefEdit -- an instruction-based editing model trained on our scalable synthetic\ndata generation pipeline. Our RefEdit, trained on only 20,000 editing triplets,\noutperforms the Flux/SD3 model-based baselines trained on millions of data.\nExtensive evaluations across various benchmarks demonstrate that our model not\nonly excels in referring expression tasks but also enhances performance on\ntraditional benchmarks, achieving state-of-the-art results comparable to\nclosed-source methods. We release data \\& checkpoint for reproducibility."}
{"id": "2506.03449", "pdf": "https://arxiv.org/pdf/2506.03449", "abs": "https://arxiv.org/abs/2506.03449", "authors": ["John W. Smutny"], "title": "The effects of using created synthetic images in computer vision training", "categories": ["cs.CV"], "comment": "Nine pages long. Main content in pages one through eight. References\n  start at page nine", "summary": "This paper investigates how rendering engines, like Unreal Engine 4 (UE), can\nbe used to create synthetic images to supplement datasets for deep computer\nvision (CV) models in image abundant and image limited use cases. Using\nrendered synthetic images from UE can provide developers and businesses with a\nmethod of accessing nearly unlimited, reproducible, agile, and cheap training\nsets for their customers and applications without the threat of poisoned images\nfrom the internet or the cost of collecting them. The validity of these\ngenerated images are examined by testing the change in model test accuracy in\ntwo different sized CV models across two binary classification cases (Cat vs\nDog and Weld Defect Detection). In addition, this paper provides an\nimplementation of how to measure the quality of synthetic images by using\npre-trained CV models as auditors. Results imply that for large (VGG16) and\nsmall (MobileNetV3-small) parameter deep CV models, adding >60% additional\nsynthetic images to a real image dataset during model training can narrow the\ntest-training accuracy gap to ~1-2% without a conclusive effect on test\naccuracy compared to using real world images alone. Likewise, adding <10%\nadditional real training images to synthetic only training sets decreased the\nclassification error rate in half, then decreasing further when adding more\nreal training images. For these cases tested, using synthetic images from\nrendering engines allow researchers to only use 10% of their real images during\ntraining, compared to the traditional 50-70%. This research serves as an\nexample of how to create synthetic images, guidelines on how to use the images,\npotential restrictions and possible performance improvements for data-scarce\nprojects."}
{"id": "2506.03461", "pdf": "https://arxiv.org/pdf/2506.03461", "abs": "https://arxiv.org/abs/2506.03461", "authors": ["Nan Xiang", "Lifeng Xing", "Dequan Jin"], "title": "RoNFA: Robust Neural Field-based Approach for Few-Shot Image Classification with Noisy Labels", "categories": ["cs.CV", "68Txx", "I.5.1"], "comment": "7 pages, 1 figure", "summary": "In few-shot learning (FSL), the labeled samples are scarce. Thus, label\nerrors can significantly reduce classification accuracy. Since label errors are\ninevitable in realistic learning tasks, improving the robustness of the model\nin the presence of label errors is critical. This paper proposes a new robust\nneural field-based image approach (RoNFA) for few-shot image classification\nwith noisy labels. RoNFA consists of two neural fields for feature and category\nrepresentation. They correspond to the feature space and category set. Each\nneuron in the field for category representation (FCR) has a receptive field\n(RF) on the field for feature representation (FFR) centered at the\nrepresentative neuron for its category generated by soft clustering. In the\nprediction stage, the range of these receptive fields adapts according to the\nneuronal activation in FCR to ensure prediction accuracy. These learning\nstrategies provide the proposed model with excellent few-shot learning\ncapability and strong robustness against label noises. The experimental results\non real-world FSL datasets with three different types of label noise\ndemonstrate that the proposed method significantly outperforms state-of-the-art\nFSL methods. Its accuracy obtained in the presence of noisy labels even\nsurpasses the results obtained by state-of-the-art FSL methods trained on clean\nsupport sets, indicating its strong robustness against noisy labels."}
{"id": "2506.03473", "pdf": "https://arxiv.org/pdf/2506.03473", "abs": "https://arxiv.org/abs/2506.03473", "authors": ["Xinru Ying", "Jiaqi Mo", "Jingyang Lin", "Canghong Jin", "Fangfang Wang", "Lina Wei"], "title": "MamFusion: Multi-Mamba with Temporal Fusion for Partially Relevant Video Retrieval", "categories": ["cs.CV"], "comment": null, "summary": "Partially Relevant Video Retrieval (PRVR) is a challenging task in the domain\nof multimedia retrieval. It is designed to identify and retrieve untrimmed\nvideos that are partially relevant to the provided query. In this work, we\ninvestigate long-sequence video content understanding to address information\nredundancy issues. Leveraging the outstanding long-term state space modeling\ncapability and linear scalability of the Mamba module, we introduce a\nmulti-Mamba module with temporal fusion framework (MamFusion) tailored for PRVR\ntask. This framework effectively captures the state-relatedness in long-term\nvideo content and seamlessly integrates it into text-video relevance\nunderstanding, thereby enhancing the retrieval process. Specifically, we\nintroduce Temporal T-to-V Fusion and Temporal V-to-T Fusion to explicitly model\ntemporal relationships between text queries and video moments, improving\ncontextual awareness and retrieval accuracy. Extensive experiments conducted on\nlarge-scale datasets demonstrate that MamFusion achieves state-of-the-art\nperformance in retrieval effectiveness. Code is available at the link:\nhttps://github.com/Vision-Multimodal-Lab-HZCU/MamFusion."}
{"id": "2506.03481", "pdf": "https://arxiv.org/pdf/2506.03481", "abs": "https://arxiv.org/abs/2506.03481", "authors": ["Hongsong Wang", "Xiaoyan Ma", "Jidong Kuang", "Jie Gui"], "title": "Heterogeneous Skeleton-Based Action Representation Learning", "categories": ["cs.CV"], "comment": "To appear in CVPR 2025", "summary": "Skeleton-based human action recognition has received widespread attention in\nrecent years due to its diverse range of application scenarios. Due to the\ndifferent sources of human skeletons, skeleton data naturally exhibit\nheterogeneity. The previous works, however, overlook the heterogeneity of human\nskeletons and solely construct models tailored for homogeneous skeletons. This\nwork addresses the challenge of heterogeneous skeleton-based action\nrepresentation learning, specifically focusing on processing skeleton data that\nvaries in joint dimensions and topological structures. The proposed framework\ncomprises two primary components: heterogeneous skeleton processing and unified\nrepresentation learning. The former first converts two-dimensional skeleton\ndata into three-dimensional skeleton via an auxiliary network, and then\nconstructs a prompted unified skeleton using skeleton-specific prompts. We also\ndesign an additional modality named semantic motion encoding to harness the\nsemantic information within skeletons. The latter module learns a unified\naction representation using a shared backbone network that processes different\nheterogeneous skeletons. Extensive experiments on the NTU-60, NTU-120, and\nPKU-MMD II datasets demonstrate the effectiveness of our method in various\ntasks of action understanding. Our approach can be applied to action\nrecognition in robots with different humanoid structures."}
{"id": "2506.03502", "pdf": "https://arxiv.org/pdf/2506.03502", "abs": "https://arxiv.org/abs/2506.03502", "authors": ["Yuxuan Chen", "Haipeng Xie"], "title": "CHIME: Conditional Hallucination and Integrated Multi-scale Enhancement for Time Series Diffusion Model", "categories": ["cs.CV", "cs.SY", "eess.SY"], "comment": null, "summary": "The denoising diffusion probabilistic model has become a mainstream\ngenerative model, achieving significant success in various computer vision\ntasks. Recently, there has been initial exploration of applying diffusion\nmodels to time series tasks. However, existing studies still face challenges in\nmulti-scale feature alignment and generative capabilities across different\nentities and long-time scales. In this paper, we propose CHIME, a conditional\nhallucination and integrated multi-scale enhancement framework for time series\ndiffusion models. By employing multi-scale decomposition and adaptive\nintegration, CHIME captures the decomposed features of time series, achieving\nin-domain distribution alignment between generated and original samples. In\naddition, we introduce a feature hallucination module in the conditional\ndenoising process, enabling the transfer of temporal features through the\ntraining of category-independent transformation layers. Experimental results on\npublicly available real-world datasets demonstrate that CHIME achieves\nstate-of-the-art performance and exhibits excellent generative generalization\ncapabilities in few-shot scenarios."}
{"id": "2506.03512", "pdf": "https://arxiv.org/pdf/2506.03512", "abs": "https://arxiv.org/abs/2506.03512", "authors": ["Daikun Liu", "Lei Cheng", "Teng Wang", "changyin Sun"], "title": "EDCFlow: Exploring Temporally Dense Difference Maps for Event-based Optical Flow Estimation", "categories": ["cs.CV"], "comment": "14 pages, 8 figures", "summary": "Recent learning-based methods for event-based optical flow estimation utilize\ncost volumes for pixel matching but suffer from redundant computations and\nlimited scalability to higher resolutions for flow refinement. In this work, we\ntake advantage of the complementarity between temporally dense feature\ndifferences of adjacent event frames and cost volume and present a lightweight\nevent-based optical flow network (EDCFlow) to achieve high-quality flow\nestimation at a higher resolution. Specifically, an attention-based multi-scale\ntemporal feature difference layer is developed to capture diverse motion\npatterns at high resolution in a computation-efficient manner. An adaptive\nfusion of high-resolution difference motion features and low-resolution\ncorrelation motion features is performed to enhance motion representation and\nmodel generalization. Notably, EDCFlow can serve as a plug-and-play refinement\nmodule for RAFT-like event-based methods to enhance flow details. Extensive\nexperiments demonstrate that EDCFlow achieves better performance with lower\ncomplexity compared to existing methods, offering superior generalization."}
{"id": "2506.03517", "pdf": "https://arxiv.org/pdf/2506.03517", "abs": "https://arxiv.org/abs/2506.03517", "authors": ["Ziyi Wu", "Anil Kag", "Ivan Skorokhodov", "Willi Menapace", "Ashkan Mirzaei", "Igor Gilitschenski", "Sergey Tulyakov", "Aliaksandr Siarohin"], "title": "DenseDPO: Fine-Grained Temporal Preference Optimization for Video Diffusion Models", "categories": ["cs.CV"], "comment": "Project page: https://snap-research.github.io/DenseDPO/", "summary": "Direct Preference Optimization (DPO) has recently been applied as a\npost-training technique for text-to-video diffusion models. To obtain training\ndata, annotators are asked to provide preferences between two videos generated\nfrom independent noise. However, this approach prohibits fine-grained\ncomparisons, and we point out that it biases the annotators towards low-motion\nclips as they often contain fewer visual artifacts. In this work, we introduce\nDenseDPO, a method that addresses these shortcomings by making three\ncontributions. First, we create each video pair for DPO by denoising corrupted\ncopies of a ground truth video. This results in aligned pairs with similar\nmotion structures while differing in local details, effectively neutralizing\nthe motion bias. Second, we leverage the resulting temporal alignment to label\npreferences on short segments rather than entire clips, yielding a denser and\nmore precise learning signal. With only one-third of the labeled data, DenseDPO\ngreatly improves motion generation over vanilla DPO, while matching it in text\nalignment, visual quality, and temporal consistency. Finally, we show that\nDenseDPO unlocks automatic preference annotation using off-the-shelf Vision\nLanguage Models (VLMs): GPT accurately predicts segment-level preferences\nsimilar to task-specifically fine-tuned video reward models, and DenseDPO\ntrained on these labels achieves performance close to using human labels."}
{"id": "2506.03521", "pdf": "https://arxiv.org/pdf/2506.03521", "abs": "https://arxiv.org/abs/2506.03521", "authors": ["Weinan He", "Zilei Wang", "Yixin Zhang"], "title": "Target Semantics Clustering via Text Representations for Robust Universal Domain Adaptation", "categories": ["cs.CV"], "comment": "Camera-ready version for AAAI 2025", "summary": "Universal Domain Adaptation (UniDA) focuses on transferring source domain\nknowledge to the target domain under both domain shift and unknown category\nshift. Its main challenge lies in identifying common class samples and aligning\nthem. Current methods typically obtain target domain semantics centers from an\nunconstrained continuous image representation space. Due to domain shift and\nthe unknown number of clusters, these centers often result in complex and less\nrobust alignment algorithm. In this paper, based on vision-language models, we\nsearch for semantic centers in a semantically meaningful and discrete text\nrepresentation space. The constrained space ensures almost no domain bias and\nappropriate semantic granularity for these centers, enabling a simple and\nrobust adaptation algorithm. Specifically, we propose TArget Semantics\nClustering (TASC) via Text Representations, which leverages information\nmaximization as a unified objective and involves two stages. First, with the\nfrozen encoders, a greedy search-based framework is used to search for an\noptimal set of text embeddings to represent target semantics. Second, with the\nsearch results fixed, encoders are refined based on gradient descent,\nsimultaneously achieving robust domain alignment and private class clustering.\nAdditionally, we propose Universal Maximum Similarity (UniMS), a scoring\nfunction tailored for detecting open-set samples in UniDA. Experimentally, we\nevaluate the universality of UniDA algorithms under four category shift\nscenarios. Extensive experiments on four benchmarks demonstrate the\neffectiveness and robustness of our method, which has achieved state-of-the-art\nperformance."}
{"id": "2506.03525", "pdf": "https://arxiv.org/pdf/2506.03525", "abs": "https://arxiv.org/abs/2506.03525", "authors": ["Daeun Lee", "Jaehong Yoon", "Jaemin Cho", "Mohit Bansal"], "title": "Video-Skill-CoT: Skill-based Chain-of-Thoughts for Domain-Adaptive Video Reasoning", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Project website: https://video-skill-cot.github.io/", "summary": "Recent advances in Chain-of-Thought (CoT) reasoning have improved complex\nvideo understanding, but existing methods often struggle to adapt to\ndomain-specific skills (e.g., event detection, spatial relation understanding,\nemotion understanding) over various video content. To address this, we propose\nVideo-Skill-CoT (a.k.a. Video-SKoT), a framework that automatically constructs\nand leverages skill-aware CoT supervisions for domain-adaptive video reasoning.\nFirst, we construct skill-based CoT annotations: we extract domain-relevant\nreasoning skills from training questions, cluster them into a shared skill\ntaxonomy, and create detailed multi-step CoT rationale tailored to each\nvideo-question pair for training. Second, we introduce a skill-specific expert\nlearning framework. Each expert module specializes in a subset of reasoning\nskills and is trained with lightweight adapters using the collected CoT\nsupervision. We demonstrate the effectiveness of the proposed approach on three\nvideo understanding benchmarks, where Video-SKoT consistently outperforms\nstrong baselines. We also provide in-depth analyses on comparing different CoT\nannotation pipelines and learned skills over multiple video domains."}
{"id": "2506.03538", "pdf": "https://arxiv.org/pdf/2506.03538", "abs": "https://arxiv.org/abs/2506.03538", "authors": ["Chengqi Li", "Zhihao Shi", "Yangdi Lu", "Wenbo He", "Xiangyu Xu"], "title": "Robust Neural Rendering in the Wild with Asymmetric Dual 3D Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "3D reconstruction from in-the-wild images remains a challenging task due to\ninconsistent lighting conditions and transient distractors. Existing methods\ntypically rely on heuristic strategies to handle the low-quality training data,\nwhich often struggle to produce stable and consistent reconstructions,\nfrequently resulting in visual artifacts. In this work, we propose Asymmetric\nDual 3DGS, a novel framework that leverages the stochastic nature of these\nartifacts: they tend to vary across different training runs due to minor\nrandomness. Specifically, our method trains two 3D Gaussian Splatting (3DGS)\nmodels in parallel, enforcing a consistency constraint that encourages\nconvergence on reliable scene geometry while suppressing inconsistent\nartifacts. To prevent the two models from collapsing into similar failure modes\ndue to confirmation bias, we introduce a divergent masking strategy that\napplies two complementary masks: a multi-cue adaptive mask and a\nself-supervised soft mask, which leads to an asymmetric training process of the\ntwo models, reducing shared error modes. In addition, to improve the efficiency\nof model training, we introduce a lightweight variant called Dynamic EMA Proxy,\nwhich replaces one of the two models with a dynamically updated Exponential\nMoving Average (EMA) proxy, and employs an alternating masking strategy to\npreserve divergence. Extensive experiments on challenging real-world datasets\ndemonstrate that our method consistently outperforms existing approaches while\nachieving high efficiency. Codes and trained models will be released."}
{"id": "2506.03555", "pdf": "https://arxiv.org/pdf/2506.03555", "abs": "https://arxiv.org/abs/2506.03555", "authors": ["Tianpei Zhang", "Jufeng Zhao", "Yiming Zhu", "Guangmang Cui"], "title": "WIFE-Fusion:Wavelet-aware Intra-inter Frequency Enhancement for Multi-model Image Fusion", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal image fusion effectively aggregates information from diverse\nmodalities, with fused images playing a crucial role in vision systems.\nHowever, existing methods often neglect frequency-domain feature exploration\nand interactive relationships. In this paper, we propose wavelet-aware\nIntra-inter Frequency Enhancement Fusion (WIFE-Fusion), a multimodal image\nfusion framework based on frequency-domain components interactions. Its core\ninnovations include: Intra-Frequency Self-Attention (IFSA) that leverages\ninherent cross-modal correlations and complementarity through interactive\nself-attention mechanisms to extract enriched frequency-domain features, and\nInter-Frequency Interaction (IFI) that enhances enriched features and filters\nlatent features via combinatorial interactions between heterogeneous\nfrequency-domain components across modalities. These processes achieve precise\nsource feature extraction and unified modeling of feature\nextraction-aggregation. Extensive experiments on five datasets across three\nmultimodal fusion tasks demonstrate WIFE-Fusion's superiority over current\nspecialized and unified fusion methods. Our code is available at\nhttps://github.com/Lmmh058/WIFE-Fusion."}
{"id": "2506.03571", "pdf": "https://arxiv.org/pdf/2506.03571", "abs": "https://arxiv.org/abs/2506.03571", "authors": ["Chong Hyun Lee", "Kibae Lee"], "title": "DiagNet: Detecting Objects using Diagonal Constraints on Adjacency Matrix of Graph Neural Network", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We propose DaigNet, a new approach to object detection with which we can\ndetect an object bounding box using diagonal constraints on adjacency matrix of\na graph convolutional network (GCN). We propose two diagonalization algorithms\nbased on hard and soft constraints on adjacency matrix and two loss functions\nusing diagonal constraint and complementary constraint. The DaigNet eliminates\nthe need for designing a set of anchor boxes commonly used. To prove\nfeasibility of our novel detector, we adopt detection head in YOLO models.\nExperiments show that the DiagNet achieves 7.5% higher mAP50 on Pascal VOC than\nYOLOv1. The DiagNet also shows 5.1% higher mAP on MS COCO than YOLOv3u, 3.7%\nhigher mAP than YOLOv5u, and 2.9% higher mAP than YOLOv8."}
{"id": "2506.03582", "pdf": "https://arxiv.org/pdf/2506.03582", "abs": "https://arxiv.org/abs/2506.03582", "authors": ["Rui Yann", "Xianglei Xing"], "title": "ViTSGMM: A Robust Semi-Supervised Image Recognition Network Using Sparse Labels", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "We present ViTSGMM, an image recognition network that leverages\nsemi-supervised learning in a highly efficient manner. Existing works often\nrely on complex training techniques and architectures, while their\ngeneralization ability when dealing with extremely limited labeled data remains\nto be improved. To address these limitations, we construct a hierarchical\nmixture density classification decision mechanism by optimizing mutual\ninformation between feature representations and target classes, compressing\nredundant information while retaining crucial discriminative components.\nExperimental results demonstrate that our method achieves state-of-the-art\nperformance on STL-10 and CIFAR-10/100 datasets when using negligible labeled\nsamples. Notably, this paper also reveals a long-overlooked data leakage issue\nin the STL-10 dataset for semi-supervised learning tasks and removes duplicates\nto ensure the reliability of experimental results. Code available at\nhttps://github.com/Shu1L0n9/ViTSGMM."}
{"id": "2506.03583", "pdf": "https://arxiv.org/pdf/2506.03583", "abs": "https://arxiv.org/abs/2506.03583", "authors": ["Zhigang Yang", "Huiguang Yao", "Linmao Tian", "Xuezhi Zhao", "Qiang Li", "Qi Wang"], "title": "A Large-Scale Referring Remote Sensing Image Segmentation Dataset and Benchmark", "categories": ["cs.CV"], "comment": null, "summary": "Referring Remote Sensing Image Segmentation is a complex and challenging task\nthat integrates the paradigms of computer vision and natural language\nprocessing. Existing datasets for RRSIS suffer from critical limitations in\nresolution, scene diversity, and category coverage, which hinders the\ngeneralization and real-world applicability of refer segmentation models. To\nfacilitate the development of this field, we introduce NWPU-Refer, the largest\nand most diverse RRSIS dataset to date, comprising 15,003 high-resolution\nimages (1024-2048px) spanning 30+ countries with 49,745 annotated targets\nsupporting single-object, multi-object, and non-object segmentation scenarios.\nAdditionally, we propose the Multi-scale Referring Segmentation Network\n(MRSNet), a novel framework tailored for the unique demands of RRSIS. MRSNet\nintroduces two key innovations: (1) an Intra-scale Feature Interaction Module\n(IFIM) that captures fine-grained details within each encoder stage, and (2) a\nHierarchical Feature Interaction Module (HFIM) to enable seamless cross-scale\nfeature fusion, preserving spatial integrity while enhancing discriminative\npower. Extensive experiments conducte on the proposed NWPU-Refer dataset\ndemonstrate that MRSNet achieves state-of-the-art performance across multiple\nevaluation metrics, validating its effectiveness. The dataset and code are\npublicly available at https://github.com/CVer-Yang/NWPU-Refer."}
{"id": "2506.03589", "pdf": "https://arxiv.org/pdf/2506.03589", "abs": "https://arxiv.org/abs/2506.03589", "authors": ["Huy Le", "Nhat Chung", "Tung Kieu", "Anh Nguyen", "Ngan Le"], "title": "BiMa: Towards Biases Mitigation for Text-Video Retrieval via Scene Element Guidance", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "22 pages, 14 figures", "summary": "Text-video retrieval (TVR) systems often suffer from visual-linguistic biases\npresent in datasets, which cause pre-trained vision-language models to overlook\nkey details. To address this, we propose BiMa, a novel framework designed to\nmitigate biases in both visual and textual representations. Our approach begins\nby generating scene elements that characterize each video by identifying\nrelevant entities/objects and activities. For visual debiasing, we integrate\nthese scene elements into the video embeddings, enhancing them to emphasize\nfine-grained and salient details. For textual debiasing, we introduce a\nmechanism to disentangle text features into content and bias components,\nenabling the model to focus on meaningful content while separately handling\nbiased information. Extensive experiments and ablation studies across five\nmajor TVR benchmarks (i.e., MSR-VTT, MSVD, LSMDC, ActivityNet, and DiDeMo)\ndemonstrate the competitive performance of BiMa. Additionally, the model's bias\nmitigation capability is consistently validated by its strong results on\nout-of-distribution retrieval tasks."}
{"id": "2506.03591", "pdf": "https://arxiv.org/pdf/2506.03591", "abs": "https://arxiv.org/abs/2506.03591", "authors": ["Jiaxing Zhang", "Xinyi Zeng", "Hao Tang"], "title": "Resolving Task Objective Conflicts in Unified Multimodal Understanding and Generation via Task-Aware Mixture-of-Experts", "categories": ["cs.CV"], "comment": null, "summary": "Unified multimodal large language models (MLLMs) based on end-to-end\nautoregressive (AR) transformers effectively integrate both understanding and\ngeneration tasks within a single framework. However, intrinsic Task Objective\nConflicts between high-level semantic abstraction in understanding and\nfine-grained detail preservation in generation pose significant challenges,\noften leading to suboptimal trade-offs and task interference. Existing\nsolutions, such as decoupling shared visual encoders, fall short of\nfundamentally resolving these conflicts due to inherent AR architecture. In\nthis paper, we propose a novel approach that decouples internal components of\nAR to resolve task objective conflicts. Specifically, we design UTAMoE, a\nUnified Task-Aware Mixture-of-Experts (MoE) framework that decouples internal\nAR modules via a Task-Aware MoE Layer to create task-specific optimization\nsubpaths. To enhance task differentiation while maintaining overall\ncoordination, we introduce a novel Two-Stage Training Strategy. Extensive\nexperiments on multimodal benchmarks demonstrate that UTAMoE mitigates task\nobjective conflicts, achieving state-of-the-art performance across various\ntasks. Visualizations and ablation studies further validate the effectiveness\nof our approach."}
{"id": "2506.03596", "pdf": "https://arxiv.org/pdf/2506.03596", "abs": "https://arxiv.org/abs/2506.03596", "authors": ["Feng Han", "Yang Jiao", "Shaoxiang Chen", "Junhao Xu", "Jingjing Chen", "Yu-Gang Jiang"], "title": "ControlThinker: Unveiling Latent Semantics for Controllable Image Generation through Visual Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "The field of controllable image generation has seen significant advancements,\nwith various architectures improving generation layout consistency with control\nsignals. However, contemporary methods still face challenges in bridging the\nsemantic gap between input text prompts with sparse semantics and the target\nimages, often over-relying on low-level control signals to infer regional\ndetails. To address this challenge, we propose ControlThinker, a novel\nframework that employs a \"comprehend-then-generate\" paradigm. Firstly, by\nincentivizing the visual reasoning capability of a MLLM, latent semantics from\ncontrol images are mined to enrich text prompts. This enriched semantic\nunderstanding then seamlessly aids in image generation without the need for\nadditional complex modifications. To further tackle the uncertainty arising\nfrom the ambiguity of control images, we encourage broader exploration of\nreasoning trajectories and select the optimal one using a metric-based output\nreward model (ORM). Extensive experimental results demonstrate that\nControlThinker effectively mitigates the semantic gap between raw text prompts\nand target images, resulting in improved visual quality and semantic\nconsistency across a wide range of benchmarks. The code and models are\navailable at https://github.com/Maplebb/ControlThinker."}
{"id": "2506.03605", "pdf": "https://arxiv.org/pdf/2506.03605", "abs": "https://arxiv.org/abs/2506.03605", "authors": ["Tomoya Yoshida", "Shuhei Kurita", "Taichi Nishimura", "Shinsuke Mori"], "title": "Generating 6DoF Object Manipulation Trajectories from Action Description in Egocentric Vision", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Learning to use tools or objects in common scenes, particularly handling them\nin various ways as instructed, is a key challenge for developing interactive\nrobots. Training models to generate such manipulation trajectories requires a\nlarge and diverse collection of detailed manipulation demonstrations for\nvarious objects, which is nearly unfeasible to gather at scale. In this paper,\nwe propose a framework that leverages large-scale ego- and exo-centric video\ndatasets -- constructed globally with substantial effort -- of Exo-Ego4D to\nextract diverse manipulation trajectories at scale. From these extracted\ntrajectories with the associated textual action description, we develop\ntrajectory generation models based on visual and point cloud-based language\nmodels. In the recently proposed egocentric vision-based in-a-quality\ntrajectory dataset of HOT3D, we confirmed that our models successfully generate\nvalid object trajectories, establishing a training dataset and baseline models\nfor the novel task of generating 6DoF manipulation trajectories from action\ndescriptions in egocentric vision."}
{"id": "2506.03607", "pdf": "https://arxiv.org/pdf/2506.03607", "abs": "https://arxiv.org/abs/2506.03607", "authors": ["Wing Man Casca Kwok", "Yip Chiu Tung", "Kunal Bhagchandani"], "title": "Analyzing Transformer Models and Knowledge Distillation Approaches for Image Captioning on Edge AI", "categories": ["cs.CV"], "comment": null, "summary": "Edge computing decentralizes processing power to network edge, enabling\nreal-time AI-driven decision-making in IoT applications. In industrial\nautomation such as robotics and rugged edge AI, real-time perception and\nintelligence are critical for autonomous operations. Deploying\ntransformer-based image captioning models at the edge can enhance machine\nperception, improve scene understanding for autonomous robots, and aid in\nindustrial inspection.\n  However, these edge or IoT devices are often constrained in computational\nresources for physical agility, yet they have strict response time\nrequirements. Traditional deep learning models can be too large and\ncomputationally demanding for these devices. In this research, we present\nfindings of transformer-based models for image captioning that operate\neffectively on edge devices. By evaluating resource-effective transformer\nmodels and applying knowledge distillation techniques, we demonstrate inference\ncan be accelerated on resource-constrained devices while maintaining model\nperformance using these techniques."}
{"id": "2506.03608", "pdf": "https://arxiv.org/pdf/2506.03608", "abs": "https://arxiv.org/abs/2506.03608", "authors": ["Di Fan", "Heng Yu", "Zhiyuan Xu"], "title": "PDSE: A Multiple Lesion Detector for CT Images using PANet and Deformable Squeeze-and-Excitation Block", "categories": ["cs.CV"], "comment": "MIUA 2024", "summary": "Detecting lesions in Computed Tomography (CT) scans is a challenging task in\nmedical image processing due to the diverse types, sizes, and locations of\nlesions. Recently, various one-stage and two-stage framework networks have been\ndeveloped to focus on lesion localization. We introduce a one-stage lesion\ndetection framework, PDSE, by redesigning Retinanet to achieve higher accuracy\nand efficiency for detecting lesions in multimodal CT images. Specifically, we\nenhance the path aggregation flow by incorporating a low-level feature map.\nAdditionally, to improve model representation, we utilize the adaptive\nSqueeze-and-Excitation (SE) block and integrate channel feature map attention.\nThis approach has resulted in achieving new state-of-the-art performance. Our\nmethod significantly improves the detection of small and multiscaled objects.\nWhen evaluated against other advanced algorithms on the public DeepLesion\nbenchmark, our algorithm achieved an mAP of over 0.20."}
{"id": "2506.03614", "pdf": "https://arxiv.org/pdf/2506.03614", "abs": "https://arxiv.org/abs/2506.03614", "authors": ["Zhanhui Zhou", "Lingjie Chen", "Chao Yang", "Chaochao Lu"], "title": "VLMs Can Aggregate Scattered Training Patches", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "One way to mitigate risks in vision-language models (VLMs) is to remove\ndangerous samples in their training data. However, such data moderation can be\neasily bypassed when harmful images are split into small, benign-looking\npatches, scattered across many training samples. VLMs may then learn to piece\nthese fragments together during training and generate harmful responses at\ninference, either from full images or text references. For instance, if trained\non image patches from a bloody scene paired with the descriptions \"safe,\" VLMs\nmay later describe, the full image or a text reference to the scene, as \"safe.\"\nWe define the core ability of VLMs enabling this attack as $\\textit{visual\nstitching}$ -- the ability to integrate visual information spread across\nmultiple training samples that share the same textual descriptions. In our\nwork, we first demonstrate visual stitching abilities in common open-source\nVLMs on three datasets where each image is labeled with a unique synthetic ID:\nwe split each $(\\texttt{image}, \\texttt{ID})$ pair into $\\{(\\texttt{patch},\n\\texttt{ID})\\}$ pairs at different granularity for finetuning, and we find that\ntuned models can verbalize the correct IDs from full images or text reference.\nBuilding on this, we simulate the adversarial data poisoning scenario mentioned\nabove by using patches from dangerous images and replacing IDs with text\ndescriptions like ``safe'' or ``unsafe'', demonstrating how harmful content can\nevade moderation in patches and later be reconstructed through visual\nstitching, posing serious VLM safety risks. Code is available at\nhttps://github.com/ZHZisZZ/visual-stitching."}
{"id": "2506.03615", "pdf": "https://arxiv.org/pdf/2506.03615", "abs": "https://arxiv.org/abs/2506.03615", "authors": ["Sarah Alyami", "Hamzah Luqman", "Sadam Al-Azani", "Maad Alowaifeer", "Yazeed Alharbi", "Yaser Alonaizan"], "title": "Isharah: A Large-Scale Multi-Scene Dataset for Continuous Sign Language Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Current benchmarks for sign language recognition (SLR) focus mainly on\nisolated SLR, while there are limited datasets for continuous SLR (CSLR), which\nrecognizes sequences of signs in a video. Additionally, existing CSLR datasets\nare collected in controlled settings, which restricts their effectiveness in\nbuilding robust real-world CSLR systems. To address these limitations, we\npresent Isharah, a large multi-scene dataset for CSLR. It is the first dataset\nof its type and size that has been collected in an unconstrained environment\nusing signers' smartphone cameras. This setup resulted in high variations of\nrecording settings, camera distances, angles, and resolutions. This variation\nhelps with developing sign language understanding models capable of handling\nthe variability and complexity of real-world scenarios. The dataset consists of\n30,000 video clips performed by 18 deaf and professional signers. Additionally,\nthe dataset is linguistically rich as it provides a gloss-level annotation for\nall dataset's videos, making it useful for developing CSLR and sign language\ntranslation (SLT) systems. This paper also introduces multiple sign language\nunderstanding benchmarks, including signer-independent and unseen-sentence\nCSLR, along with gloss-based and gloss-free SLT. The Isharah dataset is\navailable on https://snalyami.github.io/Isharah_CSLR/."}
{"id": "2506.03621", "pdf": "https://arxiv.org/pdf/2506.03621", "abs": "https://arxiv.org/abs/2506.03621", "authors": ["Chaehun Shin", "Jooyoung Choi", "Johan Barthelemy", "Jungbeom Lee", "Sungroh Yoon"], "title": "Negative-Guided Subject Fidelity Optimization for Zero-Shot Subject-Driven Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We present Subject Fidelity Optimization (SFO), a novel comparative learning\nframework for zero-shot subject-driven generation that enhances subject\nfidelity. Beyond supervised fine-tuning methods that rely only on positive\ntargets and use the diffusion loss as in the pre-training stage, SFO introduces\nsynthetic negative targets and explicitly guides the model to favor positives\nover negatives through pairwise comparison. For negative targets, we propose\nCondition-Degradation Negative Sampling (CDNS), which automatically generates\ndistinctive and informative negatives by intentionally degrading visual and\ntextual cues without expensive human annotations. Moreover, we reweight the\ndiffusion timesteps to focus finetuning on intermediate steps where subject\ndetails emerge. Extensive experiments demonstrate that SFO with CDNS\nsignificantly outperforms baselines in terms of both subject fidelity and text\nalignment on a subject-driven generation benchmark. Project page:\nhttps://subjectfidelityoptimization.github.io/"}
{"id": "2506.03635", "pdf": "https://arxiv.org/pdf/2506.03635", "abs": "https://arxiv.org/abs/2506.03635", "authors": ["Yinfan Wang", "Jie Gui", "Baosheng Yu", "Qi Li", "Zhenan Sun", "Juho Kannala", "Guoying Zhao"], "title": "FingerVeinSyn-5M: A Million-Scale Dataset and Benchmark for Finger Vein Recognition", "categories": ["cs.CV"], "comment": null, "summary": "A major challenge in finger vein recognition is the lack of large-scale\npublic datasets. Existing datasets contain few identities and limited samples\nper finger, restricting the advancement of deep learning-based methods. To\naddress this, we introduce FVeinSyn, a synthetic generator capable of producing\ndiverse finger vein patterns with rich intra-class variations. Using FVeinSyn,\nwe created FingerVeinSyn-5M -- the largest available finger vein dataset --\ncontaining 5 million samples from 50,000 unique fingers, each with 100\nvariations including shift, rotation, scale, roll, varying exposure levels,\nskin scattering blur, optical blur, and motion blur. FingerVeinSyn-5M is also\nthe first to offer fully annotated finger vein images, supporting deep learning\napplications in this field. Models pretrained on FingerVeinSyn-5M and\nfine-tuned with minimal real data achieve an average 53.91\\% performance gain\nacross multiple benchmarks. The dataset is publicly available at:\nhttps://github.com/EvanWang98/FingerVeinSyn-5M."}
{"id": "2506.03642", "pdf": "https://arxiv.org/pdf/2506.03642", "abs": "https://arxiv.org/abs/2506.03642", "authors": ["Haoyu Zhang", "Meng Liu", "Zaijing Li", "Haokun Wen", "Weili Guan", "Yaowei Wang", "Liqiang Nie"], "title": "Spatial Understanding from Videos: Structured Prompts Meet Simulation Data", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Visual-spatial understanding, the ability to infer object relationships and\nlayouts from visual input, is fundamental to downstream tasks such as robotic\nnavigation and embodied interaction. However, existing methods face spatial\nuncertainty and data scarcity, limiting the 3D spatial reasoning capability of\npre-trained vision-language models (VLMs). To address these challenges, we\npresent a unified framework for enhancing 3D spatial reasoning in pre-trained\nVLMs without modifying their architecture. This framework combines SpatialMind,\na structured prompting strategy that decomposes complex scenes and questions\ninto interpretable reasoning steps, with ScanForgeQA, a scalable\nquestion-answering dataset built from diverse 3D simulation scenes through an\nautomated construction process designed for fine-tuning. Extensive experiments\nacross multiple benchmarks demonstrate the individual and combined\neffectiveness of our prompting and fine-tuning strategies, and yield insights\nthat may inspire future research on visual-spatial understanding."}
{"id": "2506.03643", "pdf": "https://arxiv.org/pdf/2506.03643", "abs": "https://arxiv.org/abs/2506.03643", "authors": ["Lingjun Mao", "Rodolfo Corona", "Xin Liang", "Wenhao Yan", "Zineng Tang"], "title": "Images are Worth Variable Length of Representations", "categories": ["cs.CV"], "comment": null, "summary": "Most existing vision encoders map images into a fixed-length sequence of\ntokens, overlooking the fact that different images contain varying amounts of\ninformation. For example, a visually complex image (e.g., a cluttered room)\ninherently carries more information and thus deserves more tokens than a simple\nimage (e.g., a blank wall). To address this inefficiency, we propose DOVE, a\ndynamic vision encoder that produces a variable number of visual tokens (i.e.,\ncontinuous representation vectors) to reconstruct each image. Our results show\nthat DOVE significantly reduces the average number of tokens while maintaining\nhigh reconstruction quality. In several linear probing and downstream\nmultimodal tasks, it outperforms existing autoencoder-based tokenization\nmethods when using far fewer tokens, capturing more expressive semantic\nfeatures compared to fixed-length encoding. We further extend DOVE with\nquery-conditioned tokenization. By guiding the model to focus on query-relevant\nregions, it achieves more efficient and targeted semantic extraction. Our code\nand checkpoints are available at https://dove-encoder.github.io/dove-encoder."}
{"id": "2506.03645", "pdf": "https://arxiv.org/pdf/2506.03645", "abs": "https://arxiv.org/abs/2506.03645", "authors": ["Hansen Feng", "Lizhi Wang", "Yiqi Huang", "Tong Li", "Lin Zhu", "Hua Huang"], "title": "YOND: Practical Blind Raw Image Denoising Free from Camera-Specific Data Dependency", "categories": ["cs.CV", "eess.IV"], "comment": "17 pages, 19 figures, TPAMI under review", "summary": "The rapid advancement of photography has created a growing demand for a\npractical blind raw image denoising method. Recently, learning-based methods\nhave become mainstream due to their excellent performance. However, most\nexisting learning-based methods suffer from camera-specific data dependency,\nresulting in performance drops when applied to data from unknown cameras. To\naddress this challenge, we introduce a novel blind raw image denoising method\nnamed YOND, which represents You Only Need a Denoiser. Trained solely on\nsynthetic data, YOND can generalize robustly to noisy raw images captured by\ndiverse unknown cameras. Specifically, we propose three key modules to\nguarantee the practicality of YOND: coarse-to-fine noise estimation (CNE),\nexpectation-matched variance-stabilizing transform (EM-VST), and SNR-guided\ndenoiser (SNR-Net). Firstly, we propose CNE to identify the camera noise\ncharacteristic, refining the estimated noise parameters based on the coarse\ndenoised image. Secondly, we propose EM-VST to eliminate camera-specific data\ndependency, correcting the bias expectation of VST according to the noisy\nimage. Finally, we propose SNR-Net to offer controllable raw image denoising,\nsupporting adaptive adjustments and manual fine-tuning. Extensive experiments\non unknown cameras, along with flexible solutions for challenging cases,\ndemonstrate the superior practicality of our method. The source code will be\npublicly available at the\n\\href{https://fenghansen.github.io/publication/YOND}{project homepage}."}
{"id": "2506.03652", "pdf": "https://arxiv.org/pdf/2506.03652", "abs": "https://arxiv.org/abs/2506.03652", "authors": ["Cheng Zhang", "Hongxia xie", "Bin Wen", "Songhan Zuo", "Ruoxuan Zhang", "Wen-huang Cheng"], "title": "EmoArt: A Multidimensional Dataset for Emotion-Aware Artistic Generation", "categories": ["cs.CV"], "comment": null, "summary": "With the rapid advancement of diffusion models, text-to-image generation has\nachieved significant progress in image resolution, detail fidelity, and\nsemantic alignment, particularly with models like Stable Diffusion 3.5, Stable\nDiffusion XL, and FLUX 1. However, generating emotionally expressive and\nabstract artistic images remains a major challenge, largely due to the lack of\nlarge-scale, fine-grained emotional datasets. To address this gap, we present\nthe EmoArt Dataset -- one of the most comprehensive emotion-annotated art\ndatasets to date. It contains 132,664 artworks across 56 painting styles (e.g.,\nImpressionism, Expressionism, Abstract Art), offering rich stylistic and\ncultural diversity. Each image includes structured annotations: objective scene\ndescriptions, five key visual attributes (brushwork, composition, color, line,\nlight), binary arousal-valence labels, twelve emotion categories, and potential\nart therapy effects. Using EmoArt, we systematically evaluate popular\ntext-to-image diffusion models for their ability to generate emotionally\naligned images from text. Our work provides essential data and benchmarks for\nemotion-driven image synthesis and aims to advance fields such as affective\ncomputing, multimodal learning, and computational art, enabling applications in\nart therapy and creative design. The dataset and more details can be accessed\nvia our project website."}
{"id": "2506.03654", "pdf": "https://arxiv.org/pdf/2506.03654", "abs": "https://arxiv.org/abs/2506.03654", "authors": ["Xiaochun Lei", "Siqi Wu", "Weilin Wu", "Zetao Jiang"], "title": "MambaNeXt-YOLO: A Hybrid State Space Model for Real-time Object Detection", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Real-time object detection is a fundamental but challenging task in computer\nvision, particularly when computational resources are limited. Although\nYOLO-series models have set strong benchmarks by balancing speed and accuracy,\nthe increasing need for richer global context modeling has led to the use of\nTransformer-based architectures. Nevertheless, Transformers have high\ncomputational complexity because of their self-attention mechanism, which\nlimits their practicality for real-time and edge deployments. To overcome these\nchallenges, recent developments in linear state space models, such as Mamba,\nprovide a promising alternative by enabling efficient sequence modeling with\nlinear complexity. Building on this insight, we propose MambaNeXt-YOLO, a novel\nobject detection framework that balances accuracy and efficiency through three\nkey contributions: (1) MambaNeXt Block: a hybrid design that integrates CNNs\nwith Mamba to effectively capture both local features and long-range\ndependencies; (2) Multi-branch Asymmetric Fusion Pyramid Network (MAFPN): an\nenhanced feature pyramid architecture that improves multi-scale object\ndetection across various object sizes; and (3) Edge-focused Efficiency: our\nmethod achieved 66.6\\% mAP at 31.9 FPS on the PASCAL VOC dataset without any\npre-training and supports deployment on edge devices such as the NVIDIA Jetson\nXavier NX and Orin NX."}
{"id": "2506.03660", "pdf": "https://arxiv.org/pdf/2506.03660", "abs": "https://arxiv.org/abs/2506.03660", "authors": ["Wei Luo", "Haiming Yao", "Yunkang Cao", "Qiyu Chen", "Ang Gao", "Weiming Shen", "Weihang Zhang", "Wenyong Yu"], "title": "INP-Former++: Advancing Universal Anomaly Detection via Intrinsic Normal Prototypes and Residual Learning", "categories": ["cs.CV"], "comment": "15 pages, 11 figures, 13 tables", "summary": "Anomaly detection (AD) is essential for industrial inspection and medical\ndiagnosis, yet existing methods typically rely on ``comparing'' test images to\nnormal references from a training set. However, variations in appearance and\npositioning often complicate the alignment of these references with the test\nimage, limiting detection accuracy. We observe that most anomalies manifest as\nlocal variations, meaning that even within anomalous images, valuable normal\ninformation remains. We argue that this information is useful and may be more\naligned with the anomalies since both the anomalies and the normal information\noriginate from the same image. Therefore, rather than relying on external\nnormality from the training set, we propose INP-Former, a novel method that\nextracts Intrinsic Normal Prototypes (INPs) directly from the test image.\nSpecifically, we introduce the INP Extractor, which linearly combines normal\ntokens to represent INPs. We further propose an INP Coherence Loss to ensure\nINPs can faithfully represent normality for the testing image. These INPs then\nguide the INP-guided Decoder to reconstruct only normal tokens, with\nreconstruction errors serving as anomaly scores. Additionally, we propose a\nSoft Mining Loss to prioritize hard-to-optimize samples during training.\nINP-Former achieves state-of-the-art performance in single-class, multi-class,\nand few-shot AD tasks across MVTec-AD, VisA, and Real-IAD, positioning it as a\nversatile and universal solution for AD. Remarkably, INP-Former also\ndemonstrates some zero-shot AD capability. Furthermore, we propose a soft\nversion of the INP Coherence Loss and enhance INP-Former by incorporating\nresidual learning, leading to the development of INP-Former++. The proposed\nmethod significantly improves detection performance across single-class,\nmulti-class, semi-supervised, few-shot, and zero-shot settings."}
{"id": "2506.03662", "pdf": "https://arxiv.org/pdf/2506.03662", "abs": "https://arxiv.org/abs/2506.03662", "authors": ["Erhang Zhang", "Junyi Ma", "Yin-Dong Zheng", "Yixuan Zhou", "Hesheng Wang"], "title": "Zero-Shot Temporal Interaction Localization for Egocentric Videos", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Locating human-object interaction (HOI) actions within video serves as the\nfoundation for multiple downstream tasks, such as human behavior analysis and\nhuman-robot skill transfer. Current temporal action localization methods\ntypically rely on annotated action and object categories of interactions for\noptimization, which leads to domain bias and low deployment efficiency.\nAlthough some recent works have achieved zero-shot temporal action localization\n(ZS-TAL) with large vision-language models (VLMs), their coarse-grained\nestimations and open-loop pipelines hinder further performance improvements for\ntemporal interaction localization (TIL). To address these issues, we propose a\nnovel zero-shot TIL approach dubbed EgoLoc to locate the timings of grasp\nactions for human-object interaction in egocentric videos. EgoLoc introduces a\nself-adaptive sampling strategy to generate reasonable visual prompts for VLM\nreasoning. By absorbing both 2D and 3D observations, it directly samples\nhigh-quality initial guesses around the possible contact/separation timestamps\nof HOI according to 3D hand velocities, leading to high inference accuracy and\nefficiency. In addition, EgoLoc generates closed-loop feedback from visual and\ndynamic cues to further refine the localization results. Comprehensive\nexperiments on the publicly available dataset and our newly proposed benchmark\ndemonstrate that EgoLoc achieves better temporal interaction localization for\negocentric videos compared to state-of-the-art baselines. We will release our\ncode and relevant data as open-source at https://github.com/IRMVLab/EgoLoc."}
{"id": "2506.03664", "pdf": "https://arxiv.org/pdf/2506.03664", "abs": "https://arxiv.org/abs/2506.03664", "authors": ["Valerie Krug", "Sebastian Stober"], "title": "Intersectional Bias in Pre-Trained Image Recognition Models", "categories": ["cs.CV", "cs.CY", "cs.HC", "cs.LG"], "comment": "Summary paper accepted at the 3rd TRR 318 Conference: Contextualizing\n  Explanations 2025", "summary": "Deep Learning models have achieved remarkable success. Training them is often\naccelerated by building on top of pre-trained models which poses the risk of\nperpetuating encoded biases. Here, we investigate biases in the representations\nof commonly used ImageNet classifiers for facial images while considering\nintersections of sensitive variables age, race and gender. To assess the\nbiases, we use linear classifier probes and visualize activations as\ntopographic maps. We find that representations in ImageNet classifiers\nparticularly allow differentiation between ages. Less strongly pronounced, the\nmodels appear to associate certain ethnicities and distinguish genders in\nmiddle-aged groups."}
{"id": "2506.03667", "pdf": "https://arxiv.org/pdf/2506.03667", "abs": "https://arxiv.org/abs/2506.03667", "authors": ["Joji Joseph", "Bharadwaj Amrutur", "Shalabh Bhatnagar"], "title": "Accelerating SfM-based Pose Estimation with Dominating Set", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "This paper introduces a preprocessing technique to speed up\nStructure-from-Motion (SfM) based pose estimation, which is critical for\nreal-time applications like augmented reality (AR), virtual reality (VR), and\nrobotics. Our method leverages the concept of a dominating set from graph\ntheory to preprocess SfM models, significantly enhancing the speed of the pose\nestimation process without losing significant accuracy. Using the OnePose\ndataset, we evaluated our method across various SfM-based pose estimation\ntechniques. The results demonstrate substantial improvements in processing\nspeed, ranging from 1.5 to 14.48 times, and a reduction in reference images and\npoint cloud size by factors of 17-23 and 2.27-4, respectively. This work offers\na promising solution for efficient and accurate 3D pose estimation, balancing\nspeed and accuracy in real-time applications."}
{"id": "2506.03675", "pdf": "https://arxiv.org/pdf/2506.03675", "abs": "https://arxiv.org/abs/2506.03675", "authors": ["Jialei Chen", "Xu Zheng", "Danda Pani Paudel", "Luc Van Gool", "Hiroshi Murase", "Daisuke Deguchi"], "title": "BiXFormer: A Robust Framework for Maximizing Modality Effectiveness in Multi-Modal Semantic Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Utilizing multi-modal data enhances scene understanding by providing\ncomplementary semantic and geometric information. Existing methods fuse\nfeatures or distill knowledge from multiple modalities into a unified\nrepresentation, improving robustness but restricting each modality's ability to\nfully leverage its strengths in different situations. We reformulate\nmulti-modal semantic segmentation as a mask-level classification task and\npropose BiXFormer, which integrates Unified Modality Matching (UMM) and Cross\nModality Alignment (CMA) to maximize modality effectiveness and handle missing\nmodalities. Specifically, BiXFormer first categorizes multi-modal inputs into\nRGB and X, where X represents any non-RGB modalities, e.g., depth, allowing\nseparate processing for each. This design leverages the well-established\npretraining for RGB, while addressing the relative lack of attention to X\nmodalities. Then, we propose UMM, which includes Modality Agnostic Matching\n(MAM) and Complementary Matching (CM). MAM assigns labels to features from all\nmodalities without considering modality differences, leveraging each modality's\nstrengths. CM then reassigns unmatched labels to remaining unassigned features\nwithin their respective modalities, ensuring that each available modality\ncontributes to the final prediction and mitigating the impact of missing\nmodalities. Moreover, to further facilitate UMM, we introduce CMA, which\nenhances the weaker queries assigned in CM by aligning them with optimally\nmatched queries from MAM. Experiments on both synthetic and real-world\nmulti-modal benchmarks demonstrate the effectiveness of our method, achieving\nsignificant improvements in mIoU of +2.75% and +22.74% over the prior arts."}
{"id": "2506.03682", "pdf": "https://arxiv.org/pdf/2506.03682", "abs": "https://arxiv.org/abs/2506.03682", "authors": ["Melika Ayoughi", "Samira Abnar", "Chen Huang", "Chris Sandino", "Sayeri Lala", "Eeshan Gunesh Dhekane", "Dan Busbridge", "Shuangfei Zhai", "Vimal Thilak", "Josh Susskind", "Pascal Mettes", "Paul Groth", "Hanlin Goh"], "title": "How PARTs assemble into wholes: Learning the relative composition of images", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "The composition of objects and their parts, along with object-object\npositional relationships, provides a rich source of information for\nrepresentation learning. Hence, spatial-aware pretext tasks have been actively\nexplored in self-supervised learning. Existing works commonly start from a grid\nstructure, where the goal of the pretext task involves predicting the absolute\nposition index of patches within a fixed grid. However, grid-based approaches\nfall short of capturing the fluid and continuous nature of real-world object\ncompositions. We introduce PART, a self-supervised learning approach that\nleverages continuous relative transformations between off-grid patches to\novercome these limitations. By modeling how parts relate to each other in a\ncontinuous space, PART learns the relative composition of images-an off-grid\nstructural relative positioning process that generalizes beyond occlusions and\ndeformations. In tasks requiring precise spatial understanding such as object\ndetection and time series prediction, PART outperforms strong grid-based\nmethods like MAE and DropPos, while also maintaining competitive performance on\nglobal classification tasks with minimal hyperparameter tuning. By breaking\nfree from grid constraints, PART opens up an exciting new trajectory for\nuniversal self-supervised pretraining across diverse datatypes-from natural\nimages to EEG signals-with promising potential in video, medical imaging, and\naudio."}
{"id": "2506.03683", "pdf": "https://arxiv.org/pdf/2506.03683", "abs": "https://arxiv.org/abs/2506.03683", "authors": ["Qiang Fu", "Zonglei Jing", "Zonghao Ying", "Xiaoqian Li"], "title": "PRJ: Perception-Retrieval-Judgement for Generated Images", "categories": ["cs.CV"], "comment": null, "summary": "The rapid progress of generative AI has enabled remarkable creative\ncapabilities, yet it also raises urgent concerns regarding the safety of\nAI-generated visual content in real-world applications such as content\nmoderation, platform governance, and digital media regulation. This includes\nunsafe material such as sexually explicit images, violent scenes, hate symbols,\npropaganda, and unauthorized imitations of copyrighted artworks. Existing image\nsafety systems often rely on rigid category filters and produce binary outputs,\nlacking the capacity to interpret context or reason about nuanced,\nadversarially induced forms of harm. In addition, standard evaluation metrics\n(e.g., attack success rate) fail to capture the semantic severity and dynamic\nprogression of toxicity. To address these limitations, we propose\nPerception-Retrieval-Judgement (PRJ), a cognitively inspired framework that\nmodels toxicity detection as a structured reasoning process. PRJ follows a\nthree-stage design: it first transforms an image into descriptive language\n(perception), then retrieves external knowledge related to harm categories and\ntraits (retrieval), and finally evaluates toxicity based on legal or normative\nrules (judgement). This language-centric structure enables the system to detect\nboth explicit and implicit harms with improved interpretability and categorical\ngranularity. In addition, we introduce a dynamic scoring mechanism based on a\ncontextual toxicity risk matrix to quantify harmfulness across different\nsemantic dimensions. Experiments show that PRJ surpasses existing safety\ncheckers in detection accuracy and robustness while uniquely supporting\nstructured category-level toxicity interpretation."}
{"id": "2506.03684", "pdf": "https://arxiv.org/pdf/2506.03684", "abs": "https://arxiv.org/abs/2506.03684", "authors": ["Zunhui Xia", "Hongxing Li", "Libin Lan"], "title": "DSSAU-Net:U-Shaped Hybrid Network for Pubic Symphysis and Fetal Head Segmentation", "categories": ["cs.CV"], "comment": "14 pages, 3 figures, 5 tables.Accepted by MICCAI Workshop on IUGC\n  2024", "summary": "In the childbirth process, traditional methods involve invasive vaginal\nexaminations, but research has shown that these methods are both subjective and\ninaccurate. Ultrasound-assisted diagnosis offers an objective yet effective way\nto assess fetal head position via two key parameters: Angle of Progression\n(AoP) and Head-Symphysis Distance (HSD), calculated by segmenting the fetal\nhead (FH) and pubic symphysis (PS), which aids clinicians in ensuring a smooth\ndelivery process. Therefore, accurate segmentation of FH and PS is crucial. In\nthis work, we propose a sparse self-attention network architecture with good\nperformance and high computational efficiency, named DSSAU-Net, for the\nsegmentation of FH and PS. Specifically, we stack varying numbers of Dual\nSparse Selection Attention (DSSA) blocks at each stage to form a symmetric\nU-shaped encoder-decoder network architecture. For a given query, DSSA is\ndesigned to explicitly perform one sparse token selection at both the region\nand pixel levels, respectively, which is beneficial for further reducing\ncomputational complexity while extracting the most relevant features. To\ncompensate for the information loss during the upsampling process, skip\nconnections with convolutions are designed. Additionally, multiscale feature\nfusion is employed to enrich the model's global and local information. The\nperformance of DSSAU-Net has been validated using the Intrapartum Ultrasound\nGrand Challenge (IUGC) 2024 \\textit{test set} provided by the organizer in the\nMICCAI IUGC 2024\ncompetition\\footnote{\\href{https://codalab.lisn.upsaclay.fr/competitions/18413\\#learn\\_the\\_details}{https://codalab.lisn.upsaclay.fr/competitions/18413\\#learn\\_the\\_details}},\nwhere we win the fourth place on the tasks of classification and segmentation,\ndemonstrating its effectiveness. The codes will be available at\nhttps://github.com/XiaZunhui/DSSAU-Net."}
{"id": "2506.03698", "pdf": "https://arxiv.org/pdf/2506.03698", "abs": "https://arxiv.org/abs/2506.03698", "authors": ["Yuanlin Mo", "Haishan Huang", "Bocheng Liang", "Weibo Ma"], "title": "Advancements in Artificial Intelligence Applications for Cardiovascular Disease Research", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in artificial intelligence (AI) have revolutionized\ncardiovascular medicine, particularly through integration with computed\ntomography (CT), magnetic resonance imaging (MRI), electrocardiography (ECG)\nand ultrasound (US). Deep learning architectures, including convolutional\nneural networks and generative adversarial networks, enable automated analysis\nof medical imaging and physiological signals, surpassing human capabilities in\ndiagnostic accuracy and workflow efficiency. However, critical challenges\npersist, including the inability to validate input data accuracy, which may\npropagate diagnostic errors. This review highlights AI's transformative\npotential in precision diagnostics while underscoring the need for robust\nvalidation protocols to ensure clinical reliability. Future directions\nemphasize hybrid models integrating multimodal data and adaptive algorithms to\nrefine personalized cardiovascular care."}
{"id": "2506.03706", "pdf": "https://arxiv.org/pdf/2506.03706", "abs": "https://arxiv.org/abs/2506.03706", "authors": ["Aditya Gandhamal", "Aniruddh Sikdar", "Suresh Sundaram"], "title": "OV-COAST: Cost Aggregation with Optimal Transport for Open-Vocabulary Semantic Segmentation", "categories": ["cs.CV"], "comment": "Accepted at CVPR 2025 Workshop on Transformers for Vision\n  (Non-archival track)", "summary": "Open-vocabulary semantic segmentation (OVSS) entails assigning semantic\nlabels to each pixel in an image using textual descriptions, typically\nleveraging world models such as CLIP. To enhance out-of-domain generalization,\nwe propose Cost Aggregation with Optimal Transport (OV-COAST) for\nopen-vocabulary semantic segmentation. To align visual-language features within\nthe framework of optimal transport theory, we employ cost volume to construct a\ncost matrix, which quantifies the distance between two distributions. Our\napproach adopts a two-stage optimization strategy: in the first stage, the\noptimal transport problem is solved using cost volume via Sinkhorn distance to\nobtain an alignment solution; in the second stage, this solution is used to\nguide the training of the CAT-Seg model. We evaluate state-of-the-art OVSS\nmodels on the MESS benchmark, where our approach notably improves the\nperformance of the cost-aggregation model CAT-Seg with ViT-B backbone,\nachieving superior results, surpassing CAT-Seg by 1.72 % and SAN-B by 4.9 %\nmIoU. The code is available at\nhttps://github.com/adityagandhamal/OV-COAST/}{https://github.com/adityagandhamal/OV-COAST/ ."}
{"id": "2506.03709", "pdf": "https://arxiv.org/pdf/2506.03709", "abs": "https://arxiv.org/abs/2506.03709", "authors": ["Aniruddh Sikdar", "Aditya Gandhamal", "Suresh Sundaram"], "title": "AetherVision-Bench: An Open-Vocabulary RGB-Infrared Benchmark for Multi-Angle Segmentation across Aerial and Ground Perspectives", "categories": ["cs.CV"], "comment": "Accepted at Workshop on Foundation Models Meet Embodied Agents at\n  CVPR 2025 (Non-archival Track)", "summary": "Open-vocabulary semantic segmentation (OVSS) involves assigning labels to\neach pixel in an image based on textual descriptions, leveraging world models\nlike CLIP. However, they encounter significant challenges in cross-domain\ngeneralization, hindering their practical efficacy in real-world applications.\nEmbodied AI systems are transforming autonomous navigation for ground vehicles\nand drones by enhancing their perception abilities, and in this study, we\npresent AetherVision-Bench, a benchmark for multi-angle segmentation across\naerial, and ground perspectives, which facilitates an extensive evaluation of\nperformance across different viewing angles and sensor modalities. We assess\nstate-of-the-art OVSS models on the proposed benchmark and investigate the key\nfactors that impact the performance of zero-shot transfer models. Our work\npioneers the creation of a robustness benchmark, offering valuable insights and\nestablishing a foundation for future research."}
{"id": "2506.03710", "pdf": "https://arxiv.org/pdf/2506.03710", "abs": "https://arxiv.org/abs/2506.03710", "authors": ["Yisen Feng", "Haoyu Zhang", "Qiaohui Chu", "Meng Liu", "Weili Guan", "Yaowei Wang", "Liqiang Nie"], "title": "OSGNet @ Ego4D Episodic Memory Challenge 2025", "categories": ["cs.CV", "cs.AI"], "comment": "The champion solutions for the three egocentric video localization\n  tracks(Natural Language Queries, Goal Step, and Moment Queries tracks) of the\n  Ego4D Episodic Memory Challenge at CVPR EgoVis Workshop 2025", "summary": "In this report, we present our champion solutions for the three egocentric\nvideo localization tracks of the Ego4D Episodic Memory Challenge at CVPR 2025.\nAll tracks require precise localization of the interval within an untrimmed\negocentric video. Previous unified video localization approaches often rely on\nlate fusion strategies, which tend to yield suboptimal results. To address\nthis, we adopt an early fusion-based video localization model to tackle all\nthree tasks, aiming to enhance localization accuracy. Ultimately, our method\nachieved first place in the Natural Language Queries, Goal Step, and Moment\nQueries tracks, demonstrating its effectiveness. Our code can be found at\nhttps://github.com/Yisen-Feng/OSGNet."}
{"id": "2506.03713", "pdf": "https://arxiv.org/pdf/2506.03713", "abs": "https://arxiv.org/abs/2506.03713", "authors": ["Sam Bahrami", "Dylan Campbell"], "title": "PlckeRF: A Line-based 3D Representation for Few-view Reconstruction", "categories": ["cs.CV"], "comment": null, "summary": "Feed-forward 3D reconstruction methods aim to predict the 3D structure of a\nscene directly from input images, providing a faster alternative to per-scene\noptimization approaches. Significant progress has been made in single-view and\nfew-view reconstruction using learned priors that infer object shape and\nappearance, even for unobserved regions. However, there is substantial\npotential to enhance these methods by better leveraging information from\nmultiple views when available. To address this, we propose a few-view\nreconstruction model that more effectively harnesses multi-view information.\nOur approach introduces a simple mechanism that connects the 3D representation\nwith pixel rays from the input views, allowing for preferential sharing of\ninformation between nearby 3D locations and between 3D locations and nearby\npixel rays. We achieve this by defining the 3D representation as a set of\nstructured, feature-augmented lines; the Pl\\\"uckeRF representation. Using this\nrepresentation, we demonstrate improvements in reconstruction quality over the\nequivalent triplane representation and state-of-the-art feedforward\nreconstruction methods."}
{"id": "2506.03714", "pdf": "https://arxiv.org/pdf/2506.03714", "abs": "https://arxiv.org/abs/2506.03714", "authors": ["Shuai Liu", "Mingyue Cui", "Boyang Li", "Quanmin Liang", "Tinghe Hong", "Kai Huang", "Yunxiao Shan", "Kai Huang"], "title": "FSHNet: Fully Sparse Hybrid Network for 3D Object Detection", "categories": ["cs.CV"], "comment": "Accepted by CVPR2025", "summary": "Fully sparse 3D detectors have recently gained significant attention due to\ntheir efficiency in long-range detection. However, sparse 3D detectors extract\nfeatures only from non-empty voxels, which impairs long-range interactions and\ncauses the center feature missing. The former weakens the feature extraction\ncapability, while the latter hinders network optimization. To address these\nchallenges, we introduce the Fully Sparse Hybrid Network (FSHNet). FSHNet\nincorporates a proposed SlotFormer block to enhance the long-range feature\nextraction capability of existing sparse encoders. The SlotFormer divides\nsparse voxels using a slot partition approach, which, compared to traditional\nwindow partition, provides a larger receptive field. Additionally, we propose a\ndynamic sparse label assignment strategy to deeply optimize the network by\nproviding more high-quality positive samples. To further enhance performance,\nwe introduce a sparse upsampling module to refine downsampled voxels,\npreserving fine-grained details crucial for detecting small objects. Extensive\nexperiments on the Waymo, nuScenes, and Argoverse2 benchmarks demonstrate the\neffectiveness of FSHNet. The code is available at\nhttps://github.com/Say2L/FSHNet."}
{"id": "2506.03737", "pdf": "https://arxiv.org/pdf/2506.03737", "abs": "https://arxiv.org/abs/2506.03737", "authors": ["Hao Yu", "Tangyu Jiang", "Shuning Jia", "Shannan Yan", "Shunning Liu", "Haolong Qian", "Guanghao Li", "Shuting Dong", "Huaisong Zhang", "Chun Yuan"], "title": "ComRoPE: Scalable and Robust Rotary Position Embedding Parameterized by Trainable Commuting Angle Matrices", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The Transformer architecture has revolutionized various regions since it was\nproposed, and its effectiveness largely depends on the ability to encode\npositional information. Traditional position encoding methods exhibit\nsignificant limitations due to lack of robustness and flexibility of position.\nTherefore, Rotary Positional Encoding (RoPE) was proposed to alleviate these\nissues, which integrates positional information by rotating the embeddings in\nthe attention mechanism. However, RoPE requires manually defined rotation\nmatrices with limited transformation space, constraining the model's capacity.\nIn this work, we propose ComRoPE, which generalizes RoPE by defining it in\nterms of trainable commuting angle matrices. Specifically, we demonstrate that\npairwise commutativity of these matrices is essential for RoPE to achieve\nscalability and positional robustness. We formally define the RoPE Equation,\nwhich is an essential condition that ensures consistent performance with\nposition offsets. Based on the theoretical analysis, we present two types of\ntrainable commuting angle matrices as sufficient solutions to the RoPE\nequation, which significantly improve performance, surpassing the current\nstate-of-the-art method by 1.6% at training resolution and 2.9% at higher\nresolution on the ImageNet-1K dataset. Furthermore, our framework shows\nversatility in generalizing to existing RoPE formulations and offering new\ninsights for future positional encoding research. To ensure reproducibility,\nthe source code and instructions are available at\nhttps://github.com/Longin-Yu/ComRoPE"}
{"id": "2506.03740", "pdf": "https://arxiv.org/pdf/2506.03740", "abs": "https://arxiv.org/abs/2506.03740", "authors": ["Jianfeng Wu", "Nannan Xu"], "title": "SAAT: Synergistic Alternating Aggregation Transformer for Image Super-Resolution", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Single image super-resolution is a well-known downstream task which aims to\nrestore low-resolution images into high-resolution images. At present, models\nbased on Transformers have shone brightly in the field of super-resolution due\nto their ability to capture long-term dependencies in information. However,\ncurrent methods typically compute self-attention in nonoverlapping windows to\nsave computational costs, and the standard self-attention computation only\nfocuses on its results, thereby neglecting the useful information across\nchannels and the rich spatial structural information generated in the\nintermediate process. Channel attention and spatial attention have,\nrespectively, brought significant improvements to various downstream visual\ntasks in terms of extracting feature dependency and spatial structure\nrelationships, but the synergistic relationship between channel and spatial\nattention has not been fully explored yet.To address these issues, we propose a\nnovel model. Synergistic Alternating Aggregation Transformer (SAAT), which can\nbetter utilize the potential information of features. In SAAT, we introduce the\nEfficient Channel & Window Synergistic Attention Group (CWSAG) and the Spatial\n& Window Synergistic Attention Group (SWSAG). On the one hand, CWSAG combines\nefficient channel attention with shifted window attention, enhancing non-local\nfeature fusion, and producing more visually appealing results. On the other\nhand, SWSAG leverages spatial attention to capture rich structured feature\ninformation, thereby enabling SAAT to more effectively extract structural\nfeatures.Extensive experimental results and ablation studies demonstrate the\neffectiveness of SAAT in the field of super-resolution. SAAT achieves\nperformance comparable to that of the state-of-the-art (SOTA) under the same\nquantity of parameters."}
{"id": "2506.03753", "pdf": "https://arxiv.org/pdf/2506.03753", "abs": "https://arxiv.org/abs/2506.03753", "authors": ["Caiyi Sun", "Yujing Sun", "Xiao Han", "Zemin Yang", "Jiawei Liu", "Xinge Zhu", "Siu Ming Yiu", "Yuexin Ma"], "title": "HUMOF: Human Motion Forecasting in Interactive Social Scenes", "categories": ["cs.CV"], "comment": null, "summary": "Complex scenes present significant challenges for predicting human behaviour\ndue to the abundance of interaction information, such as human-human and\nhumanenvironment interactions. These factors complicate the analysis and\nunderstanding of human behaviour, thereby increasing the uncertainty in\nforecasting human motions. Existing motion prediction methods thus struggle in\nthese complex scenarios. In this paper, we propose an effective method for\nhuman motion forecasting in interactive scenes. To achieve a comprehensive\nrepresentation of interactions, we design a hierarchical interaction feature\nrepresentation so that high-level features capture the overall context of the\ninteractions, while low-level features focus on fine-grained details. Besides,\nwe propose a coarse-to-fine interaction reasoning module that leverages both\nspatial and frequency perspectives to efficiently utilize hierarchical\nfeatures, thereby enhancing the accuracy of motion predictions. Our method\nachieves state-of-the-art performance across four public datasets. Code will be\nreleased when this paper is published."}
{"id": "2506.03798", "pdf": "https://arxiv.org/pdf/2506.03798", "abs": "https://arxiv.org/abs/2506.03798", "authors": ["Fan Shi", "Haiyang Yu", "Bin Li", "Xiangyang Xue"], "title": "CoLa: Chinese Character Decomposition with Compositional Latent Components", "categories": ["cs.CV"], "comment": null, "summary": "Humans can decompose Chinese characters into compositional components and\nrecombine them to recognize unseen characters. This reflects two cognitive\nprinciples: Compositionality, the idea that complex concepts are built on\nsimpler parts; and Learning-to-learn, the ability to learn strategies for\ndecomposing and recombining components to form new concepts. These principles\nprovide inductive biases that support efficient generalization. They are\ncritical to Chinese character recognition (CCR) in solving the zero-shot\nproblem, which results from the common long-tail distribution of Chinese\ncharacter datasets. Existing methods have made substantial progress in modeling\ncompositionality via predefined radical or stroke decomposition. However, they\noften ignore the learning-to-learn capability, limiting their ability to\ngeneralize beyond human-defined schemes. Inspired by these principles, we\npropose a deep latent variable model that learns Compositional Latent\ncomponents of Chinese characters (CoLa) without relying on human-defined\ndecomposition schemes. Recognition and matching can be performed by comparing\ncompositional latent components in the latent space, enabling zero-shot\ncharacter recognition. The experiments illustrate that CoLa outperforms\nprevious methods in both character the radical zero-shot CCR. Visualization\nindicates that the learned components can reflect the structure of characters\nin an interpretable way. Moreover, despite being trained on historical\ndocuments, CoLa can analyze components of oracle bone characters, highlighting\nits cross-dataset generalization ability."}
{"id": "2506.03799", "pdf": "https://arxiv.org/pdf/2506.03799", "abs": "https://arxiv.org/abs/2506.03799", "authors": ["Fei Zhang", "Pei Zhang", "Baosong Yang", "Fei Huang", "Yanfeng Wang", "Ya Zhang"], "title": "ConText: Driving In-context Learning for Text Removal and Segmentation", "categories": ["cs.CV"], "comment": "19 pages, 9 figures, Accepted at ICML 2025", "summary": "This paper presents the first study on adapting the visual in-context\nlearning (V-ICL) paradigm to optical character recognition tasks, specifically\nfocusing on text removal and segmentation. Most existing V-ICL generalists\nemploy a reasoning-as-reconstruction approach: they turn to using a\nstraightforward image-label compositor as the prompt and query input, and then\nmasking the query label to generate the desired output. This direct prompt\nconfines the model to a challenging single-step reasoning process. To address\nthis, we propose a task-chaining compositor in the form of\nimage-removal-segmentation, providing an enhanced prompt that elicits reasoning\nwith enriched intermediates. Additionally, we introduce context-aware\naggregation, integrating the chained prompt pattern into the latent query\nrepresentation, thereby strengthening the model's in-context reasoning. We also\nconsider the issue of visual heterogeneity, which complicates the selection of\nhomogeneous demonstrations in text recognition. Accordingly, this is\neffectively addressed through a simple self-prompting strategy, preventing the\nmodel's in-context learnability from devolving into specialist-like,\ncontext-free inference. Collectively, these insights culminate in our ConText\nmodel, which achieves new state-of-the-art across both in- and out-of-domain\nbenchmarks. The code is available at https://github.com/Ferenas/ConText."}
{"id": "2506.03868", "pdf": "https://arxiv.org/pdf/2506.03868", "abs": "https://arxiv.org/abs/2506.03868", "authors": ["Zhuoyang Pan", "Boxiao Pan", "Guandao Yang", "Adam W. Harley", "Leonidas Guibas"], "title": "Animal Pose Labeling Using General-Purpose Point Trackers", "categories": ["cs.CV"], "comment": null, "summary": "Automatically estimating animal poses from videos is important for studying\nanimal behaviors. Existing methods do not perform reliably since they are\ntrained on datasets that are not comprehensive enough to capture all necessary\nanimal behaviors. However, it is very challenging to collect such datasets due\nto the large variations in animal morphology. In this paper, we propose an\nanimal pose labeling pipeline that follows a different strategy, i.e. test time\noptimization. Given a video, we fine-tune a lightweight appearance embedding\ninside a pre-trained general-purpose point tracker on a sparse set of annotated\nframes. These annotations can be obtained from human labelers or off-the-shelf\npose detectors. The fine-tuned model is then applied to the rest of the frames\nfor automatic labeling. Our method achieves state-of-the-art performance at a\nreasonable annotation cost. We believe our pipeline offers a valuable tool for\nthe automatic quantification of animal behavior. Visit our project webpage at\nhttps://zhuoyang-pan.github.io/animal-labeling."}
{"id": "2506.03872", "pdf": "https://arxiv.org/pdf/2506.03872", "abs": "https://arxiv.org/abs/2506.03872", "authors": ["Yang Xiao", "Guoan Xu", "Qiang Wu", "Wenjing Jia"], "title": "JointSplat: Probabilistic Joint Flow-Depth Optimization for Sparse-View Gaussian Splatting", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Reconstructing 3D scenes from sparse viewpoints is a long-standing challenge\nwith wide applications. Recent advances in feed-forward 3D Gaussian sparse-view\nreconstruction methods provide an efficient solution for real-time novel view\nsynthesis by leveraging geometric priors learned from large-scale multi-view\ndatasets and computing 3D Gaussian centers via back-projection. Despite\noffering strong geometric cues, both feed-forward multi-view depth estimation\nand flow-depth joint estimation face key limitations: the former suffers from\nmislocation and artifact issues in low-texture or repetitive regions, while the\nlatter is prone to local noise and global inconsistency due to unreliable\nmatches when ground-truth flow supervision is unavailable. To overcome this, we\npropose JointSplat, a unified framework that leverages the complementarity\nbetween optical flow and depth via a novel probabilistic optimization\nmechanism. Specifically, this pixel-level mechanism scales the information\nfusion between depth and flow based on the matching probability of optical flow\nduring training. Building upon the above mechanism, we further propose a novel\nmulti-view depth-consistency loss to leverage the reliability of supervision\nwhile suppressing misleading gradients in uncertain areas. Evaluated on\nRealEstate10K and ACID, JointSplat consistently outperforms state-of-the-art\n(SOTA) methods, demonstrating the effectiveness and robustness of our proposed\nprobabilistic joint flow-depth optimization approach for high-fidelity\nsparse-view 3D reconstruction."}
{"id": "2506.03885", "pdf": "https://arxiv.org/pdf/2506.03885", "abs": "https://arxiv.org/abs/2506.03885", "authors": ["Sam Pollard", "Michael Wray"], "title": "Video, How Do Your Tokens Merge?", "categories": ["cs.CV"], "comment": "Accepted at eLVM workshop at CVPR 2025", "summary": "Video transformer models require huge amounts of compute resources due to the\nspatio-temporal scaling of the input. Tackling this, recent methods have\nproposed to drop or merge tokens for image models, whether randomly or via\nlearned methods. Merging tokens has many benefits: it can be plugged into any\nvision transformer, does not require model re-training, and it propagates\ninformation that would otherwise be dropped through the model. Before now,\nvideo token merging has not been evaluated on temporally complex datasets for\nvideo understanding. In this work, we explore training-free token merging for\nvideo to provide comprehensive experiments and find best practices across four\nvideo transformers on three datasets that exhibit coarse and fine-grained\naction recognition. Our results showcase the benefits of video token merging\nwith a speedup of around $2.5$X while maintaining accuracy (avg. $-0.55\\%$ for\nViViT). Code available at\nhttps://github.com/sjpollard/video-how-do-your-tokens-merge."}
{"id": "2506.03892", "pdf": "https://arxiv.org/pdf/2506.03892", "abs": "https://arxiv.org/abs/2506.03892", "authors": ["Giyong Choi", "HyunWook Park"], "title": "Joint Video Enhancement with Deblurring, Super-Resolution, and Frame Interpolation Network", "categories": ["cs.CV"], "comment": null, "summary": "Video quality is often severely degraded by multiple factors rather than a\nsingle factor. These low-quality videos can be restored to high-quality videos\nby sequentially performing appropriate video enhancement techniques. However,\nthe sequential approach was inefficient and sub-optimal because most video\nenhancement approaches were designed without taking into account that multiple\nfactors together degrade video quality. In this paper, we propose a new joint\nvideo enhancement method that mitigates multiple degradation factors\nsimultaneously by resolving an integrated enhancement problem. Our proposed\nnetwork, named DSFN, directly produces a high-resolution, high-frame-rate, and\nclear video from a low-resolution, low-frame-rate, and blurry video. In the\nDSFN, low-resolution and blurry input frames are enhanced by a joint deblurring\nand super-resolution (JDSR) module. Meanwhile, intermediate frames between\ninput adjacent frames are interpolated by a triple-frame-based frame\ninterpolation (TFBFI) module. The proper combination of the proposed modules of\nDSFN can achieve superior performance on the joint video enhancement task.\nExperimental results show that the proposed method outperforms other sequential\nstate-of-the-art techniques on public datasets with a smaller network size and\nfaster processing time."}
{"id": "2506.03918", "pdf": "https://arxiv.org/pdf/2506.03918", "abs": "https://arxiv.org/abs/2506.03918", "authors": ["Marcin Kowalczyk", "Kamil Jeziorek", "Tomasz Kryjak"], "title": "Learning from Noise: Enhancing DNNs for Event-Based Vision through Controlled Noise Injection", "categories": ["cs.CV"], "comment": null, "summary": "Event-based sensors offer significant advantages over traditional frame-based\ncameras, especially in scenarios involving rapid motion or challenging lighting\nconditions. However, event data frequently suffers from considerable noise,\nnegatively impacting the performance and robustness of deep learning models.\nTraditionally, this problem has been addressed by applying filtering algorithms\nto the event stream, but this may also remove some of relevant data. In this\npaper, we propose a novel noise-injection training methodology designed to\nenhance the neural networks robustness against varying levels of event noise.\nOur approach introduces controlled noise directly into the training data,\nenabling models to learn noise-resilient representations. We have conducted\nextensive evaluations of the proposed method using multiple benchmark datasets\n(N-Caltech101, N-Cars, and Mini N-ImageNet) and various network architectures,\nincluding Convolutional Neural Networks, Vision Transformers, Spiking Neural\nNetworks, and Graph Convolutional Networks. Experimental results show that our\nnoise-injection training strategy achieves stable performance over a range of\nnoise intensities, consistently outperforms event-filtering techniques, and\nachieves the highest average classification accuracy, making it a viable\nalternative to traditional event-data filtering methods in an object\nclassification system. Code: https://github.com/vision-agh/DVS_Filtering"}
{"id": "2506.03926", "pdf": "https://arxiv.org/pdf/2506.03926", "abs": "https://arxiv.org/abs/2506.03926", "authors": ["Debarshi Brahma", "Soma Biswas"], "title": "Multiple Stochastic Prompt Tuning for Practical Cross-Domain Few Shot Learning", "categories": ["cs.CV"], "comment": null, "summary": "In this work, we propose a practical cross-domain few-shot learning (pCDFSL)\ntask, where a large-scale pre-trained model like CLIP can be easily deployed on\na target dataset. The goal is to simultaneously classify all unseen classes\nunder extreme domain shifts, by utilizing only a few labeled samples per class.\nThe pCDFSL paradigm is source-free and moves beyond artificially created\nepisodic training and testing regimes followed by existing CDFSL frameworks,\nmaking it more challenging and relevant to real-world applications. Towards\nthat goal, we propose a novel framework, termed MIST (MultIple STochastic\nPrompt tuning), where multiple stochastic prompts are utilized to handle\nsignificant domain and semantic shifts. Specifically, multiple prompts are\nlearnt for each class, effectively capturing multiple peaks in the input data.\nFurthermore, instead of representing the weights of the multiple prompts as\npoint-estimates, we model them as learnable Gaussian distributions with two\ndifferent strategies, encouraging an efficient exploration of the prompt\nparameter space, which mitigate overfitting due to the few labeled training\nsamples. Extensive experiments and comparison with the state-of-the-art methods\non four CDFSL benchmarks adapted to this setting, show the effectiveness of the\nproposed framework."}
{"id": "2506.03928", "pdf": "https://arxiv.org/pdf/2506.03928", "abs": "https://arxiv.org/abs/2506.03928", "authors": ["Ze Feng", "Jiang-Jiang Liu", "Sen Yang", "Lingyu Xiao", "Xiaofan Li", "Wankou Yang", "Jingdong Wang"], "title": "Vision Remember: Alleviating Visual Forgetting in Efficient MLLM with Vision Feature Resample", "categories": ["cs.CV"], "comment": null, "summary": "In this work, we study the Efficient Multimodal Large Language Model.\nRedundant vision tokens consume a significant amount of computational memory\nand resources. Therefore, many previous works compress them in the Vision\nProjector to reduce the number of vision tokens. However, simply compressing in\nthe Vision Projector can lead to the loss of visual information, especially for\ntasks that rely on fine-grained spatial relationships, such as OCR and Chart \\&\nTable Understanding. To address this problem, we propose Vision Remember, which\nis inserted between the LLM decoder layers to allow vision tokens to\nre-memorize vision features. Specifically, we retain multi-level vision\nfeatures and resample them with the vision tokens that have interacted with the\ntext token. During the resampling process, each vision token only attends to a\nlocal region in vision features, which is referred to as saliency-enhancing\nlocal attention. Saliency-enhancing local attention not only improves\ncomputational efficiency but also captures more fine-grained contextual\ninformation and spatial relationships within the region. Comprehensive\nexperiments on multiple visual understanding benchmarks validate the\neffectiveness of our method when combined with various Efficient Vision\nProjectors, showing performance gains without sacrificing efficiency. Based on\nVision Remember, LLaVA-VR with only 2B parameters is also superior to previous\nrepresentative MLLMs such as Tokenpacker-HD-7B and DeepSeek-VL-7B."}
{"id": "2506.03933", "pdf": "https://arxiv.org/pdf/2506.03933", "abs": "https://arxiv.org/abs/2506.03933", "authors": ["Jia Fu", "Yongtao Wu", "Yihang Chen", "Kunyu Peng", "Xiao Zhang", "Volkan Cevher", "Sepideh Pashami", "Anders Holst"], "title": "DiffCAP: Diffusion-based Cumulative Adversarial Purification for Vision Language Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Vision Language Models (VLMs) have shown remarkable capabilities in\nmultimodal understanding, yet their susceptibility to perturbations poses a\nsignificant threat to their reliability in real-world applications. Despite\noften being imperceptible to humans, these perturbations can drastically alter\nmodel outputs, leading to erroneous interpretations and decisions. This paper\nintroduces DiffCAP, a novel diffusion-based purification strategy that can\neffectively neutralize adversarial corruptions in VLMs. We observe that adding\nminimal noise to an adversarially corrupted image significantly alters its\nlatent embedding with respect to VLMs. Building on this insight, DiffCAP\ncumulatively injects random Gaussian noise into adversarially perturbed input\ndata. This process continues until the embeddings of two consecutive noisy\nimages reach a predefined similarity threshold, indicating a potential approach\nto neutralize the adversarial effect. Subsequently, a pretrained diffusion\nmodel is employed to denoise the stabilized image, recovering a clean\nrepresentation suitable for the VLMs to produce an output. Through extensive\nexperiments across six datasets with three VLMs under varying attack strengths\nin three task scenarios, we show that DiffCAP consistently outperforms existing\ndefense techniques by a substantial margin. Notably, DiffCAP significantly\nreduces both hyperparameter tuning complexity and the required diffusion time,\nthereby accelerating the denoising process. Equipped with strong theoretical\nand empirical support, DiffCAP provides a robust and practical solution for\nsecurely deploying VLMs in adversarial environments."}
{"id": "2506.03942", "pdf": "https://arxiv.org/pdf/2506.03942", "abs": "https://arxiv.org/abs/2506.03942", "authors": ["Theodore Barfoot", "Luis C. Garcia-Peraza-Herrera", "Samet Akcay", "Ben Glocker", "Tom Vercauteren"], "title": "Average Calibration Losses for Reliable Uncertainty in Medical Image Segmentation", "categories": ["cs.CV"], "comment": "12 pages, 5 figures, IEEE TMI submission", "summary": "Deep neural networks for medical image segmentation are often overconfident,\ncompromising both reliability and clinical utility. In this work, we propose\ndifferentiable formulations of marginal L1 Average Calibration Error (mL1-ACE)\nas an auxiliary loss that can be computed on a per-image basis. We compare both\nhard- and soft-binning approaches to directly improve pixel-wise calibration.\nOur experiments on four datasets (ACDC, AMOS, KiTS, BraTS) demonstrate that\nincorporating mL1-ACE significantly reduces calibration errors, particularly\nAverage Calibration Error (ACE) and Maximum Calibration Error (MCE), while\nlargely maintaining high Dice Similarity Coefficients (DSCs). We find that the\nsoft-binned variant yields the greatest improvements in calibration, over the\nDice plus cross-entropy loss baseline, but often compromises segmentation\nperformance, with hard-binned mL1-ACE maintaining segmentation performance,\nalbeit with weaker calibration improvement. To gain further insight into\ncalibration performance and its variability across an imaging dataset, we\nintroduce dataset reliability histograms, an aggregation of per-image\nreliability diagrams. The resulting analysis highlights improved alignment\nbetween predicted confidences and true accuracies. Overall, our approach not\nonly enhances the trustworthiness of segmentation predictions but also shows\npotential for safer integration of deep learning methods into clinical\nworkflows. We share our code here:\nhttps://github.com/cai4cai/Average-Calibration-Losses"}
{"id": "2506.03972", "pdf": "https://arxiv.org/pdf/2506.03972", "abs": "https://arxiv.org/abs/2506.03972", "authors": ["Guohua Wu", "Shengqi Chen", "Pengchao Deng", "Wenting Yu"], "title": "MS-YOLO: A Multi-Scale Model for Accurate and Efficient Blood Cell Detection", "categories": ["cs.CV"], "comment": null, "summary": "Complete blood cell detection holds significant value in clinical\ndiagnostics. Conventional manual microscopy methods suffer from time\ninefficiency and diagnostic inaccuracies. Existing automated detection\napproaches remain constrained by high deployment costs and suboptimal accuracy.\nWhile deep learning has introduced powerful paradigms to this field, persistent\nchallenges in detecting overlapping cells and multi-scale objects hinder\npractical deployment. This study proposes the multi-scale YOLO (MS-YOLO), a\nblood cell detection model based on the YOLOv11 framework, incorporating three\nkey architectural innovations to enhance detection performance. Specifically,\nthe multi-scale dilated residual module (MS-DRM) replaces the original C3K2\nmodules to improve multi-scale discriminability; the dynamic cross-path feature\nenhancement module (DCFEM) enables the fusion of hierarchical features from the\nbackbone with aggregated features from the neck to enhance feature\nrepresentations; and the light adaptive-weight downsampling module (LADS)\nimproves feature downsampling through adaptive spatial weighting while reducing\ncomputational complexity. Experimental results on the CBC benchmark demonstrate\nthat MS-YOLO achieves precise detection of overlapping cells and multi-scale\nobjects, particularly small targets such as platelets, achieving an mAP@50 of\n97.4% that outperforms existing models. Further validation on the supplementary\nWBCDD dataset confirms its robust generalization capability. Additionally, with\na lightweight architecture and real-time inference efficiency, MS-YOLO meets\nclinical deployment requirements, providing reliable technical support for\nstandardized blood pathology assessment."}
{"id": "2506.03988", "pdf": "https://arxiv.org/pdf/2506.03988", "abs": "https://arxiv.org/abs/2506.03988", "authors": ["Hicham Eddoubi", "Jonas Ricker", "Federico Cocchi", "Lorenzo Baraldi", "Angelo Sotgiu", "Maura Pintor", "Marcella Cornia", "Lorenzo Baraldi", "Asja Fischer", "Rita Cucchiara", "Battista Biggio"], "title": "RAID: A Dataset for Testing the Adversarial Robustness of AI-Generated Image Detectors", "categories": ["cs.CV", "cs.LG"], "comment": "Under review for NeurIPS 2025 Datasets and Benchmarks Track", "summary": "AI-generated images have reached a quality level at which humans are\nincapable of reliably distinguishing them from real images. To counteract the\ninherent risk of fraud and disinformation, the detection of AI-generated images\nis a pressing challenge and an active research topic. While many of the\npresented methods claim to achieve high detection accuracy, they are usually\nevaluated under idealized conditions. In particular, the adversarial robustness\nis often neglected, potentially due to a lack of awareness or the substantial\neffort required to conduct a comprehensive robustness analysis. In this work,\nwe tackle this problem by providing a simpler means to assess the robustness of\nAI-generated image detectors. We present RAID (Robust evaluation of\nAI-generated image Detectors), a dataset of 72k diverse and highly transferable\nadversarial examples. The dataset is created by running attacks against an\nensemble of seven state-of-the-art detectors and images generated by four\ndifferent text-to-image models. Extensive experiments show that our methodology\ngenerates adversarial images that transfer with a high success rate to unseen\ndetectors, which can be used to quickly provide an approximate yet still\nreliable estimate of a detector's adversarial robustnessOur findings indicate\nthat current state-of-the-art AI-generated image detectors can be easily\ndeceived by adversarial examples, highlighting the critical need for the\ndevelopment of more robust methods. We release our dataset at\nhttps://huggingface.co/datasets/aimagelab/RAID and evaluation code at\nhttps://github.com/pralab/RAID."}
{"id": "2506.04005", "pdf": "https://arxiv.org/pdf/2506.04005", "abs": "https://arxiv.org/abs/2506.04005", "authors": ["Maxime Zanella", "Clment Fuchs", "Ismail Ben Ayed", "Christophe De Vleeschouwer"], "title": "Vocabulary-free few-shot learning for Vision-Language Models", "categories": ["cs.CV"], "comment": "Accepted at CVPR Workshops 2025", "summary": "Recent advances in few-shot adaptation for Vision-Language Models (VLMs) have\ngreatly expanded their ability to generalize across tasks using only a few\nlabeled examples. However, existing approaches primarily build upon the strong\nzero-shot priors of these models by leveraging carefully designed,\ntask-specific prompts. This dependence on predefined class names can restrict\ntheir applicability, especially in scenarios where exact class names are\nunavailable or difficult to specify. To address this limitation, we introduce\nvocabulary-free few-shot learning for VLMs, a setting where target class\ninstances - that is, images - are available but their corresponding names are\nnot. We propose Similarity Mapping (SiM), a simple yet effective baseline that\nclassifies target instances solely based on similarity scores with a set of\ngeneric prompts (textual or visual), eliminating the need for carefully\nhandcrafted prompts. Although conceptually straightforward, SiM demonstrates\nstrong performance, operates with high computational efficiency (learning the\nmapping typically takes less than one second), and provides interpretability by\nlinking target classes to generic prompts. We believe that our approach could\nserve as an important baseline for future research in vocabulary-free few-shot\nlearning. Code is available at\nhttps://github.com/MaxZanella/vocabulary-free-FSL."}
{"id": "2506.04034", "pdf": "https://arxiv.org/pdf/2506.04034", "abs": "https://arxiv.org/abs/2506.04034", "authors": ["Qing Jiang", "Xingyu Chen", "Zhaoyang Zeng", "Junzhi Yu", "Lei Zhang"], "title": "Rex-Thinker: Grounded Object Referring via Chain-of-Thought Reasoning", "categories": ["cs.CV"], "comment": "homepage: https://rexthinker.github.io/", "summary": "Object referring aims to detect all objects in an image that match a given\nnatural language description. We argue that a robust object referring model\nshould be grounded, meaning its predictions should be both explainable and\nfaithful to the visual content. Specifically, it should satisfy two key\nproperties: 1) Verifiable, by producing interpretable reasoning that justifies\nits predictions and clearly links them to visual evidence; and 2) Trustworthy,\nby learning to abstain when no object in the image satisfies the given\nexpression. However, most methods treat referring as a direct bounding box\nprediction task, offering limited interpretability and struggling to reject\nexpressions with no matching object. In this work, we propose Rex-Thinker, a\nmodel that formulates object referring as an explicit CoT reasoning task. Given\na referring expression, we first identify all candidate object instances\ncorresponding to the referred object category. Rex-Thinker then performs\nstep-by-step reasoning over each candidate to assess whether it matches the\ngiven expression, before making a final prediction. To support this paradigm,\nwe construct a large-scale CoT-style referring dataset named HumanRef-CoT by\nprompting GPT-4o on the HumanRef dataset. Each reasoning trace follows a\nstructured planning, action, and summarization format, enabling the model to\nlearn decomposed, interpretable reasoning over object candidates. We then train\nRex-Thinker in two stages: a cold-start supervised fine-tuning phase to teach\nthe model how to perform structured reasoning, followed by GRPO-based RL\nlearning to improve accuracy and generalization. Experiments show that our\napproach outperforms standard baselines in both precision and interpretability\non in-domain evaluation, while also demonstrating improved ability to reject\nhallucinated outputs and strong generalization in out-of-domain settings."}
{"id": "2506.04039", "pdf": "https://arxiv.org/pdf/2506.04039", "abs": "https://arxiv.org/abs/2506.04039", "authors": ["Jiulong Wu", "Zhengliang Shi", "Shuaiqiang Wang", "Jizhou Huang", "Dawei Yin", "Lingyong Yan", "Min Cao", "Min Zhang"], "title": "Mitigating Hallucinations in Large Vision-Language Models via Entity-Centric Multimodal Preference Optimization", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Large Visual Language Models (LVLMs) have demonstrated impressive\ncapabilities across multiple tasks. However, their trustworthiness is often\nchallenged by hallucinations, which can be attributed to the modality\nmisalignment and the inherent hallucinations of their underlying Large Language\nModels (LLMs) backbone. Existing preference alignment methods focus on aligning\nmodel responses with human preferences while neglecting image-text modality\nalignment, resulting in over-reliance on LLMs and hallucinations. In this\npaper, we propose Entity-centric Multimodal Preference Optimization (EMPO),\nwhich achieves enhanced modality alignment than existing human preference\nalignment methods. Besides, to overcome the scarcity of high-quality multimodal\npreference data, we utilize open-source instruction datasets to automatically\nconstruct high-quality preference data across three aspects: image,\ninstruction, and response. Experiments on two human preference datasets and\nfive multimodal hallucination benchmarks demonstrate the effectiveness of EMPO,\ne.g., reducing hallucination rates by 85.9% on Object-HalBench and 49.8% on\nMM-HalBench."}
{"id": "2506.04048", "pdf": "https://arxiv.org/pdf/2506.04048", "abs": "https://arxiv.org/abs/2506.04048", "authors": ["Gabriele Magrini", "Federico Becattini", "Giovanni Colombo", "Pietro Pala"], "title": "EV-Flying: an Event-based Dataset for In-The-Wild Recognition of Flying Objects", "categories": ["cs.CV"], "comment": null, "summary": "Monitoring aerial objects is crucial for security, wildlife conservation, and\nenvironmental studies. Traditional RGB-based approaches struggle with\nchallenges such as scale variations, motion blur, and high-speed object\nmovements, especially for small flying entities like insects and drones. In\nthis work, we explore the potential of event-based vision for detecting and\nrecognizing flying objects, in particular animals that may not follow short and\nlong-term predictable patters. Event cameras offer high temporal resolution,\nlow latency, and robustness to motion blur, making them well-suited for this\ntask. We introduce EV-Flying, an event-based dataset of flying objects,\ncomprising manually annotated birds, insects and drones with spatio-temporal\nbounding boxes and track identities. To effectively process the asynchronous\nevent streams, we employ a point-based approach leveraging lightweight\narchitectures inspired by PointNet. Our study investigates the classification\nof flying objects using point cloud-based event representations. The proposed\ndataset and methodology pave the way for more efficient and reliable aerial\nobject recognition in real-world scenarios."}
{"id": "2506.04054", "pdf": "https://arxiv.org/pdf/2506.04054", "abs": "https://arxiv.org/abs/2506.04054", "authors": ["Giyong Choi", "HyunWook Park"], "title": "Video Deblurring with Deconvolution and Aggregation Networks", "categories": ["cs.CV"], "comment": null, "summary": "In contrast to single-image deblurring, video deblurring has the advantage\nthat neighbor frames can be utilized to deblur a target frame. However,\nexisting video deblurring algorithms often fail to properly employ the neighbor\nframes, resulting in sub-optimal performance. In this paper, we propose a\ndeconvolution and aggregation network (DAN) for video deblurring that utilizes\nthe information of neighbor frames well. In DAN, both deconvolution and\naggregation strategies are achieved through three sub-networks: the\npreprocessing network (PPN) and the alignment-based deconvolution network\n(ABDN) for the deconvolution scheme; the frame aggregation network (FAN) for\nthe aggregation scheme. In the deconvolution part, blurry inputs are first\npreprocessed by the PPN with non-local operations. Then, the output frames from\nthe PPN are deblurred by the ABDN based on the frame alignment. In the FAN,\nthese deblurred frames from the deconvolution part are combined into a latent\nframe according to reliability maps which infer pixel-wise sharpness. The\nproper combination of three sub-networks can achieve favorable performance on\nvideo deblurring by using the neighbor frames suitably. In experiments, the\nproposed DAN was demonstrated to be superior to existing state-of-the-art\nmethods through both quantitative and qualitative evaluations on the public\ndatasets."}
{"id": "2506.04081", "pdf": "https://arxiv.org/pdf/2506.04081", "abs": "https://arxiv.org/abs/2506.04081", "authors": ["Abdelouahed Laazoufi", "Mohammed El Hassouni", "Hocine Cherifi"], "title": "Point Cloud Quality Assessment Using the Perceptual Clustering Weighted Graph (PCW-Graph) and Attention Fusion Network", "categories": ["cs.CV"], "comment": null, "summary": "No-Reference Point Cloud Quality Assessment (NR-PCQA) is critical for\nevaluating 3D content in real-world applications where reference models are\nunavailable."}
{"id": "2506.04106", "pdf": "https://arxiv.org/pdf/2506.04106", "abs": "https://arxiv.org/abs/2506.04106", "authors": ["Xiao Xiang Zhu", "Sining Chen", "Fahong Zhang", "Yilei Shi", "Yuanyuan Wang"], "title": "GlobalBuildingAtlas: An Open Global and Complete Dataset of Building Polygons, Heights and LoD1 3D Models", "categories": ["cs.CV"], "comment": null, "summary": "We introduce GlobalBuildingAtlas, a publicly available dataset providing\nglobal and complete coverage of building polygons, heights and Level of Detail\n1 (LoD1) 3D building models. This is the first open dataset to offer high\nquality, consistent, and complete building data in 2D and 3D form at the\nindividual building level on a global scale. Towards this dataset, we developed\nmachine learning-based pipelines to derive building polygons and heights\n(called GBA.Height) from global PlanetScope satellite data, respectively. Also\na quality-based fusion strategy was employed to generate higher-quality\npolygons (called GBA.Polygon) based on existing open building polygons,\nincluding our own derived one. With more than 2.75 billion buildings worldwide,\nGBA.Polygon surpasses the most comprehensive database to date by more than 1\nbillion buildings. GBA.Height offers the most detailed and accurate global 3D\nbuilding height maps to date, achieving a spatial resolution of 3x3 meters-30\ntimes finer than previous global products (90 m), enabling a high-resolution\nand reliable analysis of building volumes at both local and global scales.\nFinally, we generated a global LoD1 building model (called GBA.LoD1) from the\nresulting GBA.Polygon and GBA.Height. GBA.LoD1 represents the first complete\nglobal LoD1 building models, including 2.68 billion building instances with\npredicted heights, i.e., with a height completeness of more than 97%, achieving\nRMSEs ranging from 1.5 m to 8.9 m across different continents. With its height\naccuracy, comprehensive global coverage and rich spatial details,\nGlobalBuildingAltas offers novel insights on the status quo of global\nbuildings, which unlocks unprecedented geospatial analysis possibilities, as\nshowcased by a better illustration of where people live and a more\ncomprehensive monitoring of the progress on the 11th Sustainable Development\nGoal of the United Nations."}
{"id": "2506.04115", "pdf": "https://arxiv.org/pdf/2506.04115", "abs": "https://arxiv.org/abs/2506.04115", "authors": ["Robin Bruneau", "Baptiste Brument", "Yvain Quau", "Jean Mlou", "Franois Bernard Lauze", "Jean-Denis Durou", "Lilian Calvet"], "title": "Multi-view Surface Reconstruction Using Normal and Reflectance Cues", "categories": ["cs.CV"], "comment": "22 pages, 15 figures, 11 tables. A thorough qualitative and\n  quantitive study is available in the supplementary material at\n  https://drive.google.com/file/d/1KDfCKediXNP5Os954TL_QldaUWS0nKcD/view?usp=drive_link", "summary": "Achieving high-fidelity 3D surface reconstruction while preserving fine\ndetails remains challenging, especially in the presence of materials with\ncomplex reflectance properties and without a dense-view setup. In this paper,\nwe introduce a versatile framework that incorporates multi-view normal and\noptionally reflectance maps into radiance-based surface reconstruction. Our\napproach employs a pixel-wise joint re-parametrization of reflectance and\nsurface normals, representing them as a vector of radiances under simulated,\nvarying illumination. This formulation enables seamless incorporation into\nstandard surface reconstruction pipelines, such as traditional multi-view\nstereo (MVS) frameworks or modern neural volume rendering (NVR) ones. Combined\nwith the latter, our approach achieves state-of-the-art performance on\nmulti-view photometric stereo (MVPS) benchmark datasets, including DiLiGenT-MV,\nLUCES-MV and Skoltech3D. In particular, our method excels in reconstructing\nfine-grained details and handling challenging visibility conditions. The\npresent paper is an extended version of the earlier conference paper by Brument\net al. (in Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), 2024), featuring an accelerated and more robust\nalgorithm as well as a broader empirical evaluation. The code and data relative\nto this article is available at https://github.com/RobinBruneau/RNb-NeuS2."}
{"id": "2506.04122", "pdf": "https://arxiv.org/pdf/2506.04122", "abs": "https://arxiv.org/abs/2506.04122", "authors": ["Sharang Kaul", "Mario Berk", "Thiemo Gerbich", "Abhinav Valada"], "title": "Contour Errors: An Ego-Centric Metric for Reliable 3D Multi-Object Tracking", "categories": ["cs.CV"], "comment": null, "summary": "Finding reliable matches is essential in multi-object tracking to ensure the\naccuracy and reliability of perception systems in safety-critical applications\nsuch as autonomous vehicles. Effective matching mitigates perception errors,\nenhancing object identification and tracking for improved performance and\nsafety. However, traditional metrics such as Intersection over Union (IoU) and\nCenter Point Distances (CPDs), which are effective in 2D image planes, often\nfail to find critical matches in complex 3D scenes. To address this limitation,\nwe introduce Contour Errors (CEs), an ego or object-centric metric for\nidentifying matches of interest in tracking scenarios from a functional\nperspective. By comparing bounding boxes in the ego vehicle's frame, contour\nerrors provide a more functionally relevant assessment of object matches.\nExtensive experiments on the nuScenes dataset demonstrate that contour errors\nimprove the reliability of matches over the state-of-the-art 2D IoU and CPD\nmetrics in tracking-by-detection methods. In 3D car tracking, our results show\nthat Contour Errors reduce functional failures (FPs/FNs) by 80% at close ranges\nand 60% at far ranges compared to IoU in the evaluation stage."}
{"id": "2506.04134", "pdf": "https://arxiv.org/pdf/2506.04134", "abs": "https://arxiv.org/abs/2506.04134", "authors": ["Jinting Wang", "Shan Yang", "Li Liu"], "title": "UniCUE: Unified Recognition and Generation Framework for Chinese Cued Speech Video-to-Speech Generation", "categories": ["cs.CV", "cs.SD", "eess.AS"], "comment": "10 pages, 10 figures", "summary": "Cued Speech (CS) enhances lipreading through hand coding, providing precise\nspeech perception support for the hearing-impaired. CS Video-to-Speech\ngeneration (CSV2S) task aims to convert the CS visual expressions (CS videos)\nof hearing-impaired individuals into comprehensible speech signals. Direct\ngeneration of speech from CS video (called single CSV2S) yields poor\nperformance due to insufficient CS data. Current research mostly focuses on CS\nRecognition (CSR), which convert video content into linguistic text. Based on\nthis, one straightforward way of CSV2S is to combine CSR with a Text-to-Speech\nsystem. This combined architecture relies on text as an intermediate medium for\nstepwise cross-modal alignment, which may lead to error propagation and\ntemporal misalignment between speech and video dynamics. To address these\nchallenges, we propose a novel approach that directly generates speech from CS\nvideos without relying on intermediate text. Building upon this, we propose\nUniCUE, the first unified framework for CSV2S, whose core innovation lies in\nthe integration of the CSR task that provides fine-grained visual-semantic\ninformation to facilitate speech generation from CS videos. More precisely, (1)\na novel fine-grained semantic alignment pool to ensure precise mapping between\nvisual features and speech contents; (2) a VisioPhonetic adapter to bridge\ncross-task representations, ensuring seamless compatibility between two\ndistinct tasks (i.e., CSV2S and CSR); (3) a pose-aware visual processor is\nintroduced to enhance fine-grained spatiotemporal correlations between lip and\nhand movements in CS video. Experiments on our new established Chinese CS\ndataset (14 cuers1: 8 hearing-impaired and 6 normal-hearing) show that our\nUniCUE significantly reduces Word Error Rate by 78.3% and improves lip-speech\nsynchronization by 32% compared to the single CSV2S."}
{"id": "2506.04141", "pdf": "https://arxiv.org/pdf/2506.04141", "abs": "https://arxiv.org/abs/2506.04141", "authors": ["Kejian Zhu", "Zhuoran Jin", "Hongbang Yuan", "Jiachun Li", "Shangqing Tu", "Pengfei Cao", "Yubo Chen", "Kang Liu", "Jun Zhao"], "title": "MMR-V: What's Left Unsaid? A Benchmark for Multimodal Deep Reasoning in Videos", "categories": ["cs.CV", "cs.CL"], "comment": "Project Page: https://mmr-v.github.io", "summary": "The sequential structure of videos poses a challenge to the ability of\nmultimodal large language models (MLLMs) to locate multi-frame evidence and\nconduct multimodal reasoning. However, existing video benchmarks mainly focus\non understanding tasks, which only require models to match frames mentioned in\nthe question (hereafter referred to as \"question frame\") and perceive a few\nadjacent frames. To address this gap, we propose MMR-V: A Benchmark for\nMultimodal Deep Reasoning in Videos. The benchmark is characterized by the\nfollowing features. (1) Long-range, multi-frame reasoning: Models are required\nto infer and analyze evidence frames that may be far from the question frame.\n(2) Beyond perception: Questions cannot be answered through direct perception\nalone but require reasoning over hidden information. (3) Reliability: All tasks\nare manually annotated, referencing extensive real-world user understanding to\nalign with common perceptions. (4) Confusability: Carefully designed distractor\nannotation strategies to reduce model shortcuts. MMR-V consists of 317 videos\nand 1,257 tasks. Our experiments reveal that current models still struggle with\nmulti-modal reasoning; even the best-performing model, o4-mini, achieves only\n52.5% accuracy. Additionally, current reasoning enhancement strategies\n(Chain-of-Thought and scaling test-time compute) bring limited gains. Further\nanalysis indicates that the CoT demanded for multi-modal reasoning differs from\nit in textual reasoning, which partly explains the limited performance gains.\nWe hope that MMR-V can inspire further research into enhancing multi-modal\nreasoning capabilities."}
{"id": "2506.04143", "pdf": "https://arxiv.org/pdf/2506.04143", "abs": "https://arxiv.org/abs/2506.04143", "authors": ["Ngoc Q. Ly", "Hieu N. M. Cao", "Thi T. Nguyen"], "title": "Person Re-Identification System at Semantic Level based on Pedestrian Attributes Ontology", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Person Re-Identification (Re-ID) is a very important task in video\nsurveillance systems such as tracking people, finding people in public places,\nor analysing customer behavior in supermarkets. Although there have been many\nworks to solve this problem, there are still remaining challenges such as\nlarge-scale datasets, imbalanced data, viewpoint, fine grained data\n(attributes), the Local Features are not employed at semantic level in online\nstage of Re-ID task, furthermore, the imbalanced data problem of attributes are\nnot taken into consideration. This paper has proposed a Unified Re-ID system\nconsisted of three main modules such as Pedestrian Attribute Ontology (PAO),\nLocal Multi-task DCNN (Local MDCNN), Imbalance Data Solver (IDS). The new main\npoint of our Re-ID system is the power of mutual support of PAO, Local MDCNN\nand IDS to exploit the inner-group correlations of attributes and pre-filter\nthe mismatch candidates from Gallery set based on semantic information as\nFashion Attributes and Facial Attributes, to solve the imbalanced data of\nattributes without adjusting network architecture and data augmentation. We\nexperimented on the well-known Market1501 dataset. The experimental results\nhave shown the effectiveness of our Re-ID system and it could achieve the\nhigher performance on Market1501 dataset in comparison to some state-of-the-art\nRe-ID methods."}
{"id": "2506.04158", "pdf": "https://arxiv.org/pdf/2506.04158", "abs": "https://arxiv.org/abs/2506.04158", "authors": ["Yujia Hu", "Songhua Liu", "Zhenxiong Tan", "Xingyi Yang", "Xinchao Wang"], "title": "Image Editing As Programs with Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "While diffusion models have achieved remarkable success in text-to-image\ngeneration, they encounter significant challenges with instruction-driven image\nediting. Our research highlights a key challenge: these models particularly\nstruggle with structurally inconsistent edits that involve substantial layout\nchanges. To mitigate this gap, we introduce Image Editing As Programs (IEAP), a\nunified image editing framework built upon the Diffusion Transformer (DiT)\narchitecture. At its core, IEAP approaches instructional editing through a\nreductionist lens, decomposing complex editing instructions into sequences of\natomic operations. Each operation is implemented via a lightweight adapter\nsharing the same DiT backbone and is specialized for a specific type of edit.\nProgrammed by a vision-language model (VLM)-based agent, these operations\ncollaboratively support arbitrary and structurally inconsistent\ntransformations. By modularizing and sequencing edits in this way, IEAP\ngeneralizes robustly across a wide range of editing tasks, from simple\nadjustments to substantial structural changes. Extensive experiments\ndemonstrate that IEAP significantly outperforms state-of-the-art methods on\nstandard benchmarks across various editing scenarios. In these evaluations, our\nframework delivers superior accuracy and semantic fidelity, particularly for\ncomplex, multi-step instructions. Codes are available at\nhttps://github.com/YujiaHu1109/IEAP."}
{"id": "2506.04174", "pdf": "https://arxiv.org/pdf/2506.04174", "abs": "https://arxiv.org/abs/2506.04174", "authors": ["Hengyu Liu", "Yuehao Wang", "Chenxin Li", "Ruisi Cai", "Kevin Wang", "Wuyang Li", "Pavlo Molchanov", "Peihao Wang", "Zhangyang Wang"], "title": "FlexGS: Train Once, Deploy Everywhere with Many-in-One Flexible 3D Gaussian Splatting", "categories": ["cs.CV"], "comment": "CVPR 2025; Project Page: https://flexgs.github.io", "summary": "3D Gaussian splatting (3DGS) has enabled various applications in 3D scene\nrepresentation and novel view synthesis due to its efficient rendering\ncapabilities. However, 3DGS demands relatively significant GPU memory, limiting\nits use on devices with restricted computational resources. Previous approaches\nhave focused on pruning less important Gaussians, effectively compressing 3DGS\nbut often requiring a fine-tuning stage and lacking adaptability for the\nspecific memory needs of different devices. In this work, we present an elastic\ninference method for 3DGS. Given an input for the desired model size, our\nmethod selects and transforms a subset of Gaussians, achieving substantial\nrendering performance without additional fine-tuning. We introduce a tiny\nlearnable module that controls Gaussian selection based on the input\npercentage, along with a transformation module that adjusts the selected\nGaussians to complement the performance of the reduced model. Comprehensive\nexperiments on ZipNeRF, MipNeRF and Tanks\\&Temples scenes demonstrate the\neffectiveness of our approach. Code is available at https://flexgs.github.io."}
{"id": "2506.04209", "pdf": "https://arxiv.org/pdf/2506.04209", "abs": "https://arxiv.org/abs/2506.04209", "authors": ["Jingfeng Yang", "Ziyang Wu", "Yue Zhao", "Yi Ma"], "title": "Language-Image Alignment with Fixed Text Encoders", "categories": ["cs.CV"], "comment": null, "summary": "Currently, the most dominant approach to establishing language-image\nalignment is to pre-train text and image encoders jointly through contrastive\nlearning, such as CLIP and its variants. In this work, we question whether such\na costly joint training is necessary. In particular, we investigate if a\npre-trained fixed large language model (LLM) offers a good enough text encoder\nto guide visual representation learning. That is, we propose to learn\nLanguage-Image alignment with a Fixed Text encoder (LIFT) from an LLM by\ntraining only the image encoder. Somewhat surprisingly, through comprehensive\nbenchmarking and ablation studies, we find that this much simplified framework\nLIFT is highly effective and it outperforms CLIP in most scenarios that involve\ncompositional understanding and long captions, while achieving considerable\ngains in computational efficiency. Our work takes a first step towards\nsystematically exploring how text embeddings from LLMs can guide visual\nlearning and suggests an alternative design choice for learning\nlanguage-aligned visual representations."}
{"id": "2506.04211", "pdf": "https://arxiv.org/pdf/2506.04211", "abs": "https://arxiv.org/abs/2506.04211", "authors": ["Boyong He", "Yuxiang Ji", "Zhuoyue Tan", "Liaoni Wu"], "title": "Diffusion Domain Teacher: Diffusion Guided Domain Adaptive Object Detector", "categories": ["cs.CV"], "comment": "MM2024 poster, with appendix and codes", "summary": "Object detectors often suffer a decrease in performance due to the large\ndomain gap between the training data (source domain) and real-world data\n(target domain). Diffusion-based generative models have shown remarkable\nabilities in generating high-quality and diverse images, suggesting their\npotential for extracting valuable feature from various domains. To effectively\nleverage the cross-domain feature representation of diffusion models, in this\npaper, we train a detector with frozen-weight diffusion model on the source\ndomain, then employ it as a teacher model to generate pseudo labels on the\nunlabeled target domain, which are used to guide the supervised learning of the\nstudent model on the target domain. We refer to this approach as Diffusion\nDomain Teacher (DDT). By employing this straightforward yet potent framework,\nwe significantly improve cross-domain object detection performance without\ncompromising the inference speed. Our method achieves an average mAP\nimprovement of 21.2% compared to the baseline on 6 datasets from three common\ncross-domain detection benchmarks (Cross-Camera, Syn2Real, Real2Artistic},\nsurpassing the current state-of-the-art (SOTA) methods by an average of 5.7%\nmAP. Furthermore, extensive experiments demonstrate that our method\nconsistently brings improvements even in more powerful and complex models,\nhighlighting broadly applicable and effective domain adaptation capability of\nour DDT. The code is available at\nhttps://github.com/heboyong/Diffusion-Domain-Teacher."}
{"id": "2506.04213", "pdf": "https://arxiv.org/pdf/2506.04213", "abs": "https://arxiv.org/abs/2506.04213", "authors": ["Xuanhua He", "Quande Liu", "Zixuan Ye", "Wecai Ye", "Qiulin Wang", "Xintao Wang", "Qifeng Chen", "Pengfei Wan", "Di Zhang", "Kun Gai"], "title": "FullDiT2: Efficient In-Context Conditioning for Video Diffusion Transformers", "categories": ["cs.CV"], "comment": null, "summary": "Fine-grained and efficient controllability on video diffusion transformers\nhas raised increasing desires for the applicability. Recently, In-context\nConditioning emerged as a powerful paradigm for unified conditional video\ngeneration, which enables diverse controls by concatenating varying context\nconditioning signals with noisy video latents into a long unified token\nsequence and jointly processing them via full-attention, e.g., FullDiT. Despite\ntheir effectiveness, these methods face quadratic computation overhead as task\ncomplexity increases, hindering practical deployment. In this paper, we study\nthe efficiency bottleneck neglected in original in-context conditioning video\ngeneration framework. We begin with systematic analysis to identify two key\nsources of the computation inefficiencies: the inherent redundancy within\ncontext condition tokens and the computational redundancy in context-latent\ninteractions throughout the diffusion process. Based on these insights, we\npropose FullDiT2, an efficient in-context conditioning framework for general\ncontrollability in both video generation and editing tasks, which innovates\nfrom two key perspectives. Firstly, to address the token redundancy, FullDiT2\nleverages a dynamic token selection mechanism to adaptively identify important\ncontext tokens, reducing the sequence length for unified full-attention.\nAdditionally, a selective context caching mechanism is devised to minimize\nredundant interactions between condition tokens and video latents. Extensive\nexperiments on six diverse conditional video editing and generation tasks\ndemonstrate that FullDiT2 achieves significant computation reduction and 2-3\ntimes speedup in averaged time cost per diffusion step, with minimal\ndegradation or even higher performance in video generation quality. The project\npage is at \\href{https://fulldit2.github.io/}{https://fulldit2.github.io/}."}
{"id": "2506.04214", "pdf": "https://arxiv.org/pdf/2506.04214", "abs": "https://arxiv.org/abs/2506.04214", "authors": ["Tingle Li", "Baihe Huang", "Xiaobin Zhuang", "Dongya Jia", "Jiawei Chen", "Yuping Wang", "Zhuo Chen", "Gopala Anumanchipalli", "Yuxuan Wang"], "title": "Sounding that Object: Interactive Object-Aware Image to Audio Generation", "categories": ["cs.CV", "cs.LG", "cs.MM", "cs.SD", "eess.AS"], "comment": "ICML 2025", "summary": "Generating accurate sounds for complex audio-visual scenes is challenging,\nespecially in the presence of multiple objects and sound sources. In this\npaper, we propose an {\\em interactive object-aware audio generation} model that\ngrounds sound generation in user-selected visual objects within images. Our\nmethod integrates object-centric learning into a conditional latent diffusion\nmodel, which learns to associate image regions with their corresponding sounds\nthrough multi-modal attention. At test time, our model employs image\nsegmentation to allow users to interactively generate sounds at the {\\em\nobject} level. We theoretically validate that our attention mechanism\nfunctionally approximates test-time segmentation masks, ensuring the generated\naudio aligns with selected objects. Quantitative and qualitative evaluations\nshow that our model outperforms baselines, achieving better alignment between\nobjects and their associated sounds. Project page:\nhttps://tinglok.netlify.app/files/avobject/"}
{"id": "2506.04216", "pdf": "https://arxiv.org/pdf/2506.04216", "abs": "https://arxiv.org/abs/2506.04216", "authors": ["Zixuan Ye", "Xuanhua He", "Quande Liu", "Qiulin Wang", "Xintao Wang", "Pengfei Wan", "Di Zhang", "Kun Gai", "Qifeng Chen", "Wenhan Luo"], "title": "UNIC: Unified In-Context Video Editing", "categories": ["cs.CV"], "comment": "The project page is at\n  \\href{https://zixuan-ye.github.io/UNIC}{https://zixuan-ye.github.io/UNIC}", "summary": "Recent advances in text-to-video generation have sparked interest in\ngenerative video editing tasks. Previous methods often rely on task-specific\narchitectures (e.g., additional adapter modules) or dedicated customizations\n(e.g., DDIM inversion), which limit the integration of versatile editing\nconditions and the unification of various editing tasks. In this paper, we\nintroduce UNified In-Context Video Editing (UNIC), a simple yet effective\nframework that unifies diverse video editing tasks within a single model in an\nin-context manner. To achieve this unification, we represent the inputs of\nvarious video editing tasks as three types of tokens: the source video tokens,\nthe noisy video latent, and the multi-modal conditioning tokens that vary\naccording to the specific editing task. Based on this formulation, our key\ninsight is to integrate these three types into a single consecutive token\nsequence and jointly model them using the native attention operations of DiT,\nthereby eliminating the need for task-specific adapter designs. Nevertheless,\ndirect task unification under this framework is challenging, leading to severe\ntoken collisions and task confusion due to the varying video lengths and\ndiverse condition modalities across tasks. To address these, we introduce\ntask-aware RoPE to facilitate consistent temporal positional encoding, and\ncondition bias that enables the model to clearly differentiate different\nediting tasks. This allows our approach to adaptively perform different video\nediting tasks by referring the source video and varying condition tokens \"in\ncontext\", and support flexible task composition. To validate our method, we\nconstruct a unified video editing benchmark containing six representative video\nediting tasks. Results demonstrate that our unified approach achieves superior\nperformance on each task and exhibits emergent task composition abilities."}
{"id": "2506.04220", "pdf": "https://arxiv.org/pdf/2506.04220", "abs": "https://arxiv.org/abs/2506.04220", "authors": ["Fangrui Zhu", "Hanhui Wang", "Yiming Xie", "Jing Gu", "Tianye Ding", "Jianwei Yang", "Huaizu Jiang"], "title": "Struct2D: A Perception-Guided Framework for Spatial Reasoning in Large Multimodal Models", "categories": ["cs.CV"], "comment": "https://github.com/neu-vi/struct2d", "summary": "Unlocking spatial reasoning in Large Multimodal Models (LMMs) is crucial for\nenabling intelligent interaction with 3D environments. While prior efforts\noften rely on explicit 3D inputs or specialized model architectures, we ask:\ncan LMMs reason about 3D space using only structured 2D representations derived\nfrom perception? We introduce Struct2D, a perception-guided prompting framework\nthat combines bird's-eye-view (BEV) images with object marks and object-centric\nmetadata, optionally incorporating egocentric keyframes when needed. Using\nStruct2D, we conduct an in-depth zero-shot analysis of closed-source LMMs\n(e.g., GPT-o3) and find that they exhibit surprisingly strong spatial reasoning\nabilities when provided with structured 2D inputs, effectively handling tasks\nsuch as relative direction estimation and route planning. Building on these\ninsights, we construct Struct2D-Set, a large-scale instruction tuning dataset\nwith 200K fine-grained QA pairs across eight spatial reasoning categories,\ngenerated automatically from 3D indoor scenes. We fine-tune an open-source LMM\n(Qwen2.5VL) on Struct2D-Set, achieving competitive performance on multiple\nbenchmarks, including 3D question answering, dense captioning, and object\ngrounding. Our approach demonstrates that structured 2D inputs can effectively\nbridge perception and language reasoning in LMMs-without requiring explicit 3D\nrepresentations as input. We will release both our code and dataset to support\nfuture research."}
{"id": "2506.04224", "pdf": "https://arxiv.org/pdf/2506.04224", "abs": "https://arxiv.org/abs/2506.04224", "authors": ["Zirui Wang", "Wenjing Bian", "Xinghui Li", "Yifu Tao", "Jianeng Wang", "Maurice Fallon", "Victor Adrian Prisacariu"], "title": "Seeing in the Dark: Benchmarking Egocentric 3D Vision with the Oxford Day-and-Night Dataset", "categories": ["cs.CV"], "comment": "Project page: https://oxdan.active.vision/", "summary": "We introduce Oxford Day-and-Night, a large-scale, egocentric dataset for\nnovel view synthesis (NVS) and visual relocalisation under challenging lighting\nconditions. Existing datasets often lack crucial combinations of features such\nas ground-truth 3D geometry, wide-ranging lighting variation, and full 6DoF\nmotion. Oxford Day-and-Night addresses these gaps by leveraging Meta ARIA\nglasses to capture egocentric video and applying multi-session SLAM to estimate\ncamera poses, reconstruct 3D point clouds, and align sequences captured under\nvarying lighting conditions, including both day and night. The dataset spans\nover 30 $\\mathrm{km}$ of recorded trajectories and covers an area of 40,000\n$\\mathrm{m}^2$, offering a rich foundation for egocentric 3D vision research.\nIt supports two core benchmarks, NVS and relocalisation, providing a unique\nplatform for evaluating models in realistic and diverse environments."}
{"id": "2506.04225", "pdf": "https://arxiv.org/pdf/2506.04225", "abs": "https://arxiv.org/abs/2506.04225", "authors": ["Tianyu Huang", "Wangguandong Zheng", "Tengfei Wang", "Yuhao Liu", "Zhenwei Wang", "Junta Wu", "Jie Jiang", "Hui Li", "Rynson W. H. Lau", "Wangmeng Zuo", "Chunchao Guo"], "title": "Voyager: Long-Range and World-Consistent Video Diffusion for Explorable 3D Scene Generation", "categories": ["cs.CV"], "comment": null, "summary": "Real-world applications like video gaming and virtual reality often demand\nthe ability to model 3D scenes that users can explore along custom camera\ntrajectories. While significant progress has been made in generating 3D objects\nfrom text or images, creating long-range, 3D-consistent, explorable 3D scenes\nremains a complex and challenging problem. In this work, we present Voyager, a\nnovel video diffusion framework that generates world-consistent 3D point-cloud\nsequences from a single image with user-defined camera path. Unlike existing\napproaches, Voyager achieves end-to-end scene generation and reconstruction\nwith inherent consistency across frames, eliminating the need for 3D\nreconstruction pipelines (e.g., structure-from-motion or multi-view stereo).\nOur method integrates three key components: 1) World-Consistent Video\nDiffusion: A unified architecture that jointly generates aligned RGB and depth\nvideo sequences, conditioned on existing world observation to ensure global\ncoherence 2) Long-Range World Exploration: An efficient world cache with point\nculling and an auto-regressive inference with smooth video sampling for\niterative scene extension with context-aware consistency, and 3) Scalable Data\nEngine: A video reconstruction pipeline that automates camera pose estimation\nand metric depth prediction for arbitrary videos, enabling large-scale, diverse\ntraining data curation without manual 3D annotations. Collectively, these\ndesigns result in a clear improvement over existing methods in visual quality\nand geometric accuracy, with versatile applications."}
{"id": "2506.04228", "pdf": "https://arxiv.org/pdf/2506.04228", "abs": "https://arxiv.org/abs/2506.04228", "authors": ["Sihui Ji", "Hao Luo", "Xi Chen", "Yuanpeng Tu", "Yiyang Wang", "Hengshuang Zhao"], "title": "LayerFlow: A Unified Model for Layer-aware Video Generation", "categories": ["cs.CV"], "comment": "Project Page: https://sihuiji.github.io/LayerFlow-Page/", "summary": "We present LayerFlow, a unified solution for layer-aware video generation.\nGiven per-layer prompts, LayerFlow generates videos for the transparent\nforeground, clean background, and blended scene. It also supports versatile\nvariants like decomposing a blended video or generating the background for the\ngiven foreground and vice versa. Starting from a text-to-video diffusion\ntransformer, we organize the videos for different layers as sub-clips, and\nleverage layer embeddings to distinguish each clip and the corresponding\nlayer-wise prompts. In this way, we seamlessly support the aforementioned\nvariants in one unified framework. For the lack of high-quality layer-wise\ntraining videos, we design a multi-stage training strategy to accommodate\nstatic images with high-quality layer annotations. Specifically, we first train\nthe model with low-quality video data. Then, we tune a motion LoRA to make the\nmodel compatible with static frames. Afterward, we train the content LoRA on\nthe mixture of image data with high-quality layered images along with\ncopy-pasted video data. During inference, we remove the motion LoRA thus\ngenerating smooth videos with desired layers."}
{"id": "2505.24073", "pdf": "https://arxiv.org/pdf/2505.24073", "abs": "https://arxiv.org/abs/2505.24073", "authors": ["Chan-Wei Hu", "Yueqi Wang", "Shuo Xing", "Chia-Ju Chen", "Zhengzhong Tu"], "title": "mRAG: Elucidating the Design Space of Multi-modal Retrieval-Augmented Generation", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": "16 pages, 11 figures", "summary": "Large Vision-Language Models (LVLMs) have made remarkable strides in\nmultimodal tasks such as visual question answering, visual grounding, and\ncomplex reasoning. However, they remain limited by static training data,\nsusceptibility to hallucinations, and inability to verify claims against\nup-to-date, external evidence, compromising their performance in dynamic\nreal-world applications. Retrieval-Augmented Generation (RAG) offers a\npractical solution to mitigate these challenges by allowing the LVLMs to access\nlarge-scale knowledge databases via retrieval mechanisms, thereby grounding\nmodel outputs in factual, contextually relevant information. Here in this\npaper, we conduct the first systematic dissection of the multimodal RAG\npipeline for LVLMs, explicitly investigating (1) the retrieval phase: on the\nmodality configurations and retrieval strategies, (2) the re-ranking stage: on\nstrategies to mitigate positional biases and improve the relevance of retrieved\nevidence, and (3) the generation phase: we further investigate how to best\nintegrate retrieved candidates into the final generation process. Finally, we\nextend to explore a unified agentic framework that integrates re-ranking and\ngeneration through self-reflection, enabling LVLMs to select relevant evidence\nand suppress irrelevant context dynamically. Our full-stack exploration of RAG\nfor LVLMs yields substantial insights, resulting in an average performance\nboost of 5% without any fine-tuning."}
{"id": "2506.03152", "pdf": "https://arxiv.org/pdf/2506.03152", "abs": "https://arxiv.org/abs/2506.03152", "authors": ["Robert Bayer", "Julian Priest", "Daniel Kjellberg", "Jeppe Lindhard", "Nikolaj Srenesen", "Nicolaj Valsted", "var li", "Pnar Tzn"], "title": "Adaptive and Robust Image Processing on CubeSats", "categories": ["eess.IV", "cs.CV", "cs.DC", "cs.LG"], "comment": null, "summary": "CubeSats offer a low-cost platform for space research, particularly for Earth\nobservation. However, their resource-constrained nature and being in space,\nchallenge the flexibility and complexity of the deployed image processing\npipelines and their orchestration. This paper introduces two novel systems,\nDIPP and DISH, to address these challenges. DIPP is a modular and configurable\nimage processing pipeline framework that allows for adaptability to changing\nmission goals even after deployment, while preserving robustness. DISH is a\ndomain-specific language (DSL) and runtime system designed to schedule complex\nimaging workloads on low-power and memory-constrained processors.\n  Our experiments demonstrate that DIPP's decomposition of the processing\npipelines adds negligible overhead, while significantly reducing the network\nrequirements of updating pipelines and being robust against erroneous module\nuploads. Furthermore, we compare DISH to Lua, a general purpose scripting\nlanguage, and demonstrate its comparable expressiveness and lower memory\nrequirement."}
{"id": "2506.03158", "pdf": "https://arxiv.org/pdf/2506.03158", "abs": "https://arxiv.org/abs/2506.03158", "authors": ["Jiahao Qin", "Bei Peng", "Feng Liu", "Guangliang Cheng", "Lu Zong"], "title": "DUAL: Dynamic Uncertainty-Aware Learning", "categories": ["cs.LG", "cs.CV"], "comment": "12 pages, 3 figures", "summary": "Deep learning models frequently encounter feature uncertainty in diverse\nlearning scenarios, significantly impacting their performance and reliability.\nThis challenge is particularly complex in multi-modal scenarios, where models\nmust integrate information from different sources with inherent uncertainties.\nWe propose Dynamic Uncertainty-Aware Learning (DUAL), a unified framework that\neffectively handles feature uncertainty in both single-modal and multi-modal\nscenarios. DUAL introduces three key innovations: Dynamic Feature Uncertainty\nModeling, which continuously refines uncertainty estimates through joint\nconsideration of feature characteristics and learning dynamics; Adaptive\nDistribution-Aware Modulation, which maintains balanced feature distributions\nthrough dynamic sample influence adjustment; and Uncertainty-aware Cross-Modal\nRelationship Learning, which explicitly models uncertainties in cross-modal\ninteractions. Through extensive experiments, we demonstrate DUAL's\neffectiveness across multiple domains: in computer vision tasks, it achieves\nsubstantial improvements of 7.1% accuracy on CIFAR-10, 6.5% accuracy on\nCIFAR-100, and 2.3% accuracy on Tiny-ImageNet; in multi-modal learning, it\ndemonstrates consistent gains of 4.1% accuracy on CMU-MOSEI and 2.8% accuracy\non CMU-MOSI for sentiment analysis, while achieving 1.4% accuracy improvements\non MISR. The code will be available on GitHub soon."}
{"id": "2506.03175", "pdf": "https://arxiv.org/pdf/2506.03175", "abs": "https://arxiv.org/abs/2506.03175", "authors": ["Youshen Xiao", "Yiling Shi", "Ruixi Sun", "Hongjiang Wei", "Fei Gao", "Yuyao Zhang"], "title": "Super-temporal-resolution Photoacoustic Imaging with Dynamic Reconstruction through Implicit Neural Representation in Sparse-view", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Dynamic Photoacoustic Computed Tomography (PACT) is an important imaging\ntechnique for monitoring physiological processes, capable of providing\nhigh-contrast images of optical absorption at much greater depths than\ntraditional optical imaging methods. However, practical instrumentation and\ngeometric constraints limit the number of acoustic sensors available around the\nimaging target, leading to sparsity in sensor data. Traditional photoacoustic\n(PA) image reconstruction methods, when directly applied to sparse PA data,\nproduce severe artifacts. Additionally, these traditional methods do not\nconsider the inter-frame relationships in dynamic imaging. Temporal resolution\nis crucial for dynamic photoacoustic imaging, which is fundamentally limited by\nthe low repetition rate (e.g., 20 Hz) and high cost of high-power laser\ntechnology. Recently, Implicit Neural Representation (INR) has emerged as a\npowerful deep learning tool for solving inverse problems with sparse data, by\ncharacterizing signal properties as continuous functions of their coordinates\nin an unsupervised manner. In this work, we propose an INR-based method to\nimprove dynamic photoacoustic image reconstruction from sparse-views and\nenhance temporal resolution, using only spatiotemporal coordinates as input.\nSpecifically, the proposed INR represents dynamic photoacoustic images as\nimplicit functions and encodes them into a neural network. The weights of the\nnetwork are learned solely from the acquired sparse sensor data, without the\nneed for external training datasets or prior images. Benefiting from the strong\nimplicit continuity regularization provided by INR, as well as explicit\nregularization for low-rank and sparsity, our proposed method outperforms\ntraditional reconstruction methods under two different sparsity conditions,\neffectively suppressing artifacts and ensuring image quality."}
{"id": "2506.03177", "pdf": "https://arxiv.org/pdf/2506.03177", "abs": "https://arxiv.org/abs/2506.03177", "authors": ["Isarun Chamveha", "Supphanut Chaiyungyuen", "Sasinun Worakriangkrai", "Nattawadee Prasawang", "Warasinee Chaisangmongkon", "Pornpim Korpraphong", "Voraparee Suvannarerg", "Shanigarn Thiravit", "Chalermdej Kannawat", "Kewalin Rungsinaporn", "Suwara Issaragrisil", "Payia Chadbunchachai", "Pattiya Gatechumpol", "Chawiporn Muktabhant", "Patarachai Sereerat"], "title": "Deep Learning-Based Breast Cancer Detection in Mammography: A Multi-Center Validation Study in Thai Population", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "This study presents a deep learning system for breast cancer detection in\nmammography, developed using a modified EfficientNetV2 architecture with\nenhanced attention mechanisms. The model was trained on mammograms from a major\nThai medical center and validated on three distinct datasets: an in-domain test\nset (9,421 cases), a biopsy-confirmed set (883 cases), and an out-of-domain\ngeneralizability set (761 cases) collected from two different hospitals. For\ncancer detection, the model achieved AUROCs of 0.89, 0.96, and 0.94 on the\nrespective datasets. The system's lesion localization capability, evaluated\nusing metrics including Lesion Localization Fraction (LLF) and Non-Lesion\nLocalization Fraction (NLF), demonstrated robust performance in identifying\nsuspicious regions. Clinical validation through concordance tests showed strong\nagreement with radiologists: 83.5% classification and 84.0% localization\nconcordance for biopsy-confirmed cases, and 78.1% classification and 79.6%\nlocalization concordance for out-of-domain cases. Expert radiologists'\nacceptance rate also averaged 96.7% for biopsy-confirmed cases, and 89.3% for\nout-of-domain cases. The system achieved a System Usability Scale score of\n74.17 for source hospital, and 69.20 for validation hospitals, indicating good\nclinical acceptance. These results demonstrate the model's effectiveness in\nassisting mammogram interpretation, with the potential to enhance breast cancer\nscreening workflows in clinical practice."}
{"id": "2506.03178", "pdf": "https://arxiv.org/pdf/2506.03178", "abs": "https://arxiv.org/abs/2506.03178", "authors": ["Md. Zihad Bin Jahangir", "Muhammad Ashad Kabir", "Sumaiya Akter", "Israt Jahan", "Minh Chau"], "title": "LLaMA-XR: A Novel Framework for Radiology Report Generation using LLaMA and QLoRA Fine Tuning", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "25 pages", "summary": "Automated radiology report generation holds significant potential to reduce\nradiologists' workload and enhance diagnostic accuracy. However, generating\nprecise and clinically meaningful reports from chest radiographs remains\nchallenging due to the complexity of medical language and the need for\ncontextual understanding. Existing models often struggle with maintaining both\naccuracy and contextual relevance. In this paper, we present LLaMA-XR, a novel\nframework that integrates LLaMA 3.1 with DenseNet-121-based image embeddings\nand Quantized Low-Rank Adaptation (QLoRA) fine-tuning. LLaMA-XR achieves\nimproved coherence and clinical accuracy while maintaining computational\nefficiency. This efficiency is driven by an optimization strategy that enhances\nparameter utilization and reduces memory overhead, enabling faster report\ngeneration with lower computational resource demands. Extensive experiments\nconducted on the IU X-ray benchmark dataset demonstrate that LLaMA-XR\noutperforms a range of state-of-the-art methods. Our model achieves a ROUGE-L\nscore of 0.433 and a METEOR score of 0.336, establishing new performance\nbenchmarks in the domain. These results underscore LLaMA-XR's potential as an\neffective and efficient AI system for automated radiology reporting, offering\nenhanced clinical utility and reliability."}
{"id": "2506.03180", "pdf": "https://arxiv.org/pdf/2506.03180", "abs": "https://arxiv.org/abs/2506.03180", "authors": ["Jan Ignatowicz", "Krzysztof Kutt", "Grzegorz J. Nalepa"], "title": "Knowledge Graphs for Digitized Manuscripts in Jagiellonian Digital Library Application", "categories": ["cs.DL", "cs.CV"], "comment": null, "summary": "Digitizing cultural heritage collections has become crucial for preservation\nof historical artifacts and enhancing their availability to the wider public.\nGalleries, libraries, archives and museums (GLAM institutions) are actively\ndigitizing their holdings and creates extensive digital collections. Those\ncollections are often enriched with metadata describing items but not exactly\ntheir contents. The Jagiellonian Digital Library, standing as a good example of\nsuch an effort, offers datasets accessible through protocols like OAI-PMH.\nDespite these improvements, metadata completeness and standardization continue\nto pose substantial obstacles, limiting the searchability and potential\nconnections between collections. To deal with these challenges, we explore an\nintegrated methodology of computer vision (CV), artificial intelligence (AI),\nand semantic web technologies to enrich metadata and construct knowledge graphs\nfor digitized manuscripts and incunabula."}
{"id": "2506.03181", "pdf": "https://arxiv.org/pdf/2506.03181", "abs": "https://arxiv.org/abs/2506.03181", "authors": ["Wangting Zhou", "Jiangshan He", "Tong Cai", "Lin Wang", "Zhen Yuan", "Xunbin Wei", "Xueli Chen"], "title": "Dc-EEMF: Pushing depth-of-field limit of photoacoustic microscopy via decision-level constrained learning", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Photoacoustic microscopy holds the potential to measure biomarkers'\nstructural and functional status without labels, which significantly aids in\ncomprehending pathophysiological conditions in biomedical research. However,\nconventional optical-resolution photoacoustic microscopy (OR-PAM) is hindered\nby a limited depth-of-field (DoF) due to the narrow depth range focused on a\nGaussian beam. Consequently, it fails to resolve sufficient details in the\ndepth direction. Herein, we propose a decision-level constrained end-to-end\nmulti-focus image fusion (Dc-EEMF) to push DoF limit of PAM. The DC-EEMF method\nis a lightweight siamese network that incorporates an artifact-resistant\nchannel-wise spatial frequency as its feature fusion rule. The meticulously\ncrafted U-Net-based perceptual loss function for decision-level focus\nproperties in end-to-end fusion seamlessly integrates the complementary\nadvantages of spatial domain and transform domain methods within Dc-EEMF. This\napproach can be trained end-to-end without necessitating post-processing\nprocedures. Experimental results and numerical analyses collectively\ndemonstrate our method's robust performance, achieving an impressive fusion\nresult for PAM images without a substantial sacrifice in lateral resolution.\nThe utilization of Dc-EEMF-powered PAM has the potential to serve as a\npractical tool in preclinical and clinical studies requiring extended DoF for\nvarious applications."}
{"id": "2506.03183", "pdf": "https://arxiv.org/pdf/2506.03183", "abs": "https://arxiv.org/abs/2506.03183", "authors": ["Yaar Utku Alalar", "Yu Cao", "Mehmet Akakaya"], "title": "Edge Computing for Physics-Driven AI in Computational MRI: A Feasibility Study", "categories": ["eess.IV", "cs.AI", "cs.AR", "cs.CV", "cs.LG", "physics.med-ph"], "comment": "IEEE International Conference on Future Internet of Things and Cloud\n  (FiCloud), 2025", "summary": "Physics-driven artificial intelligence (PD-AI) reconstruction methods have\nemerged as the state-of-the-art for accelerating MRI scans, enabling higher\nspatial and temporal resolutions. However, the high resolution of these scans\ngenerates massive data volumes, leading to challenges in transmission, storage,\nand real-time processing. This is particularly pronounced in functional MRI,\nwhere hundreds of volumetric acquisitions further exacerbate these demands.\nEdge computing with FPGAs presents a promising solution for enabling PD-AI\nreconstruction near the MRI sensors, reducing data transfer and storage\nbottlenecks. However, this requires optimization of PD-AI models for hardware\nefficiency through quantization and bypassing traditional FFT-based approaches,\nwhich can be a limitation due to their computational demands. In this work, we\npropose a novel PD-AI computational MRI approach optimized for FPGA-based edge\ncomputing devices, leveraging 8-bit complex data quantization and eliminating\nredundant FFT/IFFT operations. Our results show that this strategy improves\ncomputational efficiency while maintaining reconstruction quality comparable to\nconventional PD-AI methods, and outperforms standard clinical methods. Our\napproach presents an opportunity for high-resolution MRI reconstruction on\nresource-constrained devices, highlighting its potential for real-world\ndeployment."}
{"id": "2506.03185", "pdf": "https://arxiv.org/pdf/2506.03185", "abs": "https://arxiv.org/abs/2506.03185", "authors": ["Liangrui Pan", "Xingchen Li", "Zhongyi Chen", "Ling Chu", "Shaoliang Peng"], "title": "DLiPath: A Benchmark for the Comprehensive Assessment of Donor Liver Based on Histopathological Image Dataset", "categories": ["eess.IV", "cs.AI", "cs.CV", "q-bio.QM"], "comment": "Submit to ACM MM2025", "summary": "Pathologists comprehensive evaluation of donor liver biopsies provides\ncrucial information for accepting or discarding potential grafts. However,\nrapidly and accurately obtaining these assessments intraoperatively poses a\nsignificant challenge for pathologists. Features in donor liver biopsies, such\nas portal tract fibrosis, total steatosis, macrovesicular steatosis, and\nhepatocellular ballooning are correlated with transplant outcomes, yet\nquantifying these indicators suffers from substantial inter- and intra-observer\nvariability. To address this, we introduce DLiPath, the first benchmark for\ncomprehensive donor liver assessment based on a histopathology image dataset.\nWe collected and publicly released 636 whole slide images from 304 donor liver\npatients at the Department of Pathology, the Third Xiangya Hospital, with\nexpert annotations for key pathological features (including cholestasis, portal\ntract fibrosis, portal inflammation, total steatosis, macrovesicular steatosis,\nand hepatocellular ballooning). We selected nine state-of-the-art\nmultiple-instance learning (MIL) models based on the DLiPath dataset as\nbaselines for extensive comparative analysis. The experimental results\ndemonstrate that several MIL models achieve high accuracy across donor liver\nassessment indicators on DLiPath, charting a clear course for future automated\nand intelligent donor liver assessment research. Data and code are available at\nhttps://github.com/panliangrui/ACM_MM_2025."}
{"id": "2506.03186", "pdf": "https://arxiv.org/pdf/2506.03186", "abs": "https://arxiv.org/abs/2506.03186", "authors": ["Duaa Kareem Qasim", "Sabah Abdulazeez Jebur", "Lafta Raheem Ali", "Abdul Jalil M. Khalaf", "Abir Jaafar Hussain"], "title": "Lightweight Convolutional Neural Networks for Retinal Disease Classification", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG", "cs.NE"], "comment": null, "summary": "Retinal diseases such as Diabetic Retinopathy (DR) and Macular Hole (MH)\nsignificantly impact vision and affect millions worldwide. Early detection is\ncrucial, as DR, a complication of diabetes, damages retinal blood vessels,\npotentially leading to blindness, while MH disrupts central vision, affecting\ntasks like reading and facial recognition. This paper employed two lightweight\nand efficient Convolution Neural Network architectures, MobileNet and\nNASNetMobile, for the classification of Normal, DR, and MH retinal images. The\nmodels were trained on the RFMiD dataset, consisting of 3,200 fundus images,\nafter undergoing preprocessing steps such as resizing, normalization, and\naugmentation. To address data scarcity, this study leveraged transfer learning\nand data augmentation techniques, enhancing model generalization and\nperformance. The experimental results demonstrate that MobileNetV2 achieved the\nhighest accuracy of 90.8%, outperforming NASNetMobile, which achieved 89.5%\naccuracy. These findings highlight the effectiveness of CNNs in retinal disease\nclassification, providing a foundation for AI-assisted ophthalmic diagnosis and\nearly intervention."}
{"id": "2506.03188", "pdf": "https://arxiv.org/pdf/2506.03188", "abs": "https://arxiv.org/abs/2506.03188", "authors": ["Madhu Babu Sikha", "Lalith Appari", "Gurudatt Nanjanagudu Ganesh", "Amay Bandodkar", "Imon Banerjee"], "title": "Multi-Analyte, Swab-based Automated Wound Monitor with AI", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.HC"], "comment": "4 pages conference paper", "summary": "Diabetic foot ulcers (DFUs), a class of chronic wounds, affect ~750,000\nindividuals every year in the US alone and identifying non-healing DFUs that\ndevelop to chronic wounds early can drastically reduce treatment costs and\nminimize risks of amputation. There is therefore a pressing need for diagnostic\ntools that can detect non-healing DFUs early. We develop a low cost,\nmulti-analyte 3D printed assays seamlessly integrated on swabs that can\nidentify non-healing DFUs and a Wound Sensor iOS App - an innovative mobile\napplication developed for the controlled acquisition and automated analysis of\nwound sensor data. By comparing both the original base image (before exposure\nto the wound) and the wound-exposed image, we developed automated computer\nvision techniques to compare density changes between the two assay images,\nwhich allow us to automatically determine the severity of the wound. The iOS\napp ensures accurate data collection and presents actionable insights, despite\nchallenges such as variations in camera configurations and ambient conditions.\nThe proposed integrated sensor and iOS app will allow healthcare professionals\nto monitor wound conditions real-time, track healing progress, and assess\ncritical parameters related to wound care."}
{"id": "2506.03192", "pdf": "https://arxiv.org/pdf/2506.03192", "abs": "https://arxiv.org/abs/2506.03192", "authors": ["Basudha Pal", "Rama Chellappa", "Muhammad Umair"], "title": "Encoding of Demographic and Anatomical Information in Chest X-Ray-based Severe Left Ventricular Hypertrophy Classifiers", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "While echocardiography and MRI are clinical standards for evaluating cardiac\nstructure, their use is limited by cost and accessibility.We introduce a direct\nclassification framework that predicts severe left ventricular hypertrophy from\nchest X-rays, without relying on anatomical measurements or demographic inputs.\nOur approach achieves high AUROC and AUPRC, and employs Mutual Information\nNeural Estimation to quantify feature expressivity. This reveals clinically\nmeaningful attribute encoding and supports transparent model interpretation."}
{"id": "2506.03202", "pdf": "https://arxiv.org/pdf/2506.03202", "abs": "https://arxiv.org/abs/2506.03202", "authors": ["Itxasne Antnez Senz", "Ane Alberdi Aramendi", "David Dunaway", "Juling Ong", "Lara Delige", "Amparo Senz", "Anita Ahmadi Birjandi", "Noor UI Owase Jeelani", "Silvia Schievano", "Alessandro Borghi"], "title": "A combined Machine Learning and Finite Element Modelling tool for the surgical planning of craniosynostosis correction", "categories": ["eess.IV", "cs.CV", "cs.LG", "physics.med-ph"], "comment": "11 pages, 16 figures", "summary": "Craniosynostosis is a medical condition that affects the growth of babies'\nheads, caused by an early fusion of cranial sutures. In recent decades,\nsurgical treatments for craniosynostosis have significantly improved, leading\nto reduced invasiveness, faster recovery, and less blood loss. At Great Ormond\nStreet Hospital (GOSH), the main surgical treatment for patients diagnosed with\nsagittal craniosynostosis (SC) is spring assisted cranioplasty (SAC). This\nprocedure involves a 15x15 mm2 osteotomy, where two springs are inserted to\ninduce distraction. Despite the numerous advantages of this surgical technique\nfor patients, the outcome remains unpredictable due to the lack of efficient\npreoperative planning tools. The surgeon's experience and the baby's age are\ncurrently relied upon to determine the osteotomy location and spring selection.\nPrevious tools for predicting the surgical outcome of SC relied on finite\nelement modeling (FEM), which involved computed tomography (CT) imaging and\nrequired engineering expertise and lengthy calculations. The main goal of this\nresearch is to develop a real-time prediction tool for the surgical outcome of\npatients, eliminating the need for CT scans to minimise radiation exposure\nduring preoperative planning. The proposed methodology involves creating\npersonalised synthetic skulls based on three-dimensional (3D) photographs,\nincorporating population average values of suture location, skull thickness,\nand soft tissue properties. A machine learning (ML) surrogate model is employed\nto achieve the desired surgical outcome. The resulting multi-output support\nvector regressor model achieves a R2 metric of 0.95 and MSE and MAE below 0.13.\nFurthermore, in the future, this model could not only simulate various surgical\nscenarios but also provide optimal parameters for achieving a maximum cranial\nindex (CI)."}
{"id": "2506.03216", "pdf": "https://arxiv.org/pdf/2506.03216", "abs": "https://arxiv.org/abs/2506.03216", "authors": ["Arbind Agrahari Baniya", "Tsz-Kwan Lee", "Peter Eklund", "Sunil Aryal"], "title": "A Survey of Deep Learning Video Super-Resolution", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": "This paper has been published in IEEE Transactions on Emerging Topics\n  in Computational Intelligence, vol. 8, no. 4, pp. 2655-2676, Aug. 2024, doi:\n  10.1109/TETCI.2024.3398015", "summary": "Video super-resolution (VSR) is a prominent research topic in low-level\ncomputer vision, where deep learning technologies have played a significant\nrole. The rapid progress in deep learning and its applications in VSR has led\nto a proliferation of tools and techniques in the literature. However, the\nusage of these methods is often not adequately explained, and decisions are\nprimarily driven by quantitative improvements. Given the significance of VSR's\npotential influence across multiple domains, it is imperative to conduct a\ncomprehensive analysis of the elements and deep learning methodologies employed\nin VSR research. This methodical analysis will facilitate the informed\ndevelopment of models tailored to specific application needs. In this paper, we\npresent an overarching overview of deep learning-based video super-resolution\nmodels, investigating each component and discussing its implications.\nFurthermore, we provide a synopsis of key components and technologies employed\nby state-of-the-art and earlier VSR models. By elucidating the underlying\nmethodologies and categorising them systematically, we identified trends,\nrequirements, and challenges in the domain. As a first-of-its-kind survey of\ndeep learning-based VSR models, this work also establishes a multi-level\ntaxonomy to guide current and future VSR research, enhancing the maturation and\ninterpretation of VSR practices for various practical applications."}
{"id": "2506.03217", "pdf": "https://arxiv.org/pdf/2506.03217", "abs": "https://arxiv.org/abs/2506.03217", "authors": ["Pierrick Coup", "Boris Mansencal", "Floral Morandat", "Sergio Morell-Ortega", "Nicolas Villain", "Jose V. Manjn", "Vincent Planche"], "title": "petBrain: A New Pipeline for Amyloid, Tau Tangles and Neurodegeneration Quantification Using PET and MRI", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "INTRODUCTION: Quantification of amyloid plaques (A), neurofibrillary tangles\n(T2), and neurodegeneration (N) using PET and MRI is critical for Alzheimer's\ndisease (AD) diagnosis and prognosis. Existing pipelines face limitations\nregarding processing time, variability in tracer types, and challenges in\nmultimodal integration.\n  METHODS: We developed petBrain, a novel end-to-end processing pipeline for\namyloid-PET, tau-PET, and structural MRI. It leverages deep learning-based\nsegmentation, standardized biomarker quantification (Centiloid, CenTauR,\nHAVAs), and simultaneous estimation of A, T2, and N biomarkers. The pipeline is\nimplemented as a web-based platform, requiring no local computational\ninfrastructure or specialized software knowledge.\n  RESULTS: petBrain provides reliable and rapid biomarker quantification, with\nresults comparable to existing pipelines for A and T2. It shows strong\nconcordance with data processed in ADNI databases. The staging and\nquantification of A/T2/N by petBrain demonstrated good agreement with\nCSF/plasma biomarkers, clinical status, and cognitive performance.\n  DISCUSSION: petBrain represents a powerful and openly accessible platform for\nstandardized AD biomarker analysis, facilitating applications in clinical\nresearch."}
{"id": "2506.03238", "pdf": "https://arxiv.org/pdf/2506.03238", "abs": "https://arxiv.org/abs/2506.03238", "authors": ["Ziheng Zhao", "Lisong Dai", "Ya Zhang", "Yanfeng Wang", "Weidi Xie"], "title": "Rethinking Whole-Body CT Image Interpretation: An Abnormality-Centric Approach", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Automated interpretation of CT images-particularly localizing and describing\nabnormal findings across multi-plane and whole-body scans-remains a significant\nchallenge in clinical radiology. This work aims to address this challenge\nthrough four key contributions: (i) On taxonomy, we collaborate with senior\nradiologists to propose a comprehensive hierarchical classification system,\nwith 404 representative abnormal findings across all body regions; (ii) On\ndata, we contribute a dataset containing over 14.5K CT images from multiple\nplanes and all human body regions, and meticulously provide grounding\nannotations for over 19K abnormalities, each linked to the detailed description\nand cast into the taxonomy; (iii) On model development, we propose\nOminiAbnorm-CT, which can automatically ground and describe abnormal findings\non multi-plane and whole-body CT images based on text queries, while also\nallowing flexible interaction through visual prompts; (iv) On benchmarks, we\nestablish three representative evaluation tasks based on real clinical\nscenarios. Through extensive experiments, we show that OminiAbnorm-CT can\nsignificantly outperform existing methods on all the tasks and metrics."}
{"id": "2506.03317", "pdf": "https://arxiv.org/pdf/2506.03317", "abs": "https://arxiv.org/abs/2506.03317", "authors": ["Yuntian Wang", "Zafer Yilmaz", "Yuhang Li", "Edward Liu", "Eric Ahlberg", "Farid Ghahari", "Ertugrul Taciroglu", "Aydogan Ozcan"], "title": "Structural Vibration Monitoring with Diffractive Optical Processors", "categories": ["physics.optics", "cs.CV", "cs.LG", "physics.app-ph"], "comment": "33 Pages, 8 Figures, 1 Table", "summary": "Structural Health Monitoring (SHM) is vital for maintaining the safety and\nlongevity of civil infrastructure, yet current solutions remain constrained by\ncost, power consumption, scalability, and the complexity of data processing.\nHere, we present a diffractive vibration monitoring system, integrating a\njointly optimized diffractive layer with a shallow neural network-based backend\nto remotely extract 3D structural vibration spectra, offering a low-power,\ncost-effective and scalable solution. This architecture eliminates the need for\ndense sensor arrays or extensive data acquisition; instead, it uses a\nspatially-optimized passive diffractive layer that encodes 3D structural\ndisplacements into modulated light, captured by a minimal number of detectors\nand decoded in real-time by shallow and low-power neural networks to\nreconstruct the 3D displacement spectra of structures. The diffractive system's\nefficacy was demonstrated both numerically and experimentally using\nmillimeter-wave illumination on a laboratory-scale building model with a\nprogrammable shake table. Our system achieves more than an order-of-magnitude\nimprovement in accuracy over conventional optics or separately trained modules,\nestablishing a foundation for high-throughput 3D monitoring of structures.\nBeyond SHM, the 3D vibration monitoring capabilities of this cost-effective and\ndata-efficient framework establish a new computational sensing modality with\npotential applications in disaster resilience, aerospace diagnostics, and\nautonomous navigation, where energy efficiency, low latency, and\nhigh-throughput are critical."}
{"id": "2506.03355", "pdf": "https://arxiv.org/pdf/2506.03355", "abs": "https://arxiv.org/abs/2506.03355", "authors": ["Elias Abad Rocamora", "Christian Schlarmann", "Naman Deep Singh", "Yongtao Wu", "Matthias Hein", "Volkan Cevher"], "title": "Robustness in Both Domains: CLIP Needs a Robust Text Encoder", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Adversarial input attacks can cause a significant shift of CLIP embeddings.\nThis can affect the downstream robustness of models incorporating CLIP in the\npipeline, such as text-to-image generative models or large vision language\nmodels. While some efforts have been done towards making the CLIP image\nencoders robust, the robustness of text encoders remains unexplored. In this\nwork, we cover this gap in the literature. We propose LEAF: an efficient\nadversarial finetuning method for the text domain, with the ability to scale to\nlarge CLIP models. Our models significantly improve the zero-shot adversarial\naccuracy in the text domain, while maintaining the vision performance provided\nby robust image encoders. When combined with text-to-image diffusion models, we\ncan improve the generation quality under adversarial noise. When employing our\nrobust CLIP encoders in multimodal retrieval tasks, we improve the recall under\nadversarial noise over standard CLIP models. Finally, we show that robust text\nencoders facilitate better reconstruction of input text from its embedding via\ndirect optimization."}
{"id": "2506.03365", "pdf": "https://arxiv.org/pdf/2506.03365", "abs": "https://arxiv.org/abs/2506.03365", "authors": ["Artur Grigorev", "Adriana-Simona Mihaita"], "title": "Urban Visibility Hotspots: Quantifying Building Vertex Visibility from Connected Vehicle Trajectories using Spatial Indexing", "categories": ["eess.SY", "cs.CV", "cs.SY", "stat.CO"], "comment": null, "summary": "Effective placement of Out-of-Home advertising and street furniture requires\naccurate identification of locations offering maximum visual exposure to target\naudiences, particularly vehicular traffic. Traditional site selection methods\noften rely on static traffic counts or subjective assessments. This research\nintroduces a data-driven methodology to objectively quantify location\nvisibility by analyzing large-scale connected vehicle trajectory data (sourced\nfrom Compass IoT) within urban environments. We model the dynamic driver\nfield-of-view using a forward-projected visibility area for each vehicle\nposition derived from interpolated trajectories. By integrating this with\nbuilding vertex locations extracted from OpenStreetMap, we quantify the\ncumulative visual exposure, or ``visibility count'', for thousands of potential\npoints of interest near roadways. The analysis reveals that visibility is\nhighly concentrated, identifying specific ``visual hotspots'' that receive\ndisproportionately high exposure compared to average locations. The core\ntechnical contribution involves the construction of a BallTree spatial index\nover building vertices. This enables highly efficient (O(logN) complexity)\nradius queries to determine which vertices fall within the viewing circles of\nmillions of trajectory points across numerous trips, significantly\noutperforming brute-force geometric checks. Analysis reveals two key findings:\n1) Visibility is highly concentrated, identifying distinct 'visual hotspots'\nreceiving disproportionately high exposure compared to average locations. 2)\nThe aggregated visibility counts across vertices conform to a Log-Normal\ndistribution."}
{"id": "2506.03378", "pdf": "https://arxiv.org/pdf/2506.03378", "abs": "https://arxiv.org/abs/2506.03378", "authors": ["Orchid Chetia Phukan", "Mohd Mujtaba Akhtar", "Girish", "Swarup Ranjan Behera", "Abu Osama Siddiqui", "Sarthak Jain", "Priyabrata Mallick", "Jaya Sai Kiran Patibandla", "Pailla Balakrishna Reddy", "Arun Balaji Buduru", "Rajesh Sharma"], "title": "SNIFR : Boosting Fine-Grained Child Harmful Content Detection Through Audio-Visual Alignment with Cascaded Cross-Transformer", "categories": ["eess.AS", "cs.CV", "cs.MM"], "comment": "Accepted to INTERSPEECH 2025", "summary": "As video-sharing platforms have grown over the past decade, child viewership\nhas surged, increasing the need for precise detection of harmful content like\nviolence or explicit scenes. Malicious users exploit moderation systems by\nembedding unsafe content in minimal frames to evade detection. While prior\nresearch has focused on visual cues and advanced such fine-grained detection,\naudio features remain underexplored. In this study, we embed audio cues with\nvisual for fine-grained child harmful content detection and introduce SNIFR, a\nnovel framework for effective alignment. SNIFR employs a transformer encoder\nfor intra-modality interaction, followed by a cascaded cross-transformer for\ninter-modality alignment. Our approach achieves superior performance over\nunimodal and baseline fusion methods, setting a new state-of-the-art."}
{"id": "2506.03407", "pdf": "https://arxiv.org/pdf/2506.03407", "abs": "https://arxiv.org/abs/2506.03407", "authors": ["Lukas Meyer", "Josef Grn", "Maximilian Weiherer", "Bernhard Egger", "Marc Stamminger", "Linus Franke"], "title": "Multi-Spectral Gaussian Splatting with Neural Color Representation", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "We present MS-Splatting -- a multi-spectral 3D Gaussian Splatting (3DGS)\nframework that is able to generate multi-view consistent novel views from\nimages of multiple, independent cameras with different spectral domains. In\ncontrast to previous approaches, our method does not require cross-modal camera\ncalibration and is versatile enough to model a variety of different spectra,\nincluding thermal and near-infra red, without any algorithmic changes.\n  Unlike existing 3DGS-based frameworks that treat each modality separately (by\noptimizing per-channel spherical harmonics) and therefore fail to exploit the\nunderlying spectral and spatial correlations, our method leverages a novel\nneural color representation that encodes multi-spectral information into a\nlearned, compact, per-splat feature embedding. A shallow multi-layer perceptron\n(MLP) then decodes this embedding to obtain spectral color values, enabling\njoint learning of all bands within a unified representation.\n  Our experiments show that this simple yet effective strategy is able to\nimprove multi-spectral rendering quality, while also leading to improved\nper-spectra rendering quality over state-of-the-art methods. We demonstrate the\neffectiveness of this new technique in agricultural applications to render\nvegetation indices, such as normalized difference vegetation index (NDVI)."}
{"id": "2506.03408", "pdf": "https://arxiv.org/pdf/2506.03408", "abs": "https://arxiv.org/abs/2506.03408", "authors": ["Yi Xu", "Ruining Yang", "Yitian Zhang", "Yizhou Wang", "Jianglin Lu", "Mingyuan Zhang", "Lili Su", "Yun Fu"], "title": "Trajectory Prediction Meets Large Language Models: A Survey", "categories": ["cs.CL", "cs.CV"], "comment": "16 pages, GitHub:\n  https://github.com/colorfulfuture/Awesome-Trajectory-Motion-Prediction-Papers", "summary": "Recent advances in large language models (LLMs) have sparked growing interest\nin integrating language-driven techniques into trajectory prediction. By\nleveraging their semantic and reasoning capabilities, LLMs are reshaping how\nautonomous systems perceive, model, and predict trajectories. This survey\nprovides a comprehensive overview of this emerging field, categorizing recent\nwork into five directions: (1) Trajectory prediction via language modeling\nparadigms, (2) Direct trajectory prediction with pretrained language models,\n(3) Language-guided scene understanding for trajectory prediction, (4)\nLanguage-driven data generation for trajectory prediction, (5) Language-based\nreasoning and interpretability for trajectory prediction. For each, we analyze\nrepresentative methods, highlight core design choices, and identify open\nchallenges. This survey bridges natural language processing and trajectory\nprediction, offering a unified perspective on how language can enrich\ntrajectory prediction."}
{"id": "2506.03420", "pdf": "https://arxiv.org/pdf/2506.03420", "abs": "https://arxiv.org/abs/2506.03420", "authors": ["Muhammad Zubair Hasan", "Fahmida Yasmin Rifat"], "title": "Hybrid Ensemble of Segmentation-Assisted Classification and GBDT for Skin Cancer Detection with Engineered Metadata and Synthetic Lesions from ISIC 2024 Non-Dermoscopic 3D-TBP Images", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": "Written as per the requirements of CVPR 2025. It is a 8 page paper\n  without reference", "summary": "Skin cancer is among the most prevalent and life-threatening diseases\nworldwide, with early detection being critical to patient outcomes. This work\npresents a hybrid machine and deep learning-based approach for classifying\nmalignant and benign skin lesions using the SLICE-3D dataset from ISIC 2024,\nwhich comprises 401,059 cropped lesion images extracted from 3D Total Body\nPhotography (TBP), emulating non-dermoscopic, smartphone-like conditions. Our\nmethod combines vision transformers (EVA02) and our designed convolutional ViT\nhybrid (EdgeNeXtSAC) to extract robust features, employing a\nsegmentation-assisted classification pipeline to enhance lesion localization.\nPredictions from these models are fused with a gradient-boosted decision tree\n(GBDT) ensemble enriched by engineered features and patient-specific relational\nmetrics. To address class imbalance and improve generalization, we augment\nmalignant cases with Stable Diffusion-generated synthetic lesions and apply a\ndiagnosis-informed relabeling strategy to harmonize external datasets into a\n3-class format. Using partial AUC (pAUC) above 80 percent true positive rate\n(TPR) as the evaluation metric, our approach achieves a pAUC of 0.1755 -- the\nhighest among all configurations. These results underscore the potential of\nhybrid, interpretable AI systems for skin cancer triage in telemedicine and\nresource-constrained settings."}
{"id": "2506.03478", "pdf": "https://arxiv.org/pdf/2506.03478", "abs": "https://arxiv.org/abs/2506.03478", "authors": ["Yuxuan Han", "Junfeng Lyu", "Kuan Sheng", "Minghao Que", "Qixuan Zhang", "Lan Xu", "Feng Xu"], "title": "Facial Appearance Capture at Home with Patch-Level Reflectance Prior", "categories": ["cs.GR", "cs.CV"], "comment": "ACM Transactions on Graphics (Proc. of SIGGRAPH), 2025. Code:\n  https://github.com/yxuhan/DoRA; Project Page: https://yxuhan.github.io/DoRA", "summary": "Existing facial appearance capture methods can reconstruct plausible facial\nreflectance from smartphone-recorded videos. However, the reconstruction\nquality is still far behind the ones based on studio recordings. This paper\nfills the gap by developing a novel daily-used solution with a co-located\nsmartphone and flashlight video capture setting in a dim room. To enhance the\nquality, our key observation is to solve facial reflectance maps within the\ndata distribution of studio-scanned ones. Specifically, we first learn a\ndiffusion prior over the Light Stage scans and then steer it to produce the\nreflectance map that best matches the captured images. We propose to train the\ndiffusion prior at the patch level to improve generalization ability and\ntraining stability, as current Light Stage datasets are in ultra-high\nresolution but limited in data size. Tailored to this prior, we propose a\npatch-level posterior sampling technique to sample seamless full-resolution\nreflectance maps from this patch-level diffusion model. Experiments demonstrate\nour method closes the quality gap between low-cost and studio recordings by a\nlarge margin, opening the door for everyday users to clone themselves to the\ndigital world. Our code will be released at https://github.com/yxuhan/DoRA."}
{"id": "2506.03530", "pdf": "https://arxiv.org/pdf/2506.03530", "abs": "https://arxiv.org/abs/2506.03530", "authors": ["Guanzhou Ke", "Yi Xie", "Xiaoli Wang", "Guoqing Chao", "Bo Wang", "Shengfeng He"], "title": "How Far Are We from Predicting Missing Modalities with Foundation Models?", "categories": ["cs.MM", "cs.CL", "cs.CV"], "comment": null, "summary": "Multimodal foundation models have demonstrated impressive capabilities across\ndiverse tasks. However, their potential as plug-and-play solutions for missing\nmodality prediction remains underexplored. To investigate this, we categorize\nexisting approaches into three representative paradigms, encompassing a total\nof 42 model variants, and conduct a comprehensive evaluation in terms of\nprediction accuracy and adaptability to downstream tasks. Our analysis reveals\nthat current foundation models often fall short in two critical aspects: (i)\nfine-grained semantic extraction from the available modalities, and (ii) robust\nvalidation of generated modalities. These limitations lead to suboptimal and,\nat times, misaligned predictions. To address these challenges, we propose an\nagentic framework tailored for missing modality prediction. This framework\ndynamically formulates modality-aware mining strategies based on the input\ncontext, facilitating the extraction of richer and more discriminative semantic\nfeatures. In addition, we introduce a \\textit{self-refinement mechanism}, which\niteratively verifies and enhances the quality of generated modalities through\ninternal feedback. Experimental results show that our method reduces FID for\nmissing image prediction by at least 14% and MER for missing text prediction by\nat least 10% compared to baselines."}
{"id": "2506.03594", "pdf": "https://arxiv.org/pdf/2506.03594", "abs": "https://arxiv.org/abs/2506.03594", "authors": ["Shengjie Lin", "Jiading Fang", "Muhammad Zubair Irshad", "Vitor Campagnolo Guizilini", "Rares Andrei Ambrus", "Greg Shakhnarovich", "Matthew R. Walter"], "title": "SplArt: Articulation Estimation and Part-Level Reconstruction with 3D Gaussian Splatting", "categories": ["cs.GR", "cs.CV", "cs.LG", "cs.MM", "cs.RO"], "comment": "https://github.com/ripl/splart", "summary": "Reconstructing articulated objects prevalent in daily environments is crucial\nfor applications in augmented/virtual reality and robotics. However, existing\nmethods face scalability limitations (requiring 3D supervision or costly\nannotations), robustness issues (being susceptible to local optima), and\nrendering shortcomings (lacking speed or photorealism). We introduce SplArt, a\nself-supervised, category-agnostic framework that leverages 3D Gaussian\nSplatting (3DGS) to reconstruct articulated objects and infer kinematics from\ntwo sets of posed RGB images captured at different articulation states,\nenabling real-time photorealistic rendering for novel viewpoints and\narticulations. SplArt augments 3DGS with a differentiable mobility parameter\nper Gaussian, achieving refined part segmentation. A multi-stage optimization\nstrategy is employed to progressively handle reconstruction, part segmentation,\nand articulation estimation, significantly enhancing robustness and accuracy.\nSplArt exploits geometric self-supervision, effectively addressing challenging\nscenarios without requiring 3D annotations or category-specific priors.\nEvaluations on established and newly proposed benchmarks, along with\napplications to real-world scenarios using a handheld RGB camera, demonstrate\nSplArt's state-of-the-art performance and real-world practicality. Code is\npublicly available at https://github.com/ripl/splart."}
{"id": "2506.03665", "pdf": "https://arxiv.org/pdf/2506.03665", "abs": "https://arxiv.org/abs/2506.03665", "authors": ["Hernn Maina", "Guido Ivetta", "Mateo Lione Stuto", "Julian Martin Eisenschlos", "Jorge Snchez", "Luciana Benotti"], "title": "ROSA: Addressing text understanding challenges in photographs via ROtated SAmpling", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Visually impaired people could benefit from Visual Question Answering (VQA)\nsystems to interpret text in their surroundings. However, current models often\nstruggle with recognizing text in the photos taken by this population. Through\nin-depth interviews with visually impaired individuals, we identified common\nframing conventions that frequently result in misaligned text. Existing VQA\nbenchmarks primarily feature well-oriented text captured by sighted users,\nunder-representing these challenges. To address this gap, we introduce ROtated\nSAmpling (ROSA), a decoding strategy that enhances VQA performance in text-rich\nimages with incorrectly oriented text. ROSA outperforms Greedy decoding by 11.7\nabsolute points in the best-performing model."}
{"id": "2506.03792", "pdf": "https://arxiv.org/pdf/2506.03792", "abs": "https://arxiv.org/abs/2506.03792", "authors": ["Qianwei Qu", "Christian M. Schleptz", "Marco Stampanoni"], "title": "Analytical Reconstruction of Periodically Deformed Objects in Time-resolved CT", "categories": ["physics.med-ph", "cs.CV"], "comment": null, "summary": "Time-resolved CT is an advanced measurement technique that has been widely\nused to observe dynamic objects, including periodically varying structures such\nas hearts, lungs, or hearing structures. To reconstruct these objects from CT\nprojections, a common approach is to divide the projections into several\ncollections based on their motion phases and perform reconstruction within each\ncollection, assuming they originate from a static object. This describes the\ngating-based method, which is the standard approach for time-periodic\nreconstruction. However, the gating-based reconstruction algorithm only\nutilizes a limited subset of projections within each collection and ignores the\ncorrelation between different collections, leading to inefficient use of the\nradiation dose. To address this issue, we propose two analytical reconstruction\npipelines in this paper, and validate them with experimental data captured\nusing tomographic synchrotron microscopy. We demonstrate that our approaches\nsignificantly reduce random noise in the reconstructed images without blurring\nthe sharp features of the observed objects. Equivalently, our methods can\nachieve the same reconstruction quality as gating-based methods but with a\nlower radiation dose. Our code is available at github.com/PeriodRecon."}
{"id": "2506.03804", "pdf": "https://arxiv.org/pdf/2506.03804", "abs": "https://arxiv.org/abs/2506.03804", "authors": ["George Webber", "Alexander Hammers", "Andrew P. King", "Andrew J. Reader"], "title": "Personalized MR-Informed Diffusion Models for 3D PET Image Reconstruction", "categories": ["physics.med-ph", "cs.CV"], "comment": "10 pages, 10 figures", "summary": "Recent work has shown improved lesion detectability and flexibility to\nreconstruction hyperparameters (e.g. scanner geometry or dose level) when PET\nimages are reconstructed by leveraging pre-trained diffusion models. Such\nmethods train a diffusion model (without sinogram data) on high-quality, but\nstill noisy, PET images. In this work, we propose a simple method for\ngenerating subject-specific PET images from a dataset of multi-subject PET-MR\nscans, synthesizing \"pseudo-PET\" images by transforming between different\npatients' anatomy using image registration. The images we synthesize retain\ninformation from the subject's MR scan, leading to higher resolution and the\nretention of anatomical features compared to the original set of PET images.\nWith simulated and real [$^{18}$F]FDG datasets, we show that pre-training a\npersonalized diffusion model with subject-specific \"pseudo-PET\" images improves\nreconstruction accuracy with low-count data. In particular, the method shows\npromise in combining information from a guidance MR scan without overly\nimposing anatomical features, demonstrating an improved trade-off between\nreconstructing PET-unique image features versus features present in both PET\nand MR. We believe this approach for generating and utilizing synthetic data\nhas further applications to medical imaging tasks, particularly because\npatient-specific PET images can be generated without resorting to generative\ndeep learning or large training datasets."}
{"id": "2506.03884", "pdf": "https://arxiv.org/pdf/2506.03884", "abs": "https://arxiv.org/abs/2506.03884", "authors": ["Utkarsh Pathak", "Chandra Sai Krishna Gunda", "Anusha Prakash", "Keshav Agarwal", "Hema A. Murthy"], "title": "Kinship in Speech: Leveraging Linguistic Relatedness for Zero-Shot TTS in Indian Languages", "categories": ["cs.CL", "cs.CV", "I.5.4"], "comment": "Accepted at INTERSPEECH 2025", "summary": "Text-to-speech (TTS) systems typically require high-quality studio data and\naccurate transcriptions for training. India has 1369 languages, with 22\nofficial using 13 scripts. Training a TTS system for all these languages, most\nof which have no digital resources, seems a Herculean task. Our work focuses on\nzero-shot synthesis, particularly for languages whose scripts and phonotactics\ncome from different families. The novelty of our work is in the augmentation of\na shared phone representation and modifying the text parsing rules to match the\nphonotactics of the target language, thus reducing the synthesiser overhead and\nenabling rapid adaptation. Intelligible and natural speech was generated for\nSanskrit, Maharashtrian and Canara Konkani, Maithili and Kurukh by leveraging\nlinguistic connections across languages with suitable synthesisers. Evaluations\nconfirm the effectiveness of this approach, highlighting its potential to\nexpand speech technology access for under-represented languages."}
{"id": "2506.03890", "pdf": "https://arxiv.org/pdf/2506.03890", "abs": "https://arxiv.org/abs/2506.03890", "authors": ["Christian Tinauer", "Maximilian Sackl", "Stefan Ropele", "Christian Langkammer"], "title": "Identifying Alzheimer's Disease Prediction Strategies of Convolutional Neural Network Classifiers using R2* Maps and Spectral Clustering", "categories": ["eess.IV", "cs.CV"], "comment": "Accepted for the conference EUSIPCO2025 (https://eusipco2025.org/)", "summary": "Deep learning models have shown strong performance in classifying Alzheimer's\ndisease (AD) from R2* maps, but their decision-making remains opaque, raising\nconcerns about interpretability. Previous studies suggest biases in model\ndecisions, necessitating further analysis. This study uses Layer-wise Relevance\nPropagation (LRP) and spectral clustering to explore classifier decision\nstrategies across preprocessing and training configurations using R2* maps. We\ntrained a 3D convolutional neural network on R2* maps, generating relevance\nheatmaps via LRP and applied spectral clustering to identify dominant patterns.\nt-Stochastic Neighbor Embedding (t-SNE) visualization was used to assess\nclustering structure. Spectral clustering revealed distinct decision patterns,\nwith the relevance-guided model showing the clearest separation between AD and\nnormal control (NC) cases. The t-SNE visualization confirmed that this model\naligned heatmap groupings with the underlying subject groups. Our findings\nhighlight the significant impact of preprocessing and training choices on deep\nlearning models trained on R2* maps, even with similar performance metrics.\nSpectral clustering offers a structured method to identify classification\nstrategy differences, emphasizing the importance of explainability in medical\nAI."}
{"id": "2506.03951", "pdf": "https://arxiv.org/pdf/2506.03951", "abs": "https://arxiv.org/abs/2506.03951", "authors": ["Aojun Lu", "Hangjie Yuan", "Tao Feng", "Yanan Sun"], "title": "Rethinking the Stability-Plasticity Trade-off in Continual Learning from an Architectural Perspective", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "The quest for Continual Learning (CL) seeks to empower neural networks with\nthe ability to learn and adapt incrementally. Central to this pursuit is\naddressing the stability-plasticity dilemma, which involves striking a balance\nbetween two conflicting objectives: preserving previously learned knowledge and\nacquiring new knowledge. While numerous CL methods aim to achieve this\ntrade-off, they often overlook the impact of network architecture on stability\nand plasticity, restricting the trade-off to the parameter level. In this\npaper, we delve into the conflict between stability and plasticity at the\narchitectural level. We reveal that under an equal parameter constraint, deeper\nnetworks exhibit better plasticity, while wider networks are characterized by\nsuperior stability. To address this architectural-level dilemma, we introduce a\nnovel framework denoted Dual-Arch, which serves as a plug-in component for CL.\nThis framework leverages the complementary strengths of two distinct and\nindependent networks: one dedicated to plasticity and the other to stability.\nEach network is designed with a specialized and lightweight architecture,\ntailored to its respective objective. Extensive experiments demonstrate that\nDual-Arch enhances the performance of existing CL methods while being up to 87%\nmore compact in terms of parameters."}
{"id": "2506.03956", "pdf": "https://arxiv.org/pdf/2506.03956", "abs": "https://arxiv.org/abs/2506.03956", "authors": ["Aojun Lu", "Tao Feng", "Hangjie Yuan", "Chunhui Ding", "Yanan Sun"], "title": "Adapt before Continual Learning", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Continual Learning (CL) seeks to enable neural networks to incrementally\nacquire new knowledge (plasticity) while retaining existing knowledge\n(stability). While pre-trained models (PTMs) have become pivotal in CL,\nprevailing approaches freeze the PTM backbone to preserve stability, limiting\ntheir plasticity, particularly when encountering significant domain gaps in\nincremental tasks. Conversely, sequentially finetuning the entire PTM risks\ncatastrophic forgetting of generalizable knowledge, exposing a critical\nstability-plasticity trade-off. To address this challenge, we propose Adapting\nPTMs before the core CL process (ACL), a novel framework that refines the PTM\nbackbone through a plug-and-play adaptation phase before learning each new task\nwith existing CL approaches (e.g., prompt tuning). ACL enhances plasticity by\naligning embeddings with their original class prototypes while distancing them\nfrom others, theoretically and empirically shown to balance stability and\nplasticity. Extensive experiments demonstrate that ACL significantly improves\nCL performance across benchmarks and integrated methods, offering a versatile\nsolution for PTM-based CL."}
{"id": "2506.03979", "pdf": "https://arxiv.org/pdf/2506.03979", "abs": "https://arxiv.org/abs/2506.03979", "authors": ["Haoxuan Chen", "Yinuo Ren", "Martin Renqiang Min", "Lexing Ying", "Zachary Izzo"], "title": "Solving Inverse Problems via Diffusion-Based Priors: An Approximation-Free Ensemble Sampling Approach", "categories": ["cs.LG", "cs.CV", "cs.NA", "eess.IV", "math.NA", "stat.ML"], "comment": "45 pages", "summary": "Diffusion models (DMs) have proven to be effective in modeling\nhigh-dimensional distributions, leading to their widespread adoption for\nrepresenting complex priors in Bayesian inverse problems (BIPs). However,\ncurrent DM-based posterior sampling methods proposed for solving common BIPs\nrely on heuristic approximations to the generative process. To exploit the\ngenerative capability of DMs and avoid the usage of such approximations, we\npropose an ensemble-based algorithm that performs posterior sampling without\nthe use of heuristic approximations. Our algorithm is motivated by existing\nworks that combine DM-based methods with the sequential Monte Carlo (SMC)\nmethod. By examining how the prior evolves through the diffusion process\nencoded by the pre-trained score function, we derive a modified partial\ndifferential equation (PDE) governing the evolution of the corresponding\nposterior distribution. This PDE includes a modified diffusion term and a\nreweighting term, which can be simulated via stochastic weighted particle\nmethods. Theoretically, we prove that the error between the true posterior\ndistribution can be bounded in terms of the training error of the pre-trained\nscore function and the number of particles in the ensemble. Empirically, we\nvalidate our algorithm on several inverse problems in imaging to show that our\nmethod gives more accurate reconstructions compared to existing DM-based\nmethods."}
{"id": "2506.03990", "pdf": "https://arxiv.org/pdf/2506.03990", "abs": "https://arxiv.org/abs/2506.03990", "authors": ["Hongzhi Zhang", "Jingyuan Zhang", "Xingguang Ji", "Qi Wang", "Fuzheng Zhang"], "title": "DynTok: Dynamic Compression of Visual Tokens for Efficient and Effective Video Understanding", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Typical video modeling methods, such as LLava, represent videos as sequences\nof visual tokens, which are then processed by the LLM backbone for effective\nvideo understanding. However, this approach leads to a massive number of visual\ntokens, especially for long videos. A practical solution is to first extract\nrelevant visual information from the large visual context before feeding it\ninto the LLM backbone, thereby reducing computational overhead. In this work,\nwe introduce DynTok, a novel \\textbf{Dyn}amic video \\textbf{Tok}en compression\nstrategy. DynTok adaptively splits visual tokens into groups and merges them\nwithin each group, achieving high compression in regions with low information\ndensity while preserving essential content. Our method reduces the number of\ntokens to 44.4% of the original size while maintaining comparable performance.\nIt further benefits from increasing the number of video frames and achieves\n65.3% on Video-MME and 72.5% on MLVU. By applying this simple yet effective\ncompression method, we expose the redundancy in video token representations and\noffer insights for designing more efficient video modeling techniques."}
{"id": "2506.03994", "pdf": "https://arxiv.org/pdf/2506.03994", "abs": "https://arxiv.org/abs/2506.03994", "authors": ["Dan Oneata", "Desmond Elliott", "Stella Frank"], "title": "Seeing What Tastes Good: Revisiting Multimodal Distributional Semantics in the Billion Parameter Era", "categories": ["cs.CL", "cs.CV"], "comment": "ACL Findings 2025", "summary": "Human learning and conceptual representation is grounded in sensorimotor\nexperience, in contrast to state-of-the-art foundation models. In this paper,\nwe investigate how well such large-scale models, trained on vast quantities of\ndata, represent the semantic feature norms of concrete object concepts, e.g. a\nROSE is red, smells sweet, and is a flower. More specifically, we use probing\ntasks to test which properties of objects these models are aware of. We\nevaluate image encoders trained on image data alone, as well as\nmultimodally-trained image encoders and language-only models, on predicting an\nextended denser version of the classic McRae norms and the newer Binder dataset\nof attribute ratings. We find that multimodal image encoders slightly\noutperform language-only approaches, and that image-only encoders perform\ncomparably to the language models, even on non-visual attributes that are\nclassified as \"encyclopedic\" or \"function\". These results offer new insights\ninto what can be learned from pure unimodal learning, and the complementarity\nof the modalities."}
{"id": "2506.04016", "pdf": "https://arxiv.org/pdf/2506.04016", "abs": "https://arxiv.org/abs/2506.04016", "authors": ["Adam Ranon", "Ulysse Ranon", "Tomislav Ivek", "Ivan Balog"], "title": "Dreaming up scale invariance via inverse renormalization group", "categories": ["cond-mat.stat-mech", "cs.CV", "cs.LG"], "comment": "v1: 12 pages, 11 figures, 55 references", "summary": "We explore how minimal neural networks can invert the renormalization group\n(RG) coarse-graining procedure in the two-dimensional Ising model, effectively\n\"dreaming up\" microscopic configurations from coarse-grained states. This\ntask-formally impossible at the level of configurations-can be approached\nprobabilistically, allowing machine learning models to reconstruct\nscale-invariant distributions without relying on microscopic input. We\ndemonstrate that even neural networks with as few as three trainable parameters\ncan learn to generate critical configurations, reproducing the scaling behavior\nof observables such as magnetic susceptibility, heat capacity, and Binder\nratios. A real-space renormalization group analysis of the generated\nconfigurations confirms that the models capture not only scale invariance but\nalso reproduce nontrivial eigenvalues of the RG transformation. Surprisingly,\nwe find that increasing network complexity by introducing multiple layers\noffers no significant benefit. These findings suggest that simple local rules,\nakin to those generating fractal structures, are sufficient to encode the\nuniversality of critical phenomena, opening the door to efficient generative\nmodels of statistical ensembles in physics."}
{"id": "2506.04030", "pdf": "https://arxiv.org/pdf/2506.04030", "abs": "https://arxiv.org/abs/2506.04030", "authors": ["Olivier Jaubert", "Salman Mohammadi", "Keith A. Goatman", "Shadia S. Mikhael", "Conor Bradley", "Rebecca Hughes", "Richard Good", "John H. Hipwell", "Sonia Dahdouh"], "title": "Conformal coronary calcification volume estimation with conditional coverage via histogram clustering", "categories": ["eess.IV", "cs.CV"], "comment": "IEEE 22nd International Symposium on Biomedical Imaging (ISBI)", "summary": "Incidental detection and quantification of coronary calcium in CT scans could\nlead to the early introduction of lifesaving clinical interventions. However,\nover-reporting could negatively affect patient wellbeing and unnecessarily\nburden the medical system. Therefore, careful considerations should be taken\nwhen automatically reporting coronary calcium scores. A cluster-based\nconditional conformal prediction framework is proposed to provide score\nintervals with calibrated coverage from trained segmentation networks without\nretraining. The proposed method was tuned and used to calibrate predictive\nintervals for 3D UNet models (deterministic, MCDropout and deep ensemble)\nreaching similar coverage with better triage metrics compared to conventional\nconformal prediction. Meaningful predictive intervals of calcium scores could\nhelp triage patients according to the confidence of their risk category\nprediction."}
{"id": "2506.04058", "pdf": "https://arxiv.org/pdf/2506.04058", "abs": "https://arxiv.org/abs/2506.04058", "authors": ["Bulat Maksudov", "Kathleen Curran", "Alessandra Mileo"], "title": "Towards generating more interpretable counterfactuals via concept vectors: a preliminary study on chest X-rays", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "An essential step in deploying medical imaging models is ensuring alignment\nwith clinical knowledge and interpretability. We focus on mapping clinical\nconcepts into the latent space of generative models to identify Concept\nActivation Vectors (CAVs). Using a simple reconstruction autoencoder, we link\nuser-defined concepts to image-level features without explicit label training.\nThe extracted concepts are stable across datasets, enabling visual explanations\nthat highlight clinically relevant features. By traversing latent space along\nconcept directions, we produce counterfactuals that exaggerate or reduce\nspecific clinical features. Preliminary results on chest X-rays show promise\nfor large pathologies like cardiomegaly, while smaller pathologies remain\nchallenging due to reconstruction limits. Although not outperforming baselines,\nthis approach offers a path toward interpretable, concept-based explanations\naligned with clinical knowledge."}
{"id": "2506.04071", "pdf": "https://arxiv.org/pdf/2506.04071", "abs": "https://arxiv.org/abs/2506.04071", "authors": ["Luiz Manella Pereira", "M. Hadi Amini"], "title": "Optimal Transport-based Domain Alignment as a Preprocessing Step for Federated Learning", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Federated learning (FL) is a subfield of machine learning that avoids sharing\nlocal data with a central server, which can enhance privacy and scalability.\nThe inability to consolidate data leads to a unique problem called dataset\nimbalance, where agents in a network do not have equal representation of the\nlabels one is trying to learn to predict. In FL, fusing locally-trained models\nwith unbalanced datasets may deteriorate the performance of global model\naggregation, and reduce the quality of updated local models and the accuracy of\nthe distributed agents' decisions. In this work, we introduce an Optimal\nTransport-based preprocessing algorithm that aligns the datasets by minimizing\nthe distributional discrepancy of data along the edge devices. We accomplish\nthis by leveraging Wasserstein barycenters when computing channel-wise\naverages. These barycenters are collected in a trusted central server where\nthey collectively generate a target RGB space. By projecting our dataset\ntowards this target space, we minimize the distributional discrepancy on a\nglobal level, which facilitates the learning process due to a minimization of\nvariance across the samples. We demonstrate the capabilities of the proposed\napproach over the CIFAR-10 dataset, where we show its capability of reaching\nhigher degrees of generalization in fewer communication rounds."}
{"id": "2506.04088", "pdf": "https://arxiv.org/pdf/2506.04088", "abs": "https://arxiv.org/abs/2506.04088", "authors": ["Jun-Peng Jiang", "Yu Xia", "Hai-Long Sun", "Shiyin Lu", "Qing-Guo Chen", "Weihua Luo", "Kaifu Zhang", "De-Chuan Zhan", "Han-Jia Ye"], "title": "Multimodal Tabular Reasoning with Privileged Structured Information", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Tabular reasoning involves multi-step information extraction and logical\ninference over tabular data. While recent advances have leveraged large\nlanguage models (LLMs) for reasoning over structured tables, such high-quality\ntextual representations are often unavailable in real-world settings, where\ntables typically appear as images. In this paper, we tackle the task of tabular\nreasoning from table images, leveraging privileged structured information\navailable during training to enhance multimodal large language models (MLLMs).\nThe key challenges lie in the complexity of accurately aligning structured\ninformation with visual representations, and in effectively transferring\nstructured reasoning skills to MLLMs despite the input modality gap. To address\nthese, we introduce TabUlar Reasoning with Bridged infOrmation ({\\sc Turbo}), a\nnew framework for multimodal tabular reasoning with privileged structured\ntables. {\\sc Turbo} benefits from a structure-aware reasoning trace generator\nbased on DeepSeek-R1, contributing to high-quality modality-bridged data. On\nthis basis, {\\sc Turbo} repeatedly generates and selects the advantageous\nreasoning paths, further enhancing the model's tabular reasoning ability.\nExperimental results demonstrate that, with limited ($9$k) data, {\\sc Turbo}\nachieves state-of-the-art performance ($+7.2\\%$ vs. previous SOTA) across\nmultiple datasets."}
{"id": "2506.04116", "pdf": "https://arxiv.org/pdf/2506.04116", "abs": "https://arxiv.org/abs/2506.04116", "authors": ["Xuanru Zhou", "Jiarun Liu", "Shoujun Yu", "Hao Yang", "Cheng Li", "Tao Tan", "Shanshan Wang"], "title": "A Diffusion-Driven Temporal Super-Resolution and Spatial Consistency Enhancement Framework for 4D MRI imaging", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "In medical imaging, 4D MRI enables dynamic 3D visualization, yet the\ntrade-off between spatial and temporal resolution requires prolonged scan time\nthat can compromise temporal fidelity--especially during rapid, large-amplitude\nmotion. Traditional approaches typically rely on registration-based\ninterpolation to generate intermediate frames. However, these methods struggle\nwith large deformations, resulting in misregistration, artifacts, and\ndiminished spatial consistency. To address these challenges, we propose\nTSSC-Net, a novel framework that generates intermediate frames while preserving\nspatial consistency. To improve temporal fidelity under fast motion, our\ndiffusion-based temporal super-resolution network generates intermediate frames\nusing the start and end frames as key references, achieving 6x temporal\nsuper-resolution in a single inference step. Additionally, we introduce a novel\ntri-directional Mamba-based module that leverages long-range contextual\ninformation to effectively resolve spatial inconsistencies arising from\ncross-slice misalignment, thereby enhancing volumetric coherence and correcting\ncross-slice errors. Extensive experiments were performed on the public ACDC\ncardiac MRI dataset and a real-world dynamic 4D knee joint dataset. The results\ndemonstrate that TSSC-Net can generate high-resolution dynamic MRI from\nfast-motion data while preserving structural fidelity and spatial consistency."}
{"id": "2506.04121", "pdf": "https://arxiv.org/pdf/2506.04121", "abs": "https://arxiv.org/abs/2506.04121", "authors": ["Loan Dao", "Ngoc Quoc Ly"], "title": "A Comprehensive Study on Medical Image Segmentation using Deep Neural Networks", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Over the past decade, Medical Image Segmentation (MIS) using Deep Neural\nNetworks (DNNs) has achieved significant performance improvements and holds\ngreat promise for future developments. This paper presents a comprehensive\nstudy on MIS based on DNNs. Intelligent Vision Systems are often evaluated\nbased on their output levels, such as Data, Information, Knowledge,\nIntelligence, and Wisdom (DIKIW),and the state-of-the-art solutions in MIS at\nthese levels are the focus of research. Additionally, Explainable Artificial\nIntelligence (XAI) has become an important research direction, as it aims to\nuncover the \"black box\" nature of previous DNN architectures to meet the\nrequirements of transparency and ethics. The study emphasizes the importance of\nMIS in disease diagnosis and early detection, particularly for increasing the\nsurvival rate of cancer patients through timely diagnosis. XAI and early\nprediction are considered two important steps in the journey from\n\"intelligence\" to \"wisdom.\" Additionally, the paper addresses existing\nchallenges and proposes potential solutions to enhance the efficiency of\nimplementing DNN-based MIS."}
{"id": "2506.04129", "pdf": "https://arxiv.org/pdf/2506.04129", "abs": "https://arxiv.org/abs/2506.04129", "authors": ["Loan Dao", "Ngoc Quoc Ly"], "title": "Recent Advances in Medical Image Classification", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Medical image classification is crucial for diagnosis and treatment,\nbenefiting significantly from advancements in artificial intelligence. The\npaper reviews recent progress in the field, focusing on three levels of\nsolutions: basic, specific, and applied. It highlights advances in traditional\nmethods using deep learning models like Convolutional Neural Networks and\nVision Transformers, as well as state-of-the-art approaches with Vision\nLanguage Models. These models tackle the issue of limited labeled data, and\nenhance and explain predictive results through Explainable Artificial\nIntelligence."}
{"id": "2506.04207", "pdf": "https://arxiv.org/pdf/2506.04207", "abs": "https://arxiv.org/abs/2506.04207", "authors": ["Shuang Chen", "Yue Guo", "Zhaochen Su", "Yafu Li", "Yulun Wu", "Jiacheng Chen", "Jiayu Chen", "Weijie Wang", "Xiaoye Qu", "Yu Cheng"], "title": "Advancing Multimodal Reasoning: From Optimized Cold Start to Staged Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": "19 pages, 6 figures", "summary": "Inspired by the remarkable reasoning capabilities of Deepseek-R1 in complex\ntextual tasks, many works attempt to incentivize similar capabilities in\nMultimodal Large Language Models (MLLMs) by directly applying reinforcement\nlearning (RL). However, they still struggle to activate complex reasoning. In\nthis paper, rather than examining multimodal RL in isolation, we delve into\ncurrent training pipelines and identify three crucial phenomena: 1) Effective\ncold start initialization is critical for enhancing MLLM reasoning.\nIntriguingly, we find that initializing with carefully selected text data alone\ncan lead to performance surpassing many recent multimodal reasoning models,\neven before multimodal RL. 2) Standard GRPO applied to multimodal RL suffers\nfrom gradient stagnation, which degrades training stability and performance. 3)\nSubsequent text-only RL training, following the multimodal RL phase, further\nenhances multimodal reasoning. This staged training approach effectively\nbalances perceptual grounding and cognitive reasoning development. By\nincorporating the above insights and addressing multimodal RL issues, we\nintroduce ReVisual-R1, achieving a new state-of-the-art among open-source 7B\nMLLMs on challenging benchmarks including MathVerse, MathVision, WeMath,\nLogicVista, DynaMath, and challenging AIME2024 and AIME2025."}
{"id": "2506.04218", "pdf": "https://arxiv.org/pdf/2506.04218", "abs": "https://arxiv.org/abs/2506.04218", "authors": ["Wei Cao", "Marcel Hallgarten", "Tianyu Li", "Daniel Dauner", "Xunjiang Gu", "Caojun Wang", "Yakov Miron", "Marco Aiello", "Hongyang Li", "Igor Gilitschenski", "Boris Ivanovic", "Marco Pavone", "Andreas Geiger", "Kashyap Chitta"], "title": "Pseudo-Simulation for Autonomous Driving", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Existing evaluation paradigms for Autonomous Vehicles (AVs) face critical\nlimitations. Real-world evaluation is often challenging due to safety concerns\nand a lack of reproducibility, whereas closed-loop simulation can face\ninsufficient realism or high computational costs. Open-loop evaluation, while\nbeing efficient and data-driven, relies on metrics that generally overlook\ncompounding errors. In this paper, we propose pseudo-simulation, a novel\nparadigm that addresses these limitations. Pseudo-simulation operates on real\ndatasets, similar to open-loop evaluation, but augments them with synthetic\nobservations generated prior to evaluation using 3D Gaussian Splatting. Our key\nidea is to approximate potential future states the AV might encounter by\ngenerating a diverse set of observations that vary in position, heading, and\nspeed. Our method then assigns a higher importance to synthetic observations\nthat best match the AV's likely behavior using a novel proximity-based\nweighting scheme. This enables evaluating error recovery and the mitigation of\ncausal confusion, as in closed-loop benchmarks, without requiring sequential\ninteractive simulation. We show that pseudo-simulation is better correlated\nwith closed-loop simulations (R^2=0.8) than the best existing open-loop\napproach (R^2=0.7). We also establish a public leaderboard for the community to\nbenchmark new methodologies with pseudo-simulation. Our code is available at\nhttps://github.com/autonomousvision/navsim."}
{"id": "2506.04227", "pdf": "https://arxiv.org/pdf/2506.04227", "abs": "https://arxiv.org/abs/2506.04227", "authors": ["Zhao-Heng Yin", "Sherry Yang", "Pieter Abbeel"], "title": "Object-centric 3D Motion Field for Robot Learning from Human Videos", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "cs.SY", "eess.SY"], "comment": "Project: https://zhaohengyin.github.io/3DMF", "summary": "Learning robot control policies from human videos is a promising direction\nfor scaling up robot learning. However, how to extract action knowledge (or\naction representations) from videos for policy learning remains a key\nchallenge. Existing action representations such as video frames, pixelflow, and\npointcloud flow have inherent limitations such as modeling complexity or loss\nof information. In this paper, we propose to use object-centric 3D motion field\nto represent actions for robot learning from human videos, and present a novel\nframework for extracting this representation from videos for zero-shot control.\nWe introduce two novel components in its implementation. First, a novel\ntraining pipeline for training a ''denoising'' 3D motion field estimator to\nextract fine object 3D motions from human videos with noisy depth robustly.\nSecond, a dense object-centric 3D motion field prediction architecture that\nfavors both cross-embodiment transfer and policy generalization to background.\nWe evaluate the system in real world setups. Experiments show that our method\nreduces 3D motion estimation error by over 50% compared to the latest method,\nachieve 55% average success rate in diverse tasks where prior approaches\nfail~($\\lesssim 10$\\%), and can even acquire fine-grained manipulation skills\nlike insertion."}
