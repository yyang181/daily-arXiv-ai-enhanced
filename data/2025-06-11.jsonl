{"id": "2506.08048", "pdf": "https://arxiv.org/pdf/2506.08048", "abs": "https://arxiv.org/abs/2506.08048", "authors": ["Zheng Han", "Jun Zhou", "Jialun Pei", "Jing Qin", "Yingfang Fan", "Qi Dou"], "title": "Towards Reliable AR-Guided Surgical Navigation: Interactive Deformation Modeling with Data-Driven Biomechanics and Prompts", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": null, "summary": "In augmented reality (AR)-guided surgical navigation, preoperative organ\nmodels are superimposed onto the patient's intraoperative anatomy to visualize\ncritical structures such as vessels and tumors. Accurate deformation modeling\nis essential to maintain the reliability of AR overlays by ensuring alignment\nbetween preoperative models and the dynamically changing anatomy. Although the\nfinite element method (FEM) offers physically plausible modeling, its high\ncomputational cost limits intraoperative applicability. Moreover, existing\nalgorithms often fail to handle large anatomical changes, such as those induced\nby pneumoperitoneum or ligament dissection, leading to inaccurate anatomical\ncorrespondences and compromised AR guidance. To address these challenges, we\npropose a data-driven biomechanics algorithm that preserves FEM-level accuracy\nwhile improving computational efficiency. In addition, we introduce a novel\nhuman-in-the-loop mechanism into the deformation modeling process. This enables\nsurgeons to interactively provide prompts to correct anatomical misalignments,\nthereby incorporating clinical expertise and allowing the model to adapt\ndynamically to complex surgical scenarios. Experiments on a publicly available\ndataset demonstrate that our algorithm achieves a mean target registration\nerror of 3.42 mm. Incorporating surgeon prompts through the interactive\nframework further reduces the error to 2.78 mm, surpassing state-of-the-art\nmethods in volumetric accuracy. These results highlight the ability of our\nframework to deliver efficient and accurate deformation modeling while\nenhancing surgeon-algorithm collaboration, paving the way for safer and more\nreliable computer-assisted surgeries."}
{"id": "2506.08052", "pdf": "https://arxiv.org/pdf/2506.08052", "abs": "https://arxiv.org/abs/2506.08052", "authors": ["Yongkang Li", "Kaixin Xiong", "Xiangyu Guo", "Fang Li", "Sixu Yan", "Gangwei Xu", "Lijun Zhou", "Long Chen", "Haiyang Sun", "Bing Wang", "Guang Chen", "Hangjun Ye", "Wenyu Liu", "Xinggang Wang"], "title": "ReCogDrive: A Reinforced Cognitive Framework for End-to-End Autonomous Driving", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Although end-to-end autonomous driving has made remarkable progress, its\nperformance degrades significantly in rare and long-tail scenarios. Recent\napproaches attempt to address this challenge by leveraging the rich world\nknowledge of Vision-Language Models (VLMs), but these methods suffer from\nseveral limitations: (1) a significant domain gap between the pre-training data\nof VLMs and real-world driving data, (2) a dimensionality mismatch between the\ndiscrete language space and the continuous action space, and (3) imitation\nlearning tends to capture the average behavior present in the dataset, which\nmay be suboptimal even dangerous. In this paper, we propose ReCogDrive, an\nautonomous driving system that integrates VLMs with diffusion planner, which\nadopts a three-stage paradigm for training. In the first stage, we use a\nlarge-scale driving question-answering datasets to train the VLMs, mitigating\nthe domain discrepancy between generic content and real-world driving\nscenarios. In the second stage, we employ a diffusion-based planner to perform\nimitation learning, mapping representations from the latent language space to\ncontinuous driving actions. Finally, we fine-tune the diffusion planner using\nreinforcement learning with NAVSIM non-reactive simulator, enabling the model\nto generate safer, more human-like driving trajectories. We evaluate our\napproach on the planning-oriented NAVSIM benchmark, achieving a PDMS of 89.6\nand setting a new state-of-the-art that surpasses the previous vision-only SOTA\nby 5.6 PDMS."}
{"id": "2506.08071", "pdf": "https://arxiv.org/pdf/2506.08071", "abs": "https://arxiv.org/abs/2506.08071", "authors": ["Aniket Rege", "Zinnia Nie", "Mahesh Ramesh", "Unmesh Raskar", "Zhuoran Yu", "Aditya Kusupati", "Yong Jae Lee", "Ramya Korlakai Vinayak"], "title": "CuRe: Cultural Gaps in the Long Tail of Text-to-Image Systems", "categories": ["cs.CV"], "comment": "41 pages, 22 figures, 17 tables", "summary": "Popular text-to-image (T2I) systems are trained on web-scraped data, which is\nheavily Amero and Euro-centric, underrepresenting the cultures of the Global\nSouth. To analyze these biases, we introduce CuRe, a novel and scalable\nbenchmarking and scoring suite for cultural representativeness that leverages\nthe marginal utility of attribute specification to T2I systems as a proxy for\nhuman judgments. Our CuRe benchmark dataset has a novel categorical hierarchy\nbuilt from the crowdsourced Wikimedia knowledge graph, with 300 cultural\nartifacts across 32 cultural subcategories grouped into six broad cultural axes\n(food, art, fashion, architecture, celebrations, and people). Our dataset's\ncategorical hierarchy enables CuRe scorers to evaluate T2I systems by analyzing\ntheir response to increasing the informativeness of text conditioning, enabling\nfine-grained cultural comparisons. We empirically observe much stronger\ncorrelations of our class of scorers to human judgments of perceptual\nsimilarity, image-text alignment, and cultural diversity across image encoders\n(SigLIP 2, AIMV2 and DINOv2), vision-language models (OpenCLIP, SigLIP 2,\nGemini 2.0 Flash) and state-of-the-art text-to-image systems, including three\nvariants of Stable Diffusion (1.5, XL, 3.5 Large), FLUX.1 [dev], Ideogram 2.0,\nand DALL-E 3. The code and dataset is open-sourced and available at\nhttps://aniketrege.github.io/cure/."}
{"id": "2506.08137", "pdf": "https://arxiv.org/pdf/2506.08137", "abs": "https://arxiv.org/abs/2506.08137", "authors": ["Oishee Bintey Hoque", "Abhijin Adiga", "Aniruddha Adiga", "Siddharth Chaudhary", "Madhav V. Marathe", "S. S. Ravi", "Kirti Rajagopalan", "Amanda Wilson", "Samarth Swarup"], "title": "IGraSS: Learning to Identify Infrastructure Networks from Satellite Imagery by Iterative Graph-constrained Semantic Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Accurate canal network mapping is essential for water management, including\nirrigation planning and infrastructure maintenance. State-of-the-art semantic\nsegmentation models for infrastructure mapping, such as roads, rely on large,\nwell-annotated remote sensing datasets. However, incomplete or inadequate\nground truth can hinder these learning approaches. Many infrastructure networks\nhave graph-level properties such as reachability to a source (like canals) or\nconnectivity (roads) that can be leveraged to improve these existing ground\ntruth. This paper develops a novel iterative framework IGraSS, combining a\nsemantic segmentation module-incorporating RGB and additional modalities (NDWI,\nDEM)-with a graph-based ground-truth refinement module. The segmentation module\nprocesses satellite imagery patches, while the refinement module operates on\nthe entire data viewing the infrastructure network as a graph. Experiments show\nthat IGraSS reduces unreachable canal segments from around 18% to 3%, and\ntraining with refined ground truth significantly improves canal identification.\nIGraSS serves as a robust framework for both refining noisy ground truth and\nmapping canal networks from remote sensing imagery. We also demonstrate the\neffectiveness and generalizability of IGraSS using road networks as an example,\napplying a different graph-theoretic constraint to complete road networks."}
{"id": "2506.08163", "pdf": "https://arxiv.org/pdf/2506.08163", "abs": "https://arxiv.org/abs/2506.08163", "authors": ["Harshvardhan Takawale", "Nirupam Roy"], "title": "Spectral Domain Neural Reconstruction for Passband FMCW Radars", "categories": ["cs.CV"], "comment": "arXiv admin note: substantial text overlap with arXiv:2503.23313", "summary": "We present SpINRv2, a neural framework for high-fidelity volumetric\nreconstruction using Frequency-Modulated Continuous-Wave (FMCW) radar.\nExtending our prior work (SpINR), this version introduces enhancements that\nallow accurate learning under high start frequencies-where phase aliasing and\nsub-bin ambiguity become prominent. Our core contribution is a fully\ndifferentiable frequency-domain forward model that captures the complex radar\nresponse using closed-form synthesis, paired with an implicit neural\nrepresentation (INR) for continuous volumetric scene modeling. Unlike\ntime-domain baselines, SpINRv2 directly supervises the complex frequency\nspectrum, preserving spectral fidelity while drastically reducing computational\noverhead. Additionally, we introduce sparsity and smoothness regularization to\ndisambiguate sub-bin ambiguities that arise at fine range resolutions.\nExperimental results show that SpINRv2 significantly outperforms both classical\nand learning-based baselines, especially under high-frequency regimes,\nestablishing a new benchmark for neural radar-based 3D imaging."}
{"id": "2506.08185", "pdf": "https://arxiv.org/pdf/2506.08185", "abs": "https://arxiv.org/abs/2506.08185", "authors": ["Huixin Zhan", "Jason H. Moore"], "title": "Surgeon Style Fingerprinting and Privacy Risk Quantification via Discrete Diffusion Models in a Vision-Language-Action Framework", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Surgeons exhibit distinct operating styles due to differences in training,\nexperience, and motor behavior - yet current AI systems often ignore this\npersonalization signal. We propose a novel approach to model fine-grained,\nsurgeon-specific fingerprinting in robotic surgery using a discrete diffusion\nframework integrated with a vision-language-action (VLA) pipeline. Our method\nformulates gesture prediction as a structured sequence denoising task,\nconditioned on multimodal inputs including endoscopic video, surgical intent\nlanguage, and a privacy-aware embedding of surgeon identity and skill.\nPersonalized surgeon fingerprinting is encoded through natural language prompts\nusing third-party language models, allowing the model to retain individual\nbehavioral style without exposing explicit identity. We evaluate our method on\nthe JIGSAWS dataset and demonstrate that it accurately reconstructs gesture\nsequences while learning meaningful motion fingerprints unique to each surgeon.\nTo quantify the privacy implications of personalization, we perform membership\ninference attacks and find that more expressive embeddings improve task\nperformance but simultaneously increase susceptibility to identity leakage.\nThese findings demonstrate that while personalized embeddings improve\nperformance, they also increase vulnerability to identity leakage, revealing\nthe importance of balancing personalization with privacy risk in surgical\nmodeling. Code is available at:\nhttps://github.com/huixin-zhan-ai/Surgeon_style_fingerprinting."}
{"id": "2506.08189", "pdf": "https://arxiv.org/pdf/2506.08189", "abs": "https://arxiv.org/abs/2506.08189", "authors": ["Amartya Dutta", "Kazi Sajeed Mehrab", "Medha Sawhney", "Abhilash Neog", "Mridul Khurana", "Sepideh Fatemi", "Aanish Pradhan", "M. Maruf", "Ismini Lourentzou", "Arka Daw", "Anuj Karpatne"], "title": "Open World Scene Graph Generation using Vision Language Models", "categories": ["cs.CV"], "comment": "Accepted in CVPR 2025 Workshop (CVinW)", "summary": "Scene-Graph Generation (SGG) seeks to recognize objects in an image and\ndistill their salient pairwise relationships. Most methods depend on\ndataset-specific supervision to learn the variety of interactions, restricting\ntheir usefulness in open-world settings, involving novel objects and/or\nrelations. Even methods that leverage large Vision Language Models (VLMs)\ntypically require benchmark-specific fine-tuning. We introduce Open-World SGG,\na training-free, efficient, model-agnostic framework that taps directly into\nthe pretrained knowledge of VLMs to produce scene graphs with zero additional\nlearning. Casting SGG as a zero-shot structured-reasoning problem, our method\ncombines multimodal prompting, embedding alignment, and a lightweight\npair-refinement strategy, enabling inference over unseen object vocabularies\nand relation sets. To assess this setting, we formalize an Open-World\nevaluation protocol that measures performance when no SGG-specific data have\nbeen observed either in terms of objects and relations. Experiments on Visual\nGenome, Open Images V6, and the Panoptic Scene Graph (PSG) dataset demonstrate\nthe capacity of pretrained VLMs to perform relational understanding without\ntask-level training."}
{"id": "2506.08191", "pdf": "https://arxiv.org/pdf/2506.08191", "abs": "https://arxiv.org/abs/2506.08191", "authors": ["Antoni Nowinowski", "Krzysztof Krawiec"], "title": "Generative Learning of Differentiable Object Models for Compositional Interpretation of Complex Scenes", "categories": ["cs.CV"], "comment": null, "summary": "This study builds on the architecture of the Disentangler of Visual Priors\n(DVP), a type of autoencoder that learns to interpret scenes by decomposing the\nperceived objects into independent visual aspects of shape, size, orientation,\nand color appearance. These aspects are expressed as latent parameters which\ncontrol a differentiable renderer that performs image reconstruction, so that\nthe model can be trained end-to-end with gradient using reconstruction loss. In\nthis study, we extend the original DVP so that it can handle multiple objects\nin a scene. We also exploit the interpretability of its latent by using the\ndecoder to sample additional training examples and devising alternative\ntraining modes that rely on loss functions defined not only in the image space,\nbut also in the latent space. This significantly facilitates training, which is\notherwise challenging due to the presence of extensive plateaus in the\nimage-space reconstruction loss. To examine the performance of this approach,\nwe propose a new benchmark featuring multiple 2D objects, which subsumes the\npreviously proposed Multi-dSprites dataset while being more parameterizable. We\ncompare the DVP extended in these ways with two baselines (MONet and LIVE) and\ndemonstrate its superiority in terms of reconstruction quality and capacity to\ndecompose overlapping objects. We also analyze the gradients induced by the\nconsidered loss functions, explain how they impact the efficacy of training,\nand discuss the limitations of differentiable rendering in autoencoders and the\nways in which they can be addressed."}
{"id": "2506.08194", "pdf": "https://arxiv.org/pdf/2506.08194", "abs": "https://arxiv.org/abs/2506.08194", "authors": ["Mateusz Michalkiewicz", "Anekha Sokhal", "Tadeusz Michalkiewicz", "Piotr Pawlikowski", "Mahsa Baktashmotlagh", "Varun Jampani", "Guha Balakrishnan"], "title": "GIQ: Benchmarking 3D Geometric Reasoning of Vision Foundation Models with Simulated and Real Polyhedra", "categories": ["cs.CV", "68T45", "I.5.4; I.2.10; I.3.5"], "comment": "15 pages, 4 figures", "summary": "Monocular 3D reconstruction methods and vision-language models (VLMs)\ndemonstrate impressive results on standard benchmarks, yet their true\nunderstanding of geometric properties remains unclear. We introduce GIQ , a\ncomprehensive benchmark specifically designed to evaluate the geometric\nreasoning capabilities of vision and vision-language foundation models. GIQ\ncomprises synthetic and real-world images of 224 diverse polyhedra - including\nPlatonic, Archimedean, Johnson, and Catalan solids, as well as stellations and\ncompound shapes - covering varying levels of complexity and symmetry. Through\nsystematic experiments involving monocular 3D reconstruction, 3D symmetry\ndetection, mental rotation tests, and zero-shot shape classification tasks, we\nreveal significant shortcomings in current models. State-of-the-art\nreconstruction algorithms trained on extensive 3D datasets struggle to\nreconstruct even basic geometric forms accurately. While foundation models\neffectively detect specific 3D symmetry elements via linear probing, they\nfalter significantly in tasks requiring detailed geometric differentiation,\nsuch as mental rotation. Moreover, advanced vision-language assistants exhibit\nremarkably low accuracy on complex polyhedra, systematically misinterpreting\nbasic properties like face geometry, convexity, and compound structures. GIQ is\npublicly available, providing a structured platform to highlight and address\ncritical gaps in geometric intelligence, facilitating future progress in\nrobust, geometry-aware representation learning."}
{"id": "2506.08210", "pdf": "https://arxiv.org/pdf/2506.08210", "abs": "https://arxiv.org/abs/2506.08210", "authors": ["Andrew Z. Wang", "Songwei Ge", "Tero Karras", "Ming-Yu Liu", "Yogesh Balaji"], "title": "A Comprehensive Study of Decoder-Only LLMs for Text-to-Image Generation", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "CVPR 2025", "summary": "Both text-to-image generation and large language models (LLMs) have made\nsignificant advancements. However, many text-to-image models still employ the\nsomewhat outdated T5 and CLIP as their text encoders. In this work, we\ninvestigate the effectiveness of using modern decoder-only LLMs as text\nencoders for text-to-image diffusion models. We build a standardized training\nand evaluation pipeline that allows us to isolate and evaluate the effect of\ndifferent text embeddings. We train a total of 27 text-to-image models with 12\ndifferent text encoders to analyze the critical aspects of LLMs that could\nimpact text-to-image generation, including the approaches to extract\nembeddings, different LLMs variants, and model sizes. Our experiments reveal\nthat the de facto way of using last-layer embeddings as conditioning leads to\ninferior performance. Instead, we explore embeddings from various layers and\nfind that using layer-normalized averaging across all layers significantly\nimproves alignment with complex prompts. Most LLMs with this conditioning\noutperform the baseline T5 model, showing enhanced performance in advanced\nvisio-linguistic reasoning skills."}
{"id": "2506.08214", "pdf": "https://arxiv.org/pdf/2506.08214", "abs": "https://arxiv.org/abs/2506.08214", "authors": ["Ioannis Iakovidis", "Zahra Kalantari", "Amir Hossein Payberah", "Fernando Jaramillo", "Francisco Pena Escobar"], "title": "Using Satellite Images And Self-supervised Machine Learning Networks To Detect Water Hidden Under Vegetation", "categories": ["cs.CV"], "comment": "16 pages, 9 figures", "summary": "In recent years the wide availability of high-resolution radar satellite\nimages along with the advancement of computer vision models have enabled the\nremote monitoring of the surface area of wetlands. However, these models\nrequire large amounts of manually annotated satellite images, which are slow\nand expensive to produce. To overcome this problem, self-supervised training\nmethods have been deployed to train models without using annotated data. In\nthis paper we use a combination of deep clustering and negative sampling to\ntrain a model to segment radar satellite images into areas that separate water\nfrom land without the use of any manual annotations. Furthermore, we implement\nan ensemble version of the model to reduce variance and improve performance.\nCompared to a single fully-supervised model using the same architecture, our\nensemble of self-supervised models achieves a 0.02 improvement in the\nIntersection Over Union metric over our test dataset."}
{"id": "2506.08220", "pdf": "https://arxiv.org/pdf/2506.08220", "abs": "https://arxiv.org/abs/2506.08220", "authors": ["Octave Mariotti", "Zhipeng Du", "Yash Bhalgat", "Oisin Mac Aodha", "Hakan Bilen"], "title": "Jamais Vu: Exposing the Generalization Gap in Supervised Semantic Correspondence", "categories": ["cs.CV"], "comment": null, "summary": "Semantic correspondence (SC) aims to establish semantically meaningful\nmatches across different instances of an object category. We illustrate how\nrecent supervised SC methods remain limited in their ability to generalize\nbeyond sparsely annotated training keypoints, effectively acting as keypoint\ndetectors. To address this, we propose a novel approach for learning dense\ncorrespondences by lifting 2D keypoints into a canonical 3D space using\nmonocular depth estimation. Our method constructs a continuous canonical\nmanifold that captures object geometry without requiring explicit 3D\nsupervision or camera annotations. Additionally, we introduce SPair-U, an\nextension of SPair-71k with novel keypoint annotations, to better assess\ngeneralization. Experiments not only demonstrate that our model significantly\noutperforms supervised baselines on unseen keypoints, highlighting its\neffectiveness in learning robust correspondences, but that unsupervised\nbaselines outperform supervised counterparts when generalized across different\ndatasets."}
{"id": "2506.08227", "pdf": "https://arxiv.org/pdf/2506.08227", "abs": "https://arxiv.org/abs/2506.08227", "authors": ["Vishaal Udandarao", "Mehdi Cherti", "Shyamgopal Karthik", "Jenia Jitsev", "Samuel Albanie", "Matthias Bethge"], "title": "A Good CREPE needs more than just Sugar: Investigating Biases in Compositional Vision-Language Benchmarks", "categories": ["cs.CV"], "comment": null, "summary": "We investigate 17 benchmarks (e.g. SugarCREPE, VALSE) commonly used for\nmeasuring compositional understanding capabilities of vision-language models\n(VLMs). We scrutinize design choices in their construction, including data\nsource (e.g. MS-COCO) and curation procedures (e.g. constructing negative\nimages/captions), uncovering several inherent biases across most benchmarks. We\nfind that blind heuristics (e.g. token-length, log-likelihood under a language\nmodel) perform on par with CLIP models, indicating that these benchmarks do not\neffectively measure compositional understanding. We demonstrate that the\nunderlying factor is a distribution asymmetry between positive and negative\nimages/captions, induced by the benchmark construction procedures. To mitigate\nthese issues, we provide a few key recommendations for constructing more robust\nvision-language compositional understanding benchmarks, that would be less\nprone to such simple attacks."}
{"id": "2506.08257", "pdf": "https://arxiv.org/pdf/2506.08257", "abs": "https://arxiv.org/abs/2506.08257", "authors": ["L. Lao Beyer", "T. Li", "X. Chen", "S. Karaman", "K. He"], "title": "Highly Compressed Tokenizer Can Generate Without Training", "categories": ["cs.CV", "cs.AI"], "comment": "Main manuscript: 9 pages, 7 figures. Appendix: 8 pages, 9 figures. To\n  appear in the Proceedings of the 42nd International Conference on Machine\n  Learning", "summary": "Commonly used image tokenizers produce a 2D grid of spatially arranged\ntokens. In contrast, so-called 1D image tokenizers represent images as highly\ncompressed one-dimensional sequences of as few as 32 discrete tokens. We find\nthat the high degree of compression achieved by a 1D tokenizer with vector\nquantization enables image editing and generative capabilities through\nheuristic manipulation of tokens, demonstrating that even very crude\nmanipulations -- such as copying and replacing tokens between latent\nrepresentations of images -- enable fine-grained image editing by transferring\nappearance and semantic attributes. Motivated by the expressivity of the 1D\ntokenizer's latent space, we construct an image generation pipeline leveraging\ngradient-based test-time optimization of tokens with plug-and-play loss\nfunctions such as reconstruction or CLIP similarity. Our approach is\ndemonstrated for inpainting and text-guided image editing use cases, and can\ngenerate diverse and realistic samples without requiring training of any\ngenerative model."}
{"id": "2506.08279", "pdf": "https://arxiv.org/pdf/2506.08279", "abs": "https://arxiv.org/abs/2506.08279", "authors": ["Aditi Sundararaman", "Amogh Adishesha", "Andrew Jaegle", "Dan Bigioi", "Hyoung-Kyu Song", "Jon Kyl", "Justin Mao", "Kevin Lan", "Mojtaba Komeili", "ShahRukh Athar", "Sheila Babayan", "Stanislau Beliasau", "William Buchwalter"], "title": "Seeing Voices: Generating A-Roll Video from Audio with Mirage", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Technical report website: mirage.app/research/seeing-voices, product\n  website: mirage.app", "summary": "From professional filmmaking to user-generated content, creators and\nconsumers have long recognized that the power of video depends on the\nharmonious integration of what we hear (the video's audio track) with what we\nsee (the video's image sequence). Current approaches to video generation either\nignore sound to focus on general-purpose but silent image sequence generation\nor address both visual and audio elements but focus on restricted application\ndomains such as re-dubbing. We introduce Mirage, an audio-to-video foundation\nmodel that excels at generating realistic, expressive output imagery from\nscratch given an audio input. When integrated with existing methods for speech\nsynthesis (text-to-speech, or TTS), Mirage results in compelling multimodal\nvideo. When trained on audio-video footage of people talking (A-roll) and\nconditioned on audio containing speech, Mirage generates video of people\ndelivering a believable interpretation of the performance implicit in input\naudio. Our central technical contribution is a unified method for training\nself-attention-based audio-to-video generation models, either from scratch or\ngiven existing weights. This methodology allows Mirage to retain generality as\nan approach to audio-to-video generation while producing outputs of superior\nsubjective quality to methods that incorporate audio-specific architectures or\nloss components specific to people, speech, or details of how images or audio\nare captured. We encourage readers to watch and listen to the results of Mirage\nfor themselves (see paper and comments for links)."}
{"id": "2506.08297", "pdf": "https://arxiv.org/pdf/2506.08297", "abs": "https://arxiv.org/abs/2506.08297", "authors": ["Nhat Thanh Tran", "Fanghui Xue", "Shuai Zhang", "Jiancheng Lyu", "Yunling Zheng", "Yingyong Qi", "Jack Xin"], "title": "SEMA: a Scalable and Efficient Mamba like Attention via Token Localization and Averaging", "categories": ["cs.CV", "cs.AI"], "comment": "15 pages, figures 3", "summary": "Attention is the critical component of a transformer. Yet the quadratic\ncomputational complexity of vanilla full attention in the input size and the\ninability of its linear attention variant to focus have been challenges for\ncomputer vision tasks. We provide a mathematical definition of generalized\nattention and formulate both vanilla softmax attention and linear attention\nwithin the general framework. We prove that generalized attention disperses,\nthat is, as the number of keys tends to infinity, the query assigns equal\nweights to all keys. Motivated by the dispersion property and recent\ndevelopment of Mamba form of attention, we design Scalable and Efficient Mamba\nlike Attention (SEMA) which utilizes token localization to avoid dispersion and\nmaintain focusing, complemented by theoretically consistent arithmetic\naveraging to capture global aspect of attention. We support our approach on\nImagenet-1k where classification results show that SEMA is a scalable and\neffective alternative beyond linear attention, outperforming recent vision\nMamba models on increasingly larger scales of images at similar model parameter\nsizes."}
{"id": "2506.08299", "pdf": "https://arxiv.org/pdf/2506.08299", "abs": "https://arxiv.org/abs/2506.08299", "authors": ["Kangning Yang", "Ling Ouyang", "Huiming Sun", "Jie Cai", "Lan Fu", "Jiaming Ding", "Chiu Man Ho", "Zibo Meng"], "title": "OpenRR-1k: A Scalable Dataset for Real-World Reflection Removal", "categories": ["cs.CV"], "comment": null, "summary": "Reflection removal technology plays a crucial role in photography and\ncomputer vision applications. However, existing techniques are hindered by the\nlack of high-quality in-the-wild datasets. In this paper, we propose a novel\nparadigm for collecting reflection datasets from a fresh perspective. Our\napproach is convenient, cost-effective, and scalable, while ensuring that the\ncollected data pairs are of high quality, perfectly aligned, and represent\nnatural and diverse scenarios. Following this paradigm, we collect a\nReal-world, Diverse, and Pixel-aligned dataset (named OpenRR-1k dataset), which\ncontains 1,000 high-quality transmission-reflection image pairs collected in\nthe wild. Through the analysis of several reflection removal methods and\nbenchmark evaluation experiments on our dataset, we demonstrate its\neffectiveness in improving robustness in challenging real-world environments.\nOur dataset is available at https://github.com/caijie0620/OpenRR-1k."}
{"id": "2506.08324", "pdf": "https://arxiv.org/pdf/2506.08324", "abs": "https://arxiv.org/abs/2506.08324", "authors": ["Guandong Li", "Mengxia Ye"], "title": "Hyperspectral Image Classification via Transformer-based Spectral-Spatial Attention Decoupling and Adaptive Gating", "categories": ["cs.CV"], "comment": "arXiv admin note: substantial text overlap with arXiv:2504.15155,\n  arXiv:2504.13045, arXiv:2503.23472", "summary": "Deep neural networks face several challenges in hyperspectral image\nclassification, including high-dimensional data, sparse distribution of ground\nobjects, and spectral redundancy, which often lead to classification\noverfitting and limited generalization capability. To more effectively extract\nand fuse spatial context with fine spectral information in hyperspectral image\n(HSI) classification, this paper proposes a novel network architecture called\nSTNet. The core advantage of STNet stems from the dual innovative design of its\nSpatial-Spectral Transformer module: first, the fundamental explicit decoupling\nof spatial and spectral attention ensures targeted capture of key information\nin HSI; second, two functionally distinct gating mechanisms perform intelligent\nregulation at both the fusion level of attention flows (adaptive attention\nfusion gating) and the internal level of feature transformation (GFFN). This\ncharacteristic demonstrates superior feature extraction and fusion capabilities\ncompared to traditional convolutional neural networks, while reducing\noverfitting risks in small-sample and high-noise scenarios. STNet enhances\nmodel representation capability without increasing network depth or width. The\nproposed method demonstrates superior performance on IN, UP, and KSC datasets,\noutperforming mainstream hyperspectral image classification approaches."}
{"id": "2506.08327", "pdf": "https://arxiv.org/pdf/2506.08327", "abs": "https://arxiv.org/abs/2506.08327", "authors": ["Yuto Kase", "Kai Ishibe", "Ryoma Yasuda", "Yudai Washida", "Sakiko Hashimoto"], "title": "Locating Tennis Ball Impact on the Racket in Real Time Using an Event Camera", "categories": ["cs.CV"], "comment": "17 pages, 10 figures, 3 tables", "summary": "In racket sports, such as tennis, locating the ball's position at impact is\nimportant in clarifying player and equipment characteristics, thereby aiding in\npersonalized equipment design. High-speed cameras are used to measure the\nimpact location; however, their excessive memory consumption limits prolonged\nscene capture, and manual digitization for position detection is time-consuming\nand prone to human error. These limitations make it difficult to effectively\ncapture the entire playing scene, hindering the ability to analyze the player's\nperformance. We propose a method for locating the tennis ball impact on the\nracket in real time using an event camera. Event cameras efficiently measure\nbrightness changes (called `events') with microsecond accuracy under high-speed\nmotion while using lower memory consumption. These cameras enable users to\ncontinuously monitor their performance over extended periods. Our method\nconsists of three identification steps: time range of swing, timing at impact,\nand contours of ball and racket. Conventional computer vision techniques are\nutilized along with an original event-based processing to detect the timing at\nimpact (PATS: the amount of polarity asymmetry in time symmetry). The results\nof the experiments were within the permissible range for measuring tennis\nplayers' performance. Moreover, the computation time was sufficiently short for\nreal-time applications."}
{"id": "2506.08351", "pdf": "https://arxiv.org/pdf/2506.08351", "abs": "https://arxiv.org/abs/2506.08351", "authors": ["Huixuan Zhang", "Junzhe Zhang", "Xiaojun Wan"], "title": "How Much To Guide: Revisiting Adaptive Guidance in Classifier-Free Guidance Text-to-Vision Diffusion Models", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "With the rapid development of text-to-vision generation diffusion models,\nclassifier-free guidance has emerged as the most prevalent method for\nconditioning. However, this approach inherently requires twice as many steps\nfor model forwarding compared to unconditional generation, resulting in\nsignificantly higher costs. While previous study has introduced the concept of\nadaptive guidance, it lacks solid analysis and empirical results, making\nprevious method unable to be applied to general diffusion models. In this work,\nwe present another perspective of applying adaptive guidance and propose Step\nAG, which is a simple, universally applicable adaptive guidance strategy. Our\nevaluations focus on both image quality and image-text alignment. whose results\nindicate that restricting classifier-free guidance to the first several\ndenoising steps is sufficient for generating high-quality, well-conditioned\nimages, achieving an average speedup of 20% to 30%. Such improvement is\nconsistent across different settings such as inference steps, and various\nmodels including video generation models, highlighting the superiority of our\nmethod."}
{"id": "2506.08356", "pdf": "https://arxiv.org/pdf/2506.08356", "abs": "https://arxiv.org/abs/2506.08356", "authors": ["Shivang Chopra", "Lingchao Mao", "Gabriela Sanchez-Rodriguez", "Andrew J Feola", "Jing Li", "Zsolt Kira"], "title": "MedMoE: Modality-Specialized Mixture of Experts for Medical Vision-Language Understanding", "categories": ["cs.CV"], "comment": null, "summary": "Different medical imaging modalities capture diagnostic information at\nvarying spatial resolutions, from coarse global patterns to fine-grained\nlocalized structures. However, most existing vision-language frameworks in the\nmedical domain apply a uniform strategy for local feature extraction,\noverlooking the modality-specific demands. In this work, we present MedMoE, a\nmodular and extensible vision-language processing framework that dynamically\nadapts visual representation based on the diagnostic context. MedMoE\nincorporates a Mixture-of-Experts (MoE) module conditioned on the report type,\nwhich routes multi-scale image features through specialized expert branches\ntrained to capture modality-specific visual semantics. These experts operate\nover feature pyramids derived from a Swin Transformer backbone, enabling\nspatially adaptive attention to clinically relevant regions. This framework\nproduces localized visual representations aligned with textual descriptions,\nwithout requiring modality-specific supervision at inference. Empirical results\non diverse medical benchmarks demonstrate that MedMoE improves alignment and\nretrieval performance across imaging modalities, underscoring the value of\nmodality-specialized visual representations in clinical vision-language\nsystems."}
{"id": "2506.08361", "pdf": "https://arxiv.org/pdf/2506.08361", "abs": "https://arxiv.org/abs/2506.08361", "authors": ["Yanting Mei", "Zhilu Zhang", "Xiaohe Wu", "Wangmeng Zuo"], "title": "Image Demoir√©ing Using Dual Camera Fusion on Mobile Phones", "categories": ["cs.CV"], "comment": "ICME 2025", "summary": "When shooting electronic screens, moir\\'e patterns usually appear in captured\nimages, which seriously affects the image quality. Existing image demoir\\'eing\nmethods face great challenges in removing large and heavy moir\\'e. To address\nthe issue, we propose to utilize Dual Camera fusion for Image Demoir\\'eing\n(DCID), \\ie, using the ultra-wide-angle (UW) image to assist the moir\\'e\nremoval of wide-angle (W) image. This is inspired by two motivations: (1) the\ntwo lenses are commonly equipped with modern smartphones, (2) the UW image\ngenerally can provide normal colors and textures when moir\\'e exists in the W\nimage mainly due to their different focal lengths. In particular, we propose an\nefficient DCID method, where a lightweight UW image encoder is integrated into\nan existing demoir\\'eing network and a fast two-stage image alignment manner is\npresent. Moreover, we construct a large-scale real-world dataset with diverse\nmobile phones and monitors, containing about 9,000 samples. Experiments on the\ndataset show our method performs better than state-of-the-art methods. Code and\ndataset are available at https://github.com/Mrduckk/DCID."}
{"id": "2506.08391", "pdf": "https://arxiv.org/pdf/2506.08391", "abs": "https://arxiv.org/abs/2506.08391", "authors": ["Woohyeon Park", "Woojin Kim", "Jaeik Kim", "Jaeyoung Do"], "title": "SECOND: Mitigating Perceptual Hallucination in Vision-Language Models via Selective and Contrastive Decoding", "categories": ["cs.CV"], "comment": null, "summary": "Despite significant advancements in Vision-Language Models (VLMs), the\nperformance of existing VLMs remains hindered by object hallucination, a\ncritical challenge to achieving accurate visual understanding. To address this\nissue, we propose SECOND: Selective and Contrastive Decoding, a novel approach\nthat enables VLMs to effectively leverage multi-scale visual information with\nan object-centric manner, closely aligning with human visual perception. SECOND\nprogressively selects and integrates multi-scale visual information,\nfacilitating a more precise interpretation of images. By contrasting these\nvisual information iteratively, SECOND significantly reduces perceptual\nhallucinations and outperforms a wide range of benchmarks. Our theoretical\nanalysis and experiments highlight the largely unexplored potential of\nmulti-scale application in VLMs, showing that prioritizing and contrasting\nacross scales outperforms existing methods."}
{"id": "2506.08418", "pdf": "https://arxiv.org/pdf/2506.08418", "abs": "https://arxiv.org/abs/2506.08418", "authors": ["Taiqin Chen", "Zikun Zhou", "Zheng Fang", "Wenzhen Zou", "Kanjun Liu", "Ke Chen", "Yongbing Zhang", "Yaowei Wang"], "title": "RadioDUN: A Physics-Inspired Deep Unfolding Network for Radio Map Estimation", "categories": ["cs.CV", "eess.SP"], "comment": null, "summary": "The radio map represents the spatial distribution of spectrum resources\nwithin a region, supporting efficient resource allocation and interference\nmitigation. However, it is difficult to construct a dense radio map as a\nlimited number of samples can be measured in practical scenarios. While\nexisting works have used deep learning to estimate dense radio maps from sparse\nsamples, they are hard to integrate with the physical characteristics of the\nradio map. To address this challenge, we cast radio map estimation as the\nsparse signal recovery problem. A physical propagation model is further\nincorporated to decompose the problem into multiple factor optimization\nsub-problems, thereby reducing recovery complexity. Inspired by the existing\ncompressive sensing methods, we propose the Radio Deep Unfolding Network\n(RadioDUN) to unfold the optimization process, achieving adaptive parameter\nadjusting and prior fitting in a learnable manner. To account for the radio\npropagation characteristics, we develop a dynamic reweighting module (DRM) to\nadaptively model the importance of each factor for the radio map. Inspired by\nthe shadowing factor in the physical propagation model, we integrate\nobstacle-related factors to express the obstacle-induced signal stochastic\ndecay. The shadowing loss is further designed to constrain the factor\nprediction and act as a supplementary supervised objective, which enhances the\nperformance of RadioDUN. Extensive experiments have been conducted to\ndemonstrate that the proposed method outperforms the state-of-the-art methods.\nOur code will be made publicly available upon publication."}
{"id": "2506.08429", "pdf": "https://arxiv.org/pdf/2506.08429", "abs": "https://arxiv.org/abs/2506.08429", "authors": ["Mingjie Xu", "Andrew Estornell", "Hongzheng Yang", "Yuzhi Zhao", "Zhaowei Zhu", "Qi Xuan", "Jiaheng Wei"], "title": "Better Reasoning with Less Data: Enhancing VLMs Through Unified Modality Scoring", "categories": ["cs.CV"], "comment": null, "summary": "The application of visual instruction tuning and other post-training\ntechniques has significantly enhanced the capabilities of Large Language Models\n(LLMs) in visual understanding, enriching Vision-Language Models (VLMs) with\nmore comprehensive visual language datasets. However, the effectiveness of VLMs\nis highly dependent on large-scale, high-quality datasets that ensure precise\nrecognition and accurate reasoning. Two key challenges hinder progress: (1)\nnoisy alignments between images and the corresponding text, which leads to\nmisinterpretation, and (2) ambiguous or misleading text, which obscures visual\ncontent. To address these challenges, we propose SCALE (Single modality data\nquality and Cross modality Alignment Evaluation), a novel quality-driven data\nselection pipeline for VLM instruction tuning datasets. Specifically, SCALE\nintegrates a cross-modality assessment framework that first assigns each data\nentry to its appropriate vision-language task, generates general and\ntask-specific captions (covering scenes, objects, style, etc.), and evaluates\nthe alignment, clarity, task rarity, text coherence, and image clarity of each\nentry based on the generated captions. We reveal that: (1) current unimodal\nquality assessment methods evaluate one modality while overlooking the rest,\nwhich can underestimate samples essential for specific tasks and discard the\nlower-quality instances that help build model robustness; and (2) appropriately\ngenerated image captions provide an efficient way to transfer the image-text\nmultimodal task into a unified text modality."}
{"id": "2506.08456", "pdf": "https://arxiv.org/pdf/2506.08456", "abs": "https://arxiv.org/abs/2506.08456", "authors": ["June Suk Choi", "Kyungmin Lee", "Sihyun Yu", "Yisol Choi", "Jinwoo Shin", "Kimin Lee"], "title": "Enhancing Motion Dynamics of Image-to-Video Models via Adaptive Low-Pass Guidance", "categories": ["cs.CV"], "comment": "Preprint. Under review. Project page available at\n  http://choi403.github.io/ALG", "summary": "Recent text-to-video (T2V) models have demonstrated strong capabilities in\nproducing high-quality, dynamic videos. To improve the visual controllability,\nrecent works have considered fine-tuning pre-trained T2V models to support\nimage-to-video (I2V) generation. However, such adaptation frequently suppresses\nmotion dynamics of generated outputs, resulting in more static videos compared\nto their T2V counterparts. In this work, we analyze this phenomenon and\nidentify that it stems from the premature exposure to high-frequency details in\nthe input image, which biases the sampling process toward a shortcut trajectory\nthat overfits to the static appearance of the reference image. To address this,\nwe propose adaptive low-pass guidance (ALG), a simple fix to the I2V model\nsampling procedure to generate more dynamic videos without compromising\nper-frame image quality. Specifically, ALG adaptively modulates the frequency\ncontent of the conditioning image by applying low-pass filtering at the early\nstage of denoising. Extensive experiments demonstrate that ALG significantly\nimproves the temporal dynamics of generated videos, while preserving image\nfidelity and text alignment. Especially, under VBench-I2V test suite, ALG\nachieves an average improvement of 36% in dynamic degree without a significant\ndrop in video quality or image fidelity."}
{"id": "2506.08470", "pdf": "https://arxiv.org/pdf/2506.08470", "abs": "https://arxiv.org/abs/2506.08470", "authors": ["Siyuan Shen", "Ziheng Wang", "Xingyue Peng", "Suan Xia", "Ruiqian Li", "Shiying Li", "Jingyi Yu"], "title": "MARMOT: Masked Autoencoder for Modeling Transient Imaging", "categories": ["cs.CV"], "comment": null, "summary": "Pretrained models have demonstrated impressive success in many modalities\nsuch as language and vision. Recent works facilitate the pretraining paradigm\nin imaging research. Transients are a novel modality, which are captured for an\nobject as photon counts versus arrival times using a precisely time-resolved\nsensor. In particular for non-line-of-sight (NLOS) scenarios, transients of\nhidden objects are measured beyond the sensor's direct line of sight. Using\nNLOS transients, the majority of previous works optimize volume density or\nsurfaces to reconstruct the hidden objects and do not transfer priors learned\nfrom datasets. In this work, we present a masked autoencoder for modeling\ntransient imaging, or MARMOT, to facilitate NLOS applications. Our MARMOT is a\nself-supervised model pretrianed on massive and diverse NLOS transient\ndatasets. Using a Transformer-based encoder-decoder, MARMOT learns features\nfrom partially masked transients via a scanning pattern mask (SPM), where the\nunmasked subset is functionally equivalent to arbitrary sampling, and predicts\nfull measurements. Pretrained on TransVerse-a synthesized transient dataset of\n500K 3D models-MARMOT adapts to downstream imaging tasks using direct feature\ntransfer or decoder finetuning. Comprehensive experiments are carried out in\ncomparisons with state-of-the-art methods. Quantitative and qualitative results\ndemonstrate the efficiency of our MARMOT."}
{"id": "2506.08493", "pdf": "https://arxiv.org/pdf/2506.08493", "abs": "https://arxiv.org/abs/2506.08493", "authors": ["Qilin Yin", "Wei Lu", "Xiangyang Luo", "Xiaochun Cao"], "title": "Context-aware TFL: A Universal Context-aware Contrastive Learning Framework for Temporal Forgery Localization", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Most research efforts in the multimedia forensics domain have focused on\ndetecting forgery audio-visual content and reached sound achievements. However,\nthese works only consider deepfake detection as a classification task and\nignore the case where partial segments of the video are tampered with. Temporal\nforgery localization (TFL) of small fake audio-visual clips embedded in real\nvideos is still challenging and more in line with realistic application\nscenarios. To resolve this issue, we propose a universal context-aware\ncontrastive learning framework (UniCaCLF) for TFL. Our approach leverages\nsupervised contrastive learning to discover and identify forged instants by\nmeans of anomaly detection, allowing for the precise localization of temporal\nforged segments. To this end, we propose a novel context-aware perception layer\nthat utilizes a heterogeneous activation operation and an adaptive context\nupdater to construct a context-aware contrastive objective, which enhances the\ndiscriminability of forged instant features by contrasting them with genuine\ninstant features in terms of their distances to the global context. An\nefficient context-aware contrastive coding is introduced to further push the\nlimit of instant feature distinguishability between genuine and forged instants\nin a supervised sample-by-sample manner, suppressing the cross-sample influence\nto improve temporal forgery localization performance. Extensive experimental\nresults over five public datasets demonstrate that our proposed UniCaCLF\nsignificantly outperforms the state-of-the-art competing algorithms."}
{"id": "2506.08512", "pdf": "https://arxiv.org/pdf/2506.08512", "abs": "https://arxiv.org/abs/2506.08512", "authors": ["Zhiyi Zhu", "Xiaoyu Wu", "Zihao Liu", "Linlin Yang"], "title": "MLVTG: Mamba-Based Feature Alignment and LLM-Driven Purification for Multi-Modal Video Temporal Grounding", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Video Temporal Grounding (VTG), which aims to localize video clips\ncorresponding to natural language queries, is a fundamental yet challenging\ntask in video understanding. Existing Transformer-based methods often suffer\nfrom redundant attention and suboptimal multi-modal alignment. To address these\nlimitations, we propose MLVTG, a novel framework that integrates two key\nmodules: MambaAligner and LLMRefiner. MambaAligner uses stacked Vision Mamba\nblocks as a backbone instead of Transformers to model temporal dependencies and\nextract robust video representations for multi-modal alignment. LLMRefiner\nleverages the specific frozen layer of a pre-trained Large Language Model (LLM)\nto implicitly transfer semantic priors, enhancing multi-modal alignment without\nfine-tuning. This dual alignment strategy, temporal modeling via structured\nstate-space dynamics and semantic purification via textual priors, enables more\nprecise localization. Extensive experiments on QVHighlights, Charades-STA, and\nTVSum demonstrate that MLVTG achieves state-of-the-art performance and\nsignificantly outperforms existing baselines."}
{"id": "2506.08526", "pdf": "https://arxiv.org/pdf/2506.08526", "abs": "https://arxiv.org/abs/2506.08526", "authors": ["Zhongtao Tian", "Wenhao Huang", "Zhidong Chen", "Xiao Wei Sun"], "title": "Robust Visual Localization via Semantic-Guided Multi-Scale Transformer", "categories": ["cs.CV"], "comment": null, "summary": "Visual localization remains challenging in dynamic environments where\nfluctuating lighting, adverse weather, and moving objects disrupt appearance\ncues. Despite advances in feature representation, current absolute pose\nregression methods struggle to maintain consistency under varying conditions.\nTo address this challenge, we propose a framework that synergistically combines\nmulti-scale feature learning with semantic scene understanding. Our approach\nemploys a hierarchical Transformer with cross-scale attention to fuse geometric\ndetails and contextual cues, preserving spatial precision while adapting to\nenvironmental changes. We improve the performance of this architecture with\nsemantic supervision via neural scene representation during training, guiding\nthe network to learn view-invariant features that encode persistent structural\ninformation while suppressing complex environmental interference. Experiments\non TartanAir demonstrate that our approach outperforms existing pose regression\nmethods in challenging scenarios with dynamic objects, illumination changes,\nand occlusions. Our findings show that integrating multi-scale processing with\nsemantic guidance offers a promising strategy for robust visual localization in\nreal-world dynamic environments."}
{"id": "2506.08529", "pdf": "https://arxiv.org/pdf/2506.08529", "abs": "https://arxiv.org/abs/2506.08529", "authors": ["Xijun Wang", "Xin Li", "Bingchen Li", "Zhibo Chen"], "title": "LiftVSR: Lifting Image Diffusion to Video Super-Resolution via Hybrid Temporal Modeling with Only 4$\\times$RTX 4090s", "categories": ["cs.CV"], "comment": "Project page: https://kopperx.github.io/projects/liftvsr", "summary": "Diffusion models have significantly advanced video super-resolution (VSR) by\nenhancing perceptual quality, largely through elaborately designed temporal\nmodeling to ensure inter-frame consistency. However, existing methods usually\nsuffer from limited temporal coherence and prohibitively high computational\ncosts (e.g., typically requiring over 8 NVIDIA A100-80G GPUs), especially for\nlong videos. In this work, we propose LiftVSR, an efficient VSR framework that\nleverages and elevates the image-wise diffusion prior from PixArt-$\\alpha$,\nachieving state-of-the-art results using only 4$\\times$RTX 4090 GPUs. To\nbalance long-term consistency and efficiency, we introduce a hybrid temporal\nmodeling mechanism that decomposes temporal learning into two complementary\ncomponents: (i) Dynamic Temporal Attention (DTA) for fine-grained temporal\nmodeling within short frame segment ($\\textit{i.e.}$, low complexity), and (ii)\nAttention Memory Cache (AMC) for long-term temporal modeling across segments\n($\\textit{i.e.}$, consistency). Specifically, DTA identifies multiple token\nflows across frames within multi-head query and key tokens to warp inter-frame\ncontexts in the value tokens. AMC adaptively aggregates historical segment\ninformation via a cache unit, ensuring long-term coherence with minimal\noverhead. To further stabilize the cache interaction during inference, we\nintroduce an asymmetric sampling strategy that mitigates feature mismatches\narising from different diffusion sampling steps. Extensive experiments on\nseveral typical VSR benchmarks have demonstrated that LiftVSR achieves\nimpressive performance with significantly lower computational costs."}
{"id": "2506.08541", "pdf": "https://arxiv.org/pdf/2506.08541", "abs": "https://arxiv.org/abs/2506.08541", "authors": ["Qi Yan", "Brian Zhang", "Yutong Zhang", "Daniel Yang", "Joshua White", "Di Chen", "Jiachao Liu", "Langechuan Liu", "Binnan Zhuang", "Shaoshuai Shi", "Renjie Liao"], "title": "TrajFlow: Multi-modal Motion Prediction via Flow Matching", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Efficient and accurate motion prediction is crucial for ensuring safety and\ninformed decision-making in autonomous driving, particularly under dynamic\nreal-world conditions that necessitate multi-modal forecasts. We introduce\nTrajFlow, a novel flow matching-based motion prediction framework that\naddresses the scalability and efficiency challenges of existing generative\ntrajectory prediction methods. Unlike conventional generative approaches that\nemploy i.i.d. sampling and require multiple inference passes to capture diverse\noutcomes, TrajFlow predicts multiple plausible future trajectories in a single\npass, significantly reducing computational overhead while maintaining coherence\nacross predictions. Moreover, we propose a ranking loss based on the\nPlackett-Luce distribution to improve uncertainty estimation of predicted\ntrajectories. Additionally, we design a self-conditioning training technique\nthat reuses the model's own predictions to construct noisy inputs during a\nsecond forward pass, thereby improving generalization and accelerating\ninference. Extensive experiments on the large-scale Waymo Open Motion Dataset\n(WOMD) demonstrate that TrajFlow achieves state-of-the-art performance across\nvarious key metrics, underscoring its effectiveness for safety-critical\nautonomous driving applications. The code and other details are available on\nthe project website https://traj-flow.github.io/."}
{"id": "2506.08543", "pdf": "https://arxiv.org/pdf/2506.08543", "abs": "https://arxiv.org/abs/2506.08543", "authors": ["Bowei Tian", "Xuntao Lyu", "Meng Liu", "Hongyi Wang", "Ang Li"], "title": "Convergence of Spectral Principal Paths: How Deep Networks Distill Linear Representations from Noisy Inputs", "categories": ["cs.CV"], "comment": "arXiv admin note: text overlap with arXiv:2503.22720", "summary": "High-level representations have become a central focus in enhancing AI\ntransparency and control, shifting attention from individual neurons or\ncircuits to structured semantic directions that align with human-interpretable\nconcepts. Motivated by the Linear Representation Hypothesis (LRH), we propose\nthe Input-Space Linearity Hypothesis (ISLH), which posits that concept-aligned\ndirections originate in the input space and are selectively amplified with\nincreasing depth. We then introduce the Spectral Principal Path (SPP)\nframework, which formalizes how deep networks progressively distill linear\nrepresentations along a small set of dominant spectral directions. Building on\nthis framework, we further demonstrate the multimodal robustness of these\nrepresentations in Vision-Language Models (VLMs). By bridging theoretical\ninsights with empirical validation, this work advances a structured theory of\nrepresentation formation in deep networks, paving the way for improving AI\nrobustness, fairness, and transparency."}
{"id": "2506.08553", "pdf": "https://arxiv.org/pdf/2506.08553", "abs": "https://arxiv.org/abs/2506.08553", "authors": ["Agnese Taluzzi", "Davide Gesualdi", "Riccardo Santambrogio", "Chiara Plizzari", "Francesca Palermo", "Simone Mentasti", "Matteo Matteucci"], "title": "From Pixels to Graphs: using Scene and Knowledge Graphs for HD-EPIC VQA Challenge", "categories": ["cs.CV"], "comment": "Technical report for the HD-EPIC VQA Challenge 2025 (1st place)", "summary": "This report presents SceneNet and KnowledgeNet, our approaches developed for\nthe HD-EPIC VQA Challenge 2025. SceneNet leverages scene graphs generated with\na multi-modal large language model (MLLM) to capture fine-grained object\ninteractions, spatial relationships, and temporally grounded events. In\nparallel, KnowledgeNet incorporates ConceptNet's external commonsense knowledge\nto introduce high-level semantic connections between entities, enabling\nreasoning beyond directly observable visual evidence. Each method demonstrates\ndistinct strengths across the seven categories of the HD-EPIC benchmark, and\ntheir combination within our framework results in an overall accuracy of 44.21%\non the challenge, highlighting its effectiveness for complex egocentric VQA\ntasks."}
{"id": "2506.08555", "pdf": "https://arxiv.org/pdf/2506.08555", "abs": "https://arxiv.org/abs/2506.08555", "authors": ["Xinyue Niu", "Akira Furui"], "title": "Towards Cross-Subject EMG Pattern Recognition via Dual-Branch Adversarial Feature Disentanglement", "categories": ["cs.CV", "cs.HC"], "comment": "6 pages, 3 figures. This work has been accepted for presentation at\n  the IEEE Engineering in Medicine and Biology Conference (EMBC) 2025", "summary": "Cross-subject electromyography (EMG) pattern recognition faces significant\nchallenges due to inter-subject variability in muscle anatomy, electrode\nplacement, and signal characteristics. Traditional methods rely on\nsubject-specific calibration data to adapt models to new users, an approach\nthat is both time-consuming and impractical for large-scale, real-world\ndeployment. This paper presents an approach to eliminate calibration\nrequirements through feature disentanglement, enabling effective cross-subject\ngeneralization. We propose an end-to-end dual-branch adversarial neural network\nthat simultaneously performs pattern recognition and individual identification\nby disentangling EMG features into pattern-specific and subject-specific\ncomponents. The pattern-specific components facilitate robust pattern\nrecognition for new users without model calibration, while the subject-specific\ncomponents enable downstream applications such as task-invariant biometric\nidentification. Experimental results demonstrate that the proposed model\nachieves robust performance on data from unseen users, outperforming various\nbaseline methods in cross-subject scenarios. Overall, this study offers a new\nperspective for cross-subject EMG pattern recognition without model calibration\nand highlights the proposed model's potential for broader applications, such as\ntask-independent biometric systems."}
{"id": "2506.08562", "pdf": "https://arxiv.org/pdf/2506.08562", "abs": "https://arxiv.org/abs/2506.08562", "authors": ["Duc Thanh Pham", "Hong Dang Nguyen", "Nhat Minh Nguyen Quoc", "Linh Ngo Van", "Sang Dinh Viet", "Duc Anh Nguyen"], "title": "Hierarchical Neural Collapse Detection Transformer for Class Incremental Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "Recently, object detection models have witnessed notable performance\nimprovements, particularly with transformer-based models. However, new objects\nfrequently appear in the real world, requiring detection models to continually\nlearn without suffering from catastrophic forgetting. Although Incremental\nObject Detection (IOD) has emerged to address this challenge, these existing\nmodels are still not practical due to their limited performance and prolonged\ninference time. In this paper, we introduce a novel framework for IOD, called\nHier-DETR: Hierarchical Neural Collapse Detection Transformer, ensuring both\nefficiency and competitive performance by leveraging Neural Collapse for\nimbalance dataset and Hierarchical relation of classes' labels."}
{"id": "2506.08566", "pdf": "https://arxiv.org/pdf/2506.08566", "abs": "https://arxiv.org/abs/2506.08566", "authors": ["Yibo Cui", "Liang Xie", "Yu Zhao", "Jiawei Sun", "Erwei Yin"], "title": "Generating Vision-Language Navigation Instructions Incorporated Fine-Grained Alignment Annotations", "categories": ["cs.CV"], "comment": null, "summary": "Vision-Language Navigation (VLN) enables intelligent agents to navigate\nenvironments by integrating visual perception and natural language\ninstructions, yet faces significant challenges due to the scarcity of\nfine-grained cross-modal alignment annotations. Existing datasets primarily\nfocus on global instruction-trajectory matching, neglecting\nsub-instruction-level and entity-level alignments critical for accurate\nnavigation action decision-making. To address this limitation, we propose\nFCA-NIG, a generative framework that automatically constructs navigation\ninstructions with dual-level fine-grained cross-modal annotations. In this\nframework, an augmented trajectory is first divided into sub-trajectories,\nwhich are then processed through GLIP-based landmark detection, crafted\ninstruction construction, OFA-Speaker based R2R-like instruction generation,\nand CLIP-powered entity selection, generating sub-instruction-trajectory pairs\nwith entity-landmark annotations. Finally, these sub-pairs are aggregated to\nform a complete instruction-trajectory pair. The framework generates the\nFCA-R2R dataset, the first large-scale augmentation dataset featuring precise\nsub-instruction-sub-trajectory and entity-landmark alignments. Extensive\nexperiments demonstrate that training with FCA-R2R significantly improves the\nperformance of multiple state-of-the-art VLN agents, including SF, EnvDrop,\nRecBERT, and HAMT. Incorporating sub-instruction-trajectory alignment enhances\nagents' state awareness and decision accuracy, while entity-landmark alignment\nfurther boosts navigation performance and generalization. These results\nhighlight the effectiveness of FCA-NIG in generating high-quality, scalable\ntraining data without manual annotation, advancing fine-grained cross-modal\nlearning in complex navigation tasks."}
{"id": "2506.08591", "pdf": "https://arxiv.org/pdf/2506.08591", "abs": "https://arxiv.org/abs/2506.08591", "authors": ["Chengchao Shen", "Hourun Zhu", "Gongfan Fang", "Jianxin Wang", "Xinchao Wang"], "title": "Diversity-Guided MLP Reduction for Efficient Large Vision Transformers", "categories": ["cs.CV", "cs.LG", "cs.MM"], "comment": null, "summary": "Transformer models achieve excellent scaling property, where the performance\nis improved with the increment of model capacity. However, large-scale model\nparameters lead to an unaffordable cost of computing and memory. We analyze\npopular transformer architectures and find that multilayer perceptron (MLP)\nmodules take up the majority of model parameters. To this end, we focus on the\nrecoverability of the compressed models and propose a Diversity-Guided MLP\nReduction (DGMR) method to significantly reduce the parameters of large vision\ntransformers with only negligible performance degradation. Specifically, we\nconduct a Gram-Schmidt weight pruning strategy to eliminate redundant neurons\nof MLP hidden layer, while preserving weight diversity for better performance\nrecover during distillation. Compared to the model trained from scratch, our\npruned model only requires 0.06\\% data of LAION-2B (for the training of large\nvision transformers) without labels (ImageNet-1K) to recover the original\nperformance. Experimental results on several state-of-the-art large vision\ntransformers demonstrate that our method achieves a more than 57.0\\% parameter\nand FLOPs reduction in a near lossless manner. Notably, for EVA-CLIP-E (4.4B),\nour method accomplishes a 71.5\\% parameter and FLOPs reduction without\nperformance degradation. The source code and trained weights are available at\nhttps://github.com/visresearch/DGMR."}
{"id": "2506.08596", "pdf": "https://arxiv.org/pdf/2506.08596", "abs": "https://arxiv.org/abs/2506.08596", "authors": ["Guyang Zhang", "Waleed Abdulla"], "title": "Transformers Meet Hyperspectral Imaging: A Comprehensive Study of Models, Challenges and Open Problems", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Transformers have become the architecture of choice for learning long-range\ndependencies, yet their adoption in hyperspectral imaging (HSI) is still\nemerging. We reviewed more than 300 papers published up to 2025 and present the\nfirst end-to-end survey dedicated to Transformer-based HSI classification. The\nstudy categorizes every stage of a typical pipeline-pre-processing, patch or\npixel tokenization, positional encoding, spatial-spectral feature extraction,\nmulti-head self-attention variants, skip connections, and loss design-and\ncontrasts alternative design choices with the unique spatial-spectral\nproperties of HSI. We map the field's progress against persistent obstacles:\nscarce labeled data, extreme spectral dimensionality, computational overhead,\nand limited model explainability. Finally, we outline a research agenda\nprioritizing valuable public data sets, lightweight on-edge models,\nillumination and sensor shifts robustness, and intrinsically interpretable\nattention mechanisms. Our goal is to guide researchers in selecting, combining,\nor extending Transformer components that are truly fit for purpose for\nnext-generation HSI applications."}
{"id": "2506.08611", "pdf": "https://arxiv.org/pdf/2506.08611", "abs": "https://arxiv.org/abs/2506.08611", "authors": ["Shiji Zhao", "Chi Chen", "Ranjie Duan", "Xizhe Wang", "Xingxing Wei"], "title": "Towards Class-wise Fair Adversarial Training via Anti-Bias Soft Label Distillation", "categories": ["cs.CV"], "comment": "arXiv admin note: text overlap with arXiv:2312.05508", "summary": "Adversarial Training (AT) is widely recognized as an effective approach to\nenhance the adversarial robustness of Deep Neural Networks. As a variant of AT,\nAdversarial Robustness Distillation (ARD) has shown outstanding performance in\nenhancing the robustness of small models. However, both AT and ARD face robust\nfairness issue: these models tend to display strong adversarial robustness\nagainst some classes (easy classes) while demonstrating weak adversarial\nrobustness against others (hard classes). This paper explores the underlying\nfactors of this problem and points out the smoothness degree of soft labels for\ndifferent classes significantly impacts the robust fairness from both empirical\nobservation and theoretical analysis. Based on the above exploration, we\npropose Anti-Bias Soft Label Distillation (ABSLD) within the Knowledge\nDistillation framework to enhance the adversarial robust fairness.\nSpecifically, ABSLD adaptively reduces the student's error risk gap between\ndifferent classes, which is accomplished by adjusting the class-wise smoothness\ndegree of teacher's soft labels during the training process, and the adjustment\nis managed by assigning varying temperatures to different classes.\nAdditionally, as a label-based approach, ABSLD is highly adaptable and can be\nintegrated with the sample-based methods. Extensive experiments demonstrate\nABSLD outperforms state-of-the-art methods on the comprehensive performance of\nrobustness and fairness."}
{"id": "2506.08612", "pdf": "https://arxiv.org/pdf/2506.08612", "abs": "https://arxiv.org/abs/2506.08612", "authors": ["Robert-Jan Bruintjes", "Attila Lengyel", "Osman Semih Kayhan", "Davide Zambrano", "Nergis T√∂men", "Hadi Jamali-Rad", "Jan van Gemert"], "title": "Data-Efficient Challenges in Visual Inductive Priors: A Retrospective", "categories": ["cs.CV"], "comment": null, "summary": "Deep Learning requires large amounts of data to train models that work well.\nIn data-deficient settings, performance can be degraded. We investigate which\nDeep Learning methods benefit training models in a data-deficient setting, by\norganizing the \"VIPriors: Visual Inductive Priors for Data-Efficient Deep\nLearning\" workshop series, featuring four editions of data-impaired challenges.\nThese challenges address the problem of training deep learning models for\ncomputer vision tasks with limited data. Participants are limited to training\nmodels from scratch using a low number of training samples and are not allowed\nto use any form of transfer learning. We aim to stimulate the development of\nnovel approaches that incorporate prior knowledge to improve the data\nefficiency of deep learning models. Successful challenge entries make use of\nlarge model ensembles that mix Transformers and CNNs, as well as heavy data\naugmentation. Novel prior knowledge-based methods contribute to success in some\nentries."}
{"id": "2506.08613", "pdf": "https://arxiv.org/pdf/2506.08613", "abs": "https://arxiv.org/abs/2506.08613", "authors": ["Joost van Dalen", "Yuki M. Asano", "Marc Russwurm"], "title": "SAMSelect: A Spectral Index Search for Marine Debris Visualization using Segment Anything", "categories": ["cs.CV"], "comment": null, "summary": "This work proposes SAMSelect, an algorithm to obtain a salient three-channel\nvisualization for multispectral images. We develop SAMSelect and show its use\nfor marine scientists visually interpreting floating marine debris in\nSentinel-2 imagery. These debris are notoriously difficult to visualize due to\ntheir compositional heterogeneity in medium-resolution imagery. Out of these\ndifficulties, a visual interpretation of imagery showing marine debris remains\na common practice by domain experts, who select bands and spectral indices on a\ncase-by-case basis informed by common practices and heuristics. SAMSelect\nselects the band or index combination that achieves the best classification\naccuracy on a small annotated dataset through the Segment Anything Model. Its\ncentral assumption is that the three-channel visualization achieves the most\naccurate segmentation results also provide good visual information for\nphoto-interpretation.\n  We evaluate SAMSelect in three Sentinel-2 scenes containing generic marine\ndebris in Accra, Ghana, and Durban, South Africa, and deployed plastic targets\nfrom the Plastic Litter Project. This reveals the potential of new previously\nunused band combinations (e.g., a normalized difference index of B8, B2), which\ndemonstrate improved performance compared to literature-based indices. We\ndescribe the algorithm in this paper and provide an open-source code repository\nthat will be helpful for domain scientists doing visual photo interpretation,\nespecially in the marine field."}
{"id": "2506.08619", "pdf": "https://arxiv.org/pdf/2506.08619", "abs": "https://arxiv.org/abs/2506.08619", "authors": ["Gon√ßalo Dias Pais", "Valter Piedade", "Moitreya Chatterjee", "Marcus Greiff", "Pedro Miraldo"], "title": "A Probability-guided Sampler for Neural Implicit Surface Rendering", "categories": ["cs.CV"], "comment": "Accepted in ECCV 2024", "summary": "Several variants of Neural Radiance Fields (NeRFs) have significantly\nimproved the accuracy of synthesized images and surface reconstruction of 3D\nscenes/objects. In all of these methods, a key characteristic is that none can\ntrain the neural network with every possible input data, specifically, every\npixel and potential 3D point along the projection rays due to scalability\nissues. While vanilla NeRFs uniformly sample both the image pixels and 3D\npoints along the projection rays, some variants focus only on guiding the\nsampling of the 3D points along the projection rays. In this paper, we leverage\nthe implicit surface representation of the foreground scene and model a\nprobability density function in a 3D image projection space to achieve a more\ntargeted sampling of the rays toward regions of interest, resulting in improved\nrendering. Additionally, a new surface reconstruction loss is proposed for\nimproved performance. This new loss fully explores the proposed 3D image\nprojection space model and incorporates near-to-surface and empty space\ncomponents. By integrating our novel sampling strategy and novel loss into\ncurrent state-of-the-art neural implicit surface renderers, we achieve more\naccurate and detailed 3D reconstructions and improved image rendering,\nespecially for the regions of interest in any given scene."}
{"id": "2506.08629", "pdf": "https://arxiv.org/pdf/2506.08629", "abs": "https://arxiv.org/abs/2506.08629", "authors": ["Feixiang Du", "Shengkun Wu"], "title": "ECMNet:Lightweight Semantic Segmentation with Efficient CNN-Mamba Network", "categories": ["cs.CV", "cs.AI"], "comment": "16 pages, 2 figures, 4 tables", "summary": "In the past decade, Convolutional Neural Networks (CNNs) and Transformers\nhave achieved wide applicaiton in semantic segmentation tasks. Although CNNs\nwith Transformer models greatly improve performance, the global context\nmodeling remains inadequate. Recently, Mamba achieved great potential in vision\ntasks, showing its advantages in modeling long-range dependency. In this paper,\nwe propose a lightweight Efficient CNN-Mamba Network for semantic segmentation,\ndubbed as ECMNet. ECMNet combines CNN with Mamba skillfully in a capsule-based\nframework to address their complementary weaknesses. Specifically, We design a\nEnhanced Dual-Attention Block (EDAB) for lightweight bottleneck. In order to\nimprove the representations ability of feature, We devise a Multi-Scale\nAttention Unit (MSAU) to integrate multi-scale feature aggregation, spatial\naggregation and channel aggregation. Moreover, a Mamba enhanced Feature Fusion\nModule (FFM) merges diverse level feature, significantly enhancing segmented\naccuracy. Extensive experiments on two representative datasets demonstrate that\nthe proposed model excels in accuracy and efficiency balance, achieving 70.6%\nmIoU on Cityscapes and 73.6% mIoU on CamVid test datasets, with 0.87M\nparameters and 8.27G FLOPs on a single RTX 3090 GPU platform."}
{"id": "2506.08632", "pdf": "https://arxiv.org/pdf/2506.08632", "abs": "https://arxiv.org/abs/2506.08632", "authors": ["Yang Bai", "Liudi Yang", "George Eskandar", "Fengyi Shen", "Dong Chen", "Mohammad Altillawi", "Ziyuan Liu", "Gitta Kutyniok"], "title": "RoboSwap: A GAN-driven Video Diffusion Framework For Unsupervised Robot Arm Swapping", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in generative models have revolutionized video synthesis\nand editing. However, the scarcity of diverse, high-quality datasets continues\nto hinder video-conditioned robotic learning, limiting cross-platform\ngeneralization. In this work, we address the challenge of swapping a robotic\narm in one video with another: a key step for crossembodiment learning. Unlike\nprevious methods that depend on paired video demonstrations in the same\nenvironmental settings, our proposed framework, RoboSwap, operates on unpaired\ndata from diverse environments, alleviating the data collection needs. RoboSwap\nintroduces a novel video editing pipeline integrating both GANs and diffusion\nmodels, combining their isolated advantages. Specifically, we segment robotic\narms from their backgrounds and train an unpaired GAN model to translate one\nrobotic arm to another. The translated arm is blended with the original video\nbackground and refined with a diffusion model to enhance coherence, motion\nrealism and object interaction. The GAN and diffusion stages are trained\nindependently. Our experiments demonstrate that RoboSwap outperforms\nstate-of-the-art video and image editing models on three benchmarks in terms of\nboth structural coherence and motion consistency, thereby offering a robust\nsolution for generating reliable, cross-embodiment data in robotic learning."}
{"id": "2506.08635", "pdf": "https://arxiv.org/pdf/2506.08635", "abs": "https://arxiv.org/abs/2506.08635", "authors": ["Siddhant Ranade", "Gon√ßalo Dias Pais", "Ross Tyler Whitaker", "Jacinto C. Nascimento", "Pedro Miraldo", "Srikumar Ramalingam"], "title": "SurfR: Surface Reconstruction with Multi-scale Attention", "categories": ["cs.CV"], "comment": "Accepted in 3DV 2025", "summary": "We propose a fast and accurate surface reconstruction algorithm for\nunorganized point clouds using an implicit representation. Recent learning\nmethods are either single-object representations with small neural models that\nallow for high surface details but require per-object training or generalized\nrepresentations that require larger models and generalize to newer shapes but\nlack details, and inference is slow. We propose a new implicit representation\nfor general 3D shapes that is faster than all the baselines at their optimum\nresolution, with only a marginal loss in performance compared to the\nstate-of-the-art. We achieve the best accuracy-speed trade-off using three key\ncontributions. Many implicit methods extract features from the point cloud to\nclassify whether a query point is inside or outside the object. First, to speed\nup the reconstruction, we show that this feature extraction does not need to\nuse the query point at an early stage (lazy query). Second, we use a parallel\nmulti-scale grid representation to develop robust features for different noise\nlevels and input resolutions. Finally, we show that attention across scales can\nprovide improved reconstruction results."}
{"id": "2506.08640", "pdf": "https://arxiv.org/pdf/2506.08640", "abs": "https://arxiv.org/abs/2506.08640", "authors": ["Yichong Lu", "Yuzhuo Tian", "Zijin Jiang", "Yikun Zhao", "Yuanbo Yang", "Hao Ouyang", "Haoji Hu", "Huimin Yu", "Yujun Shen", "Yiyi Liao"], "title": "Orientation Matters: Making 3D Generative Models Orientation-Aligned", "categories": ["cs.CV"], "comment": "Project Page: https://xdimlab.github.io/Orientation_Matters", "summary": "Humans intuitively perceive object shape and orientation from a single image,\nguided by strong priors about canonical poses. However, existing 3D generative\nmodels often produce misaligned results due to inconsistent training data,\nlimiting their usability in downstream tasks. To address this gap, we introduce\nthe task of orientation-aligned 3D object generation: producing 3D objects from\nsingle images with consistent orientations across categories. To facilitate\nthis, we construct Objaverse-OA, a dataset of 14,832 orientation-aligned 3D\nmodels spanning 1,008 categories. Leveraging Objaverse-OA, we fine-tune two\nrepresentative 3D generative models based on multi-view diffusion and 3D\nvariational autoencoder frameworks to produce aligned objects that generalize\nwell to unseen objects across various categories. Experimental results\ndemonstrate the superiority of our method over post-hoc alignment approaches.\nFurthermore, we showcase downstream applications enabled by our aligned object\ngeneration, including zero-shot object orientation estimation via\nanalysis-by-synthesis and efficient arrow-based object rotation manipulation."}
{"id": "2506.08649", "pdf": "https://arxiv.org/pdf/2506.08649", "abs": "https://arxiv.org/abs/2506.08649", "authors": ["Zhiyi Zhu", "Xiaoyu Wu", "Youwei Lu"], "title": "Enhancing Video Memorability Prediction with Text-Motion Cross-modal Contrastive Loss and Its Application in Video Summarization", "categories": ["cs.CV"], "comment": null, "summary": "Video memorability refers to the ability of videos to be recalled after\nviewing, playing a crucial role in creating content that remains memorable.\nExisting models typically focus on extracting multimodal features to predict\nvideo memorability scores but often fail to fully utilize motion cues. The\nrepresentation of motion features is compromised during the fine-tuning phase\nof the motion feature extractor due to a lack of labeled data. In this paper,\nwe introduce the Text-Motion Cross-modal Contrastive Loss (TMCCL), a multimodal\nvideo memorability prediction model designed to enhance the representation of\nmotion features. We tackle the challenge of improving motion feature\nrepresentation by leveraging text description similarities across videos to\nestablish positive and negative motion sample sets for a given target. This\nenhancement allows the model to learn similar feature representations for\nsemantically related motion content, resulting in more accurate memorability\npredictions. Our model achieves state-of-the-art performance on two video\nmemorability prediction datasets. Moreover, the potential applications of video\nmemorability prediction have been underexplored. To address this gap, we\npresent Memorability Weighted Correction for Video Summarization (MWCVS), using\nvideo memorability prediction to reduce subjectivity in video summarization\nlabels. Experimental results on two video summarization datasets demonstrate\nthe effectiveness of MWCVS, showcasing the promising applications of video\nmemorability prediction."}
{"id": "2506.08650", "pdf": "https://arxiv.org/pdf/2506.08650", "abs": "https://arxiv.org/abs/2506.08650", "authors": ["Peter Gr√∂nquist", "Stepan Tulyakov", "Dengxin Dai"], "title": "Beyond Calibration: Physically Informed Learning for Raw-to-Raw Mapping", "categories": ["cs.CV"], "comment": null, "summary": "Achieving consistent color reproduction across multiple cameras is essential\nfor seamless image fusion and Image Processing Pipeline (ISP) compatibility in\nmodern devices, but it is a challenging task due to variations in sensors and\noptics. Existing raw-to-raw conversion methods face limitations such as poor\nadaptability to changing illumination, high computational costs, or impractical\nrequirements such as simultaneous camera operation and overlapping\nfields-of-view. We introduce the Neural Physical Model (NPM), a lightweight,\nphysically-informed approach that simulates raw images under specified\nillumination to estimate transformations between devices. The NPM effectively\nadapts to varying illumination conditions, can be initialized with physical\nmeasurements, and supports training with or without paired data. Experiments on\npublic datasets like NUS and BeyondRGB demonstrate that NPM outperforms recent\nstate-of-the-art methods, providing robust chromatic consistency across\ndifferent sensors and optical systems."}
{"id": "2506.08666", "pdf": "https://arxiv.org/pdf/2506.08666", "abs": "https://arxiv.org/abs/2506.08666", "authors": ["Wenzhuo Liu", "Fei Zhu", "Haiyang Guo", "Longhui Wei", "Cheng-Lin Liu"], "title": "LLaVA-c: Continual Improved Visual Instruction Tuning", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal models like LLaVA-1.5 achieve state-of-the-art visual\nunderstanding through visual instruction tuning on multitask datasets, enabling\nstrong instruction-following and multimodal performance. However, multitask\nlearning faces challenges such as task balancing, requiring careful adjustment\nof data proportions, and expansion costs, where new tasks risk catastrophic\nforgetting and need costly retraining. Continual learning provides a promising\nalternative to acquiring new knowledge incrementally while preserving existing\ncapabilities. However, current methods prioritize task-specific performance,\nneglecting base model degradation from overfitting to specific instructions,\nwhich undermines general capabilities. In this work, we propose a simple but\neffective method with two modifications on LLaVA-1.5: spectral-aware\nconsolidation for improved task balance and unsupervised inquiry regularization\nto prevent base model degradation. We evaluate both general and task-specific\nperformance across continual pretraining and fine-tuning. Experiments\ndemonstrate that LLaVA-c consistently enhances standard benchmark performance\nand preserves general capabilities. For the first time, we show that\ntask-by-task continual learning can achieve results that match or surpass\nmultitask joint learning. The code will be publicly released."}
{"id": "2506.08678", "pdf": "https://arxiv.org/pdf/2506.08678", "abs": "https://arxiv.org/abs/2506.08678", "authors": ["Juan Yeo", "Soonwoo Cha", "Jiwoo Song", "Hyunbin Jin", "Taesup Kim"], "title": "ATAS: Any-to-Any Self-Distillation for Enhanced Open-Vocabulary Dense Prediction", "categories": ["cs.CV"], "comment": null, "summary": "Vision-language models such as CLIP have recently propelled open-vocabulary\ndense prediction tasks by enabling recognition of a broad range of visual\nconcepts. However, CLIP still struggles with fine-grained, region-level\nunderstanding, hindering its effectiveness on these dense prediction tasks. We\nidentify two pivotal factors required to address this limitation: semantic\ncoherence and fine-grained vision-language alignment. Current adaptation\nmethods often improve fine-grained alignment at the expense of semantic\ncoherence, and often rely on extra modules or supervised fine-tuning. To\novercome these issues, we propose Any-to-Any Self-Distillation (ATAS), a novel\napproach that simultaneously enhances semantic coherence and fine-grained\nalignment by leveraging own knowledge of a model across all representation\nlevels. Unlike prior methods, ATAS uses only unlabeled images and an internal\nself-distillation process to refine representations of CLIP vision encoders,\npreserving local semantic consistency while sharpening local detail\nrecognition. On open-vocabulary object detection and semantic segmentation\nbenchmarks, ATAS achieves substantial performance gains, outperforming baseline\nCLIP models. These results validate the effectiveness of our approach and\nunderscore the importance of jointly maintaining semantic coherence and\nfine-grained alignment for advanced open-vocabulary dense prediction."}
{"id": "2506.08690", "pdf": "https://arxiv.org/pdf/2506.08690", "abs": "https://arxiv.org/abs/2506.08690", "authors": ["Hugo Porta", "Emanuele Dalsasso", "Jessica L. McCarty", "Devis Tuia"], "title": "CanadaFireSat: Toward high-resolution wildfire forecasting with multiple modalities", "categories": ["cs.CV"], "comment": "34 pages, 11 figures", "summary": "Canada experienced in 2023 one of the most severe wildfire seasons in recent\nhistory, causing damage across ecosystems, destroying communities, and emitting\nlarge quantities of CO2. This extreme wildfire season is symptomatic of a\nclimate-change-induced increase in the length and severity of the fire season\nthat affects the boreal ecosystem. Therefore, it is critical to empower\nwildfire management in boreal communities with better mitigation solutions.\nWildfire probability maps represent an important tool for understanding the\nlikelihood of wildfire occurrence and the potential severity of future\nwildfires. The massive increase in the availability of Earth observation data\nhas enabled the development of deep learning-based wildfire forecasting models,\naiming at providing precise wildfire probability maps at different spatial and\ntemporal scales. A main limitation of such methods is their reliance on\ncoarse-resolution environmental drivers and satellite products, leading to\nwildfire occurrence prediction of reduced resolution, typically around $\\sim\n0.1${\\deg}. This paper presents a benchmark dataset: CanadaFireSat, and\nbaseline methods for high-resolution: 100 m wildfire forecasting across Canada,\nleveraging multi-modal data from high-resolution multi-spectral satellite\nimages (Sentinel-2 L1C), mid-resolution satellite products (MODIS), and\nenvironmental factors (ERA5 reanalysis data). Our experiments consider two\nmajor deep learning architectures. We observe that using multi-modal temporal\ninputs outperforms single-modal temporal inputs across all metrics, achieving a\npeak performance of 60.3% in F1 score for the 2023 wildfire season, a season\nnever seen during model training. This demonstrates the potential of\nmulti-modal deep learning models for wildfire forecasting at high-resolution\nand continental scale."}
{"id": "2506.08691", "pdf": "https://arxiv.org/pdf/2506.08691", "abs": "https://arxiv.org/abs/2506.08691", "authors": ["Congzhi Zhang", "Jiawei Peng", "Zhenglin Wang", "Yilong Lai", "Haowen Sun", "Heng Chang", "Fei Ma", "Weijiang Yu"], "title": "VReST: Enhancing Reasoning in Large Vision-Language Models through Tree Search and Self-Reward Mechanism", "categories": ["cs.CV"], "comment": "Accepted by ACL 2025 main", "summary": "Large Vision-Language Models (LVLMs) have shown exceptional performance in\nmultimodal tasks, but their effectiveness in complex visual reasoning is still\nconstrained, especially when employing Chain-of-Thought prompting techniques.\nIn this paper, we propose VReST, a novel training-free approach that enhances\nReasoning in LVLMs through Monte Carlo Tree Search and Self-Reward mechanisms.\nVReST meticulously traverses the reasoning landscape by establishing a search\ntree, where each node encapsulates a reasoning step, and each path delineates a\ncomprehensive reasoning sequence. Our innovative multimodal Self-Reward\nmechanism assesses the quality of reasoning steps by integrating the utility of\nsub-questions, answer correctness, and the relevance of vision-language clues,\nall without the need for additional models. VReST surpasses current prompting\nmethods and secures state-of-the-art performance across three multimodal\nmathematical reasoning benchmarks. Furthermore, it substantiates the efficacy\nof test-time scaling laws in multimodal tasks, offering a promising direction\nfor future research."}
{"id": "2506.08694", "pdf": "https://arxiv.org/pdf/2506.08694", "abs": "https://arxiv.org/abs/2506.08694", "authors": ["Mohammadreza Salehi", "Shashanka Venkataramanan", "Ioana Simion", "Efstratios Gavves", "Cees G. M. Snoek", "Yuki M Asano"], "title": "MoSiC: Optimal-Transport Motion Trajectory for Dense Self-Supervised Learning", "categories": ["cs.CV"], "comment": "preprint", "summary": "Dense self-supervised learning has shown great promise for learning pixel-\nand patch-level representations, but extending it to videos remains challenging\ndue to the complexity of motion dynamics. Existing approaches struggle as they\nrely on static augmentations that fail under object deformations, occlusions,\nand camera movement, leading to inconsistent feature learning over time. We\npropose a motion-guided self-supervised learning framework that clusters dense\npoint tracks to learn spatiotemporally consistent representations. By\nleveraging an off-the-shelf point tracker, we extract long-range motion\ntrajectories and optimize feature clustering through a momentum-encoder-based\noptimal transport mechanism. To ensure temporal coherence, we propagate cluster\nassignments along tracked points, enforcing feature consistency across views\ndespite viewpoint changes. Integrating motion as an implicit supervisory\nsignal, our method learns representations that generalize across frames,\nimproving robustness in dynamic scenes and challenging occlusion scenarios. By\ninitializing from strong image-pretrained models and leveraging video data for\ntraining, we improve state-of-the-art by 1% to 6% on six image and video\ndatasets and four evaluation benchmarks. The implementation is publicly\navailable at our GitHub repository: https://github.com/SMSD75/MoSiC/tree/main"}
{"id": "2506.08699", "pdf": "https://arxiv.org/pdf/2506.08699", "abs": "https://arxiv.org/abs/2506.08699", "authors": ["Frederik Hagelskjaer"], "title": "ArrowPose: Segmentation, Detection, and 5 DoF Pose Estimation Network for Colorless Point Clouds", "categories": ["cs.CV"], "comment": "6 pages, 5 figures, 4 tables", "summary": "This paper presents a fast detection and 5 DoF (Degrees of Freedom) pose\nestimation network for colorless point clouds. The pose estimation is\ncalculated from center and top points of the object, predicted by the neural\nnetwork. The network is trained on synthetic data, and tested on a benchmark\ndataset, where it demonstrates state-of-the-art performance and outperforms all\ncolorless methods. The network is able to run inference in only 250\nmilliseconds making it usable in many scenarios. Project page with code at\narrowpose.github.io"}
{"id": "2506.08704", "pdf": "https://arxiv.org/pdf/2506.08704", "abs": "https://arxiv.org/abs/2506.08704", "authors": ["Xiaohan Zhang", "Sitong Wang", "Yushen Yan", "Yi Yang", "Mingda Xu", "Qi Liu"], "title": "TraGraph-GS: Trajectory Graph-based Gaussian Splatting for Arbitrary Large-Scale Scene Rendering", "categories": ["cs.CV"], "comment": null, "summary": "High-quality novel view synthesis for large-scale scenes presents a\nchallenging dilemma in 3D computer vision. Existing methods typically partition\nlarge scenes into multiple regions, reconstruct a 3D representation using\nGaussian splatting for each region, and eventually merge them for novel view\nrendering. They can accurately render specific scenes, yet they do not\ngeneralize effectively for two reasons: (1) rigid spatial partition techniques\nstruggle with arbitrary camera trajectories, and (2) the merging of regions\nresults in Gaussian overlap to distort texture details. To address these\nchallenges, we propose TraGraph-GS, leveraging a trajectory graph to enable\nhigh-precision rendering for arbitrarily large-scale scenes. We present a\nspatial partitioning method for large-scale scenes based on graphs, which\nincorporates a regularization constraint to enhance the rendering of textures\nand distant objects, as well as a progressive rendering strategy to mitigate\nartifacts caused by Gaussian overlap. Experimental results demonstrate its\nsuperior performance both on four aerial and four ground datasets and highlight\nits remarkable efficiency: our method achieves an average improvement of 1.86\ndB in PSNR on aerial datasets and 1.62 dB on ground datasets compared to\nstate-of-the-art approaches."}
{"id": "2506.08710", "pdf": "https://arxiv.org/pdf/2506.08710", "abs": "https://arxiv.org/abs/2506.08710", "authors": ["Mengjiao Ma", "Qi Ma", "Yue Li", "Jiahuan Cheng", "Runyi Yang", "Bin Ren", "Nikola Popovic", "Mingqiang Wei", "Nicu Sebe", "Luc Van Gool", "Theo Gevers", "Martin R. Oswald", "Danda Pani Paudel"], "title": "SceneSplat++: A Large Dataset and Comprehensive Benchmark for Language Gaussian Splatting", "categories": ["cs.CV"], "comment": "15 pages, codes, data and benchmark will be released", "summary": "3D Gaussian Splatting (3DGS) serves as a highly performant and efficient\nencoding of scene geometry, appearance, and semantics. Moreover, grounding\nlanguage in 3D scenes has proven to be an effective strategy for 3D scene\nunderstanding. Current Language Gaussian Splatting line of work fall into three\nmain groups: (i) per-scene optimization-based, (ii) per-scene\noptimization-free, and (iii) generalizable approach. However, most of them are\nevaluated only on rendered 2D views of a handful of scenes and viewpoints close\nto the training views, limiting ability and insight into holistic 3D\nunderstanding. To address this gap, we propose the first large-scale benchmark\nthat systematically assesses these three groups of methods directly in 3D\nspace, evaluating on 1060 scenes across three indoor datasets and one outdoor\ndataset. Benchmark results demonstrate a clear advantage of the generalizable\nparadigm, particularly in relaxing the scene-specific limitation, enabling fast\nfeed-forward inference on novel scenes, and achieving superior segmentation\nperformance. We further introduce GaussianWorld-49K a carefully curated 3DGS\ndataset comprising around 49K diverse indoor and outdoor scenes obtained from\nmultiple sources, with which we demonstrate the generalizable approach could\nharness strong data priors. Our codes, benchmark, and datasets will be made\npublic to accelerate research in generalizable 3DGS scene understanding."}
{"id": "2506.08729", "pdf": "https://arxiv.org/pdf/2506.08729", "abs": "https://arxiv.org/abs/2506.08729", "authors": ["Dieuwertje Alblas", "Patryk Rygiel", "Julian Suk", "Kaj O. Kappe", "Marieke Hofman", "Christoph Brune", "Kak Khee Yeung", "Jelmer M. Wolterink"], "title": "Geometric deep learning for local growth prediction on abdominal aortic aneurysm surfaces", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Abdominal aortic aneurysms (AAAs) are progressive focal dilatations of the\nabdominal aorta. AAAs may rupture, with a survival rate of only 20\\%. Current\nclinical guidelines recommend elective surgical repair when the maximum AAA\ndiameter exceeds 55 mm in men or 50 mm in women. Patients that do not meet\nthese criteria are periodically monitored, with surveillance intervals based on\nthe maximum AAA diameter. However, this diameter does not take into account the\ncomplex relation between the 3D AAA shape and its growth, making standardized\nintervals potentially unfit. Personalized AAA growth predictions could improve\nmonitoring strategies. We propose to use an SE(3)-symmetric transformer model\nto predict AAA growth directly on the vascular model surface enriched with\nlocal, multi-physical features. In contrast to other works which have\nparameterized the AAA shape, this representation preserves the vascular\nsurface's anatomical structure and geometric fidelity. We train our model using\na longitudinal dataset of 113 computed tomography angiography (CTA) scans of 24\nAAA patients at irregularly sampled intervals. After training, our model\npredicts AAA growth to the next scan moment with a median diameter error of\n1.18 mm. We further demonstrate our model's utility to identify whether a\npatient will become eligible for elective repair within two years (acc = 0.93).\nFinally, we evaluate our model's generalization on an external validation set\nconsisting of 25 CTAs from 7 AAA patients from a different hospital. Our\nresults show that local directional AAA growth prediction from the vascular\nsurface is feasible and may contribute to personalized surveillance strategies."}
{"id": "2506.08735", "pdf": "https://arxiv.org/pdf/2506.08735", "abs": "https://arxiv.org/abs/2506.08735", "authors": ["Yuhang Wang", "Jun Li", "Zhijian Wu", "Jianhua Xu"], "title": "InceptionMamba: An Efficient Hybrid Network with Large Band Convolution and Bottleneck Mamba", "categories": ["cs.CV"], "comment": null, "summary": "Within the family of convolutional neural networks, InceptionNeXt has shown\nexcellent competitiveness in image classification and a number of downstream\ntasks. Built on parallel one-dimensional strip convolutions, however, it\nsuffers from limited ability of capturing spatial dependencies along different\ndimensions and fails to fully explore spatial modeling in local neighborhood.\nBesides, inherent locality constraints of convolution operations are\ndetrimental to effective global context modeling. To overcome these\nlimitations, we propose a novel backbone architecture termed InceptionMamba in\nthis study. More specifically, the traditional one-dimensional strip\nconvolutions are replaced by orthogonal band convolutions in our InceptionMamba\nto achieve cohesive spatial modeling. Furthermore, global contextual modeling\ncan be achieved via a bottleneck Mamba module, facilitating enhanced\ncross-channel information fusion and enlarged receptive field. Extensive\nevaluations on classification and various downstream tasks demonstrate that the\nproposed InceptionMamba achieves state-of-the-art performance with superior\nparameter and computational efficiency. The source code will be available at\nhttps://github.com/Wake1021/InceptionMamba."}
{"id": "2506.08772", "pdf": "https://arxiv.org/pdf/2506.08772", "abs": "https://arxiv.org/abs/2506.08772", "authors": ["Jiayi Song", "Kaiyu Li", "Xiangyong Cao", "Deyu Meng"], "title": "RS-MTDF: Multi-Teacher Distillation and Fusion for Remote Sensing Semi-Supervised Semantic Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Semantic segmentation in remote sensing images is crucial for various\napplications, yet its performance is heavily reliant on large-scale,\nhigh-quality pixel-wise annotations, which are notoriously expensive and\ntime-consuming to acquire. Semi-supervised semantic segmentation (SSS) offers a\npromising alternative to mitigate this data dependency. However, existing SSS\nmethods often struggle with the inherent distribution mismatch between limited\nlabeled data and abundant unlabeled data, leading to suboptimal generalization.\nWe propose that Vision Foundation Models (VFMs), pre-trained on vast and\ndiverse datasets, possess robust generalization capabilities that can\neffectively bridge this distribution gap and provide strong semantic priors for\nSSS. Inspired by this, we introduce RS-MTDF (Multi-Teacher Distillation and\nFusion), a novel framework that leverages the powerful semantic knowledge\nembedded in VFMs to guide semi-supervised learning in remote sensing.\nSpecifically, RS-MTDF employs multiple frozen VFMs (\\textit{e.g.}, DINOv2 and\nCLIP) as expert teachers, utilizing feature-level distillation to align student\nfeatures with their robust representations. To further enhance discriminative\npower, the distilled knowledge is seamlessly fused into the student decoder.\nExtensive experiments on three challenging remote sensing datasets (ISPRS\nPotsdam, LoveDA, and DeepGlobe) demonstrate that RS-MTDF consistently achieves\nstate-of-the-art performance. Notably, our method outperforms existing\napproaches across various label ratios on LoveDA and secures the highest IoU in\nthe majority of semantic categories. These results underscore the efficacy of\nmulti-teacher VFM guidance in significantly enhancing both generalization and\nsemantic understanding for remote sensing segmentation. Ablation studies\nfurther validate the contribution of each proposed module."}
{"id": "2506.08777", "pdf": "https://arxiv.org/pdf/2506.08777", "abs": "https://arxiv.org/abs/2506.08777", "authors": ["Keyi Liu", "Weidong Yang", "Ben Fei", "Ying He"], "title": "Gaussian2Scene: 3D Scene Representation Learning via Self-supervised Learning with 3D Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "Self-supervised learning (SSL) for point cloud pre-training has become a\ncornerstone for many 3D vision tasks, enabling effective learning from\nlarge-scale unannotated data. At the scene level, existing SSL methods often\nincorporate volume rendering into the pre-training framework, using RGB-D\nimages as reconstruction signals to facilitate cross-modal learning. This\nstrategy promotes alignment between 2D and 3D modalities and enables the model\nto benefit from rich visual cues in the RGB-D inputs. However, these approaches\nare limited by their reliance on implicit scene representations and high memory\ndemands. Furthermore, since their reconstruction objectives are applied only in\n2D space, they often fail to capture underlying 3D geometric structures. To\naddress these challenges, we propose Gaussian2Scene, a novel scene-level SSL\nframework that leverages the efficiency and explicit nature of 3D Gaussian\nSplatting (3DGS) for pre-training. The use of 3DGS not only alleviates the\ncomputational burden associated with volume rendering but also supports direct\n3D scene reconstruction, thereby enhancing the geometric understanding of the\nbackbone network. Our approach follows a progressive two-stage training\nstrategy. In the first stage, a dual-branch masked autoencoder learns both 2D\nand 3D scene representations. In the second stage, we initialize training with\nreconstructed point clouds and further supervise learning using the geometric\nlocations of Gaussian primitives and rendered RGB images. This process\nreinforces both geometric and cross-modal learning. We demonstrate the\neffectiveness of Gaussian2Scene across several downstream 3D object detection\ntasks, showing consistent improvements over existing pre-training methods."}
{"id": "2506.08780", "pdf": "https://arxiv.org/pdf/2506.08780", "abs": "https://arxiv.org/abs/2506.08780", "authors": ["Isaac Corley", "Lakshay Sharma", "Ruth Crasto"], "title": "Landsat-Bench: Datasets and Benchmarks for Landsat Foundation Models", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "The Landsat program offers over 50 years of globally consistent Earth\nimagery. However, the lack of benchmarks for this data constrains progress\ntowards Landsat-based Geospatial Foundation Models (GFM). In this paper, we\nintroduce Landsat-Bench, a suite of three benchmarks with Landsat imagery that\nadapt from existing remote sensing datasets -- EuroSAT-L, BigEarthNet-L, and\nLC100-L. We establish baseline and standardized evaluation methods across both\ncommon architectures and Landsat foundation models pretrained on the SSL4EO-L\ndataset. Notably, we provide evidence that SSL4EO-L pretrained GFMs extract\nbetter representations for downstream tasks in comparison to ImageNet,\nincluding performance gains of +4% OA and +5.1% mAP on EuroSAT-L and\nBigEarthNet-L."}
{"id": "2506.08784", "pdf": "https://arxiv.org/pdf/2506.08784", "abs": "https://arxiv.org/abs/2506.08784", "authors": ["Jongyub Seok", "Chanjin Kang"], "title": "HomographyAD: Deep Anomaly Detection Using Self Homography Learning", "categories": ["cs.CV"], "comment": null, "summary": "Anomaly detection (AD) is a task that distinguishes normal and abnormal data,\nwhich is important for applying automation technologies of the manufacturing\nfacilities. For MVTec dataset that is a representative AD dataset for\nindustrial environment, many recent works have shown remarkable performances.\nHowever, the existing anomaly detection works have a limitation of showing good\nperformance for fully-aligned datasets only, unlike real-world industrial\nenvironments. To solve this limitation, we propose HomographyAD, a novel deep\nanomaly detection methodology based on the ImageNet-pretrained network, which\nis specially designed for actual industrial dataset. Specifically, we first\nsuggest input foreground alignment using the deep homography estimation method.\nIn addition, we fine-tune the model by self homography learning to learn\nadditional shape information from normal samples. Finally, we conduct anomaly\ndetection based on the measure of how far the feature of test sample is from\nthe distribution of the extracted normal features. By applying our proposed\nmethod to various existing AD approaches, we show performance enhancement\nthrough extensive experiments."}
{"id": "2506.08793", "pdf": "https://arxiv.org/pdf/2506.08793", "abs": "https://arxiv.org/abs/2506.08793", "authors": ["Zhuoran Zheng"], "title": "A PDE-Based Image Dehazing Method via Atmospheric Scattering Theory", "categories": ["cs.CV", "eess.IV"], "comment": "report", "summary": "This paper presents a novel partial differential equation (PDE) framework for\nsingle-image dehazing. By integrating the atmospheric scattering model with\nnonlocal regularization and dark channel prior, we propose the improved PDE: \\[\n-\\text{div}\\left(D(\\nabla u)\\nabla u\\right) + \\lambda(t) G(u) = \\Phi(I,t,A) \\]\nwhere $D(\\nabla u) = (|\\nabla u| + \\epsilon)^{-1}$ is the edge-preserving\ndiffusion coefficient, $G(u)$ is the Gaussian convolution operator, and\n$\\lambda(t)$ is the adaptive regularization parameter based on transmission map\n$t$. We prove the existence and uniqueness of weak solutions in $H_0^1(\\Omega)$\nusing Lax-Milgram theorem, and implement an efficient fixed-point iteration\nscheme accelerated by PyTorch GPU computation. The experimental results\ndemonstrate that this method is a promising deghazing solution that can be\ngeneralized to the deep model paradigm."}
{"id": "2506.08796", "pdf": "https://arxiv.org/pdf/2506.08796", "abs": "https://arxiv.org/abs/2506.08796", "authors": ["Zhiyuan Ma", "Ruixun Liu", "Sixian Liu", "Jianjun Li", "Bowen Zhou"], "title": "Flow Diverse and Efficient: Learning Momentum Flow Matching via Stochastic Velocity Field Sampling", "categories": ["cs.CV"], "comment": null, "summary": "Recently, the rectified flow (RF) has emerged as the new state-of-the-art\namong flow-based diffusion models due to its high efficiency advantage in\nstraight path sampling, especially with the amazing images generated by a\nseries of RF models such as Flux 1.0 and SD 3.0. Although a straight-line\nconnection between the noisy and natural data distributions is intuitive, fast,\nand easy to optimize, it still inevitably leads to: 1) Diversity concerns,\nwhich arise since straight-line paths only cover a fairly restricted sampling\nspace. 2) Multi-scale noise modeling concerns, since the straight line flow\nonly needs to optimize the constant velocity field $\\bm v$ between the two\ndistributions $\\bm\\pi_0$ and $\\bm\\pi_1$. In this work, we present\nDiscretized-RF, a new family of rectified flow (also called momentum flow\nmodels since they refer to the previous velocity component and the random\nvelocity component in each diffusion step), which discretizes the straight path\ninto a series of variable velocity field sub-paths (namely ``momentum fields'')\nto expand the search space, especially when close to the distribution\n$p_\\text{noise}$. Different from the previous case where noise is directly\nsuperimposed on $\\bm x$, we introduce noise on the velocity $\\bm v$ of the\nsub-path to change its direction in order to improve the diversity and\nmulti-scale noise modeling abilities. Experimental results on several\nrepresentative datasets demonstrate that learning momentum flow matching by\nsampling random velocity fields will produce trajectories that are both diverse\nand efficient, and can consistently generate high-quality and diverse results.\nCode is available at https://github.com/liuruixun/momentum-fm."}
{"id": "2506.08797", "pdf": "https://arxiv.org/pdf/2506.08797", "abs": "https://arxiv.org/abs/2506.08797", "authors": ["Ziyao Huang", "Zixiang Zhou", "Juan Cao", "Yifeng Ma", "Yi Chen", "Zejing Rao", "Zhiyong Xu", "Hongmei Wang", "Qin Lin", "Yuan Zhou", "Qinglin Lu", "Fan Tang"], "title": "HunyuanVideo-HOMA: Generic Human-Object Interaction in Multimodal Driven Human Animation", "categories": ["cs.CV"], "comment": null, "summary": "To address key limitations in human-object interaction (HOI) video generation\n-- specifically the reliance on curated motion data, limited generalization to\nnovel objects/scenarios, and restricted accessibility -- we introduce\nHunyuanVideo-HOMA, a weakly conditioned multimodal-driven framework.\nHunyuanVideo-HOMA enhances controllability and reduces dependency on precise\ninputs through sparse, decoupled motion guidance. It encodes appearance and\nmotion signals into the dual input space of a multimodal diffusion transformer\n(MMDiT), fusing them within a shared context space to synthesize temporally\nconsistent and physically plausible interactions. To optimize training, we\nintegrate a parameter-space HOI adapter initialized from pretrained MMDiT\nweights, preserving prior knowledge while enabling efficient adaptation, and a\nfacial cross-attention adapter for anatomically accurate audio-driven lip\nsynchronization. Extensive experiments confirm state-of-the-art performance in\ninteraction naturalness and generalization under weak supervision. Finally,\nHunyuanVideo-HOMA demonstrates versatility in text-conditioned generation and\ninteractive object manipulation, supported by a user-friendly demo interface.\nThe project page is at https://anonymous.4open.science/w/homa-page-0FBE/."}
{"id": "2506.08809", "pdf": "https://arxiv.org/pdf/2506.08809", "abs": "https://arxiv.org/abs/2506.08809", "authors": ["Jiaze E", "Srutarshi Banerjee", "Tekin Bicer", "Guannan Wang", "Yanfu Zhang", "Bin Ren"], "title": "HiSin: Efficient High-Resolution Sinogram Inpainting via Resolution-Guided Progressive Inference", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "High-resolution sinogram inpainting is essential for computed tomography\nreconstruction, as missing high-frequency projections can lead to visible\nartifacts and diagnostic errors. Diffusion models are well-suited for this task\ndue to their robustness and detail-preserving capabilities, but their\napplication to high-resolution inputs is limited by excessive memory and\ncomputational demands. To address this limitation, we propose HiSin, a novel\ndiffusion based framework for efficient sinogram inpainting via\nresolution-guided progressive inference. It progressively extracts global\nstructure at low resolution and defers high-resolution inference to small\npatches, enabling memory-efficient inpainting. It further incorporates\nfrequency-aware patch skipping and structure-adaptive step allocation to reduce\nredundant computation. Experimental results show that HiSin reduces peak memory\nusage by up to 31.25% and inference time by up to 18.15%, and maintains\ninpainting accuracy across datasets, resolutions, and mask conditions."}
{"id": "2506.08817", "pdf": "https://arxiv.org/pdf/2506.08817", "abs": "https://arxiv.org/abs/2506.08817", "authors": ["Shuyi Zhang", "Xiaoshuai Hao", "Yingbo Tang", "Lingfeng Zhang", "Pengwei Wang", "Zhongyuan Wang", "Hongxuan Ma", "Shanghang Zhang"], "title": "Video-CoT: A Comprehensive Dataset for Spatiotemporal Understanding of Videos Based on Chain-of-Thought", "categories": ["cs.CV"], "comment": null, "summary": "Video content comprehension is essential for various applications, ranging\nfrom video analysis to interactive systems. Despite advancements in large-scale\nvision-language models (VLMs), these models often struggle to capture the\nnuanced, spatiotemporal details essential for thorough video analysis. To\naddress this gap, we introduce Video-CoT, a groundbreaking dataset designed to\nenhance spatiotemporal understanding using Chain-of-Thought (CoT)\nmethodologies. Video-CoT contains 192,000 fine-grained spa-tiotemporal\nquestion-answer pairs and 23,000 high-quality CoT-annotated samples, providing\na solid foundation for evaluating spatiotemporal understanding in video\ncomprehension. Additionally, we provide a comprehensive benchmark for assessing\nthese tasks, with each task featuring 750 images and tailored evaluation\nmetrics. Our extensive experiments reveal that current VLMs face significant\nchallenges in achieving satisfactory performance, high-lighting the\ndifficulties of effective spatiotemporal understanding. Overall, the Video-CoT\ndataset and benchmark open new avenues for research in multimedia understanding\nand support future innovations in intelligent systems requiring advanced video\nanalysis capabilities. By making these resources publicly available, we aim to\nencourage further exploration in this critical area. Project\nwebsite:https://video-cot.github.io/ ."}
{"id": "2506.08835", "pdf": "https://arxiv.org/pdf/2506.08835", "abs": "https://arxiv.org/abs/2506.08835", "authors": ["Shravan Nayak", "Mehar Bhatia", "Xiaofeng Zhang", "Verena Rieser", "Lisa Anne Hendricks", "Sjoerd van Steenkiste", "Yash Goyal", "Karolina Sta≈Ñczak", "Aishwarya Agrawal"], "title": "CulturalFrames: Assessing Cultural Expectation Alignment in Text-to-Image Models and Evaluation Metrics", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "The increasing ubiquity of text-to-image (T2I) models as tools for visual\ncontent generation raises concerns about their ability to accurately represent\ndiverse cultural contexts. In this work, we present the first study to\nsystematically quantify the alignment of T2I models and evaluation metrics with\nrespect to both explicit as well as implicit cultural expectations. To this\nend, we introduce CulturalFrames, a novel benchmark designed for rigorous human\nevaluation of cultural representation in visual generations. Spanning 10\ncountries and 5 socio-cultural domains, CulturalFrames comprises 983 prompts,\n3637 corresponding images generated by 4 state-of-the-art T2I models, and over\n10k detailed human annotations. We find that T2I models not only fail to meet\nthe more challenging implicit expectations but also the less challenging\nexplicit expectations. Across models and countries, cultural expectations are\nmissed an average of 44% of the time. Among these failures, explicit\nexpectations are missed at a surprisingly high average rate of 68%, while\nimplicit expectation failures are also significant, averaging 49%. Furthermore,\nwe demonstrate that existing T2I evaluation metrics correlate poorly with human\njudgments of cultural alignment, irrespective of their internal reasoning.\nCollectively, our findings expose critical gaps, providing actionable\ndirections for developing more culturally informed T2I models and evaluation\nmethodologies."}
{"id": "2506.08849", "pdf": "https://arxiv.org/pdf/2506.08849", "abs": "https://arxiv.org/abs/2506.08849", "authors": ["Jingguo Qu", "Xinyang Han", "Tonghuan Xiao", "Jia Ai", "Juan Wu", "Tong Zhao", "Jing Qin", "Ann Dorothy King", "Winnie Chiu-Wing Chu", "Jing Cai", "Michael Tin-Cheung Yingƒ±nst"], "title": "Adapting Vision-Language Foundation Model for Next Generation Medical Ultrasound Image Analysis", "categories": ["cs.CV"], "comment": null, "summary": "Medical ultrasonography is an essential imaging technique for examining\nsuperficial organs and tissues, including lymph nodes, breast, and thyroid. It\nemploys high-frequency ultrasound waves to generate detailed images of the\ninternal structures of the human body. However, manually contouring regions of\ninterest in these images is a labor-intensive task that demands expertise and\noften results in inconsistent interpretations among individuals.\nVision-language foundation models, which have excelled in various computer\nvision applications, present new opportunities for enhancing ultrasound image\nanalysis. Yet, their performance is hindered by the significant differences\nbetween natural and medical imaging domains. This research seeks to overcome\nthese challenges by developing domain adaptation methods for vision-language\nfoundation models. In this study, we explore the fine-tuning pipeline for\nvision-language foundation models by utilizing large language model as text\nrefiner with special-designed adaptation strategies and task-driven heads. Our\napproach has been extensively evaluated on six ultrasound datasets and two\ntasks: segmentation and classification. The experimental results show that our\nmethod can effectively improve the performance of vision-language foundation\nmodels for ultrasound image analysis, and outperform the existing\nstate-of-the-art vision-language and pure foundation models. The source code of\nthis study is available at\n\\href{https://github.com/jinggqu/NextGen-UIA}{GitHub}."}
{"id": "2506.08854", "pdf": "https://arxiv.org/pdf/2506.08854", "abs": "https://arxiv.org/abs/2506.08854", "authors": ["Junzhuo Liu", "Markus Eckstein", "Zhixiang Wang", "Friedrich Feuerhake", "Dorit Merhof"], "title": "Spatial Transcriptomics Expression Prediction from Histopathology Based on Cross-Modal Mask Reconstruction and Contrastive Learning", "categories": ["cs.CV", "cs.AI"], "comment": "20 pages, 7 figures", "summary": "Spatial transcriptomics is a technology that captures gene expression levels\nat different spatial locations, widely used in tumor microenvironment analysis\nand molecular profiling of histopathology, providing valuable insights into\nresolving gene expression and clinical diagnosis of cancer. Due to the high\ncost of data acquisition, large-scale spatial transcriptomics data remain\nchallenging to obtain. In this study, we develop a contrastive learning-based\ndeep learning method to predict spatially resolved gene expression from\nwhole-slide images. Evaluation across six different disease datasets\ndemonstrates that, compared to existing studies, our method improves Pearson\nCorrelation Coefficient (PCC) in the prediction of highly expressed genes,\nhighly variable genes, and marker genes by 6.27%, 6.11%, and 11.26%\nrespectively. Further analysis indicates that our method preserves gene-gene\ncorrelations and applies to datasets with limited samples. Additionally, our\nmethod exhibits potential in cancer tissue localization based on biomarker\nexpression."}
{"id": "2506.08862", "pdf": "https://arxiv.org/pdf/2506.08862", "abs": "https://arxiv.org/abs/2506.08862", "authors": ["Zike Wu", "Qi Yan", "Xuanyu Yi", "Lele Wang", "Renjie Liao"], "title": "StreamSplat: Towards Online Dynamic 3D Reconstruction from Uncalibrated Video Streams", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Real-time reconstruction of dynamic 3D scenes from uncalibrated video streams\nis crucial for numerous real-world applications. However, existing methods\nstruggle to jointly address three key challenges: 1) processing uncalibrated\ninputs in real time, 2) accurately modeling dynamic scene evolution, and 3)\nmaintaining long-term stability and computational efficiency. To this end, we\nintroduce StreamSplat, the first fully feed-forward framework that transforms\nuncalibrated video streams of arbitrary length into dynamic 3D Gaussian\nSplatting (3DGS) representations in an online manner, capable of recovering\nscene dynamics from temporally local observations. We propose two key technical\ninnovations: a probabilistic sampling mechanism in the static encoder for 3DGS\nposition prediction, and a bidirectional deformation field in the dynamic\ndecoder that enables robust and efficient dynamic modeling. Extensive\nexperiments on static and dynamic benchmarks demonstrate that StreamSplat\nconsistently outperforms prior works in both reconstruction quality and dynamic\nscene modeling, while uniquely supporting online reconstruction of arbitrarily\nlong video streams. Code and models are available at\nhttps://github.com/nickwzk/StreamSplat."}
{"id": "2506.08887", "pdf": "https://arxiv.org/pdf/2506.08887", "abs": "https://arxiv.org/abs/2506.08887", "authors": ["Leqi Shen", "Guoqiang Gong", "Tianxiang Hao", "Tao He", "Yifeng Zhang", "Pengzhang Liu", "Sicheng Zhao", "Jungong Han", "Guiguang Ding"], "title": "DiscoVLA: Discrepancy Reduction in Vision, Language, and Alignment for Parameter-Efficient Video-Text Retrieval", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "The parameter-efficient adaptation of the image-text pretraining model CLIP\nfor video-text retrieval is a prominent area of research. While CLIP is focused\non image-level vision-language matching, video-text retrieval demands\ncomprehensive understanding at the video level. Three key discrepancies emerge\nin the transfer from image-level to video-level: vision, language, and\nalignment. However, existing methods mainly focus on vision while neglecting\nlanguage and alignment. In this paper, we propose Discrepancy Reduction in\nVision, Language, and Alignment (DiscoVLA), which simultaneously mitigates all\nthree discrepancies. Specifically, we introduce Image-Video Features Fusion to\nintegrate image-level and video-level features, effectively tackling both\nvision and language discrepancies. Additionally, we generate pseudo image\ncaptions to learn fine-grained image-level alignment. To mitigate alignment\ndiscrepancies, we propose Image-to-Video Alignment Distillation, which\nleverages image-level alignment knowledge to enhance video-level alignment.\nExtensive experiments demonstrate the superiority of our DiscoVLA. In\nparticular, on MSRVTT with CLIP (ViT-B/16), DiscoVLA outperforms previous\nmethods by 1.5% in R@1, reaching a final score of 50.5% R@1. The code is\navailable at https://github.com/LunarShen/DsicoVLA."}
{"id": "2506.08894", "pdf": "https://arxiv.org/pdf/2506.08894", "abs": "https://arxiv.org/abs/2506.08894", "authors": ["Yunzhi Zhang", "Carson Murtuza-Lanier", "Zizhang Li", "Yilun Du", "Jiajun Wu"], "title": "Product of Experts for Visual Generation", "categories": ["cs.CV", "cs.AI"], "comment": "Project page: https://product-of-experts.github.io/", "summary": "Modern neural models capture rich priors and have complementary knowledge\nover shared data domains, e.g., images and videos. Integrating diverse\nknowledge from multiple sources -- including visual generative models, visual\nlanguage models, and sources with human-crafted knowledge such as graphics\nengines and physics simulators -- remains under-explored. We propose a Product\nof Experts (PoE) framework that performs inference-time knowledge composition\nfrom heterogeneous models. This training-free approach samples from the product\ndistribution across experts via Annealed Importance Sampling (AIS). Our\nframework shows practical benefits in image and video synthesis tasks, yielding\nbetter controllability than monolithic methods and additionally providing\nflexible user interfaces for specifying visual generation goals."}
{"id": "2506.08896", "pdf": "https://arxiv.org/pdf/2506.08896", "abs": "https://arxiv.org/abs/2506.08896", "authors": ["Negin Ghamsarian", "Raphael Sznitman", "Klaus Schoeffmann", "Jens Kowal"], "title": "WetCat: Automating Skill Assessment in Wetlab Cataract Surgery Videos", "categories": ["cs.CV"], "comment": "9 pages, 6 figures", "summary": "To meet the growing demand for systematic surgical training, wetlab\nenvironments have become indispensable platforms for hands-on practice in\nophthalmology. Yet, traditional wetlab training depends heavily on manual\nperformance evaluations, which are labor-intensive, time-consuming, and often\nsubject to variability. Recent advances in computer vision offer promising\navenues for automated skill assessment, enhancing both the efficiency and\nobjectivity of surgical education. Despite notable progress in ophthalmic\nsurgical datasets, existing resources predominantly focus on real surgeries or\nisolated tasks, falling short of supporting comprehensive skill evaluation in\ncontrolled wetlab settings. To address these limitations, we introduce WetCat,\nthe first dataset of wetlab cataract surgery videos specifically curated for\nautomated skill assessment. WetCat comprises high-resolution recordings of\nsurgeries performed by trainees on artificial eyes, featuring comprehensive\nphase annotations and semantic segmentations of key anatomical structures.\nThese annotations are meticulously designed to facilitate skill assessment\nduring the critical capsulorhexis and phacoemulsification phases, adhering to\nstandardized surgical skill assessment frameworks. By focusing on these\nessential phases, WetCat enables the development of interpretable, AI-driven\nevaluation tools aligned with established clinical metrics. This dataset lays a\nstrong foundation for advancing objective, scalable surgical education and sets\na new benchmark for automated workflow analysis and skill assessment in\nophthalmology training. The dataset and annotations are publicly available in\nSynapse https://www.synapse.org/Synapse:syn66401174/files."}
{"id": "2506.08900", "pdf": "https://arxiv.org/pdf/2506.08900", "abs": "https://arxiv.org/abs/2506.08900", "authors": ["Jos√© Morano", "Botond Fazekas", "Emese S√ºkei", "Ronald Fecso", "Taha Emre", "Markus Gumpinger", "Georg Faustmann", "Marzieh Oghbaie", "Ursula Schmidt-Erfurth", "Hrvoje Bogunoviƒá"], "title": "MIRAGE: Multimodal foundation model and benchmark for comprehensive retinal OCT image analysis", "categories": ["cs.CV"], "comment": null, "summary": "Artificial intelligence (AI) has become a fundamental tool for assisting\nclinicians in analyzing ophthalmic images, such as optical coherence tomography\n(OCT). However, developing AI models often requires extensive annotation, and\nexisting models tend to underperform on independent, unseen data. Foundation\nmodels (FMs), large AI models trained on vast unlabeled datasets, have shown\npromise in overcoming these challenges. Nonetheless, available FMs for\nophthalmology lack extensive validation, especially for segmentation tasks, and\nfocus on a single imaging modality. In this context, we propose MIRAGE, a novel\nmultimodal FM for the analysis of OCT and scanning laser ophthalmoscopy (SLO)\nimages. Additionally, we propose a new evaluation benchmark with OCT/SLO\nclassification and segmentation tasks. The comparison with general and\nspecialized FMs and segmentation methods shows the superiority of MIRAGE in\nboth types of tasks, highlighting its suitability as a basis for the\ndevelopment of robust AI systems for retinal OCT image analysis. Both MIRAGE\nand the evaluation benchmark are publicly available:\nhttps://github.com/j-morano/MIRAGE."}
{"id": "2506.08906", "pdf": "https://arxiv.org/pdf/2506.08906", "abs": "https://arxiv.org/abs/2506.08906", "authors": ["Peilin Yu", "Yuwei Wu", "Zhi Gao", "Xiaomeng Fan", "Shuo Yang", "Yunde Jia"], "title": "Hyperbolic Dual Feature Augmentation for Open-Environment", "categories": ["cs.CV"], "comment": "arXiv admin note: text overlap with arXiv:2207.03824,\n  arXiv:2304.11855 by other authors", "summary": "Feature augmentation generates novel samples in the feature space, providing\nan effective way to enhance the generalization ability of learning algorithms\nwith hyperbolic geometry. Most hyperbolic feature augmentation is confined to\nclosed-environment, assuming the number of classes is fixed (\\emph{i.e.}, seen\nclasses) and generating features only for these classes. In this paper, we\npropose a hyperbolic dual feature augmentation method for open-environment,\nwhich augments features for both seen and unseen classes in the hyperbolic\nspace. To obtain a more precise approximation of the real data distribution for\nefficient training, (1) we adopt a neural ordinary differential equation\nmodule, enhanced by meta-learning, estimating the feature distributions of both\nseen and unseen classes; (2) we then introduce a regularizer to preserve the\nlatent hierarchical structures of data in the hyperbolic space; (3) we also\nderive an upper bound for the hyperbolic dual augmentation loss, allowing us to\ntrain a hyperbolic model using infinite augmentations for seen and unseen\nclasses. Extensive experiments on five open-environment tasks:\nclass-incremental learning, few-shot open-set recognition, few-shot learning,\nzero-shot learning, and general image classification, demonstrate that our\nmethod effectively enhances the performance of hyperbolic algorithms in\nopen-environment."}
{"id": "2506.08908", "pdf": "https://arxiv.org/pdf/2506.08908", "abs": "https://arxiv.org/abs/2506.08908", "authors": ["Jiajun Li", "Yue Ma", "Xinyu Zhang", "Qingyan Wei", "Songhua Liu", "Linfeng Zhang"], "title": "SkipVAR: Accelerating Visual Autoregressive Modeling via Adaptive Frequency-Aware Skipping", "categories": ["cs.CV"], "comment": null, "summary": "Recent studies on Visual Autoregressive (VAR) models have highlighted that\nhigh-frequency components, or later steps, in the generation process contribute\ndisproportionately to inference latency. However, the underlying computational\nredundancy involved in these steps has yet to be thoroughly investigated. In\nthis paper, we conduct an in-depth analysis of the VAR inference process and\nidentify two primary sources of inefficiency: step redundancy and unconditional\nbranch redundancy. To address step redundancy, we propose an automatic\nstep-skipping strategy that selectively omits unnecessary generation steps to\nimprove efficiency. For unconditional branch redundancy, we observe that the\ninformation gap between the conditional and unconditional branches is minimal.\nLeveraging this insight, we introduce unconditional branch replacement, a\ntechnique that bypasses the unconditional branch to reduce computational cost.\nNotably, we observe that the effectiveness of acceleration strategies varies\nsignificantly across different samples. Motivated by this, we propose SkipVAR,\na sample-adaptive framework that leverages frequency information to dynamically\nselect the most suitable acceleration strategy for each instance. To evaluate\nthe role of high-frequency information, we introduce high-variation benchmark\ndatasets that test model sensitivity to fine details. Extensive experiments\nshow SkipVAR achieves over 0.88 average SSIM with up to 1.81x overall\nacceleration and 2.62x speedup on the GenEval benchmark, maintaining model\nquality. These results confirm the effectiveness of frequency-aware,\ntraining-free adaptive acceleration for scalable autoregressive image\ngeneration. Our code is available at https://github.com/fakerone-li/SkipVAR and\nhas been publicly released."}
{"id": "2506.08915", "pdf": "https://arxiv.org/pdf/2506.08915", "abs": "https://arxiv.org/abs/2506.08915", "authors": ["Ananthu Aniraj", "Cassio F. Dantas", "Dino Ienco", "Diego Marcos"], "title": "Inherently Faithful Attention Maps for Vision Transformers", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We introduce an attention-based method that uses learned binary attention\nmasks to ensure that only attended image regions influence the prediction.\nContext can strongly affect object perception, sometimes leading to biased\nrepresentations, particularly when objects appear in out-of-distribution\nbackgrounds. At the same time, many image-level object-centric tasks require\nidentifying relevant regions, often requiring context. To address this\nconundrum, we propose a two-stage framework: stage 1 processes the full image\nto discover object parts and identify task-relevant regions, while stage 2\nleverages input attention masking to restrict its receptive field to these\nregions, enabling a focused analysis while filtering out potentially spurious\ninformation. Both stages are trained jointly, allowing stage 2 to refine stage\n1. Extensive experiments across diverse benchmarks demonstrate that our\napproach significantly improves robustness against spurious correlations and\nout-of-distribution backgrounds."}
{"id": "2506.08927", "pdf": "https://arxiv.org/pdf/2506.08927", "abs": "https://arxiv.org/abs/2506.08927", "authors": ["David Acuna", "Ximing Lu", "Jaehun Jung", "Hyunwoo Kim", "Amlan Kar", "Sanja Fidler", "Yejin Choi"], "title": "Socratic-MCTS: Test-Time Visual Reasoning by Asking the Right Questions", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Recent research in vision-language models (VLMs) has centered around the\npossibility of equipping them with implicit long-form chain-of-thought\nreasoning -- akin to the success observed in language models -- via\ndistillation and reinforcement learning. But what about the non-reasoning\nmodels already trained and deployed across the internet? Should we simply\nabandon them, or is there hope for a search mechanism that can elicit hidden\nknowledge and induce long reasoning traces -- without any additional training\nor supervision? In this paper, we explore this possibility using a Monte Carlo\nTree Search (MCTS)-inspired algorithm, which injects subquestion-subanswer\npairs into the model's output stream. We show that framing reasoning as a\nsearch process -- where subquestions act as latent decisions within a broader\ninference trajectory -- helps the model \"connect the dots\" between fragmented\nknowledge and produce extended reasoning traces in non-reasoning models. We\nevaluate our method across three benchmarks and observe consistent\nimprovements. Notably, our approach yields a 2% overall improvement on\nMMMU-PRO, including a significant 9% gain in Liberal Arts."}
{"id": "2506.08933", "pdf": "https://arxiv.org/pdf/2506.08933", "abs": "https://arxiv.org/abs/2506.08933", "authors": ["Wendong Bu", "Yang Wu", "Qifan Yu", "Minghe Gao", "Bingchen Miao", "Zhenkui Zhang", "Kaihang Pan", "Yunfei Li", "Mengze Li", "Wei Ji", "Juncheng Li", "Siliang Tang", "Yueting Zhuang"], "title": "What Limits Virtual Agent Application? OmniBench: A Scalable Multi-Dimensional Benchmark for Essential Virtual Agent Capabilities", "categories": ["cs.CV"], "comment": "Accepted by ICML 2025 (Oral)", "summary": "As multimodal large language models (MLLMs) advance, MLLM-based virtual\nagents have demonstrated remarkable performance. However, existing benchmarks\nface significant limitations, including uncontrollable task complexity,\nextensive manual annotation with limited scenarios, and a lack of\nmultidimensional evaluation. In response to these challenges, we introduce\nOmniBench, a self-generating, cross-platform, graph-based benchmark with an\nautomated pipeline for synthesizing tasks of controllable complexity through\nsubtask composition. To evaluate the diverse capabilities of virtual agents on\nthe graph, we further present OmniEval, a multidimensional evaluation framework\nthat includes subtask-level evaluation, graph-based metrics, and comprehensive\ntests across 10 capabilities. Our synthesized dataset contains 36k\ngraph-structured tasks across 20 scenarios, achieving a 91\\% human acceptance\nrate. Training on our graph-structured data shows that it can more efficiently\nguide agents compared to manually annotated data. We conduct multidimensional\nevaluations for various open-source and closed-source models, revealing their\nperformance across various capabilities and paving the way for future\nadvancements. Our project is available at https://omni-bench.github.io/."}
{"id": "2506.08949", "pdf": "https://arxiv.org/pdf/2506.08949", "abs": "https://arxiv.org/abs/2506.08949", "authors": ["Hongjie Zhu", "Xiwei Liu", "Rundong Xue", "Zeyu Zhang", "Yong Xu", "Daji Ergu", "Ying Cai", "Yang Zhao"], "title": "SSS: Semi-Supervised SAM-2 with Efficient Prompting for Medical Imaging Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "In the era of information explosion, efficiently leveraging large-scale\nunlabeled data while minimizing the reliance on high-quality pixel-level\nannotations remains a critical challenge in the field of medical imaging.\nSemi-supervised learning (SSL) enhances the utilization of unlabeled data by\nfacilitating knowledge transfer, significantly improving the performance of\nfully supervised models and emerging as a highly promising research direction\nin medical image analysis. Inspired by the ability of Vision Foundation Models\n(e.g., SAM-2) to provide rich prior knowledge, we propose SSS (Semi-Supervised\nSAM-2), a novel approach that leverages SAM-2's robust feature extraction\ncapabilities to uncover latent knowledge in unlabeled medical images, thus\neffectively enhancing feature support for fully supervised medical image\nsegmentation. Specifically, building upon the single-stream \"weak-to-strong\"\nconsistency regularization framework, this paper introduces a Discriminative\nFeature Enhancement (DFE) mechanism to further explore the feature\ndiscrepancies introduced by various data augmentation strategies across\nmultiple views. By leveraging feature similarity and dissimilarity across\nmulti-scale augmentation techniques, the method reconstructs and models the\nfeatures, thereby effectively optimizing the salient regions. Furthermore, a\nprompt generator is developed that integrates Physical Constraints with a\nSliding Window (PCSW) mechanism to generate input prompts for unlabeled data,\nfulfilling SAM-2's requirement for additional prompts. Extensive experiments\ndemonstrate the superiority of the proposed method for semi-supervised medical\nimage segmentation on two multi-label datasets, i.e., ACDC and BHSD. Notably,\nSSS achieves an average Dice score of 53.15 on BHSD, surpassing the previous\nstate-of-the-art method by +3.65 Dice. Code will be available at\nhttps://github.com/AIGeeksGroup/SSS."}
{"id": "2506.08953", "pdf": "https://arxiv.org/pdf/2506.08953", "abs": "https://arxiv.org/abs/2506.08953", "authors": ["Anirudh Nanduri", "Siyuan Huang", "Rama Chellappa"], "title": "Cross-Spectral Body Recognition with Side Information Embedding: Benchmarks on LLCM and Analyzing Range-Induced Occlusions on IJB-MDF", "categories": ["cs.CV"], "comment": null, "summary": "Vision Transformers (ViTs) have demonstrated impressive performance across a\nwide range of biometric tasks, including face and body recognition. In this\nwork, we adapt a ViT model pretrained on visible (VIS) imagery to the\nchallenging problem of cross-spectral body recognition, which involves matching\nimages captured in the visible and infrared (IR) domains. Recent ViT\narchitectures have explored incorporating additional embeddings beyond\ntraditional positional embeddings. Building on this idea, we integrate Side\nInformation Embedding (SIE) and examine the impact of encoding domain and\ncamera information to enhance cross-spectral matching. Surprisingly, our\nresults show that encoding only camera information - without explicitly\nincorporating domain information - achieves state-of-the-art performance on the\nLLCM dataset. While occlusion handling has been extensively studied in\nvisible-spectrum person re-identification (Re-ID), occlusions in\nvisible-infrared (VI) Re-ID remain largely underexplored - primarily because\nexisting VI-ReID datasets, such as LLCM, SYSU-MM01, and RegDB, predominantly\nfeature full-body, unoccluded images. To address this gap, we analyze the\nimpact of range-induced occlusions using the IARPA Janus Benchmark Multi-Domain\nFace (IJB-MDF) dataset, which provides a diverse set of visible and infrared\nimages captured at various distances, enabling cross-range, cross-spectral\nevaluations."}
{"id": "2506.08955", "pdf": "https://arxiv.org/pdf/2506.08955", "abs": "https://arxiv.org/abs/2506.08955", "authors": ["Chunming He", "Kai Li", "Yachao Zhang", "Ziyun Yang", "Youwei Pang", "Longxiang Tang", "Chengyu Fang", "Yulun Zhang", "Linghe Kong", "Xiu Li", "Sina Farsiu"], "title": "Segment Concealed Objects with Incomplete Supervision", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "IEEE TPAMI", "summary": "Incompletely-Supervised Concealed Object Segmentation (ISCOS) involves\nsegmenting objects that seamlessly blend into their surrounding environments,\nutilizing incompletely annotated data, such as weak and semi-annotations, for\nmodel training. This task remains highly challenging due to (1) the limited\nsupervision provided by the incompletely annotated training data, and (2) the\ndifficulty of distinguishing concealed objects from the background, which\narises from the intrinsic similarities in concealed scenarios. In this paper,\nwe introduce the first unified method for ISCOS to address these challenges. To\ntackle the issue of incomplete supervision, we propose a unified mean-teacher\nframework, SEE, that leverages the vision foundation model, ``\\emph{Segment\nAnything Model (SAM)}'', to generate pseudo-labels using coarse masks produced\nby the teacher model as prompts. To mitigate the effect of low-quality\nsegmentation masks, we introduce a series of strategies for pseudo-label\ngeneration, storage, and supervision. These strategies aim to produce\ninformative pseudo-labels, store the best pseudo-labels generated, and select\nthe most reliable components to guide the student model, thereby ensuring\nrobust network training. Additionally, to tackle the issue of intrinsic\nsimilarity, we design a hybrid-granularity feature grouping module that groups\nfeatures at different granularities and aggregates these results. By clustering\nsimilar features, this module promotes segmentation coherence, facilitating\nmore complete segmentation for both single-object and multiple-object images.\nWe validate the effectiveness of our approach across multiple ISCOS tasks, and\nexperimental results demonstrate that our method achieves state-of-the-art\nperformance. Furthermore, SEE can serve as a plug-and-play solution, enhancing\nthe performance of existing models."}
{"id": "2506.08956", "pdf": "https://arxiv.org/pdf/2506.08956", "abs": "https://arxiv.org/abs/2506.08956", "authors": ["DaeEun Yoon", "Semin Kim", "SangWook Yoo", "Jongha Lee"], "title": "Data Augmentation For Small Object using Fast AutoAugment", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted and published in the USB Proceedings of the 20th\n  International Conference on Modeling Decisions for Artificial Intelligence\n  (MDAI 2023), Ume{\\aa}, Sweden, June 19--22, 2023, ISBN 978-91-527-7293-5,\n  pp.\\ 12--21", "summary": "In recent years, there has been tremendous progress in object detection\nperformance. However, despite these advances, the detection performance for\nsmall objects is significantly inferior to that of large objects. Detecting\nsmall objects is one of the most challenging and important problems in computer\nvision. To improve the detection performance for small objects, we propose an\noptimal data augmentation method using Fast AutoAugment. Through our proposed\nmethod, we can quickly find optimal augmentation policies that can overcome\ndegradation when detecting small objects, and we achieve a 20% performance\nimprovement on the DOTA dataset."}
{"id": "2506.08964", "pdf": "https://arxiv.org/pdf/2506.08964", "abs": "https://arxiv.org/abs/2506.08964", "authors": ["Jinwoo Kim", "Sangmin Han", "Jinho Jeong", "Jiwoo Choi", "Dongyoung Kim", "Seon Joo Kim"], "title": "ORIDa: Object-centric Real-world Image Composition Dataset", "categories": ["cs.CV"], "comment": "Accepted at CVPR 2025", "summary": "Object compositing, the task of placing and harmonizing objects in images of\ndiverse visual scenes, has become an important task in computer vision with the\nrise of generative models. However, existing datasets lack the diversity and\nscale required to comprehensively explore real-world scenarios. We introduce\nORIDa (Object-centric Real-world Image Composition Dataset), a large-scale,\nreal-captured dataset containing over 30,000 images featuring 200 unique\nobjects, each of which is presented across varied positions and scenes. ORIDa\nhas two types of data: factual-counterfactual sets and factual-only scenes. The\nfactual-counterfactual sets consist of four factual images showing an object in\ndifferent positions within a scene and a single counterfactual (or background)\nimage of the scene without the object, resulting in five images per scene. The\nfactual-only scenes include a single image containing an object in a specific\ncontext, expanding the variety of environments. To our knowledge, ORIDa is the\nfirst publicly available dataset with its scale and complexity for real-world\nimage composition. Extensive analysis and experiments highlight the value of\nORIDa as a resource for advancing further research in object compositing."}
{"id": "2506.08968", "pdf": "https://arxiv.org/pdf/2506.08968", "abs": "https://arxiv.org/abs/2506.08968", "authors": ["Amirreza Rouhi", "Solmaz Arezoomandan", "Knut Peterson", "Joseph T. Woods", "David K. Han"], "title": "ADAM: Autonomous Discovery and Annotation Model using LLMs for Context-Aware Annotations", "categories": ["cs.CV"], "comment": null, "summary": "Object detection models typically rely on predefined categories, limiting\ntheir ability to identify novel objects in open-world scenarios. To overcome\nthis constraint, we introduce ADAM: Autonomous Discovery and Annotation Model,\na training-free, self-refining framework for open-world object labeling. ADAM\nleverages large language models (LLMs) to generate candidate labels for unknown\nobjects based on contextual information from known entities within a scene.\nThese labels are paired with visual embeddings from CLIP to construct an\nEmbedding-Label Repository (ELR) that enables inference without category\nsupervision. For a newly encountered unknown object, ADAM retrieves visually\nsimilar instances from the ELR and applies frequency-based voting and\ncross-modal re-ranking to assign a robust label. To further enhance\nconsistency, we introduce a self-refinement loop that re-evaluates repository\nlabels using visual cohesion analysis and k-nearest-neighbor-based majority\nre-labeling. Experimental results on the COCO and PASCAL datasets demonstrate\nthat ADAM effectively annotates novel categories using only visual and\ncontextual signals, without requiring any fine-tuning or retraining."}
{"id": "2506.08979", "pdf": "https://arxiv.org/pdf/2506.08979", "abs": "https://arxiv.org/abs/2506.08979", "authors": ["Longyu Yang", "Ping Hu", "Lu Zhang", "Jun Liu", "Yap-Peng Tan", "Heng Tao Shen", "Xiaofeng Zhu"], "title": "Rethinking Range-View LiDAR Segmentation in Adverse Weather", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "LiDAR segmentation has emerged as an important task to enrich multimedia\nexperiences and analysis. Range-view-based methods have gained popularity due\nto their high computational efficiency and compatibility with real-time\ndeployment. However, their generalized performance under adverse weather\nconditions remains underexplored, limiting their reliability in real-world\nenvironments. In this work, we identify and analyze the unique challenges that\naffect the generalization of range-view LiDAR segmentation in severe weather.\nTo address these challenges, we propose a modular and lightweight framework\nthat enhances robustness without altering the core architecture of existing\nmodels. Our method reformulates the initial stem block of standard range-view\nnetworks into two branches to process geometric attributes and reflectance\nintensity separately. Specifically, a Geometric Abnormality Suppression (GAS)\nmodule reduces the influence of weather-induced spatial noise, and a\nReflectance Distortion Calibration (RDC) module corrects reflectance\ndistortions through memory-guided adaptive instance normalization. The\nprocessed features are then fused and passed to the original segmentation\npipeline. Extensive experiments on different benchmarks and baseline models\ndemonstrate that our approach significantly improves generalization to adverse\nweather with minimal inference overhead, offering a practical and effective\nsolution for real-world LiDAR segmentation."}
{"id": "2506.08990", "pdf": "https://arxiv.org/pdf/2506.08990", "abs": "https://arxiv.org/abs/2506.08990", "authors": ["Chenyu Lian", "Hong-Yu Zhou", "Dongyun Liang", "Jing Qin", "Liansheng Wang"], "title": "Efficient Medical Vision-Language Alignment Through Adapting Masked Vision Models", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "TMI 2025", "summary": "Medical vision-language alignment through cross-modal contrastive learning\nshows promising performance in image-text matching tasks, such as retrieval and\nzero-shot classification. However, conventional cross-modal contrastive\nlearning (CLIP-based) methods suffer from suboptimal visual representation\ncapabilities, which also limits their effectiveness in vision-language\nalignment. In contrast, although the models pretrained via multimodal masked\nmodeling struggle with direct cross-modal matching, they excel in visual\nrepresentation. To address this contradiction, we propose ALTA (ALign Through\nAdapting), an efficient medical vision-language alignment method that utilizes\nonly about 8% of the trainable parameters and less than 1/5 of the\ncomputational consumption required for masked record modeling. ALTA achieves\nsuperior performance in vision-language matching tasks like retrieval and\nzero-shot classification by adapting the pretrained vision model from masked\nrecord modeling. Additionally, we integrate temporal-multiview radiograph\ninputs to enhance the information consistency between radiographs and their\ncorresponding descriptions in reports, further improving the vision-language\nalignment. Experimental evaluations show that ALTA outperforms the\nbest-performing counterpart by over 4% absolute points in text-to-image\naccuracy and approximately 6% absolute points in image-to-text retrieval\naccuracy. The adaptation of vision-language models during efficient alignment\nalso promotes better vision and language understanding. Code is publicly\navailable at https://github.com/DopamineLcy/ALTA."}
{"id": "2506.08991", "pdf": "https://arxiv.org/pdf/2506.08991", "abs": "https://arxiv.org/abs/2506.08991", "authors": ["Anudeep Das", "Gurjot Singh", "Prach Chantasantitam", "N. Asokan"], "title": "Do Concept Replacement Techniques Really Erase Unacceptable Concepts?", "categories": ["cs.CV", "cs.CR"], "comment": null, "summary": "Generative models, particularly diffusion-based text-to-image (T2I) models,\nhave demonstrated astounding success. However, aligning them to avoid\ngenerating content with unacceptable concepts (e.g., offensive or copyrighted\ncontent, or celebrity likenesses) remains a significant challenge. Concept\nreplacement techniques (CRTs) aim to address this challenge, often by trying to\n\"erase\" unacceptable concepts from models. Recently, model providers have\nstarted offering image editing services which accept an image and a text prompt\nas input, to produce an image altered as specified by the prompt. These are\nknown as image-to-image (I2I) models. In this paper, we first use an I2I model\nto empirically demonstrate that today's state-of-the-art CRTs do not in fact\nerase unacceptable concepts. Existing CRTs are thus likely to be ineffective in\nemerging I2I scenarios, despite their proven ability to remove unwanted\nconcepts in T2I pipelines, highlighting the need to understand this discrepancy\nbetween T2I and I2I settings. Next, we argue that a good CRT, while replacing\nunacceptable concepts, should preserve other concepts specified in the inputs\nto generative models. We call this fidelity. Prior work on CRTs have neglected\nfidelity in the case of unacceptable concepts. Finally, we propose the use of\ntargeted image-editing techniques to achieve both effectiveness and fidelity.\nWe present such a technique, AntiMirror, and demonstrate its viability."}
{"id": "2506.08997", "pdf": "https://arxiv.org/pdf/2506.08997", "abs": "https://arxiv.org/abs/2506.08997", "authors": ["Fabian Immel", "Jan-Hendrik Pauls", "Richard Fehler", "Frank Bieder", "Jonas Merkert", "Christoph Stiller"], "title": "SDTagNet: Leveraging Text-Annotated Navigation Maps for Online HD Map Construction", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Autonomous vehicles rely on detailed and accurate environmental information\nto operate safely. High definition (HD) maps offer a promising solution, but\ntheir high maintenance cost poses a significant barrier to scalable deployment.\nThis challenge is addressed by online HD map construction methods, which\ngenerate local HD maps from live sensor data. However, these methods are\ninherently limited by the short perception range of onboard sensors. To\novercome this limitation and improve general performance, recent approaches\nhave explored the use of standard definition (SD) maps as prior, which are\nsignificantly easier to maintain. We propose SDTagNet, the first online HD map\nconstruction method that fully utilizes the information of widely available SD\nmaps, like OpenStreetMap, to enhance far range detection accuracy. Our approach\nintroduces two key innovations. First, in contrast to previous work, we\nincorporate not only polyline SD map data with manually selected classes, but\nadditional semantic information in the form of textual annotations. In this\nway, we enrich SD vector map tokens with NLP-derived features, eliminating the\ndependency on predefined specifications or exhaustive class taxonomies. Second,\nwe introduce a point-level SD map encoder together with orthogonal element\nidentifiers to uniformly integrate all types of map elements. Experiments on\nArgoverse 2 and nuScenes show that this boosts map perception performance by up\nto +5.9 mAP (+45%) w.r.t. map construction without priors and up to +3.2 mAP\n(+20%) w.r.t. previous approaches that already use SD map priors. Code is\navailable at https://github.com/immel-f/SDTagNet"}
{"id": "2506.09022", "pdf": "https://arxiv.org/pdf/2506.09022", "abs": "https://arxiv.org/abs/2506.09022", "authors": ["Daniel Shao", "Richard J. Chen", "Andrew H. Song", "Joel Runevic", "Ming Y. Lu", "Tong Ding", "Faisal Mahmood"], "title": "Do MIL Models Transfer?", "categories": ["cs.CV"], "comment": "ICML 2025 (Spotlight). 20 pages, 8 figures", "summary": "Multiple Instance Learning (MIL) is a cornerstone approach in computational\npathology (CPath) for generating clinically meaningful slide-level embeddings\nfrom gigapixel tissue images. However, MIL often struggles with small, weakly\nsupervised clinical datasets. In contrast to fields such as NLP and\nconventional computer vision, where transfer learning is widely used to address\ndata scarcity, the transferability of MIL models remains poorly understood. In\nthis study, we systematically evaluate the transfer learning capabilities of\npretrained MIL models by assessing 11 models across 21 pretraining tasks for\nmorphological and molecular subtype prediction. Our results show that\npretrained MIL models, even when trained on different organs than the target\ntask, consistently outperform models trained from scratch. Moreover,\npretraining on pancancer datasets enables strong generalization across organs\nand tasks, outperforming slide foundation models while using substantially less\npretraining data. These findings highlight the robust adaptability of MIL\nmodels and demonstrate the benefits of leveraging transfer learning to boost\nperformance in CPath. Lastly, we provide a resource which standardizes the\nimplementation of MIL models and collection of pretrained model weights on\npopular CPath tasks, available at https://github.com/mahmoodlab/MIL-Lab"}
{"id": "2506.09024", "pdf": "https://arxiv.org/pdf/2506.09024", "abs": "https://arxiv.org/abs/2506.09024", "authors": ["Felix Wagner", "Pramit Saha", "Harry Anthony", "J. Alison Noble", "Konstantinos Kamnitsas"], "title": "DIsoN: Decentralized Isolation Networks for Out-of-Distribution Detection in Medical Imaging", "categories": ["cs.CV", "cs.LG", "I.2.11; I.4.9; I.4.9; J.3; I.2.0"], "comment": null, "summary": "Safe deployment of machine learning (ML) models in safety-critical domains\nsuch as medical imaging requires detecting inputs with characteristics not seen\nduring training, known as out-of-distribution (OOD) detection, to prevent\nunreliable predictions. Effective OOD detection after deployment could benefit\nfrom access to the training data, enabling direct comparison between test\nsamples and the training data distribution to identify differences.\nState-of-the-art OOD detection methods, however, either discard training data\nafter deployment or assume that test samples and training data are centrally\nstored together, an assumption that rarely holds in real-world settings. This\nis because shipping training data with the deployed model is usually impossible\ndue to the size of training databases, as well as proprietary or privacy\nconstraints. We introduce the Isolation Network, an OOD detection framework\nthat quantifies the difficulty of separating a target test sample from the\ntraining data by solving a binary classification task. We then propose\nDecentralized Isolation Networks (DIsoN), which enables the comparison of\ntraining and test data when data-sharing is impossible, by exchanging only\nmodel parameters between the remote computational nodes of training and\ndeployment. We further extend DIsoN with class-conditioning, comparing a target\nsample solely with training data of its predicted class. We evaluate DIsoN on\nfour medical imaging datasets (dermatology, chest X-ray, breast ultrasound,\nhistopathology) across 12 OOD detection tasks. DIsoN performs favorably against\nexisting methods while respecting data-privacy. This decentralized OOD\ndetection framework opens the way for a new type of service that ML developers\ncould provide along with their models: providing remote, secure utilization of\ntheir training data for OOD detection services. Code will be available upon\nacceptance at: *****"}
{"id": "2506.09027", "pdf": "https://arxiv.org/pdf/2506.09027", "abs": "https://arxiv.org/abs/2506.09027", "authors": ["Runqian Wang", "Kaiming He"], "title": "Diffuse and Disperse: Image Generation with Representation Regularization", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "The development of diffusion-based generative models over the past decade has\nlargely proceeded independently of progress in representation learning. These\ndiffusion models typically rely on regression-based objectives and generally\nlack explicit regularization. In this work, we propose \\textit{Dispersive\nLoss}, a simple plug-and-play regularizer that effectively improves\ndiffusion-based generative models. Our loss function encourages internal\nrepresentations to disperse in the hidden space, analogous to contrastive\nself-supervised learning, with the key distinction that it requires no positive\nsample pairs and therefore does not interfere with the sampling process used\nfor regression. Compared to the recent method of representation alignment\n(REPA), our approach is self-contained and minimalist, requiring no\npre-training, no additional parameters, and no external data. We evaluate\nDispersive Loss on the ImageNet dataset across a range of models and report\nconsistent improvements over widely used and strong baselines. We hope our work\nwill help bridge the gap between generative modeling and representation\nlearning."}
{"id": "2506.09035", "pdf": "https://arxiv.org/pdf/2506.09035", "abs": "https://arxiv.org/abs/2506.09035", "authors": ["Karhan Kayan", "Stamatis Alexandropoulos", "Rishabh Jain", "Yiming Zuo", "Erich Liang", "Jia Deng"], "title": "Princeton365: A Diverse Dataset with Accurate Camera Pose", "categories": ["cs.CV"], "comment": null, "summary": "We introduce Princeton365, a large-scale diverse dataset of 365 videos with\naccurate camera pose. Our dataset bridges the gap between accuracy and data\ndiversity in current SLAM benchmarks by introducing a novel ground truth\ncollection framework that leverages calibration boards and a 360-camera. We\ncollect indoor, outdoor, and object scanning videos with synchronized monocular\nand stereo RGB video outputs as well as IMU. We further propose a new scene\nscale-aware evaluation metric for SLAM based on the the optical flow induced by\nthe camera pose estimation error. In contrast to the current metrics, our new\nmetric allows for comparison between the performance of SLAM methods across\nscenes as opposed to existing metrics such as Average Trajectory Error (ATE),\nallowing researchers to analyze the failure modes of their methods. We also\npropose a challenging Novel View Synthesis benchmark that covers cases not\ncovered by current NVS benchmarks, such as fully non-Lambertian scenes with\n360-degree camera trajectories. Please visit\nhttps://princeton365.cs.princeton.edu for the dataset, code, videos, and\nsubmission."}
{"id": "2506.09040", "pdf": "https://arxiv.org/pdf/2506.09040", "abs": "https://arxiv.org/abs/2506.09040", "authors": ["Dianyi Wang", "Wei Song", "Yikun Wang", "Siyuan Wang", "Kaicheng Yu", "Zhongyu Wei", "Jiaqi Wang"], "title": "Autoregressive Semantic Visual Reconstruction Helps VLMs Understand Better", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Typical large vision-language models (LVLMs) apply autoregressive supervision\nsolely to textual sequences, without fully incorporating the visual modality\ninto the learning process. This results in three key limitations: (1) an\ninability to utilize images without accompanying captions, (2) the risk that\ncaptions omit critical visual details, and (3) the challenge that certain\nvision-centric content cannot be adequately conveyed through text. As a result,\ncurrent LVLMs often prioritize vision-to-language alignment while potentially\noverlooking fine-grained visual information. While some prior works have\nexplored autoregressive image generation, effectively leveraging autoregressive\nvisual supervision to enhance image understanding remains an open challenge. In\nthis paper, we introduce Autoregressive Semantic Visual Reconstruction (ASVR),\nwhich enables joint learning of visual and textual modalities within a unified\nautoregressive framework. We show that autoregressively reconstructing the raw\nvisual appearance of images does not enhance and may even impair multimodal\nunderstanding. In contrast, autoregressively reconstructing the semantic\nrepresentation of images consistently improves comprehension. Notably, we find\nthat even when models are given continuous image features as input, they can\neffectively reconstruct discrete semantic tokens, resulting in stable and\nconsistent improvements across a wide range of multimodal understanding\nbenchmarks. Our approach delivers significant performance gains across varying\ndata scales (556k-2M) and types of LLM bacbones. Specifically, ASVR improves\nLLaVA-1.5 by 5% in average scores across 14 multimodal benchmarks. The code is\navailable at https://github.com/AlenjandroWang/ASVR."}
{"id": "2506.09042", "pdf": "https://arxiv.org/pdf/2506.09042", "abs": "https://arxiv.org/abs/2506.09042", "authors": ["Xuanchi Ren", "Yifan Lu", "Tianshi Cao", "Ruiyuan Gao", "Shengyu Huang", "Amirmojtaba Sabour", "Tianchang Shen", "Tobias Pfaff", "Jay Zhangjie Wu", "Runjian Chen", "Seung Wook Kim", "Jun Gao", "Laura Leal-Taixe", "Mike Chen", "Sanja Fidler", "Huan Ling"], "title": "Cosmos-Drive-Dreams: Scalable Synthetic Driving Data Generation with World Foundation Models", "categories": ["cs.CV"], "comment": "Xuanchi Ren, Yifan Lu, Tianshi Cao, Ruiyuan Gao: Equal contribution.\n  Only the core contributors are listed. The full list of contributors can be\n  found in Appendix A of this paper", "summary": "Collecting and annotating real-world data for safety-critical physical AI\nsystems, such as Autonomous Vehicle (AV), is time-consuming and costly. It is\nespecially challenging to capture rare edge cases, which play a critical role\nin training and testing of an AV system. To address this challenge, we\nintroduce the Cosmos-Drive-Dreams - a synthetic data generation (SDG) pipeline\nthat aims to generate challenging scenarios to facilitate downstream tasks such\nas perception and driving policy training. Powering this pipeline is\nCosmos-Drive, a suite of models specialized from NVIDIA Cosmos world foundation\nmodel for the driving domain and are capable of controllable, high-fidelity,\nmulti-view, and spatiotemporally consistent driving video generation. We\nshowcase the utility of these models by applying Cosmos-Drive-Dreams to scale\nthe quantity and diversity of driving datasets with high-fidelity and\nchallenging scenarios. Experimentally, we demonstrate that our generated data\nhelps in mitigating long-tail distribution problems and enhances generalization\nin downstream tasks such as 3D lane detection, 3D object detection and driving\npolicy learning. We open source our pipeline toolkit, dataset and model weights\nthrough the NVIDIA's Cosmos platform.\n  Project page: https://research.nvidia.com/labs/toronto-ai/cosmos_drive_dreams"}
{"id": "2506.09045", "pdf": "https://arxiv.org/pdf/2506.09045", "abs": "https://arxiv.org/abs/2506.09045", "authors": ["Zehong Ma", "Longhui Wei", "Feng Wang", "Shiliang Zhang", "Qi Tian"], "title": "MagCache: Fast Video Generation with Magnitude-Aware Cache", "categories": ["cs.CV"], "comment": "Project Page: https://zehong-ma.github.io/MagCache", "summary": "Existing acceleration techniques for video diffusion models often rely on\nuniform heuristics or time-embedding variants to skip timesteps and reuse\ncached features. These approaches typically require extensive calibration with\ncurated prompts and risk inconsistent outputs due to prompt-specific\noverfitting. In this paper, we introduce a novel and robust discovery: a\nunified magnitude law observed across different models and prompts.\nSpecifically, the magnitude ratio of successive residual outputs decreases\nmonotonically and steadily in most timesteps while rapidly in the last several\nsteps. Leveraging this insight, we introduce a Magnitude-aware Cache (MagCache)\nthat adaptively skips unimportant timesteps using an error modeling mechanism\nand adaptive caching strategy. Unlike existing methods requiring dozens of\ncurated samples for calibration, MagCache only requires a single sample for\ncalibration. Experimental results show that MagCache achieves 2.1x and 2.68x\nspeedups on Open-Sora and Wan 2.1, respectively, while preserving superior\nvisual fidelity. It significantly outperforms existing methods in LPIPS, SSIM,\nand PSNR, under comparable computational budgets."}
{"id": "2506.08019", "pdf": "https://arxiv.org/pdf/2506.08019", "abs": "https://arxiv.org/abs/2506.08019", "authors": ["Andrew Wells", "Geraldine Henningsen", "Brice Bolane Tchinde Kengne"], "title": "Gridding Forced Displacement using Semi-Supervised Learning", "categories": ["cs.LG", "cs.CV", "cs.CY"], "comment": null, "summary": "We present a semi-supervised approach that disaggregates refugee statistics\nfrom administrative boundaries to 0.5-degree grid cells across 25 Sub-Saharan\nAfrican countries. By integrating UNHCR's ProGres registration data with\nsatellite-derived building footprints from Google Open Buildings and location\ncoordinates from OpenStreetMap Populated Places, our label spreading algorithm\ncreates spatially explicit refugee statistics at high granularity.This\nmethodology achieves 92.9% average accuracy in placing over 10 million refugee\nobservations into appropriate grid cells, enabling the identification of\nlocalized displacement patterns previously obscured in broader regional and\nnational statistics. The resulting high-resolution dataset provides a\nfoundation for a deeper understanding of displacement drivers."}
{"id": "2506.08020", "pdf": "https://arxiv.org/pdf/2506.08020", "abs": "https://arxiv.org/abs/2506.08020", "authors": ["Zi-Ying Chen", "Chuan-Xian Ren", "Hong Yan"], "title": "Bi-level Unbalanced Optimal Transport for Partial Domain Adaptation", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Partial domain adaptation (PDA) problem requires aligning cross-domain\nsamples while distinguishing the outlier classes for accurate knowledge\ntransfer. The widely used weighting framework tries to address the outlier\nclasses by introducing the reweighed source domain with a similar label\ndistribution to the target domain. However, the empirical modeling of weights\ncan only characterize the sample-wise relations, which leads to insufficient\nexploration of cluster structures, and the weights could be sensitive to the\ninaccurate prediction and cause confusion on the outlier classes. To tackle\nthese issues, we propose a Bi-level Unbalanced Optimal Transport (BUOT) model\nto simultaneously characterize the sample-wise and class-wise relations in a\nunified transport framework. Specifically, a cooperation mechanism between\nsample-level and class-level transport is introduced, where the sample-level\ntransport provides essential structure information for the class-level\nknowledge transfer, while the class-level transport supplies discriminative\ninformation for the outlier identification. The bi-level transport plan\nprovides guidance for the alignment process. By incorporating the label-aware\ntransport cost, the local transport structure is ensured and a fast computation\nformulation is derived to improve the efficiency. Extensive experiments on\nbenchmark datasets validate the competitiveness of BUOT."}
{"id": "2506.08022", "pdf": "https://arxiv.org/pdf/2506.08022", "abs": "https://arxiv.org/abs/2506.08022", "authors": ["Chenxi Liu", "Tianyi Xiong", "Ruibo Chen", "Yihan Wu", "Junfeng Guo", "Tianyi Zhou", "Heng Huang"], "title": "Modality-Balancing Preference Optimization of Large Multimodal Models by Adversarial Negative Mining", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "The task adaptation and alignment of Large Multimodal Models (LMMs) have been\nsignificantly advanced by instruction tuning and further strengthened by recent\npreference optimization. Yet, most LMMs still suffer from severe modality\nimbalance during reasoning, i.e., outweighing language prior biases over visual\ninputs, which bottlenecks their generalization to downstream tasks and causes\nhallucinations. However, existing preference optimization approaches for LMMs\ndo not focus on restraining the internal biases of their Large Language Model\n(LLM) backbones when curating the training data. Moreover, they heavily rely on\noffline data and lack the capacity to explore diverse responses adaptive to\ndynamic distributional shifts during training. Meanwhile, Group Relative Policy\nOptimization (GRPO), a recent method using online-generated data and verified\nrewards to improve reasoning capabilities, remains largely underexplored in LMM\nalignment. In this paper, we propose a novel preference learning framework,\nModality-Balancing Preference Optimization (MBPO), to address the modality\nimbalance in LMMs. MBPO constructs a more effective offline preference dataset\nby generating hard negatives, i.e., rejected responses misled by LLM biases due\nto limited usage of visual information, through adversarial perturbation of\ninput images. Moreover, MBPO leverages the easy-to-verify nature of close-ended\ntasks to generate online responses with verified rewards. GRPO is then employed\nto train the model with offline-online hybrid data. Extensive experiments\ndemonstrate that MBPO can enhance LMM performance on challenging\nvision-language tasks and effectively reduce hallucinations."}
{"id": "2506.08023", "pdf": "https://arxiv.org/pdf/2506.08023", "abs": "https://arxiv.org/abs/2506.08023", "authors": ["Qifeng Wu", "Zhengzhe Liu", "Han Zhu", "Yizhou Zhao", "Daisuke Kihara", "Min Xu"], "title": "Aligning Proteins and Language: A Foundation Model for Protein Retrieval", "categories": ["q-bio.BM", "cs.AI", "cs.CE", "cs.CV", "cs.LG"], "comment": "4 pages for body, 3 pages for appendix, 11 figures. Accepted to CVPR\n  2025 Workshop on Multimodal Foundation Models for Biomedicine: Challenges and\n  Opportunities(MMFM-BIOMED)", "summary": "This paper aims to retrieve proteins with similar structures and semantics\nfrom large-scale protein dataset, facilitating the functional interpretation of\nprotein structures derived by structural determination methods like\ncryo-Electron Microscopy (cryo-EM). Motivated by the recent progress of\nvision-language models (VLMs), we propose a CLIP-style framework for aligning\n3D protein structures with functional annotations using contrastive learning.\nFor model training, we propose a large-scale dataset of approximately 200,000\nprotein-caption pairs with rich functional descriptors. We evaluate our model\nin both in-domain and more challenging cross-database retrieval on Protein Data\nBank (PDB) and Electron Microscopy Data Bank (EMDB) dataset, respectively. In\nboth cases, our approach demonstrates promising zero-shot retrieval\nperformance, highlighting the potential of multimodal foundation models for\nstructure-function understanding in protein biology."}
{"id": "2506.08043", "pdf": "https://arxiv.org/pdf/2506.08043", "abs": "https://arxiv.org/abs/2506.08043", "authors": ["Ashkan Shahbazi", "Kyvia Pereira", "Jon S. Heiselman", "Elaheh Akbari", "Annie C. Benson", "Sepehr Seifi", "Xinyuan Liu", "Garrison L. Johnston", "Erwin Terpstra", "Anne Draaisma", "Jan-Jaap Severes", "Jie Ying Wu", "Nabil Simaan", "Michael L. Miga", "Soheil Kolouri"], "title": "Neural-Augmented Kelvinlet: Real-Time Soft Tissue Deformation with Multiple Graspers", "categories": ["cs.GR", "cs.CV", "cs.LG", "cs.RO"], "comment": null, "summary": "Fast and accurate simulation of soft tissue deformation is a critical factor\nfor surgical robotics and medical training. In this paper, we introduce a novel\nphysics-informed neural simulator that approximates soft tissue deformations in\na realistic and real-time manner. Our framework integrates Kelvinlet-based\npriors into neural simulators, making it the first approach to leverage\nKelvinlets for residual learning and regularization in data-driven soft tissue\nmodeling. By incorporating large-scale Finite Element Method (FEM) simulations\nof both linear and nonlinear soft tissue responses, our method improves neural\nnetwork predictions across diverse architectures, enhancing accuracy and\nphysical consistency while maintaining low latency for real-time performance.\nWe demonstrate the effectiveness of our approach by performing accurate\nsurgical maneuvers that simulate the use of standard laparoscopic tissue\ngrasping tools with high fidelity. These results establish Kelvinlet-augmented\nlearning as a powerful and efficient strategy for real-time, physics-aware soft\ntissue simulation in surgical applications."}
{"id": "2506.08064", "pdf": "https://arxiv.org/pdf/2506.08064", "abs": "https://arxiv.org/abs/2506.08064", "authors": ["Livio Tenze", "Enrique Canessa"], "title": "A Real-time 3D Desktop Display", "categories": ["cs.GR", "cs.CV"], "comment": "10 pages, 5 figures", "summary": "A new extended version of the altiro3D C++ Library -- initially developed to\nget glass-free holographic displays starting from 2D images -- is here\nintroduced aiming to deal with 3D video streams from either 2D webcam images or\nflat video files. These streams are processed in real-time to synthesize\nlight-fields (in Native format) and feed realistic 3D experiences. The core\nfunction needed to recreate multiviews consists on the use of MiDaS\nConvolutional Neural Network (CNN), which allows to extract a depth map from a\nsingle 2D image. Artificial Intelligence (AI) computing techniques are applied\nto improve the overall performance of the extended altiro3D Library. Thus,\naltiro3D can now treat standard images, video streams or screen portions of a\nDesktop where other apps may be also running (like web browsers, video chats,\netc) and render them into 3D. To achieve the latter, a screen region need to be\nselected in order to feed the output directly into a light-field 3D device such\nas Looking Glass (LG) Portrait. In order to simplify the acquisition of a\nDesktop screen area by the user, a multi-platform Graphical User Interface has\nbeen also implemented. Sources available at:\nhttps://github.com/canessae/altiro3D/releases/tag/2.0.0"}
{"id": "2506.08167", "pdf": "https://arxiv.org/pdf/2506.08167", "abs": "https://arxiv.org/abs/2506.08167", "authors": ["Sunny Gupta", "Nikita Jangid", "Amit Sethi"], "title": "UniVarFL: Uniformity and Variance Regularized Federated Learning for Heterogeneous Data", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.DC", "I.2.6; C.1.4; D.1.3; I.5.1; H.3.4; I.2.10; I.4.0; I.4.1; I.4.2;\n  I.4.6; I.4.7; I.4.8; I.4.9; I.4.10; I.5.1; I.5.2; I.5.4; J.2; I.2.11; I.2.10"], "comment": null, "summary": "Federated Learning (FL) often suffers from severe performance degradation\nwhen faced with non-IID data, largely due to local classifier bias. Traditional\nremedies such as global model regularization or layer freezing either incur\nhigh computational costs or struggle to adapt to feature shifts. In this work,\nwe propose UniVarFL, a novel FL framework that emulates IID-like training\ndynamics directly at the client level, eliminating the need for global model\ndependency. UniVarFL leverages two complementary regularization strategies\nduring local training: Classifier Variance Regularization, which aligns\nclass-wise probability distributions with those expected under IID conditions,\neffectively mitigating local classifier bias; and Hyperspherical Uniformity\nRegularization, which encourages a uniform distribution of feature\nrepresentations across the hypersphere, thereby enhancing the model's ability\nto generalize under diverse data distributions. Extensive experiments on\nmultiple benchmark datasets demonstrate that UniVarFL outperforms existing\nmethods in accuracy, highlighting its potential as a highly scalable and\nefficient solution for real-world FL deployments, especially in\nresource-constrained settings. Code: https://github.com/sunnyinAI/UniVarFL"}
{"id": "2506.08183", "pdf": "https://arxiv.org/pdf/2506.08183", "abs": "https://arxiv.org/abs/2506.08183", "authors": ["Isha Puri", "David Cox"], "title": "A System for Accurate Tracking and Video Recordings of Rodent Eye Movements using Convolutional Neural Networks for Biomedical Image Segmentation", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Research in neuroscience and vision science relies heavily on careful\nmeasurements of animal subject's gaze direction. Rodents are the most widely\nstudied animal subjects for such research because of their economic advantage\nand hardiness. Recently, video based eye trackers that use image processing\ntechniques have become a popular option for gaze tracking because they are easy\nto use and are completely noninvasive. Although significant progress has been\nmade in improving the accuracy and robustness of eye tracking algorithms,\nunfortunately, almost all of the techniques have focused on human eyes, which\ndoes not account for the unique characteristics of the rodent eye images, e.g.,\nvariability in eye parameters, abundance of surrounding hair, and their small\nsize. To overcome these unique challenges, this work presents a flexible,\nrobust, and highly accurate model for pupil and corneal reflection\nidentification in rodent gaze determination that can be incrementally trained\nto account for variability in eye parameters encountered in the field. To the\nbest of our knowledge, this is the first paper that demonstrates a highly\naccurate and practical biomedical image segmentation based convolutional neural\nnetwork architecture for pupil and corneal reflection identification in eye\nimages. This new method, in conjunction with our automated infrared videobased\neye recording system, offers the state of the art technology in eye tracking\nfor neuroscience and vision science research for rodents."}
{"id": "2506.08258", "pdf": "https://arxiv.org/pdf/2506.08258", "abs": "https://arxiv.org/abs/2506.08258", "authors": ["Lorenzo Arboit", "Dennis N. Schneider", "Toby Collins", "Daniel A. Hashimoto", "Silvana Perretta", "Bernard Dallemagne", "Jacques Marescaux", "EAES Working Group", "Nicolas Padoy", "Pietro Mascagni"], "title": "Surgeons Awareness, Expectations, and Involvement with Artificial Intelligence: a Survey Pre and Post the GPT Era", "categories": ["cs.CY", "cs.CV"], "comment": "11 pages, 3 figures", "summary": "Artificial Intelligence (AI) is transforming medicine, with generative AI\nmodels like ChatGPT reshaping perceptions of its potential. This study examines\nsurgeons' awareness, expectations, and involvement with AI in surgery through\ncomparative surveys conducted in 2021 and 2024. Two cross-sectional surveys\nwere distributed globally in 2021 and 2024, the first before an IRCAD webinar\nand the second during the annual EAES meeting. The surveys assessed\ndemographics, AI awareness, expectations, involvement, and ethics (2024 only).\nThe surveys collected a total of 671 responses from 98 countries, 522 in 2021\nand 149 in 2024. Awareness of AI courses rose from 14.5% in 2021 to 44.6% in\n2024, while course attendance increased from 12.9% to 23%. Despite this,\nfamiliarity with foundational AI concepts remained limited. Expectations for\nAI's role shifted in 2024, with hospital management gaining relevance. Ethical\nconcerns gained prominence, with 87.2% of 2024 participants emphasizing\naccountability and transparency. Infrastructure limitations remained the\nprimary obstacle to implementation. Interdisciplinary collaboration and\nstructured training were identified as critical for successful AI adoption.\nOptimism about AI's transformative potential remained high, with 79.9% of\nrespondents believing AI would positively impact surgery and 96.6% willing to\nintegrate AI into their clinical practice. Surgeons' perceptions of AI are\nevolving, driven by the rise of generative AI and advancements in surgical data\nscience. While enthusiasm for integration is strong, knowledge gaps and\ninfrastructural challenges persist. Addressing these through education, ethical\nframeworks, and infrastructure development is essential."}
{"id": "2506.08277", "pdf": "https://arxiv.org/pdf/2506.08277", "abs": "https://arxiv.org/abs/2506.08277", "authors": ["Subba Reddy Oota", "Khushbu Pahwa", "Prachi Jindal", "Satya Sai Srinath Namburi", "Maneesh Singh", "Tanmoy Chakraborty", "Bapi S. Raju", "Manish Gupta"], "title": "Instruction-Tuned Video-Audio Models Elucidate Functional Specialization in the Brain", "categories": ["q-bio.NC", "cs.AI", "cs.CL", "cs.CV", "cs.LG"], "comment": "39 pages, 22 figures", "summary": "Recent voxel-wise multimodal brain encoding studies have shown that\nmultimodal large language models (MLLMs) exhibit a higher degree of brain\nalignment compared to unimodal models in both unimodal and multimodal stimulus\nsettings. More recently, instruction-tuned multimodal models have shown to\ngenerate task-specific representations that align strongly with brain activity.\nHowever, prior work evaluating the brain alignment of MLLMs has primarily\nfocused on unimodal settings or relied on non-instruction-tuned multimodal\nmodels for multimodal stimuli. To address this gap, we investigated brain\nalignment, that is, measuring the degree of predictivity of neural activity\nrecorded while participants were watching naturalistic movies (video along with\naudio) with representations derived from MLLMs. We utilized\ninstruction-specific embeddings from six video and two audio instruction-tuned\nMLLMs. Experiments with 13 video task-specific instructions show that\ninstruction-tuned video MLLMs significantly outperform non-instruction-tuned\nmultimodal (by 15%) and unimodal models (by 20%). Our evaluation of MLLMs for\nboth video and audio tasks using language-guided instructions shows clear\ndisentanglement in task-specific representations from MLLMs, leading to precise\ndifferentiation of multimodal functional processing in the brain. We also find\nthat MLLM layers align hierarchically with the brain, with early sensory areas\nshowing strong alignment with early layers, while higher-level visual and\nlanguage regions align more with middle to late layers. These findings provide\nclear evidence for the role of task-specific instructions in improving the\nalignment between brain activity and MLLMs, and open new avenues for mapping\njoint information processing in both the systems. We make the code publicly\navailable [https://github.com/subbareddy248/mllm_videos]."}
{"id": "2506.08280", "pdf": "https://arxiv.org/pdf/2506.08280", "abs": "https://arxiv.org/abs/2506.08280", "authors": ["Daniel H. Pak", "Shubh Thaker", "Kyle Baylous", "Xiaoran Zhang", "Danny Bluestein", "James S. Duncan"], "title": "Snap-and-tune: combining deep learning and test-time optimization for high-fidelity cardiovascular volumetric meshing", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "High-quality volumetric meshing from medical images is a key bottleneck for\nphysics-based simulations in personalized medicine. For volumetric meshing of\ncomplex medical structures, recent studies have often utilized deep learning\n(DL)-based template deformation approaches to enable fast test-time generation\nwith high spatial accuracy. However, these approaches still exhibit\nlimitations, such as limited flexibility at high-curvature areas and\nunrealistic inter-part distances. In this study, we introduce a simple yet\neffective snap-and-tune strategy that sequentially applies DL and test-time\noptimization, which combines fast initial shape fitting with more detailed\nsample-specific mesh corrections. Our method provides significant improvements\nin both spatial accuracy and mesh quality, while being fully automated and\nrequiring no additional training labels. Finally, we demonstrate the\nversatility and usefulness of our newly generated meshes via solid mechanics\nsimulations in two different software platforms. Our code is available at\nhttps://github.com/danpak94/Deep-Cardiac-Volumetric-Mesh."}
{"id": "2506.08334", "pdf": "https://arxiv.org/pdf/2506.08334", "abs": "https://arxiv.org/abs/2506.08334", "authors": ["Weikun Peng", "Jun Lv", "Cewu Lu", "Manolis Savva"], "title": "Generalizable Articulated Object Reconstruction from Casually Captured RGBD Videos", "categories": ["cs.GR", "cs.CV"], "comment": "Project website can be found at\n  https://3dlg-hcvc.github.io/video2articulation/", "summary": "Articulated objects are prevalent in daily life. Understanding their\nkinematic structure and reconstructing them have numerous applications in\nembodied AI and robotics. However, current methods require carefully captured\ndata for training or inference, preventing practical, scalable, and\ngeneralizable reconstruction of articulated objects. We focus on reconstruction\nof an articulated object from a casually captured RGBD video shot with a\nhand-held camera. A casually captured video of an interaction with an\narticulated object is easy to acquire at scale using smartphones. However, this\nsetting is quite challenging, as the object and camera move simultaneously and\nthere are significant occlusions as the person interacts with the object. To\ntackle these challenges, we introduce a coarse-to-fine framework that infers\njoint parameters and segments movable parts of the object from a dynamic RGBD\nvideo. To evaluate our method under this new setting, we build a 20$\\times$\nlarger synthetic dataset of 784 videos containing 284 objects across 11\ncategories. We compare our approach with existing methods that also take video\nas input. Experiments show that our method can reconstruct synthetic and real\narticulated objects across different categories from dynamic RGBD videos,\noutperforming existing methods significantly."}
{"id": "2506.08350", "pdf": "https://arxiv.org/pdf/2506.08350", "abs": "https://arxiv.org/abs/2506.08350", "authors": ["Yicheng Zhan", "Dong-Ha Shin", "Seung-Hwan Baek", "Kaan Ak≈üit"], "title": "Complex-Valued Holographic Radiance Fields", "categories": ["cs.GR", "cs.CV", "cs.ET"], "comment": "28 pages, 21 figures", "summary": "Modeling the full properties of light, including both amplitude and phase, in\n3D representations is crucial for advancing physically plausible rendering,\nparticularly in holographic displays. To support these features, we propose a\nnovel representation that optimizes 3D scenes without relying on\nintensity-based intermediaries. We reformulate 3D Gaussian splatting with\ncomplex-valued Gaussian primitives, expanding support for rendering with light\nwaves. By leveraging RGBD multi-view images, our method directly optimizes\ncomplex-valued Gaussians as a 3D holographic scene representation. This\neliminates the need for computationally expensive hologram re-optimization.\nCompared with state-of-the-art methods, our method achieves 30x-10,000x speed\nimprovements while maintaining on-par image quality, representing a first step\ntowards geometrically aligned, physically plausible holographic scene\nrepresentations."}
{"id": "2506.08353", "pdf": "https://arxiv.org/pdf/2506.08353", "abs": "https://arxiv.org/abs/2506.08353", "authors": ["Hyunseok Seung", "Jaewoo Lee", "Hyunsuk Ko"], "title": "An Adaptive Method Stabilizing Activations for Enhanced Generalization", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "We introduce AdaAct, a novel optimization algorithm that adjusts learning\nrates according to activation variance. Our method enhances the stability of\nneuron outputs by incorporating neuron-wise adaptivity during the training\nprocess, which subsequently leads to better generalization -- a complementary\napproach to conventional activation regularization methods. Experimental\nresults demonstrate AdaAct's competitive performance across standard image\nclassification benchmarks. We evaluate AdaAct on CIFAR and ImageNet, comparing\nit with other state-of-the-art methods. Importantly, AdaAct effectively bridges\nthe gap between the convergence speed of Adam and the strong generalization\ncapabilities of SGD, all while maintaining competitive execution times. Code is\navailable at https://github.com/hseung88/adaact."}
{"id": "2506.08435", "pdf": "https://arxiv.org/pdf/2506.08435", "abs": "https://arxiv.org/abs/2506.08435", "authors": ["Mingyuan Fan", "Fuyi Wang", "Cen Chen", "Jianying Zhou"], "title": "Boosting Gradient Leakage Attacks: Data Reconstruction in Realistic FL Settings", "categories": ["cs.LG", "cs.CR", "cs.CV"], "comment": "Accepted to Usenix Security 2025", "summary": "Federated learning (FL) enables collaborative model training among multiple\nclients without the need to expose raw data. Its ability to safeguard privacy,\nat the heart of FL, has recently been a hot-button debate topic. To elaborate,\nseveral studies have introduced a type of attacks known as gradient leakage\nattacks (GLAs), which exploit the gradients shared during training to\nreconstruct clients' raw data. On the flip side, some literature, however,\ncontends no substantial privacy risk in practical FL environments due to the\neffectiveness of such GLAs being limited to overly relaxed conditions, such as\nsmall batch sizes and knowledge of clients' data distributions.\n  This paper bridges this critical gap by empirically demonstrating that\nclients' data can still be effectively reconstructed, even within realistic FL\nenvironments. Upon revisiting GLAs, we recognize that their performance\nfailures stem from their inability to handle the gradient matching problem. To\nalleviate the performance bottlenecks identified above, we develop FedLeak,\nwhich introduces two novel techniques, partial gradient matching and gradient\nregularization. Moreover, to evaluate the performance of FedLeak in real-world\nFL environments, we formulate a practical evaluation protocol grounded in a\nthorough review of extensive FL literature and industry practices. Under this\nprotocol, FedLeak can still achieve high-fidelity data reconstruction, thereby\nunderscoring the significant vulnerability in FL systems and the urgent need\nfor more effective defense methods."}
{"id": "2506.08443", "pdf": "https://arxiv.org/pdf/2506.08443", "abs": "https://arxiv.org/abs/2506.08443", "authors": ["Kazuki Kawamura", "Jun Rekimoto"], "title": "SakugaFlow: A Stagewise Illustration Framework Emulating the Human Drawing Process and Providing Interactive Tutoring for Novice Drawing Skills", "categories": ["cs.HC", "cs.CV", "68T05", "H.5.2; K.3; I.2.7"], "comment": "5 pages, 1 figure; accepted as a paper to the Generative AI and HCI\n  (GenAICHI) workshop at CHI 2025 (Yokohama, 27 Apr 2025)", "summary": "While current AI illustration tools can generate high-quality images from\ntext prompts, they rarely reveal the step-by-step procedure that human artists\nfollow. We present SakugaFlow, a four-stage pipeline that pairs diffusion-based\nimage generation with a large-language-model tutor. At each stage, novices\nreceive real-time feedback on anatomy, perspective, and composition, revise any\nstep non-linearly, and branch alternative versions. By exposing intermediate\noutputs and embedding pedagogical dialogue, SakugaFlow turns a black-box\ngenerator into a scaffolded learning environment that supports both creative\nexploration and skills acquisition."}
{"id": "2506.08480", "pdf": "https://arxiv.org/pdf/2506.08480", "abs": "https://arxiv.org/abs/2506.08480", "authors": ["Huixuan Zhang", "Xiaojun Wan"], "title": "Re-Thinking the Automatic Evaluation of Image-Text Alignment in Text-to-Image Models", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Text-to-image models often struggle to generate images that precisely match\ntextual prompts. Prior research has extensively studied the evaluation of\nimage-text alignment in text-to-image generation. However, existing evaluations\nprimarily focus on agreement with human assessments, neglecting other critical\nproperties of a trustworthy evaluation framework. In this work, we first\nidentify two key aspects that a reliable evaluation should address. We then\nempirically demonstrate that current mainstream evaluation frameworks fail to\nfully satisfy these properties across a diverse range of metrics and models.\nFinally, we propose recommendations for improving image-text alignment\nevaluation."}
{"id": "2506.08518", "pdf": "https://arxiv.org/pdf/2506.08518", "abs": "https://arxiv.org/abs/2506.08518", "authors": ["Sunny Gupta", "Nikita Jangid", "Shounak Das", "Amit Sethi"], "title": "FEDTAIL: Federated Long-Tailed Domain Generalization with Sharpness-Guided Gradient Matching", "categories": ["cs.AI", "cs.CV", "cs.LG", "I.2.6; C.1.4; D.1.3; I.5.1; H.3.4; I.2.10; I.4.0; I.4.1; I.4.2;\n  I.4.6; I.4.7; I.4.8; I.4.9; I.4.10; I.5.1; I.5.2; I.5.4; J.2; I.2.11; I.2.10"], "comment": "Accepted at ICML 2025 Workshop on Collaborative and Federated Agentic\n  Workflows CFAgentic @ ICML'25", "summary": "Domain Generalization (DG) seeks to train models that perform reliably on\nunseen target domains without access to target data during training. While\nrecent progress in smoothing the loss landscape has improved generalization,\nexisting methods often falter under long-tailed class distributions and\nconflicting optimization objectives. We introduce FedTAIL, a federated domain\ngeneralization framework that explicitly addresses these challenges through\nsharpness-guided, gradient-aligned optimization. Our method incorporates a\ngradient coherence regularizer to mitigate conflicts between classification and\nadversarial objectives, leading to more stable convergence. To combat class\nimbalance, we perform class-wise sharpness minimization and propose a\ncurvature-aware dynamic weighting scheme that adaptively emphasizes\nunderrepresented tail classes. Furthermore, we enhance conditional distribution\nalignment by integrating sharpness-aware perturbations into entropy\nregularization, improving robustness under domain shift. FedTAIL unifies\noptimization harmonization, class-aware regularization, and conditional\nalignment into a scalable, federated-compatible framework. Extensive\nevaluations across standard domain generalization benchmarks demonstrate that\nFedTAIL achieves state-of-the-art performance, particularly in the presence of\ndomain shifts and label imbalance, validating its effectiveness in both\ncentralized and federated settings. Code: https://github.com/sunnyinAI/FedTail"}
{"id": "2506.08520", "pdf": "https://arxiv.org/pdf/2506.08520", "abs": "https://arxiv.org/abs/2506.08520", "authors": ["Srinivasan Kidambi", "Pravin Nair"], "title": "Plug-and-Play Linear Attention for Pre-trained Image and Video Restoration Models", "categories": ["eess.IV", "cs.CV"], "comment": "6 pages, 1 pseudo-code, 3 figure panels, 2 plot panels, 7 tables, 24\n  references", "summary": "Multi-head self-attention (MHSA) has become a core component in modern\ncomputer vision models. However, its quadratic complexity with respect to input\nlength poses a significant computational bottleneck in real-time and resource\nconstrained environments. We propose PnP-Nystra, a Nystr\\\"om based linear\napproximation of self-attention, developed as a plug-and-play (PnP) module that\ncan be integrated into the pre-trained image and video restoration models\nwithout retraining. As a drop-in replacement for MHSA, PnP-Nystra enables\nefficient acceleration in various window-based transformer architectures,\nincluding SwinIR, Uformer, and RVRT. Our experiments across diverse image and\nvideo restoration tasks, including denoising, deblurring, and super-resolution,\ndemonstrate that PnP-Nystra achieves a 2-4x speed-up on an NVIDIA RTX 4090 GPU\nand a 2-5x speed-up on CPU inference. Despite these significant gains, the\nmethod incurs a maximum PSNR drop of only 1.5 dB across all evaluated tasks. To\nthe best of our knowledge, we are the first to demonstrate a linear attention\nfunctioning as a training-free substitute for MHSA in restoration models."}
{"id": "2506.08534", "pdf": "https://arxiv.org/pdf/2506.08534", "abs": "https://arxiv.org/abs/2506.08534", "authors": ["Donglian Li", "Hui Guo", "Minglang Chen", "Huizhen Chen", "Jialing Chen", "Bocheng Liang", "Pengchen Liang", "Ying Tan"], "title": "DCD: A Semantic Segmentation Model for Fetal Ultrasound Four-Chamber View", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Accurate segmentation of anatomical structures in the apical four-chamber\n(A4C) view of fetal echocardiography is essential for early diagnosis and\nprenatal evaluation of congenital heart disease (CHD). However, precise\nsegmentation remains challenging due to ultrasound artifacts, speckle noise,\nanatomical variability, and boundary ambiguity across different gestational\nstages. To reduce the workload of sonographers and enhance segmentation\naccuracy, we propose DCD, an advanced deep learning-based model for automatic\nsegmentation of key anatomical structures in the fetal A4C view. Our model\nincorporates a Dense Atrous Spatial Pyramid Pooling (Dense ASPP) module,\nenabling superior multi-scale feature extraction, and a Convolutional Block\nAttention Module (CBAM) to enhance adaptive feature representation. By\neffectively capturing both local and global contextual information, DCD\nachieves precise and robust segmentation, contributing to improved prenatal\ncardiac assessment."}
{"id": "2506.08618", "pdf": "https://arxiv.org/pdf/2506.08618", "abs": "https://arxiv.org/abs/2506.08618", "authors": ["Xianquan Yan", "Hakan Akg√ºn", "Kenji Kawaguchi", "N. Duane Loh", "Ching Hua Lee"], "title": "HSG-12M: A Large-Scale Spatial Multigraph Dataset", "categories": ["cs.LG", "cond-mat.mes-hall", "cond-mat.other", "cs.AI", "cs.CV"], "comment": "39 pages, 13 figures, 3 tables. Code & pipeline:\n  [https://github.com/sarinstein-yan/Poly2Graph] Dataset:\n  [https://github.com/sarinstein-yan/HSG-12M] Dataset released under CC BY 4.0", "summary": "Existing graph benchmarks assume non-spatial, simple edges, collapsing\nphysically distinct paths into a single link. We introduce HSG-12M, the first\nlarge-scale dataset of $\\textbf{spatial multigraphs}-$graphs embedded in a\nmetric space where multiple geometrically distinct trajectories between two\nnodes are retained as separate edges. HSG-12M contains 11.6 million static and\n5.1 million dynamic $\\textit{Hamiltonian spectral graphs}$ across 1401\ncharacteristic-polynomial classes, derived from 177 TB of spectral potential\ndata. Each graph encodes the full geometry of a 1-D crystal's energy spectrum\non the complex plane, producing diverse, physics-grounded topologies that\ntranscend conventional node-coordinate datasets. To enable future extensions,\nwe release $\\texttt{Poly2Graph}$: a high-performance, open-source pipeline that\nmaps arbitrary 1-D crystal Hamiltonians to spectral graphs. Benchmarks with\npopular GNNs expose new challenges in learning from multi-edge geometry at\nscale. Beyond its practical utility, we show that spectral graphs serve as\nuniversal topological fingerprints of polynomials, vectors, and matrices,\nforging a new algebra-to-graph link. HSG-12M lays the groundwork for\ngeometry-aware graph learning and new opportunities of data-driven scientific\ndiscovery in condensed matter physics and beyond."}
{"id": "2506.08623", "pdf": "https://arxiv.org/pdf/2506.08623", "abs": "https://arxiv.org/abs/2506.08623", "authors": ["Rinat Prochii", "Elizaveta Dakhova", "Pavel Birulin", "Maxim Sharaev"], "title": "Biologically Inspired Deep Learning Approaches for Fetal Ultrasound Image Classification", "categories": ["eess.IV", "cs.CV"], "comment": "16 pages, 2 figures, 3 tables", "summary": "Accurate classification of second-trimester fetal ultrasound images remains\nchallenging due to low image quality, high intra-class variability, and\nsignificant class imbalance. In this work, we introduce a simple yet powerful,\nbiologically inspired deep learning ensemble framework that-unlike prior\nstudies focused on only a handful of anatomical targets-simultaneously\ndistinguishes 16 fetal structures. Drawing on the hierarchical, modular\norganization of biological vision systems, our model stacks two complementary\nbranches (a \"shallow\" path for coarse, low-resolution cues and a \"detailed\"\npath for fine, high-resolution features), concatenating their outputs for final\nprediction. To our knowledge, no existing method has addressed such a large\nnumber of classes with a comparably lightweight architecture. We trained and\nevaluated on 5,298 routinely acquired clinical images (annotated by three\nexperts and reconciled via Dawid-Skene), reflecting real-world noise and\nvariability rather than a \"cleaned\" dataset. Despite this complexity, our\nensemble (EfficientNet-B0 + EfficientNet-B6 with LDAM-Focal loss) identifies\n90% of organs with accuracy > 0.75 and 75% of organs with accuracy >\n0.85-performance competitive with more elaborate models applied to far fewer\ncategories. These results demonstrate that biologically inspired modular\nstacking can yield robust, scalable fetal anatomy recognition in challenging\nclinical settings."}
{"id": "2506.08634", "pdf": "https://arxiv.org/pdf/2506.08634", "abs": "https://arxiv.org/abs/2506.08634", "authors": ["Alvaro Becerra", "Daniel Andres", "Pablo Villegas", "Roberto Daza", "Ruth Cobos"], "title": "MOSAIC-F: A Framework for Enhancing Students' Oral Presentation Skills through Personalized Feedback", "categories": ["cs.HC", "cs.AI", "cs.CV"], "comment": "Accepted in LASI Spain 25: Learning Analytics Summer Institute Spain\n  2025", "summary": "In this article, we present a novel multimodal feedback framework called\nMOSAIC-F, an acronym for a data-driven Framework that integrates Multimodal\nLearning Analytics (MMLA), Observations, Sensors, Artificial Intelligence (AI),\nand Collaborative assessments for generating personalized feedback on student\nlearning activities. This framework consists of four key steps. First, peers\nand professors' assessments are conducted through standardized rubrics (that\ninclude both quantitative and qualitative evaluations). Second, multimodal data\nare collected during learning activities, including video recordings, audio\ncapture, gaze tracking, physiological signals (heart rate, motion data), and\nbehavioral interactions. Third, personalized feedback is generated using AI,\nsynthesizing human-based evaluations and data-based multimodal insights such as\nposture, speech patterns, stress levels, and cognitive load, among others.\nFinally, students review their own performance through video recordings and\nengage in self-assessment and feedback visualization, comparing their own\nevaluations with peers and professors' assessments, class averages, and\nAI-generated recommendations. By combining human-based and data-based\nevaluation techniques, this framework enables more accurate, personalized and\nactionable feedback. We tested MOSAIC-F in the context of improving oral\npresentation skills."}
{"id": "2506.08641", "pdf": "https://arxiv.org/pdf/2506.08641", "abs": "https://arxiv.org/abs/2506.08641", "authors": ["Simon Roschmann", "Quentin Bouniot", "Vasilii Feofanov", "Ievgen Redko", "Zeynep Akata"], "title": "Time Series Representations for Classification Lie Hidden in Pretrained Vision Transformers", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Time series classification is a fundamental task in healthcare and industry,\nyet the development of time series foundation models (TSFMs) remains limited by\nthe scarcity of publicly available time series datasets. In this work, we\npropose Time Vision Transformer (TiViT), a framework that converts time series\ninto images to leverage the representational power of frozen Vision\nTransformers (ViTs) pretrained on large-scale image datasets. First, we\ntheoretically motivate our approach by analyzing the 2D patching of ViTs for\ntime series, showing that it can increase the number of label-relevant tokens\nand reduce the sample complexity. Second, we empirically demonstrate that TiViT\nachieves state-of-the-art performance on standard time series classification\nbenchmarks by utilizing the hidden representations of large OpenCLIP models. We\nexplore the structure of TiViT representations and find that intermediate\nlayers with high intrinsic dimension are the most effective for time series\nclassification. Finally, we assess the alignment between TiViT and TSFM\nrepresentation spaces and identify a strong complementarity, with further\nperformance gains achieved by combining their features. Our findings reveal yet\nanother direction for reusing vision representations in a non-visual domain."}
{"id": "2506.08677", "pdf": "https://arxiv.org/pdf/2506.08677", "abs": "https://arxiv.org/abs/2506.08677", "authors": ["Milica ≈†kipina", "Nikola Jovi≈°iƒá", "Nicola Dall'Asen", "Vanja ≈†venda", "Anil Osman Tur", "Slobodan Iliƒá", "Elisa Ricci", "Dubravko ƒÜulibrk"], "title": "MAMBO: High-Resolution Generative Approach for Mammography Images", "categories": ["eess.IV", "cs.CV"], "comment": "21 pages, 14 figures, 7 tables", "summary": "Mammography is the gold standard for the detection and diagnosis of breast\ncancer. This procedure can be significantly enhanced with Artificial\nIntelligence (AI)-based software, which assists radiologists in identifying\nabnormalities. However, training AI systems requires large and diverse\ndatasets, which are often difficult to obtain due to privacy and ethical\nconstraints. To address this issue, the paper introduces MAMmography ensemBle\nmOdel (MAMBO), a novel patch-based diffusion approach designed to generate\nfull-resolution mammograms. Diffusion models have shown breakthrough results in\nrealistic image generation, yet few studies have focused on mammograms, and\nnone have successfully generated high-resolution outputs required to capture\nfine-grained features of small lesions. To achieve this, MAMBO integrates\nseparate diffusion models to capture both local and global (image-level)\ncontexts. The contextual information is then fed into the final patch-based\nmodel, significantly aiding the noise removal process. This thoughtful design\nenables MAMBO to generate highly realistic mammograms of up to 3840x3840\npixels. Importantly, this approach can be used to enhance the training of\nclassification models and extended to anomaly detection. Experiments, both\nnumerical and radiologist validation, assess MAMBO's capabilities in image\ngeneration, super-resolution, and anomaly detection, highlighting its potential\nto enhance mammography analysis for more accurate diagnoses and earlier lesion\ndetection."}
{"id": "2506.08700", "pdf": "https://arxiv.org/pdf/2506.08700", "abs": "https://arxiv.org/abs/2506.08700", "authors": ["Ruiran Su", "Jiasheng Si", "Zhijiang Guo", "Janet B. Pierrehumbert"], "title": "ClimateViz: A Benchmark for Statistical Reasoning and Fact Verification on Scientific Charts", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Scientific fact-checking has mostly focused on text and tables, overlooking\nscientific charts, which are key for presenting quantitative evidence and\nstatistical reasoning. We introduce ClimateViz, the first large-scale benchmark\nfor scientific fact-checking using expert-curated scientific charts. ClimateViz\ncontains 49,862 claims linked to 2,896 visualizations, each labeled as support,\nrefute, or not enough information. To improve interpretability, each example\nincludes structured knowledge graph explanations covering trends, comparisons,\nand causal relations. We evaluate state-of-the-art multimodal language models,\nincluding both proprietary and open-source systems, in zero-shot and few-shot\nsettings. Results show that current models struggle with chart-based reasoning:\neven the best systems, such as Gemini 2.5 and InternVL 2.5, reach only 76.2 to\n77.8 percent accuracy in label-only settings, far below human performance (89.3\nand 92.7 percent). Explanation-augmented outputs improve performance in some\nmodels. We released our dataset and code alongside the paper."}
{"id": "2506.08708", "pdf": "https://arxiv.org/pdf/2506.08708", "abs": "https://arxiv.org/abs/2506.08708", "authors": ["Liang Ma", "Jiajun Wen", "Min Lin", "Rongtao Xu", "Xiwen Liang", "Bingqian Lin", "Jun Ma", "Yongxin Wang", "Ziming Wei", "Haokun Lin", "Mingfei Han", "Meng Cao", "Bokui Chen", "Ivan Laptev", "Xiaodan Liang"], "title": "PhyBlock: A Progressive Benchmark for Physical Understanding and Planning via 3D Block Assembly", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "While vision-language models (VLMs) have demonstrated promising capabilities\nin reasoning and planning for embodied agents, their ability to comprehend\nphysical phenomena, particularly within structured 3D environments, remains\nseverely limited. To close this gap, we introduce PhyBlock, a progressive\nbenchmark designed to assess VLMs on physical understanding and planning\nthrough robotic 3D block assembly tasks. PhyBlock integrates a novel four-level\ncognitive hierarchy assembly task alongside targeted Visual Question Answering\n(VQA) samples, collectively aimed at evaluating progressive spatial reasoning\nand fundamental physical comprehension, including object properties, spatial\nrelationships, and holistic scene understanding. PhyBlock includes 2600 block\ntasks (400 assembly tasks, 2200 VQA tasks) and evaluates models across three\nkey dimensions: partial completion, failure diagnosis, and planning robustness.\nWe benchmark 21 state-of-the-art VLMs, highlighting their strengths and\nlimitations in physically grounded, multi-step planning. Our empirical findings\nindicate that the performance of VLMs exhibits pronounced limitations in\nhigh-level planning and reasoning capabilities, leading to a notable decline in\nperformance for the growing complexity of the tasks. Error analysis reveals\npersistent difficulties in spatial orientation and dependency reasoning.\nSurprisingly, chain-of-thought prompting offers minimal improvements,\nsuggesting spatial tasks heavily rely on intuitive model comprehension. We\nposition PhyBlock as a unified testbed to advance embodied reasoning, bridging\nvision-language understanding and real-world physical problem-solving."}
{"id": "2506.08716", "pdf": "https://arxiv.org/pdf/2506.08716", "abs": "https://arxiv.org/abs/2506.08716", "authors": ["Maximilian Tschuchnig", "Lukas Lamminger", "Philipp Steininger", "Michael Gadermayr"], "title": "Enhancing Synthetic CT from CBCT via Multimodal Fusion: A Study on the Impact of CBCT Quality and Alignment", "categories": ["eess.IV", "cs.CV"], "comment": "Data is open source. Code will be provided on acceptance. Paper\n  currently under review", "summary": "Cone-Beam Computed Tomography (CBCT) is widely used for real-time\nintraoperative imaging due to its low radiation dose and high acquisition\nspeed. However, despite its high resolution, CBCT suffers from significant\nartifacts and thereby lower visual quality, compared to conventional Computed\nTomography (CT). A recent approach to mitigate these artifacts is synthetic CT\n(sCT) generation, translating CBCT volumes into the CT domain. In this work, we\nenhance sCT generation through multimodal learning, integrating intraoperative\nCBCT with preoperative CT. Beyond validation on two real-world datasets, we use\na versatile synthetic dataset, to analyze how CBCT-CT alignment and CBCT\nquality affect sCT quality. The results demonstrate that multimodal sCT\nconsistently outperform unimodal baselines, with the most significant gains\nobserved in well-aligned, low-quality CBCT-CT cases. Finally, we demonstrate\nthat these findings are highly reproducible in real-world clinical datasets."}
{"id": "2506.08761", "pdf": "https://arxiv.org/pdf/2506.08761", "abs": "https://arxiv.org/abs/2506.08761", "authors": ["Matthias Beckmann", "Robert Beinert", "Jonas Bresch"], "title": "Normalized Radon Cumulative Distribution Transforms for Invariance and Robustness in Optimal Transport Based Image Classification", "categories": ["math.NA", "cs.CV", "cs.IT", "cs.NA", "math.IT"], "comment": null, "summary": "The Radon cumulative distribution transform (R-CDT), is an easy-to-compute\nfeature extractor that facilitates image classification tasks especially in the\nsmall data regime. It is closely related to the sliced Wasserstein distance and\nprovably guaranties the linear separability of image classes that emerge from\ntranslations or scalings. In many real-world applications, like the recognition\nof watermarks in filigranology, however, the data is subject to general affine\ntransformations originating from the measurement process. To overcome this\nissue, we recently introduced the so-called max-normalized R-CDT that only\nrequires elementary operations and guaranties the separability under arbitrary\naffine transformations. The aim of this paper is to continue our study of the\nmax-normalized R-CDT especially with respect to its robustness against\nnon-affine image deformations. Our sensitivity analysis shows that its\nseparability properties are stable provided the Wasserstein-infinity distance\nbetween the samples can be controlled. Since the Wasserstein-infinity distance\nonly allows small local image deformations, we moreover introduce a\nmean-normalized version of the R-CDT. In this case, robustness relates to the\nWasserstein-2 distance and also covers image deformations caused by impulsive\nnoise for instance. Our theoretical results are supported by numerical\nexperiments showing the effectiveness of our novel feature extractors as well\nas their robustness against local non-affine deformations and impulsive noise."}
{"id": "2506.09023", "pdf": "https://arxiv.org/pdf/2506.09023", "abs": "https://arxiv.org/abs/2506.09023", "authors": ["Julia Guerrero-Viu", "Michael Fischer", "Iliyan Georgiev", "Elena Garces", "Diego Gutierrez", "Belen Masia", "Valentin Deschaintre"], "title": "Fine-Grained Spatially Varying Material Selection in Images", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Selection is the first step in many image editing processes, enabling faster\nand simpler modifications of all pixels sharing a common modality. In this\nwork, we present a method for material selection in images, robust to lighting\nand reflectance variations, which can be used for downstream editing tasks. We\nrely on vision transformer (ViT) models and leverage their features for\nselection, proposing a multi-resolution processing strategy that yields finer\nand more stable selection results than prior methods. Furthermore, we enable\nselection at two levels: texture and subtexture, leveraging a new two-level\nmaterial selection (DuMaS) dataset which includes dense annotations for over\n800,000 synthetic images, both on the texture and subtexture levels."}
{"id": "2506.09049", "pdf": "https://arxiv.org/pdf/2506.09049", "abs": "https://arxiv.org/abs/2506.09049", "authors": ["Li Kang", "Xiufeng Song", "Heng Zhou", "Yiran Qin", "Jie Yang", "Xiaohong Liu", "Philip Torr", "Lei Bai", "Zhenfei Yin"], "title": "VIKI-R: Coordinating Embodied Multi-Agent Cooperation via Reinforcement Learning", "categories": ["cs.AI", "cs.CV", "cs.RO"], "comment": "Project page: https://faceong.github.io/VIKI-R/", "summary": "Coordinating multiple embodied agents in dynamic environments remains a core\nchallenge in artificial intelligence, requiring both perception-driven\nreasoning and scalable cooperation strategies. While recent works have\nleveraged large language models (LLMs) for multi-agent planning, a few have\nbegun to explore vision-language models (VLMs) for visual reasoning. However,\nthese VLM-based approaches remain limited in their support for diverse\nembodiment types. In this work, we introduce VIKI-Bench, the first hierarchical\nbenchmark tailored for embodied multi-agent cooperation, featuring three\nstructured levels: agent activation, task planning, and trajectory perception.\nVIKI-Bench includes diverse robot embodiments, multi-view visual observations,\nand structured supervision signals to evaluate reasoning grounded in visual\ninputs. To demonstrate the utility of VIKI-Bench, we propose VIKI-R, a\ntwo-stage framework that fine-tunes a pretrained vision-language model (VLM)\nusing Chain-of-Thought annotated demonstrations, followed by reinforcement\nlearning under multi-level reward signals. Our extensive experiments show that\nVIKI-R significantly outperforms baselines method across all task levels.\nFurthermore, we show that reinforcement learning enables the emergence of\ncompositional cooperation patterns among heterogeneous agents. Together,\nVIKI-Bench and VIKI-R offer a unified testbed and method for advancing\nmulti-agent, visual-driven cooperation in embodied AI systems."}
