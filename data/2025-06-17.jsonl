{"id": "2506.12105", "pdf": "https://arxiv.org/pdf/2506.12105", "abs": "https://arxiv.org/abs/2506.12105", "authors": ["Haoxiang Chen", "Wei Zhao", "Rufei Zhang", "Nannan Li", "Dongjin Li"], "title": "Multiple Object Tracking in Video SAR: A Benchmark and Tracking Baseline", "categories": ["cs.CV"], "comment": null, "summary": "In the context of multi-object tracking using video synthetic aperture radar\n(Video SAR), Doppler shifts induced by target motion result in artifacts that\nare easily mistaken for shadows caused by static occlusions. Moreover,\nappearance changes of the target caused by Doppler mismatch may lead to\nassociation failures and disrupt trajectory continuity. A major limitation in\nthis field is the lack of public benchmark datasets for standardized algorithm\nevaluation. To address the above challenges, we collected and annotated 45\nvideo SAR sequences containing moving targets, and named the Video SAR MOT\nBenchmark (VSMB). Specifically, to mitigate the effects of trailing and\ndefocusing in moving targets, we introduce a line feature enhancement mechanism\nthat emphasizes the positive role of motion shadows and reduces false alarms\ninduced by static occlusions. In addition, to mitigate the adverse effects of\ntarget appearance variations, we propose a motion-aware clue discarding\nmechanism that substantially improves tracking robustness in Video SAR. The\nproposed model achieves state-of-the-art performance on the VSMB, and the\ndataset and model are released at https://github.com/softwarePupil/VSMB."}
{"id": "2506.12190", "pdf": "https://arxiv.org/pdf/2506.12190", "abs": "https://arxiv.org/abs/2506.12190", "authors": ["Naomi Fridman", "Bubby Solway", "Tomer Fridman", "Itamar Barnea", "Anat Goldshtein"], "title": "BreastDCEDL: Curating a Comprehensive DCE-MRI Dataset and developing a Transformer Implementation for Breast Cancer Treatment Response Prediction", "categories": ["cs.CV", "cs.AI", "68T07, 68U10, 92C55", "I.2.0; I.2.10; I.4.5; J.3"], "comment": null, "summary": "Breast cancer remains a leading cause of cancer-related mortality worldwide,\nmaking early detection and accurate treatment response monitoring critical\npriorities. We present BreastDCEDL, a curated, deep learning-ready dataset\ncomprising pre-treatment 3D Dynamic Contrast-Enhanced MRI (DCE-MRI) scans from\n2,070 breast cancer patients drawn from the I-SPY1, I-SPY2, and Duke cohorts,\nall sourced from The Cancer Imaging Archive. The raw DICOM imaging data were\nrigorously converted into standardized 3D NIfTI volumes with preserved signal\nintegrity, accompanied by unified tumor annotations and harmonized clinical\nmetadata including pathologic complete response (pCR), hormone receptor (HR),\nand HER2 status. Although DCE-MRI provides essential diagnostic information and\ndeep learning offers tremendous potential for analyzing such complex data,\nprogress has been limited by lack of accessible, public, multicenter datasets.\nBreastDCEDL addresses this gap by enabling development of advanced models,\nincluding state-of-the-art transformer architectures that require substantial\ntraining data. To demonstrate its capacity for robust modeling, we developed\nthe first transformer-based model for breast DCE-MRI, leveraging Vision\nTransformer (ViT) architecture trained on RGB-fused images from three contrast\nphases (pre-contrast, early post-contrast, and late post-contrast). Our ViT\nmodel achieved state-of-the-art pCR prediction performance in HR+/HER2-\npatients (AUC 0.94, accuracy 0.93). BreastDCEDL includes predefined benchmark\nsplits, offering a framework for reproducible research and enabling clinically\nmeaningful modeling in breast cancer imaging."}
{"id": "2506.12198", "pdf": "https://arxiv.org/pdf/2506.12198", "abs": "https://arxiv.org/abs/2506.12198", "authors": ["Sibo Dong", "Ismail Shaheen", "Maggie Shen", "Rupayan Mallick", "Sarah Adel Bargal"], "title": "ViSTA: Visual Storytelling using Multi-modal Adapters for Text-to-Image Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-image diffusion models have achieved remarkable success, yet\ngenerating coherent image sequences for visual storytelling remains\nchallenging. A key challenge is effectively leveraging all previous text-image\npairs, referred to as history text-image pairs, which provide contextual\ninformation for maintaining consistency across frames. Existing auto-regressive\nmethods condition on all past image-text pairs but require extensive training,\nwhile training-free subject-specific approaches ensure consistency but lack\nadaptability to narrative prompts. To address these limitations, we propose a\nmulti-modal history adapter for text-to-image diffusion models, \\textbf{ViSTA}.\nIt consists of (1) a multi-modal history fusion module to extract relevant\nhistory features and (2) a history adapter to condition the generation on the\nextracted relevant features. We also introduce a salient history selection\nstrategy during inference, where the most salient history text-image pair is\nselected, improving the quality of the conditioning. Furthermore, we propose to\nemploy a Visual Question Answering-based metric TIFA to assess text-image\nalignment in visual storytelling, providing a more targeted and interpretable\nassessment of generated images. Evaluated on the StorySalon and FlintStonesSV\ndataset, our proposed ViSTA model is not only consistent across different\nframes, but also well-aligned with the narrative text descriptions."}
{"id": "2506.12208", "pdf": "https://arxiv.org/pdf/2506.12208", "abs": "https://arxiv.org/abs/2506.12208", "authors": ["Daniya Najiha Abdul Kareem", "Abdul Hannan", "Mubashir Noman", "Jean Lahoud", "Mustansar Fiaz", "Hisham Cholakkal"], "title": "InceptionMamba: Efficient Multi-Stage Feature Enhancement with Selective State Space Model for Microscopic Medical Image Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Accurate microscopic medical image segmentation plays a crucial role in\ndiagnosing various cancerous cells and identifying tumors. Driven by\nadvancements in deep learning, convolutional neural networks (CNNs) and\ntransformer-based models have been extensively studied to enhance receptive\nfields and improve medical image segmentation task. However, they often\nstruggle to capture complex cellular and tissue structures in challenging\nscenarios such as background clutter and object overlap. Moreover, their\nreliance on the availability of large datasets for improved performance, along\nwith the high computational cost, limit their practicality. To address these\nissues, we propose an efficient framework for the segmentation task, named\nInceptionMamba, which encodes multi-stage rich features and offers both\nperformance and computational efficiency. Specifically, we exploit semantic\ncues to capture both low-frequency and high-frequency regions to enrich the\nmulti-stage features to handle the blurred region boundaries (e.g., cell\nboundaries). These enriched features are input to a hybrid model that combines\nan Inception depth-wise convolution with a Mamba block, to maintain high\nefficiency and capture inherent variations in the scales and shapes of the\nregions of interest. These enriched features along with low-resolution features\nare fused to get the final segmentation mask. Our model achieves\nstate-of-the-art performance on two challenging microscopic segmentation\ndatasets (SegPC21 and GlaS) and two skin lesion segmentation datasets (ISIC2017\nand ISIC2018), while reducing computational cost by about 5 times compared to\nthe previous best performing method."}
{"id": "2506.12214", "pdf": "https://arxiv.org/pdf/2506.12214", "abs": "https://arxiv.org/abs/2506.12214", "authors": ["Ilya Ilyankou", "Natchapon Jongwiriyanurak", "Tao Cheng", "James Haworth"], "title": "CLIP the Landscape: Automated Tagging of Crowdsourced Landscape Images", "categories": ["cs.CV"], "comment": null, "summary": "We present a CLIP-based, multi-modal, multi-label classifier for predicting\ngeographical context tags from landscape photos in the Geograph dataset--a\ncrowdsourced image archive spanning the British Isles, including remote regions\nlacking POIs and street-level imagery. Our approach addresses a Kaggle\ncompetition\\footnote{https://www.kaggle.com/competitions/predict-geographic-context-from-landscape-photos}\ntask based on a subset of Geograph's 8M images, with strict evaluation: exact\nmatch accuracy is required across 49 possible tags. We show that combining\nlocation and title embeddings with image features improves accuracy over using\nimage embeddings alone. We release a lightweight\npipeline\\footnote{https://github.com/SpaceTimeLab/ClipTheLandscape} that trains\non a modest laptop, using pre-trained CLIP image and text embeddings and a\nsimple classification head. Predicted tags can support downstream tasks such as\nbuilding location embedders for GeoAI applications, enriching spatial\nunderstanding in data-sparse regions."}
{"id": "2506.12232", "pdf": "https://arxiv.org/pdf/2506.12232", "abs": "https://arxiv.org/abs/2506.12232", "authors": ["Mohammed Elhenawy", "Shadi Jaradat", "Taqwa I. Alhadidi", "Huthaifa I. Ashqar", "Ahmed Jaber", "Andry Rakotonirainy", "Mohammad Abu Tami"], "title": "Zero-Shot Scene Understanding with Multimodal Large Language Models for Automated Vehicles", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Scene understanding is critical for various downstream tasks in autonomous\ndriving, including facilitating driver-agent communication and enhancing\nhuman-centered explainability of autonomous vehicle (AV) decisions. This paper\nevaluates the capability of four multimodal large language models (MLLMs),\nincluding relatively small models, to understand scenes in a zero-shot,\nin-context learning setting. Additionally, we explore whether combining these\nmodels using an ensemble approach with majority voting can enhance scene\nunderstanding performance. Our experiments demonstrate that GPT-4o, the largest\nmodel, outperforms the others in scene understanding. However, the performance\ngap between GPT-4o and the smaller models is relatively modest, suggesting that\nadvanced techniques such as improved in-context learning, retrieval-augmented\ngeneration (RAG), or fine-tuning could further optimize the smaller models'\nperformance. We also observe mixed results with the ensemble approach: while\nsome scene attributes show improvement in performance metrics such as F1-score,\nothers experience a decline. These findings highlight the need for more\nsophisticated ensemble techniques to achieve consistent gains across all scene\nattributes. This study underscores the potential of leveraging MLLMs for scene\nunderstanding and provides insights into optimizing their performance for\nautonomous driving applications."}
{"id": "2506.12251", "pdf": "https://arxiv.org/pdf/2506.12251", "abs": "https://arxiv.org/abs/2506.12251", "authors": ["Boris Ivanovic", "Cristiano Saltori", "Yurong You", "Yan Wang", "Wenjie Luo", "Marco Pavone"], "title": "Efficient Multi-Camera Tokenization with Triplanes for End-to-End Driving", "categories": ["cs.CV", "cs.LG", "cs.RO"], "comment": "12 pages, 10 figures, 5 tables", "summary": "Autoregressive Transformers are increasingly being deployed as end-to-end\nrobot and autonomous vehicle (AV) policy architectures, owing to their\nscalability and potential to leverage internet-scale pretraining for\ngeneralization. Accordingly, tokenizing sensor data efficiently is paramount to\nensuring the real-time feasibility of such architectures on embedded hardware.\nTo this end, we present an efficient triplane-based multi-camera tokenization\nstrategy that leverages recent advances in 3D neural reconstruction and\nrendering to produce sensor tokens that are agnostic to the number of input\ncameras and their resolution, while explicitly accounting for their geometry\naround an AV. Experiments on a large-scale AV dataset and state-of-the-art\nneural simulator demonstrate that our approach yields significant savings over\ncurrent image patch-based tokenization strategies, producing up to 72% fewer\ntokens, resulting in up to 50% faster policy inference while achieving the same\nopen-loop motion planning accuracy and improved offroad rates in closed-loop\ndriving simulations."}
{"id": "2506.12258", "pdf": "https://arxiv.org/pdf/2506.12258", "abs": "https://arxiv.org/abs/2506.12258", "authors": ["Yijiang Li", "Genpei Zhang", "Jiacheng Cheng", "Yi Li", "Xiaojun Shan", "Dashan Gao", "Jiancheng Lyu", "Yuan Li", "Ning Bi", "Nuno Vasconcelos"], "title": "EgoPrivacy: What Your First-Person Camera Says About You?", "categories": ["cs.CV", "cs.CY"], "comment": "ICML 2025", "summary": "While the rapid proliferation of wearable cameras has raised significant\nconcerns about egocentric video privacy, prior work has largely overlooked the\nunique privacy threats posed to the camera wearer. This work investigates the\ncore question: How much privacy information about the camera wearer can be\ninferred from their first-person view videos? We introduce EgoPrivacy, the\nfirst large-scale benchmark for the comprehensive evaluation of privacy risks\nin egocentric vision. EgoPrivacy covers three types of privacy (demographic,\nindividual, and situational), defining seven tasks that aim to recover private\ninformation ranging from fine-grained (e.g., wearer's identity) to\ncoarse-grained (e.g., age group). To further emphasize the privacy threats\ninherent to egocentric vision, we propose Retrieval-Augmented Attack, a novel\nattack strategy that leverages ego-to-exo retrieval from an external pool of\nexocentric videos to boost the effectiveness of demographic privacy attacks. An\nextensive comparison of the different attacks possible under all threat models\nis presented, showing that private information of the wearer is highly\nsusceptible to leakage. For instance, our findings indicate that foundation\nmodels can effectively compromise wearer privacy even in zero-shot settings by\nrecovering attributes such as identity, scene, gender, and race with 70-80%\naccuracy. Our code and data are available at\nhttps://github.com/williamium3000/ego-privacy."}
{"id": "2506.12295", "pdf": "https://arxiv.org/pdf/2506.12295", "abs": "https://arxiv.org/abs/2506.12295", "authors": ["Worasit Sangjan", "Piyush Pandey", "Norman B. Best", "Jacob D. Washburn"], "title": "MatchPlant: An Open-Source Pipeline for UAV-Based Single-Plant Detection and Data Extraction", "categories": ["cs.CV"], "comment": "32 pages, 10 figures. Intended for submission to *Computers and\n  Electronics in Agriculture*. Source code is available at\n  https://github.com/JacobWashburn-USDA/MatchPlant and dataset at\n  https://doi.org/10.5281/zenodo.14856123", "summary": "Accurate identification of individual plants from unmanned aerial vehicle\n(UAV) images is essential for advancing high-throughput phenotyping and\nsupporting data-driven decision-making in plant breeding. This study presents\nMatchPlant, a modular, graphical user interface-supported, open-source Python\npipeline for UAV-based single-plant detection and geospatial trait extraction.\nMatchPlant enables end-to-end workflows by integrating UAV image processing,\nuser-guided annotation, Convolutional Neural Network model training for object\ndetection, forward projection of bounding boxes onto an orthomosaic, and\nshapefile generation for spatial phenotypic analysis. In an early-season maize\ncase study, MatchPlant achieved reliable detection performance (validation AP:\n89.6%, test AP: 85.9%) and effectively projected bounding boxes, covering 89.8%\nof manually annotated boxes with 87.5% of projections achieving an Intersection\nover Union (IoU) greater than 0.5. Trait values extracted from predicted\nbounding instances showed high agreement with manual annotations (r =\n0.87-0.97, IoU >= 0.4). Detection outputs were reused across time points to\nextract plant height and Normalized Difference Vegetation Index with minimal\nadditional annotation, facilitating efficient temporal phenotyping. By\ncombining modular design, reproducibility, and geospatial precision, MatchPlant\noffers a scalable framework for UAV-based plant-level analysis with broad\napplicability in agricultural and environmental monitoring."}
{"id": "2506.12323", "pdf": "https://arxiv.org/pdf/2506.12323", "abs": "https://arxiv.org/abs/2506.12323", "authors": ["Janet Wang", "Yunbei Zhang", "Zhengming Ding", "Jihun Hamm"], "title": "Doctor Approved: Generating Medically Accurate Skin Disease Images through AI-Expert Feedback", "categories": ["cs.CV"], "comment": null, "summary": "Paucity of medical data severely limits the generalizability of diagnostic ML\nmodels, as the full spectrum of disease variability can not be represented by a\nsmall clinical dataset. To address this, diffusion models (DMs) have been\nconsidered as a promising avenue for synthetic image generation and\naugmentation. However, they frequently produce medically inaccurate images,\ndeteriorating the model performance. Expert domain knowledge is critical for\nsynthesizing images that correctly encode clinical information, especially when\ndata is scarce and quality outweighs quantity. Existing approaches for\nincorporating human feedback, such as reinforcement learning (RL) and Direct\nPreference Optimization (DPO), rely on robust reward functions or demand\nlabor-intensive expert evaluations. Recent progress in Multimodal Large\nLanguage Models (MLLMs) reveals their strong visual reasoning capabilities,\nmaking them adept candidates as evaluators. In this work, we propose a novel\nframework, coined MAGIC (Medically Accurate Generation of Images through\nAI-Expert Collaboration), that synthesizes clinically accurate skin disease\nimages for data augmentation. Our method creatively translates expert-defined\ncriteria into actionable feedback for image synthesis of DMs, significantly\nimproving clinical accuracy while reducing the direct human workload.\nExperiments demonstrate that our method greatly improves the clinical quality\nof synthesized skin disease images, with outputs aligning with dermatologist\nassessments. Additionally, augmenting training data with these synthesized\nimages improves diagnostic accuracy by +9.02% on a challenging 20-condition\nskin disease classification task, and by +13.89% in the few-shot setting."}
{"id": "2506.12324", "pdf": "https://arxiv.org/pdf/2506.12324", "abs": "https://arxiv.org/abs/2506.12324", "authors": ["Yuantao Wang", "Haowei Yang", "Wei Zhang", "Shijian Lu"], "title": "UniDet-D: A Unified Dynamic Spectral Attention Model for Object Detection under Adverse Weathers", "categories": ["cs.CV"], "comment": null, "summary": "Real-world object detection is a challenging task where the captured\nimages/videos often suffer from complex degradations due to various adverse\nweather conditions such as rain, fog, snow, low-light, etc. Despite extensive\nprior efforts, most existing methods are designed for one specific type of\nadverse weather with constraints of poor generalization, under-utilization of\nvisual features while handling various image degradations. Leveraging a\ntheoretical analysis on how critical visual details are lost in adverse-weather\nimages, we design UniDet-D, a unified framework that tackles the challenge of\nobject detection under various adverse weather conditions, and achieves object\ndetection and image restoration within a single network. Specifically, the\nproposed UniDet-D incorporates a dynamic spectral attention mechanism that\nadaptively emphasizes informative spectral components while suppressing\nirrelevant ones, enabling more robust and discriminative feature representation\nacross various degradation types. Extensive experiments show that UniDet-D\nachieves superior detection accuracy across different types of adverse-weather\ndegradation. Furthermore, UniDet-D demonstrates superior generalization towards\nunseen adverse weather conditions such as sandstorms and rain-fog mixtures,\nhighlighting its great potential for real-world deployment."}
{"id": "2506.12326", "pdf": "https://arxiv.org/pdf/2506.12326", "abs": "https://arxiv.org/abs/2506.12326", "authors": ["Yongmin Kwon", "Namwoo Kang"], "title": "Three-dimensional Deep Shape Optimization with a Limited Dataset", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Generative models have attracted considerable attention for their ability to\nproduce novel shapes. However, their application in mechanical design remains\nconstrained due to the limited size and variability of available datasets. This\nstudy proposes a deep learning-based optimization framework specifically\ntailored for shape optimization with limited datasets, leveraging positional\nencoding and a Lipschitz regularization term to robustly learn geometric\ncharacteristics and maintain a meaningful latent space. Through extensive\nexperiments, the proposed approach demonstrates robustness, generalizability\nand effectiveness in addressing typical limitations of conventional\noptimization frameworks. The validity of the methodology is confirmed through\nmulti-objective shape optimization experiments conducted on diverse\nthree-dimensional datasets, including wheels and cars, highlighting the model's\nversatility in producing practical and high-quality design outcomes even under\ndata-constrained conditions."}
{"id": "2506.12335", "pdf": "https://arxiv.org/pdf/2506.12335", "abs": "https://arxiv.org/abs/2506.12335", "authors": ["Chuntao Ding", "Jianhang Xie", "Junna Zhang", "Salman Raza", "Shangguang Wang", "Jiannong Cao"], "title": "GroupNL: Low-Resource and Robust CNN Design over Cloud and Device", "categories": ["cs.CV", "cs.AI", "cs.DC"], "comment": "13 pages, 10 figures", "summary": "It has become mainstream to deploy Convolutional Neural Network (CNN) models\non ubiquitous Internet of Things (IoT) devices with the help of the cloud to\nprovide users with a variety of high-quality services. Most existing methods\nhave two limitations: (i) low robustness in handling corrupted image data\ncollected by IoT devices; and (ii) high consumption of computational and\ntransmission resources. To this end, we propose the Grouped NonLinear\ntransformation generation method (GroupNL), which generates diversified feature\nmaps by utilizing data-agnostic Nonlinear Transformation Functions (NLFs) to\nimprove the robustness of the CNN model. Specifically, partial convolution\nfilters are designated as seed filters in a convolutional layer, and a small\nset of feature maps, i.e., seed feature maps, are first generated based on\nvanilla convolution operation. Then, we split seed feature maps into several\ngroups, each with a set of different NLFs, to generate corresponding diverse\nfeature maps with in-place nonlinear processing. Moreover, GroupNL effectively\nreduces the parameter transmission between multiple nodes during model training\nby setting the hyperparameters of NLFs to random initialization and not\nupdating them during model training, and reduces the computing resources by\nusing NLFs to generate feature maps instead of most feature maps generated\nbased on sliding windows. Experimental results on CIFAR-10, GTSRB, CIFAR-10-C,\nIcons50, and ImageNet-1K datasets in NVIDIA RTX GPU platforms show that the\nproposed GroupNL outperforms other state-of-the-art methods in model robust and\ntraining acceleration. Specifically, on the Icons-50 dataset, the accuracy of\nGroupNL-ResNet-18 achieves approximately 2.86% higher than the vanilla\nResNet-18. GroupNL improves training speed by about 53% compared to vanilla CNN\nwhen trained on a cluster of 8 NVIDIA RTX 4090 GPUs on the ImageNet-1K dataset."}
{"id": "2506.12336", "pdf": "https://arxiv.org/pdf/2506.12336", "abs": "https://arxiv.org/abs/2506.12336", "authors": ["Youze Wang", "Zijun Chen", "Ruoyu Chen", "Shishen Gu", "Yinpeng Dong", "Hang Su", "Jun Zhu", "Meng Wang", "Richang Hong", "Wenbo Hu"], "title": "Understanding and Benchmarking the Trustworthiness in Multimodal LLMs for Video Understanding", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in multimodal large language models for video\nunderstanding (videoLLMs) have improved their ability to process dynamic\nmultimodal data. However, trustworthiness challenges factual inaccuracies,\nharmful content, biases, hallucinations, and privacy risks, undermine\nreliability due to video data's spatiotemporal complexities. This study\nintroduces Trust-videoLLMs, a comprehensive benchmark evaluating videoLLMs\nacross five dimensions: truthfulness, safety, robustness, fairness, and\nprivacy. Comprising 30 tasks with adapted, synthetic, and annotated videos, the\nframework assesses dynamic visual scenarios, cross-modal interactions, and\nreal-world safety concerns. Our evaluation of 23 state-of-the-art videoLLMs (5\ncommercial,18 open-source) reveals significant limitations in dynamic visual\nscene understanding and cross-modal perturbation resilience. Open-source\nvideoLLMs show occasional truthfulness advantages but inferior overall\ncredibility compared to commercial models, with data diversity outperforming\nscale effects. These findings highlight the need for advanced safety alignment\nto enhance capabilities. Trust-videoLLMs provides a publicly available,\nextensible toolbox for standardized trustworthiness assessments, bridging the\ngap between accuracy-focused benchmarks and critical demands for robustness,\nsafety, fairness, and privacy."}
{"id": "2506.12340", "pdf": "https://arxiv.org/pdf/2506.12340", "abs": "https://arxiv.org/abs/2506.12340", "authors": ["Zongyu Wu", "Minhua Lin", "Zhiwei Zhang", "Fali Wang", "Xianren Zhang", "Xiang Zhang", "Suhang Wang"], "title": "Image Corruption-Inspired Membership Inference Attacks against Large Vision-Language Models", "categories": ["cs.CV", "cs.CR"], "comment": "Preprint", "summary": "Large vision-language models (LVLMs) have demonstrated outstanding\nperformance in many downstream tasks. However, LVLMs are trained on large-scale\ndatasets, which can pose privacy risks if training images contain sensitive\ninformation. Therefore, it is important to detect whether an image is used to\ntrain the LVLM. Recent studies have investigated membership inference attacks\n(MIAs) against LVLMs, including detecting image-text pairs and single-modality\ncontent. In this work, we focus on detecting whether a target image is used to\ntrain the target LVLM. We design simple yet effective Image Corruption-Inspired\nMembership Inference Attacks (ICIMIA) against LLVLMs, which are inspired by\nLVLM's different sensitivity to image corruption for member and non-member\nimages. We first perform an MIA method under the white-box setting, where we\ncan obtain the embeddings of the image through the vision part of the target\nLVLM. The attacks are based on the embedding similarity between the image and\nits corrupted version. We further explore a more practical scenario where we\nhave no knowledge about target LVLMs and we can only query the target LVLMs\nwith an image and a question. We then conduct the attack by utilizing the\noutput text embeddings' similarity. Experiments on existing datasets validate\nthe effectiveness of our proposed attack methods under those two different\nsettings."}
{"id": "2506.12351", "pdf": "https://arxiv.org/pdf/2506.12351", "abs": "https://arxiv.org/abs/2506.12351", "authors": ["Huaijie Wang", "De Cheng", "Lingfeng He", "Yan Li", "Jie Li", "Nannan Wang", "Xinbo Gao"], "title": "EKPC: Elastic Knowledge Preservation and Compensation for Class-Incremental Learning", "categories": ["cs.CV"], "comment": null, "summary": "Class-Incremental Learning (CIL) aims to enable AI models to continuously\nlearn from sequentially arriving data of different classes over time while\nretaining previously acquired knowledge. Recently, Parameter-Efficient\nFine-Tuning (PEFT) methods, like prompt pool-based approaches and adapter\ntuning, have shown great attraction in CIL. However, these methods either\nintroduce additional parameters that increase memory usage, or rely on rigid\nregularization techniques which reduce forgetting but compromise model\nflexibility. To overcome these limitations, we propose the Elastic Knowledge\nPreservation and Compensation (EKPC) method, integrating Importance-aware\nParameter Regularization (IPR) and Trainable Semantic Drift Compensation (TSDC)\nfor CIL. Specifically, the IPR method assesses the sensitivity of network\nparameters to prior tasks using a novel parameter-importance algorithm. It then\nselectively constrains updates within the shared adapter according to these\nimportance values, thereby preserving previously acquired knowledge while\nmaintaining the model's flexibility. However, it still exhibits slight semantic\ndifferences in previous knowledge to accommodate new incremental tasks, leading\nto decision boundaries confusion in classifier. To eliminate this confusion,\nTSDC trains a unified classifier by compensating prototypes with trainable\nsemantic drift. Extensive experiments on five CIL benchmarks demonstrate the\neffectiveness of the proposed method, showing superior performances to existing\nstate-of-the-art methods."}
{"id": "2506.12363", "pdf": "https://arxiv.org/pdf/2506.12363", "abs": "https://arxiv.org/abs/2506.12363", "authors": ["Zahid Ullah", "Jihie Kim"], "title": "Hierarchical Deep Feature Fusion and Ensemble Learning for Enhanced Brain Tumor MRI Classification", "categories": ["cs.CV"], "comment": null, "summary": "Accurate brain tumor classification is crucial in medical imaging to ensure\nreliable diagnosis and effective treatment planning. This study introduces a\nnovel double ensembling framework that synergistically combines pre-trained\ndeep learning (DL) models for feature extraction with optimized machine\nlearning (ML) classifiers for robust classification. The framework incorporates\ncomprehensive preprocessing and data augmentation of brain magnetic resonance\nimages (MRI), followed by deep feature extraction using transfer learning with\npre-trained Vision Transformer (ViT) networks. The novelty lies in the\ndual-level ensembling strategy: feature-level ensembling, which integrates deep\nfeatures from the top-performing ViT models, and classifier-level ensembling,\nwhich aggregates predictions from hyperparameter-optimized ML classifiers.\nExperiments on two public Kaggle MRI brain tumor datasets demonstrate that this\napproach significantly surpasses state-of-the-art methods, underscoring the\nimportance of feature and classifier fusion. The proposed methodology also\nhighlights the critical roles of hyperparameter optimization (HPO) and advanced\npreprocessing techniques in improving diagnostic accuracy and reliability,\nadvancing the integration of DL and ML for clinically relevant medical image\nanalysis."}
{"id": "2506.12394", "pdf": "https://arxiv.org/pdf/2506.12394", "abs": "https://arxiv.org/abs/2506.12394", "authors": ["Haotian Zhang", "Liu Liu", "Baosheng Yu", "Jiayan Qiu", "Yanwei Ren", "Xianglong Liu"], "title": "LARGO: Low-Rank Regulated Gradient Projection for Robust Parameter Efficient Fine-Tuning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The advent of parameter-efficient fine-tuning methods has significantly\nreduced the computational burden of adapting large-scale pretrained models to\ndiverse downstream tasks. However, existing approaches often struggle to\nachieve robust performance under domain shifts while maintaining computational\nefficiency. To address this challenge, we propose Low-rAnk Regulated Gradient\nProjection (LARGO) algorithm that integrates dynamic constraints into low-rank\nadaptation methods. Specifically, LARGO incorporates parallel trainable\ngradient projections to dynamically regulate layer-wise updates, retaining the\nOut-Of-Distribution robustness of pretrained model while preserving inter-layer\nindependence. Additionally, it ensures computational efficiency by mitigating\nthe influence of gradient dependencies across layers during weight updates.\nBesides, through leveraging singular value decomposition of pretrained weights\nfor structured initialization, we incorporate an SVD-based initialization\nstrategy that minimizing deviation from pretrained knowledge. Through extensive\nexperiments on diverse benchmarks, LARGO achieves state-of-the-art performance\nacross in-domain and out-of-distribution scenarios, demonstrating improved\nrobustness under domain shifts with significantly lower computational overhead\ncompared to existing PEFT methods. The source code will be released soon."}
{"id": "2506.12400", "pdf": "https://arxiv.org/pdf/2506.12400", "abs": "https://arxiv.org/abs/2506.12400", "authors": ["Hongbi Zhou", "Zhangkai Ni"], "title": "Perceptual-GS: Scene-adaptive Perceptual Densification for Gaussian Splatting", "categories": ["cs.CV"], "comment": "Accepted to International Conference on Machine Learning (ICML) 2025", "summary": "3D Gaussian Splatting (3DGS) has emerged as a powerful technique for novel\nview synthesis. However, existing methods struggle to adaptively optimize the\ndistribution of Gaussian primitives based on scene characteristics, making it\nchallenging to balance reconstruction quality and efficiency. Inspired by human\nperception, we propose scene-adaptive perceptual densification for Gaussian\nSplatting (Perceptual-GS), a novel framework that integrates perceptual\nsensitivity into the 3DGS training process to address this challenge. We first\nintroduce a perception-aware representation that models human visual\nsensitivity while constraining the number of Gaussian primitives. Building on\nthis foundation, we develop a \\cameraready{perceptual sensitivity-adaptive\ndistribution} to allocate finer Gaussian granularity to visually critical\nregions, enhancing reconstruction quality and robustness. Extensive evaluations\non multiple datasets, including BungeeNeRF for large-scale scenes, demonstrate\nthat Perceptual-GS achieves state-of-the-art performance in reconstruction\nquality, efficiency, and robustness. The code is publicly available at:\nhttps://github.com/eezkni/Perceptual-GS"}
{"id": "2506.12401", "pdf": "https://arxiv.org/pdf/2506.12401", "abs": "https://arxiv.org/abs/2506.12401", "authors": ["Weiwei Wang", "Meijia Wang", "Haoyi Wang", "Wenqiang Guo", "Jiapan Guo", "Changming Sun", "Lingkun Ma", "Weichuan Zhang"], "title": "Feature Complementation Architecture for Visual Place Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Visual place recognition (VPR) plays a crucial role in robotic localization\nand navigation. The key challenge lies in constructing feature representations\nthat are robust to environmental changes. Existing methods typically adopt\nconvolutional neural networks (CNNs) or vision Transformers (ViTs) as feature\nextractors. However, these architectures excel in different aspects -- CNNs are\neffective at capturing local details. At the same time, ViTs are better suited\nfor modeling global context, making it difficult to leverage the strengths of\nboth. To address this issue, we propose a local-global feature complementation\nnetwork (LGCN) for VPR which integrates a parallel CNN-ViT hybrid architecture\nwith a dynamic feature fusion module (DFM). The DFM performs dynamic feature\nfusion through joint modeling of spatial and channel-wise dependencies.\nFurthermore, to enhance the expressiveness and adaptability of the ViT branch\nfor VPR tasks, we introduce lightweight frequency-to-spatial fusion adapters\ninto the frozen ViT backbone. These adapters enable task-specific adaptation\nwith controlled parameter overhead. Extensive experiments on multiple VPR\nbenchmark datasets demonstrate that the proposed LGCN consistently outperforms\nexisting approaches in terms of localization accuracy and robustness,\nvalidating its effectiveness and generalizability."}
{"id": "2506.12409", "pdf": "https://arxiv.org/pdf/2506.12409", "abs": "https://arxiv.org/abs/2506.12409", "authors": ["Ziwei Liu", "Borui Kang", "Wei Li", "Hangjie Yuan", "Yanbing Yang", "Wenbin Li", "Jun Luo", "Yifan Zhu", "Tao Feng"], "title": "Branch, or Layer? Zeroth-Order Optimization for Continual Learning of Vision-Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Continual learning in vision-language models (VLMs) faces critical challenges\nin balancing parameter efficiency, memory consumption, and optimization\nstability. While First-Order (FO) optimization (e.g., SGD) dominate current\napproaches, their deterministic gradients often trap models in suboptimal local\nminima and incur substantial memory overhead. This paper pioneers a systematic\nexploration of Zeroth-Order (ZO) optimization for vision-language continual\nlearning (VLCL). We first identify the incompatibility of naive full-ZO\nadoption in VLCL due to modality-specific instability. To resolve this, we\nselectively applying ZO to either vision or language modalities while retaining\nFO in the complementary branch. Furthermore, we develop a layer-wise\noptimization paradigm that interleaves ZO and FO across network layers,\ncapitalizing on the heterogeneous learning dynamics of shallow versus deep\nrepresentations. A key theoretical insight reveals that ZO perturbations in\nvision branches exhibit higher variance than language counterparts, prompting a\ngradient sign normalization mechanism with modality-specific perturbation\nconstraints. Extensive experiments on four benchmarks demonstrate that our\nmethod achieves state-of-the-art performance, reducing memory consumption by\n89.1% compared to baselines. Code will be available upon publication."}
{"id": "2506.12413", "pdf": "https://arxiv.org/pdf/2506.12413", "abs": "https://arxiv.org/abs/2506.12413", "authors": ["Hyeonseo Lee", "Juhyun Park", "Jihyong Oh", "Chanho Eom"], "title": "Domain Generalization for Person Re-identification: A Survey Towards Domain-Agnostic Person Matching", "categories": ["cs.CV"], "comment": "Please visit our project page at\n  https://github.com/PerceptualAI-Lab/Awesome-Domain-Generalizable-Person-Re-ID", "summary": "Person Re-identification (ReID) aims to retrieve images of the same\nindividual captured across non-overlapping camera views, making it a critical\ncomponent of intelligent surveillance systems. Traditional ReID methods assume\nthat the training and test domains share similar characteristics and primarily\nfocus on learning discriminative features within a given domain. However, they\noften fail to generalize to unseen domains due to domain shifts caused by\nvariations in viewpoint, background, and lighting conditions. To address this\nissue, Domain-Adaptive ReID (DA-ReID) methods have been proposed. These\napproaches incorporate unlabeled target domain data during training and improve\nperformance by aligning feature distributions between source and target\ndomains. Domain-Generalizable ReID (DG-ReID) tackles a more realistic and\nchallenging setting by aiming to learn domain-invariant features without\nrelying on any target domain data. Recent methods have explored various\nstrategies to enhance generalization across diverse environments, but the field\nremains relatively underexplored. In this paper, we present a comprehensive\nsurvey of DG-ReID. We first review the architectural components of DG-ReID\nincluding the overall setting, commonly used backbone networks and multi-source\ninput configurations. Then, we categorize and analyze domain generalization\nmodules that explicitly aim to learn domain-invariant and\nidentity-discriminative representations. To examine the broader applicability\nof these techniques, we further conduct a case study on a related task that\nalso involves distribution shifts. Finally, we discuss recent trends, open\nchallenges, and promising directions for future research in DG-ReID. To the\nbest of our knowledge, this is the first systematic survey dedicated to\nDG-ReID."}
{"id": "2506.12441", "pdf": "https://arxiv.org/pdf/2506.12441", "abs": "https://arxiv.org/abs/2506.12441", "authors": ["Caixu Xu", "Junming Wei", "Huizhen Chen", "Pengchen Liang", "Bocheng Liang", "Ying Tan", "Xintong Wei"], "title": "MS-UMamba: An Improved Vision Mamba Unet for Fetal Abdominal Medical Image Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recently, Mamba-based methods have become popular in medical image\nsegmentation due to their lightweight design and long-range dependency modeling\ncapabilities. However, current segmentation methods frequently encounter\nchallenges in fetal ultrasound images, such as enclosed anatomical structures,\nblurred boundaries, and small anatomical structures. To address the need for\nbalancing local feature extraction and global context modeling, we propose\nMS-UMamba, a novel hybrid convolutional-mamba model for fetal ultrasound image\nsegmentation. Specifically, we design a visual state space block integrated\nwith a CNN branch (SS-MCAT-SSM), which leverages Mamba's global modeling\nstrengths and convolutional layers' local representation advantages to enhance\nfeature learning. In addition, we also propose an efficient multi-scale feature\nfusion module that integrates spatial attention mechanisms, which Integrating\nfeature information from different layers enhances the feature representation\nability of the model. Finally, we conduct extensive experiments on a non-public\ndataset, experimental results demonstrate that MS-UMamba model has excellent\nperformance in segmentation performance."}
{"id": "2506.12447", "pdf": "https://arxiv.org/pdf/2506.12447", "abs": "https://arxiv.org/abs/2506.12447", "authors": ["Nathanael L. Baisa", "Babu Pallam", "Amudhavel Jayavel"], "title": "CLIP-HandID: Vision-Language Model for Hand-Based Person Identification", "categories": ["cs.CV"], "comment": null, "summary": "This paper introduces a new approach to person identification based on hand\nimages, designed specifically for criminal investigations. The method is\nparticularly valuable in serious crimes like sexual abuse, where hand images\nare often the sole identifiable evidence available. Our proposed method,\nCLIP-HandID, leverages pre-trained foundational vision-language model,\nparticularly CLIP, to efficiently learn discriminative deep feature\nrepresentations from hand images given as input to the image encoder of CLIP\nusing textual prompts as semantic guidance. We propose to learn pseudo-tokens\nthat represent specific visual contexts or appearance attributes using textual\ninversion network since labels of hand images are indexes instead text\ndescriptions. The learned pseudo-tokens are incorporated into textual prompts\nwhich are given as input to the text encoder of the CLIP to leverage its\nmulti-modal reasoning to enhance its generalization for identification. Through\nextensive evaluations on two large, publicly available hand datasets with\nmulti-ethnic representation, we show that our method substantially surpasses\nexisting approaches."}
{"id": "2506.12456", "pdf": "https://arxiv.org/pdf/2506.12456", "abs": "https://arxiv.org/abs/2506.12456", "authors": ["Eugene Kofi Okrah Denteh", "Andrews Danyo", "Joshua Kofi Asamoah", "Blessing Agyei Kyem", "Armstrong Aboah"], "title": "Demographics-Informed Neural Network for Multi-Modal Spatiotemporal forecasting of Urban Growth and Travel Patterns Using Satellite Imagery", "categories": ["cs.CV"], "comment": null, "summary": "This study presents a novel demographics informed deep learning framework\ndesigned to forecast urban spatial transformations by jointly modeling\ngeographic satellite imagery, socio-demographics, and travel behavior dynamics.\nThe proposed model employs an encoder-decoder architecture with temporal gated\nresidual connections, integrating satellite imagery and demographic data to\naccurately forecast future spatial transformations. The study also introduces a\ndemographics prediction component which ensures that predicted satellite\nimagery are consistent with demographic features, significantly enhancing\nphysiological realism and socioeconomic accuracy. The framework is enhanced by\na proposed multi-objective loss function complemented by a semantic loss\nfunction that balances visual realism with temporal coherence. The experimental\nresults from this study demonstrate the superior performance of the proposed\nmodel compared to state-of-the-art models, achieving higher structural\nsimilarity (SSIM: 0.8342) and significantly improved demographic consistency\n(Demo-loss: 0.14 versus 0.95 and 0.96 for baseline models). Additionally, the\nstudy validates co-evolutionary theories of urban development, demonstrating\nquantifiable bidirectional influences between built environment characteristics\nand population patterns. The study also contributes a comprehensive multimodal\ndataset pairing satellite imagery sequences (2012-2023) with corresponding\ndemographic and travel behavior attributes, addressing existing gaps in urban\nand transportation planning resources by explicitly connecting physical\nlandscape evolution with socio-demographic patterns."}
{"id": "2506.12460", "pdf": "https://arxiv.org/pdf/2506.12460", "abs": "https://arxiv.org/abs/2506.12460", "authors": ["Hao Shu"], "title": "Binarization-Aware Adjuster: Bridging Continuous Optimization and Binary Inference in Edge Detection", "categories": ["cs.CV"], "comment": "10 pages", "summary": "Image edge detection (ED) faces a fundamental mismatch between training and\ninference: models are trained using continuous-valued outputs but evaluated\nusing binary predictions. This misalignment, caused by the\nnon-differentiability of binarization, weakens the link between learning\nobjectives and actual task performance. In this paper, we propose a theoretical\nmethod to design a Binarization-Aware Adjuster (BAA), which explicitly\nincorporates binarization behavior into gradient-based optimization. At the\ncore of BAA is a novel loss adjustment mechanism based on a Distance Weight\nFunction (DWF), which reweights pixel-wise contributions according to their\ncorrectness and proximity to the decision boundary. This emphasizes\ndecision-critical regions while down-weighting less influential ones. We also\nintroduce a self-adaptive procedure to estimate the optimal binarization\nthreshold for BAA, further aligning training dynamics with inference behavior.\nExtensive experiments across various architectures and datasets demonstrate the\neffectiveness of our approach. Beyond ED, BAA offers a generalizable strategy\nfor bridging the gap between continuous optimization and discrete evaluation in\nstructured prediction tasks."}
{"id": "2506.12481", "pdf": "https://arxiv.org/pdf/2506.12481", "abs": "https://arxiv.org/abs/2506.12481", "authors": ["Runhao Zeng", "Qi Deng", "Ronghao Zhang", "Shuaicheng Niu", "Jian Chen", "Xiping Hu", "Victor C. M. Leung"], "title": "Exploring Audio Cues for Enhanced Test-Time Video Model Adaptation", "categories": ["cs.CV", "cs.LG", "cs.SD", "eess.AS"], "comment": "14 pages, 7 figures", "summary": "Test-time adaptation (TTA) aims to boost the generalization capability of a\ntrained model by conducting self-/unsupervised learning during the testing\nphase. While most existing TTA methods for video primarily utilize visual\nsupervisory signals, they often overlook the potential contribution of inherent\naudio data. To address this gap, we propose a novel approach that incorporates\naudio information into video TTA. Our method capitalizes on the rich semantic\ncontent of audio to generate audio-assisted pseudo-labels, a new concept in the\ncontext of video TTA. Specifically, we propose an audio-to-video label mapping\nmethod by first employing pre-trained audio models to classify audio signals\nextracted from videos and then mapping the audio-based predictions to video\nlabel spaces through large language models, thereby establishing a connection\nbetween the audio categories and video labels. To effectively leverage the\ngenerated pseudo-labels, we present a flexible adaptation cycle that determines\nthe optimal number of adaptation iterations for each sample, based on changes\nin loss and consistency across different views. This enables a customized\nadaptation process for each sample. Experimental results on two widely used\ndatasets (UCF101-C and Kinetics-Sounds-C), as well as on two newly constructed\naudio-video TTA datasets (AVE-C and AVMIT-C) with various corruption types,\ndemonstrate the superiority of our approach. Our method consistently improves\nadaptation performance across different video classification models and\nrepresents a significant step forward in integrating audio information into\nvideo TTA. Code: https://github.com/keikeiqi/Audio-Assisted-TTA."}
{"id": "2506.12492", "pdf": "https://arxiv.org/pdf/2506.12492", "abs": "https://arxiv.org/abs/2506.12492", "authors": ["Yanqiao Zhu"], "title": "Comparative Analysis of Deep Learning Strategies for Hypertensive Retinopathy Detection from Fundus Images: From Scratch and Pre-trained Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "This paper presents a comparative analysis of deep learning strategies for\ndetecting hypertensive retinopathy from fundus images, a central task in the\nHRDC challenge~\\cite{qian2025hrdc}. We investigate three distinct approaches: a\ncustom CNN, a suite of pre-trained transformer-based models, and an AutoML\nsolution. Our findings reveal a stark, architecture-dependent response to data\naugmentation. Augmentation significantly boosts the performance of pure Vision\nTransformers (ViTs), which we hypothesize is due to their weaker inductive\nbiases, forcing them to learn robust spatial and structural features.\nConversely, the same augmentation strategy degrades the performance of hybrid\nViT-CNN models, whose stronger, pre-existing biases from the CNN component may\nbe \"confused\" by the transformations. We show that smaller patch sizes\n(ViT-B/8) excel on augmented data, enhancing fine-grained detail capture.\nFurthermore, we demonstrate that a powerful self-supervised model like DINOv2\nfails on the original, limited dataset but is \"rescued\" by augmentation,\nhighlighting the critical need for data diversity to unlock its potential.\nPreliminary tests with a ViT-Large model show poor performance, underscoring\nthe risk of using overly-capacitive models on specialized, smaller datasets.\nThis work provides critical insights into the interplay between model\narchitecture, data augmentation, and dataset size for medical image\nclassification."}
{"id": "2506.12505", "pdf": "https://arxiv.org/pdf/2506.12505", "abs": "https://arxiv.org/abs/2506.12505", "authors": ["Mohsen Jenadeleh", "Jon Sneyers", "Davi Lazzarotto", "Shima Mohammadi", "Dominik Keller", "Atanas Boev", "Rakesh Rao Ramachandra Rao", "António Pinheiro", "Thomas Richter", "Alexander Raake", "Touradj Ebrahimi", "João Ascenso", "Dietmar Saupe"], "title": "Fine-Grained HDR Image Quality Assessment From Noticeably Distorted to Very High Fidelity", "categories": ["cs.CV"], "comment": "This paper has been accepted to QoMEX 2025. The work is funded by the\n  DFG (German Research Foundation) - Project ID 496858717, titled \"JND-based\n  Perceptual Video Quality Analysis and Modeling\". D.S. is funded by DFG\n  Project ID 251654672", "summary": "High dynamic range (HDR) and wide color gamut (WCG) technologies\nsignificantly improve color reproduction compared to standard dynamic range\n(SDR) and standard color gamuts, resulting in more accurate, richer, and more\nimmersive images. However, HDR increases data demands, posing challenges for\nbandwidth efficiency and compression techniques.\n  Advances in compression and display technologies require more precise image\nquality assessment, particularly in the high-fidelity range where perceptual\ndifferences are subtle.\n  To address this gap, we introduce AIC-HDR2025, the first such HDR dataset,\ncomprising 100 test images generated from five HDR sources, each compressed\nusing four codecs at five compression levels. It covers the high-fidelity\nrange, from visible distortions to compression levels below the visually\nlossless threshold.\n  A subjective study was conducted using the JPEG AIC-3 test methodology,\ncombining plain and boosted triplet comparisons. In total, 34,560 ratings were\ncollected from 151 participants across four fully controlled labs. The results\nconfirm that AIC-3 enables precise HDR quality estimation, with 95\\% confidence\nintervals averaging a width of 0.27 at 1 JND. In addition, several recently\nproposed objective metrics were evaluated based on their correlation with\nsubjective ratings. The dataset is publicly available."}
{"id": "2506.12514", "pdf": "https://arxiv.org/pdf/2506.12514", "abs": "https://arxiv.org/abs/2506.12514", "authors": ["Bingchen Zhao", "Oisin Mac Aodha"], "title": "Interpretable Text-Guided Image Clustering via Iterative Search", "categories": ["cs.CV"], "comment": null, "summary": "Traditional clustering methods aim to group unlabeled data points based on\ntheir similarity to each other. However, clustering, in the absence of\nadditional information, is an ill-posed problem as there may be many different,\nyet equally valid, ways to partition a dataset. Distinct users may want to use\ndifferent criteria to form clusters in the same data, e.g. shape v.s. color.\nRecently introduced text-guided image clustering methods aim to address this\nambiguity by allowing users to specify the criteria of interest using natural\nlanguage instructions. This instruction provides the necessary context and\ncontrol needed to obtain clusters that are more aligned with the users' intent.\nWe propose a new text-guided clustering approach named ITGC that uses an\niterative discovery process, guided by an unsupervised clustering objective, to\ngenerate interpretable visual concepts that better capture the criteria\nexpressed in a user's instructions. We report superior performance compared to\nexisting methods across a wide variety of image clustering and fine-grained\nclassification benchmarks."}
{"id": "2506.12515", "pdf": "https://arxiv.org/pdf/2506.12515", "abs": "https://arxiv.org/abs/2506.12515", "authors": ["Bingchen Zhao", "Kai Han"], "title": "Generalized Category Discovery under the Long-Tailed Distribution", "categories": ["cs.CV"], "comment": null, "summary": "This paper addresses the problem of Generalized Category Discovery (GCD)\nunder a long-tailed distribution, which involves discovering novel categories\nin an unlabelled dataset using knowledge from a set of labelled categories.\nExisting works assume a uniform distribution for both datasets, but real-world\ndata often exhibits a long-tailed distribution, where a few categories contain\nmost examples, while others have only a few. While the long-tailed distribution\nis well-studied in supervised and semi-supervised settings, it remains\nunexplored in the GCD context. We identify two challenges in this setting -\nbalancing classifier learning and estimating category numbers - and propose a\nframework based on confident sample selection and density-based clustering to\ntackle them. Our experiments on both long-tailed and conventional GCD datasets\ndemonstrate the effectiveness of our method."}
{"id": "2506.12517", "pdf": "https://arxiv.org/pdf/2506.12517", "abs": "https://arxiv.org/abs/2506.12517", "authors": ["Yunhao Shui", "Xuekuan Wang", "Feng Qiu", "Yuqiu Huang", "Jinzhu Li", "Haoyu Zheng", "Jinru Han", "Zhuo Zeng", "Pengpeng Zhang", "Jiarui Han", "Keqiang Sun"], "title": "Retrieval Augmented Comic Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "We present RaCig, a novel system for generating comic-style image sequences\nwith consistent characters and expressive gestures. RaCig addresses two key\nchallenges: (1) maintaining character identity and costume consistency across\nframes, and (2) producing diverse and vivid character gestures. Our approach\nintegrates a retrieval-based character assignment module, which aligns\ncharacters in textual prompts with reference images, and a regional character\ninjection mechanism that embeds character features into specified image\nregions. Experimental results demonstrate that RaCig effectively generates\nengaging comic narratives with coherent characters and dynamic interactions.\nThe source code will be publicly available to support further research in this\narea."}
{"id": "2506.12520", "pdf": "https://arxiv.org/pdf/2506.12520", "abs": "https://arxiv.org/abs/2506.12520", "authors": ["Saemee Choi", "Sohyun Jeong", "Jaegul Choo", "Jinhee Kim"], "title": "Good Noise Makes Good Edits: A Training-Free Diffusion-Based Video Editing with Image and Text Prompts", "categories": ["cs.CV"], "comment": null, "summary": "We propose ImEdit, the first zero-shot, training-free video editing method\nconditioned on both images and text. The proposed method introduces\n$\\rho$-start sampling and dilated dual masking to construct well-structured\nnoise maps for coherent and accurate edits. We further present zero image\nguidance, a controllable negative prompt strategy, for visual fidelity. Both\nquantitative and qualitative evaluations show that our method outperforms\nstate-of-the-art methods across all metrics."}
{"id": "2506.12524", "pdf": "https://arxiv.org/pdf/2506.12524", "abs": "https://arxiv.org/abs/2506.12524", "authors": ["Nuwan Bandara", "Thivya Kandappu", "Archan Misra"], "title": "Inference-Time Gaze Refinement for Micro-Expression Recognition: Enhancing Event-Based Eye Tracking with Motion-Aware Post-Processing", "categories": ["cs.CV", "cs.HC", "cs.LG", "eess.IV"], "comment": "18 pages", "summary": "Event-based eye tracking holds significant promise for fine-grained cognitive\nstate inference, offering high temporal resolution and robustness to motion\nartifacts, critical features for decoding subtle mental states such as\nattention, confusion, or fatigue. In this work, we introduce a model-agnostic,\ninference-time refinement framework designed to enhance the output of existing\nevent-based gaze estimation models without modifying their architecture or\nrequiring retraining. Our method comprises two key post-processing modules: (i)\nMotion-Aware Median Filtering, which suppresses blink-induced spikes while\npreserving natural gaze dynamics, and (ii) Optical Flow-Based Local Refinement,\nwhich aligns gaze predictions with cumulative event motion to reduce spatial\njitter and temporal discontinuities. To complement traditional spatial accuracy\nmetrics, we propose a novel Jitter Metric that captures the temporal smoothness\nof predicted gaze trajectories based on velocity regularity and local signal\ncomplexity. Together, these contributions significantly improve the consistency\nof event-based gaze signals, making them better suited for downstream tasks\nsuch as micro-expression analysis and mind-state decoding. Our results\ndemonstrate consistent improvements across multiple baseline models on\ncontrolled datasets, laying the groundwork for future integration with\nmultimodal affect recognition systems in real-world environments."}
{"id": "2506.12530", "pdf": "https://arxiv.org/pdf/2506.12530", "abs": "https://arxiv.org/abs/2506.12530", "authors": ["Xingzhong Hou", "Jie Wu", "Boxiao Liu", "Yi Zhang", "Guanglu Song", "Yunpeng Liu", "Yu Liu", "Haihang You"], "title": "Towards Seamless Borders: A Method for Mitigating Inconsistencies in Image Inpainting and Outpainting", "categories": ["cs.CV"], "comment": null, "summary": "Image inpainting is the task of reconstructing missing or damaged parts of an\nimage in a way that seamlessly blends with the surrounding content. With the\nadvent of advanced generative models, especially diffusion models and\ngenerative adversarial networks, inpainting has achieved remarkable\nimprovements in visual quality and coherence. However, achieving seamless\ncontinuity remains a significant challenge. In this work, we propose two novel\nmethods to address discrepancy issues in diffusion-based inpainting models.\nFirst, we introduce a modified Variational Autoencoder that corrects color\nimbalances, ensuring that the final inpainted results are free of color\nmismatches. Second, we propose a two-step training strategy that improves the\nblending of generated and existing image content during the diffusion process.\nThrough extensive experiments, we demonstrate that our methods effectively\nreduce discontinuity and produce high-quality inpainting results that are\ncoherent and visually appealing."}
{"id": "2506.12561", "pdf": "https://arxiv.org/pdf/2506.12561", "abs": "https://arxiv.org/abs/2506.12561", "authors": ["Mahmudul Hasan"], "title": "Parkinson's Disease Freezing of Gait (FoG) Symptom Detection Using Machine Learning from Wearable Sensor Data", "categories": ["cs.CV", "eess.SP"], "comment": null, "summary": "Freezing of gait (FoG) is a special symptom found in patients with\nParkinson's disease (PD). Patients who have FoG abruptly lose the capacity to\nwalk as they normally would. Accelerometers worn by patients can record\nmovement data during these episodes, and machine learning algorithms can be\nuseful to categorize this information. Thus, the combination may be able to\nidentify FoG in real time. In order to identify FoG events in accelerometer\ndata, we introduce the Transformer Encoder-Bi-LSTM fusion model in this paper.\nThe model's capability to differentiate between FoG episodes and normal\nmovement was used to evaluate its performance, and on the Kaggle Parkinson's\nFreezing of Gait dataset, the proposed Transformer Encoder-Bi-LSTM fusion model\nproduced 92.6% accuracy, 80.9% F1 score, and 52.06% in terms of mean average\nprecision. The findings highlight how Deep Learning-based approaches may\nprogress the field of FoG identification and help PD patients receive better\ntreatments and management plans."}
{"id": "2506.12563", "pdf": "https://arxiv.org/pdf/2506.12563", "abs": "https://arxiv.org/abs/2506.12563", "authors": ["Charith Wickrema", "Sara Leary", "Shivangi Sarkar", "Mark Giglio", "Eric Bianchi", "Eliza Mace", "Michael Twardowski"], "title": "Benchmarking Image Similarity Metrics for Novel View Synthesis Applications", "categories": ["cs.CV"], "comment": null, "summary": "Traditional image similarity metrics are ineffective at evaluating the\nsimilarity between a real image of a scene and an artificially generated\nversion of that viewpoint [6, 9, 13, 14]. Our research evaluates the\neffectiveness of a new, perceptual-based similarity metric, DreamSim [2], and\nthree popular image similarity metrics: Structural Similarity (SSIM), Peak\nSignal-to-Noise Ratio (PSNR), and Learned Perceptual Image Patch Similarity\n(LPIPS) [18, 19] in novel view synthesis (NVS) applications. We create a corpus\nof artificially corrupted images to quantify the sensitivity and discriminative\npower of each of the image similarity metrics. These tests reveal that\ntraditional metrics are unable to effectively differentiate between images with\nminor pixel-level changes and those with substantial corruption, whereas\nDreamSim is more robust to minor defects and can effectively evaluate the\nhigh-level similarity of the image. Additionally, our results demonstrate that\nDreamSim provides a more effective and useful evaluation of render quality,\nespecially for evaluating NVS renders in real-world use cases where slight\nrendering corruptions are common, but do not affect image utility for human\ntasks."}
{"id": "2506.12568", "pdf": "https://arxiv.org/pdf/2506.12568", "abs": "https://arxiv.org/abs/2506.12568", "authors": ["Chunjiang Wang", "Kun Zhang", "Yandong Liu", "Zhiyang He", "Xiaodong Tao", "S. Kevin Zhou"], "title": "MVP-CBM:Multi-layer Visual Preference-enhanced Concept Bottleneck Model for Explainable Medical Image Classification", "categories": ["cs.CV", "cs.AI"], "comment": "7 pages, 6 figures,", "summary": "The concept bottleneck model (CBM), as a technique improving interpretability\nvia linking predictions to human-understandable concepts, makes high-risk and\nlife-critical medical image classification credible. Typically, existing CBM\nmethods associate the final layer of visual encoders with concepts to explain\nthe model's predictions. However, we empirically discover the phenomenon of\nconcept preference variation, that is, the concepts are preferably associated\nwith the features at different layers than those only at the final layer; yet a\nblind last-layer-based association neglects such a preference variation and\nthus weakens the accurate correspondences between features and concepts,\nimpairing model interpretability. To address this issue, we propose a novel\nMulti-layer Visual Preference-enhanced Concept Bottleneck Model (MVP-CBM),\nwhich comprises two key novel modules: (1) intra-layer concept preference\nmodeling, which captures the preferred association of different concepts with\nfeatures at various visual layers, and (2) multi-layer concept sparse\nactivation fusion, which sparsely aggregates concept activations from multiple\nlayers to enhance performance. Thus, by explicitly modeling concept\npreferences, MVP-CBM can comprehensively leverage multi-layer visual\ninformation to provide a more nuanced and accurate explanation of model\ndecisions. Extensive experiments on several public medical classification\nbenchmarks demonstrate that MVP-CBM achieves state-of-the-art accuracy and\ninteroperability, verifying its superiority. Code is available at\nhttps://github.com/wcj6/MVP-CBM."}
{"id": "2506.12585", "pdf": "https://arxiv.org/pdf/2506.12585", "abs": "https://arxiv.org/abs/2506.12585", "authors": ["Darryl Ho", "Samuel Madden"], "title": "DejaVid: Encoder-Agnostic Learned Temporal Matching for Video Classification", "categories": ["cs.CV", "I.2.10; I.4.8; I.5.1"], "comment": "Accepted to CVPR 2025 (IEEE/CVF Conference on Computer Vision and\n  Pattern Recognition), main conference, poster presentation", "summary": "In recent years, large transformer-based video encoder models have greatly\nadvanced state-of-the-art performance on video classification tasks. However,\nthese large models typically process videos by averaging embedding outputs from\nmultiple clips over time to produce fixed-length representations. This approach\nfails to account for a variety of time-related features, such as variable video\ndurations, chronological order of events, and temporal variance in feature\nsignificance. While methods for temporal modeling do exist, they often require\nsignificant architectural changes and expensive retraining, making them\nimpractical for off-the-shelf, fine-tuned large encoders. To overcome these\nlimitations, we propose DejaVid, an encoder-agnostic method that enhances model\nperformance without the need for retraining or altering the architecture. Our\nframework converts a video into a variable-length temporal sequence of\nembeddings, which we call a multivariate time series (MTS). An MTS naturally\npreserves temporal order and accommodates variable video durations. We then\nlearn per-timestep, per-feature weights over the encoded MTS frames, allowing\nus to account for variations in feature importance over time. We introduce a\nnew neural network architecture inspired by traditional time series alignment\nalgorithms for this learning task. Our evaluation demonstrates that DejaVid\nsubstantially improves the performance of a state-of-the-art large encoder,\nachieving leading Top-1 accuracy of 77.2% on Something-Something V2, 89.1% on\nKinetics-400, and 88.6% on HMDB51, while adding fewer than 1.8% additional\nlearnable parameters and requiring less than 3 hours of training time. Our code\nis available at https://github.com/darrylho/DejaVid."}
{"id": "2506.12609", "pdf": "https://arxiv.org/pdf/2506.12609", "abs": "https://arxiv.org/abs/2506.12609", "authors": ["Lexiang Tang", "Xianwei Zhuang", "Bang Yang", "Zhiyuan Hu", "Hongxiang Li", "Lu Ma", "Jinghan Ru", "Yuexian Zou"], "title": "Not All Tokens and Heads Are Equally Important: Dual-Level Attention Intervention for Hallucination Mitigation", "categories": ["cs.CV"], "comment": null, "summary": "Large vision-language models (LVLMs) have shown remarkable capabilities\nacross a wide range of multimodal tasks. However, they remain prone to visual\nhallucination (VH), often producing confident but incorrect descriptions of\nvisual content. We present VisFlow, an efficient and training-free framework\ndesigned to mitigate VH by directly manipulating attention patterns during\ninference. Through systematic analysis, we identify three key pathological\nattention behaviors in LVLMs: (1) weak visual grounding, where attention to\nvisual tokens is insufficient or misallocated, over-focusing on uninformative\nregions; (2) language prior dominance, where excessive attention to prior\nresponse tokens reinforces autoregressive patterns and impairs multimodal\nalignment; (3) prompt redundancy, where many attention heads fixate on system\nprompt tokens, disrupting the integration of image, instruction, and response\ncontent. To address these issues, we introduce two inference-time\ninterventions: token-level attention intervention (TAI), which enhances focus\non salient visual content, and head-level attention intervention (HAI), which\nsuppresses over-attention to prompt and nearby text tokens. VisFlow operates\nwithout additional training or model modifications. Extensive experiments\nacross models and benchmarks show that VisFlow effectively reduces\nhallucinations and improves visual factuality, with negligible computational\ncost."}
{"id": "2506.12610", "pdf": "https://arxiv.org/pdf/2506.12610", "abs": "https://arxiv.org/abs/2506.12610", "authors": ["Wenxiao Cai", "Zongru Li", "Iris Wang", "Yu-Neng Wang", "Thomas H. Lee"], "title": "OscNet v1.5: Energy Efficient Hopfield Network on CMOS Oscillators for Image Classification", "categories": ["cs.CV"], "comment": null, "summary": "Machine learning has achieved remarkable advancements but at the cost of\nsignificant computational resources. This has created an urgent need for a\nnovel and energy-efficient computational fabric. CMOS Oscillator Networks\n(OscNet) is a brain inspired and specially designed hardware for low energy\nconsumption. In this paper, we propose a Hopfield Network based machine\nlearning algorithm that can be implemented on OscNet. The network is trained\nusing forward propagation alone to learn sparsely connected weights, yet\nachieves an 8% improvement in accuracy compared to conventional deep learning\nmodels on MNIST dataset. OscNet v1.5 achieves competitive accuracy on MNIST and\nis well-suited for implementation using CMOS-compatible ring oscillator arrays\nwith SHIL. In oscillator-based implementation, we utilize only 24% of the\nconnections used in a fully connected Hopfield network, with merely a 0.1% drop\nin accuracy. OscNet v1.5 relies solely on forward propagation and employs\nsparse connections, making it an energy-efficient machine learning pipeline\ndesigned for CMOS oscillator computing. The repository for OscNet family is:\nhttps://github.com/RussRobin/OscNet."}
{"id": "2506.12623", "pdf": "https://arxiv.org/pdf/2506.12623", "abs": "https://arxiv.org/abs/2506.12623", "authors": ["Yuan Zang", "Hao Tan", "Seunghyun Yoon", "Franck Dernoncourt", "Jiuxiang Gu", "Kushal Kafle", "Chen Sun", "Trung Bui"], "title": "MS4UI: A Dataset for Multi-modal Summarization of User Interface Instructional Videos", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "We study multi-modal summarization for instructional videos, whose goal is to\nprovide users an efficient way to learn skills in the form of text instructions\nand key video frames. We observe that existing benchmarks focus on generic\nsemantic-level video summarization, and are not suitable for providing\nstep-by-step executable instructions and illustrations, both of which are\ncrucial for instructional videos. We propose a novel benchmark for user\ninterface (UI) instructional video summarization to fill the gap. We collect a\ndataset of 2,413 UI instructional videos, which spans over 167 hours. These\nvideos are manually annotated for video segmentation, text summarization, and\nvideo summarization, which enable the comprehensive evaluations for concise and\nexecutable video summarization. We conduct extensive experiments on our\ncollected MS4UI dataset, which suggest that state-of-the-art multi-modal\nsummarization methods struggle on UI video summarization, and highlight the\nimportance of new methods for UI instructional video summarization."}
{"id": "2506.12633", "pdf": "https://arxiv.org/pdf/2506.12633", "abs": "https://arxiv.org/abs/2506.12633", "authors": ["Changhyun Choi", "Sungha Kim", "H. Jin Kim"], "title": "Performance Plateaus in Inference-Time Scaling for Text-to-Image Diffusion Without External Models", "categories": ["cs.CV", "cs.LG"], "comment": "MOSS workshop at ICML 2025 accepted", "summary": "Recently, it has been shown that investing computing resources in searching\nfor good initial noise for a text-to-image diffusion model helps improve\nperformance. However, previous studies required external models to evaluate the\nresulting images, which is impossible on GPUs with small VRAM. For these\nreasons, we apply Best-of-N inference-time scaling to algorithms that optimize\nthe initial noise of a diffusion model without external models across multiple\ndatasets and backbones. We demonstrate that inference-time scaling for\ntext-to-image diffusion models in this setting quickly reaches a performance\nplateau, and a relatively small number of optimization steps suffices to\nachieve the maximum achievable performance with each algorithm."}
{"id": "2506.12680", "pdf": "https://arxiv.org/pdf/2506.12680", "abs": "https://arxiv.org/abs/2506.12680", "authors": ["Chen-Bin Feng", "Kangdao Liu", "Jian Sun", "Jiping Jin", "Yiguo Jiang", "Chi-Man Vong"], "title": "3D Hand Mesh-Guided AI-Generated Malformed Hand Refinement with Hand Pose Transformation via Diffusion Model", "categories": ["cs.CV"], "comment": null, "summary": "The malformed hands in the AI-generated images seriously affect the\nauthenticity of the images. To refine malformed hands, existing depth-based\napproaches use a hand depth estimator to guide the refinement of malformed\nhands. Due to the performance limitations of the hand depth estimator, many\nhand details cannot be represented, resulting in errors in the generated hands,\nsuch as confusing the palm and the back of the hand. To solve this problem, we\npropose a 3D mesh-guided refinement framework using a diffusion pipeline. We\nuse a state-of-the-art 3D hand mesh estimator, which provides more details of\nthe hands. For training, we collect and reannotate a dataset consisting of RGB\nimages and 3D hand mesh. Then we design a diffusion inpainting model to\ngenerate refined outputs guided by 3D hand meshes. For inference, we propose a\ndouble check algorithm to facilitate the 3D hand mesh estimator to obtain\nrobust hand mesh guidance to obtain our refined results. Beyond malformed hand\nrefinement, we propose a novel hand pose transformation method. It increases\nthe flexibility and diversity of the malformed hand refinement task. We made\nthe restored images mimic the hand poses of the reference images. The pose\ntransformation requires no additional training. Extensive experimental results\ndemonstrate the superior performance of our proposed method."}
{"id": "2506.12683", "pdf": "https://arxiv.org/pdf/2506.12683", "abs": "https://arxiv.org/abs/2506.12683", "authors": ["Samarth Singhal", "Sandeep Singhal"], "title": "Evaluating Cell Type Inference in Vision Language Models Under Varying Visual Context", "categories": ["cs.CV", "q-bio.QM"], "comment": null, "summary": "Vision-Language Models (VLMs) have rapidly advanced alongside Large Language\nModels (LLMs). This study evaluates the capabilities of prominent generative\nVLMs, such as GPT-4.1 and Gemini 2.5 Pro, accessed via APIs, for histopathology\nimage classification tasks, including cell typing. Using diverse datasets from\npublic and private sources, we apply zero-shot and one-shot prompting methods\nto assess VLM performance, comparing them against custom-trained Convolutional\nNeural Networks (CNNs). Our findings demonstrate that while one-shot prompting\nsignificantly improves VLM performance over zero-shot ($p \\approx 1.005 \\times\n10^{-5}$ based on Kappa scores), these general-purpose VLMs currently\nunderperform supervised CNNs on most tasks. This work underscores both the\npromise and limitations of applying current VLMs to specialized domains like\npathology via in-context learning. All code and instructions for reproducing\nthe study can be accessed from the repository\nhttps://www.github.com/a12dongithub/VLMCCE."}
{"id": "2506.12697", "pdf": "https://arxiv.org/pdf/2506.12697", "abs": "https://arxiv.org/abs/2506.12697", "authors": ["Yuxiang Wang", "Xuecheng Bai", "Boyu Hu", "Chuanzhi Xu", "Haodong Chen", "Vera Chung", "Tingxue Li"], "title": "MGDFIS: Multi-scale Global-detail Feature Integration Strategy for Small Object Detection", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "9 pages, 5 figures, 3 tables", "summary": "Small object detection in UAV imagery is crucial for applications such as\nsearch-and-rescue, traffic monitoring, and environmental surveillance, but it\nis hampered by tiny object size, low signal-to-noise ratios, and limited\nfeature extraction. Existing multi-scale fusion methods help, but add\ncomputational burden and blur fine details, making small object detection in\ncluttered scenes difficult. To overcome these challenges, we propose the\nMulti-scale Global-detail Feature Integration Strategy (MGDFIS), a unified\nfusion framework that tightly couples global context with local detail to boost\ndetection performance while maintaining efficiency. MGDFIS comprises three\nsynergistic modules: the FusionLock-TSS Attention Module, which marries\ntoken-statistics self-attention with DynamicTanh normalization to highlight\nspectral and spatial cues at minimal cost; the Global-detail Integration\nModule, which fuses multi-scale context via directional convolution and\nparallel attention while preserving subtle shape and texture variations; and\nthe Dynamic Pixel Attention Module, which generates pixel-wise weighting maps\nto rebalance uneven foreground and background distributions and sharpen\nresponses to true object regions. Extensive experiments on the VisDrone\nbenchmark demonstrate that MGDFIS consistently outperforms state-of-the-art\nmethods across diverse backbone architectures and detection frameworks,\nachieving superior precision and recall with low inference time. By striking an\noptimal balance between accuracy and resource usage, MGDFIS provides a\npractical solution for small-object detection on resource-constrained UAV\nplatforms."}
{"id": "2506.12698", "pdf": "https://arxiv.org/pdf/2506.12698", "abs": "https://arxiv.org/abs/2506.12698", "authors": ["Cuong Manh Hoang", "Yeejin Lee", "Byeongkeun Kang"], "title": "Unsupervised Contrastive Learning Using Out-Of-Distribution Data for Long-Tailed Dataset", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "13 pages", "summary": "This work addresses the task of self-supervised learning (SSL) on a\nlong-tailed dataset that aims to learn balanced and well-separated\nrepresentations for downstream tasks such as image classification. This task is\ncrucial because the real world contains numerous object categories, and their\ndistributions are inherently imbalanced. Towards robust SSL on a\nclass-imbalanced dataset, we investigate leveraging a network trained using\nunlabeled out-of-distribution (OOD) data that are prevalently available online.\nWe first train a network using both in-domain (ID) and sampled OOD data by\nback-propagating the proposed pseudo semantic discrimination loss alongside a\ndomain discrimination loss. The OOD data sampling and loss functions are\ndesigned to learn a balanced and well-separated embedding space. Subsequently,\nwe further optimize the network on ID data by unsupervised contrastive learning\nwhile using the previously trained network as a guiding network. The guiding\nnetwork is utilized to select positive/negative samples and to control the\nstrengths of attractive/repulsive forces in contrastive learning. We also\ndistil and transfer its embedding space to the training network to maintain\nbalancedness and separability. Through experiments on four publicly available\nlong-tailed datasets, we demonstrate that the proposed method outperforms\nprevious state-of-the-art methods."}
{"id": "2506.12706", "pdf": "https://arxiv.org/pdf/2506.12706", "abs": "https://arxiv.org/abs/2506.12706", "authors": ["Jiaming Zhang", "Xin Wang", "Xingjun Ma", "Lingyu Qiu", "Yu-Gang Jiang", "Jitao Sang"], "title": "NAP-Tuning: Neural Augmented Prompt Tuning for Adversarially Robust Vision-Language Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Vision-Language Models (VLMs) such as CLIP have demonstrated remarkable\ncapabilities in understanding relationships between visual and textual data\nthrough joint embedding spaces. Despite their effectiveness, these models\nremain vulnerable to adversarial attacks, particularly in the image modality,\nposing significant security concerns. Building upon our previous work on\nAdversarial Prompt Tuning (AdvPT), which introduced learnable text prompts to\nenhance adversarial robustness in VLMs without extensive parameter training, we\npresent a significant extension by introducing the Neural Augmentor framework\nfor Multi-modal Adversarial Prompt Tuning (NAP-Tuning).Our key innovations\ninclude: (1) extending AdvPT from text-only to multi-modal prompting across\nboth text and visual modalities, (2) expanding from single-layer to multi-layer\nprompt architectures, and (3) proposing a novel architecture-level redesign\nthrough our Neural Augmentor approach, which implements feature purification to\ndirectly address the distortions introduced by adversarial attacks in feature\nspace. Our NAP-Tuning approach incorporates token refiners that learn to\nreconstruct purified features through residual connections, allowing for\nmodality-specific and layer-specific feature correction.Comprehensive\nexperiments demonstrate that NAP-Tuning significantly outperforms existing\nmethods across various datasets and attack types. Notably, our approach shows\nsignificant improvements over the strongest baselines under the challenging\nAutoAttack benchmark, outperforming them by 33.5% on ViT-B16 and 33.0% on\nViT-B32 architectures while maintaining competitive clean accuracy."}
{"id": "2506.12712", "pdf": "https://arxiv.org/pdf/2506.12712", "abs": "https://arxiv.org/abs/2506.12712", "authors": ["Zhenghao Xi", "Zhengnan Lv", "Yang Zheng", "Xiang Liu", "Zhuang Yu", "Junran Chen", "Jing Hu", "Yaqi Liu"], "title": "Combining Self-attention and Dilation Convolutional for Semantic Segmentation of Coal Maceral Groups", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "The segmentation of coal maceral groups can be described as a semantic\nsegmentation process of coal maceral group images, which is of great\nsignificance for studying the chemical properties of coal. Generally, existing\nsemantic segmentation models of coal maceral groups use the method of stacking\nparameters to achieve higher accuracy. It leads to increased computational\nrequirements and impacts model training efficiency. At the same time, due to\nthe professionalism and diversity of coal maceral group images sampling,\nobtaining the number of samples for model training requires a long time and\nprofessional personnel operation. To address these issues, We have innovatively\ndeveloped an IoT-based DA-VIT parallel network model. By utilizing this model,\nwe can continuously broaden the dataset through IoT and achieving sustained\nimprovement in the accuracy of coal maceral groups segmentation. Besides, we\ndecouple the parallel network from the backbone network to ensure the normal\nusing of the backbone network during model data updates. Secondly, DCSA\nmechanism of DA-VIT is introduced to enhance the local feature information of\ncoal microscopic images. This DCSA can decompose the large kernels of\nconvolutional attention into multiple scales and reduce 81.18% of\nparameters.Finally, we performed the contrast experiment and ablation\nexperiment between DA-VIT and state-of-the-art methods at lots of evaluation\nmetrics. Experimental results show that DA-VIT-Base achieves 92.14% pixel\naccuracy and 63.18% mIoU. Params and FLOPs of DA-VIT-Tiny are 4.95M and 8.99G,\nrespectively. All of the evaluation metrics of the proposed DA-VIT are better\nthan other state-of-the-art methods."}
{"id": "2506.12716", "pdf": "https://arxiv.org/pdf/2506.12716", "abs": "https://arxiv.org/abs/2506.12716", "authors": ["Wen-Hsuan Chu", "Lei Ke", "Jianmeng Liu", "Mingxiao Huo", "Pavel Tokmakov", "Katerina Fragkiadaki"], "title": "Generative 4D Scene Gaussian Splatting with Object View-Synthesis Priors", "categories": ["cs.CV"], "comment": "This is an updated and extended version of our CVPR paper \"Robust\n  Multi-Object 4D Generation in Complex Video Scenarios\"", "summary": "We tackle the challenge of generating dynamic 4D scenes from monocular,\nmulti-object videos with heavy occlusions, and introduce GenMOJO, a novel\napproach that integrates rendering-based deformable 3D Gaussian optimization\nwith generative priors for view synthesis. While existing models perform well\non novel view synthesis for isolated objects, they struggle to generalize to\ncomplex, cluttered scenes. To address this, GenMOJO decomposes the scene into\nindividual objects, optimizing a differentiable set of deformable Gaussians per\nobject. This object-wise decomposition allows leveraging object-centric\ndiffusion models to infer unobserved regions in novel viewpoints. It performs\njoint Gaussian splatting to render the full scene, capturing cross-object\nocclusions, and enabling occlusion-aware supervision. To bridge the gap between\nobject-centric priors and the global frame-centric coordinate system of videos,\nGenMOJO uses differentiable transformations that align generative and rendering\nconstraints within a unified framework. The resulting model generates 4D object\nreconstructions over space and time, and produces accurate 2D and 3D point\ntracks from monocular input. Quantitative evaluations and perceptual human\nstudies confirm that GenMOJO generates more realistic novel views of scenes and\nproduces more accurate point tracks compared to existing approaches."}
{"id": "2506.12723", "pdf": "https://arxiv.org/pdf/2506.12723", "abs": "https://arxiv.org/abs/2506.12723", "authors": ["Ye Li", "Yuan Meng", "Zewen Sun", "Kangye Ji", "Chen Tang", "Jiajun Fan", "Xinzhu Ma", "Shutao Xia", "Zhi Wang", "Wenwu Zhu"], "title": "SP-VLA: A Joint Model Scheduling and Token Pruning Approach for VLA Model Acceleration", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Vision-Language-Action (VLA) models have attracted increasing attention for\ntheir strong control capabilities. However, their high computational cost and\nlow execution frequency hinder their suitability for real-time tasks such as\nrobotic manipulation and autonomous navigation. Existing VLA acceleration\nmethods primarily focus on structural optimization, overlooking the fact that\nthese models operate in sequential decision-making environments. As a result,\ntemporal redundancy in sequential action generation and spatial redundancy in\nvisual input remain unaddressed. To this end, we propose SP-VLA, a unified\nframework that accelerates VLA models by jointly scheduling models and pruning\ntokens. Specifically, we design an action-aware model scheduling mechanism that\nreduces temporal redundancy by dynamically switching between VLA model and a\nlightweight generator. Inspired by the human motion pattern of focusing on key\ndecision points while relying on intuition for other actions, we categorize VLA\nactions into deliberative and intuitive, assigning the former to the VLA model\nand the latter to the lightweight generator, enabling frequency-adaptive\nexecution through collaborative model scheduling. To address spatial\nredundancy, we further develop a spatio-semantic dual-aware token pruning\nmethod. Tokens are classified into spatial and semantic types and pruned based\non their dual-aware importance to accelerate VLA inference. These two\nmechanisms work jointly to guide the VLA in focusing on critical actions and\nsalient visual information, achieving effective acceleration while maintaining\nhigh accuracy. Experimental results demonstrate that our method achieves up to\n1.5$\\times$ acceleration with less than 3% drop in accuracy, outperforming\nexisting approaches in multiple tasks."}
{"id": "2506.12724", "pdf": "https://arxiv.org/pdf/2506.12724", "abs": "https://arxiv.org/abs/2506.12724", "authors": ["Hiroshi Tanaka", "Anika Rao", "Hana Satou", "Michael Johnson", "Sofia García"], "title": "Dynamic Modality Scheduling for Multimodal Large Models via Confidence, Uncertainty, and Semantic Consistency", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal Large Models (MLLMs) have achieved remarkable progress in\nvision-language understanding and generation tasks. However, existing MLLMs\ntypically rely on static modality fusion strategies, which treat all modalities\nequally regardless of their instance-level reliability or semantic\ncontribution. This often leads to suboptimal performance, especially in\nscenarios with noisy, missing, or misaligned modalities.\n  In this paper, we propose Dynamic Modality Scheduling (DMS), a novel\nframework that adaptively adjusts the contribution of each modality at a\nper-sample level. DMS evaluates each modality based on three key factors: (1)\n\\textit{confidence}, estimated from predictive entropy; (2)\n\\textit{uncertainty}, obtained via Monte Carlo dropout; and (3)\n\\textit{semantic consistency}, computed through inter-modal similarity. These\nsignals are combined through a learnable or rule-based scheduler to generate\nsoft modality weights used in downstream fusion.To ensure stable training, we\nfurther introduce a \\textit{Modality Weight Consistency Loss}, which\nregularizes the fused representation to stay close to unimodal embeddings\nproportionally to their assigned weights. Our method is model-agnostic and can\nbe integrated into existing MLLMs such as BLIP-2 and LLaVA. Experimental\nresults on VQA, image-text retrieval, and captioning tasks show that DMS\nsignificantly improves both clean and robust performance, especially under\nmodality corruption or dropout conditions. This work provides a general and\neffective mechanism to enable instance-aware and robustness-enhanced multimodal\nmodeling."}
{"id": "2506.12727", "pdf": "https://arxiv.org/pdf/2506.12727", "abs": "https://arxiv.org/abs/2506.12727", "authors": ["Minhyuk Choi", "Injae Kim", "Hyunwoo J. Kim"], "title": "Efficient multi-view training for 3D Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) has emerged as a preferred choice alongside\nNeural Radiance Fields (NeRF) in inverse rendering due to its superior\nrendering speed. Currently, the common approach in 3DGS is to utilize\n\"single-view\" mini-batch training, where only one image is processed per\niteration, in contrast to NeRF's \"multi-view\" mini-batch training, which\nleverages multiple images. We observe that such single-view training can lead\nto suboptimal optimization due to increased variance in mini-batch stochastic\ngradients, highlighting the necessity for multi-view training. However,\nimplementing multi-view training in 3DGS poses challenges. Simply rendering\nmultiple images per iteration incurs considerable overhead and may result in\nsuboptimal Gaussian densification due to its reliance on single-view\nassumptions. To address these issues, we modify the rasterization process to\nminimize the overhead associated with multi-view training and propose a 3D\ndistance-aware D-SSIM loss and multi-view adaptive density control that better\nsuits multi-view scenarios. Our experiments demonstrate that the proposed\nmethods significantly enhance the performance of 3DGS and its variants, freeing\n3DGS from the constraints of single-view training."}
{"id": "2506.12733", "pdf": "https://arxiv.org/pdf/2506.12733", "abs": "https://arxiv.org/abs/2506.12733", "authors": ["Liam Bennett", "Mason Clark", "Lucas Anderson", "Hana Satou", "Olivia Martinez"], "title": "Learning to Fuse: Modality-Aware Adaptive Scheduling for Robust Multimodal Foundation Models", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal foundation models have achieved impressive progress across a wide\nrange of vision-language tasks. However, existing approaches often adopt fixed\nor task-specific fusion strategies, neglecting the intrinsic variability of\nmodality reliability and sample complexity. In this paper, we propose\nModality-Aware Adaptive Fusion Scheduling (MA-AFS), a general framework that\nlearns to dynamically modulate the contribution of each modality on a\nper-instance basis. MA-AFS introduces a lightweight neural scheduler that\npredicts modality fusion weights by integrating visual and textual entropy\nsignals along with cross-modal agreement cues. This enables the model to\nadaptively emphasize more reliable modalities, especially under noisy, missing,\nor misaligned inputs. We formulate the fusion process as a differentiable\nscheduling mechanism, analyze its theoretical consistency and regularization\neffect, and demonstrate that it improves robustness without increasing model\ncapacity significantly. Extensive experiments on image-text retrieval,\ncaptioning, and visual question answering show that MA-AFS achieves consistent\nperformance gains over strong baselines such as CLIP, ALBEF, and BLIP.\nMoreover, MA-AFS exhibits improved robustness under modality corruption and\nenhanced generalization under domain shifts. Our work highlights the importance\nof adaptive fusion and opens a promising direction toward reliable and\nuncertainty-aware multimodal learning."}
{"id": "2506.12737", "pdf": "https://arxiv.org/pdf/2506.12737", "abs": "https://arxiv.org/abs/2506.12737", "authors": ["Changsheng Gao", "Shan Liu", "Feng Wu", "Weisi Lin"], "title": "Cross-architecture universal feature coding via distribution alignment", "categories": ["cs.CV", "cs.DC"], "comment": null, "summary": "Feature coding has become increasingly important in scenarios where semantic\nrepresentations rather than raw pixels are transmitted and stored. However,\nmost existing methods are architecture-specific, targeting either CNNs or\nTransformers. This design limits their applicability in real-world scenarios\nwhere features from both architectures coexist. To address this gap, we\nintroduce a new research problem: cross-architecture universal feature coding\n(CAUFC), which seeks to build a unified codec that can effectively compress\nfeatures from heterogeneous architectures. To tackle this challenge, we propose\na two-step distribution alignment method. First, we design the format alignment\nmethod that unifies CNN and Transformer features into a consistent 2D token\nformat. Second, we propose the feature value alignment method that harmonizes\nstatistical distributions via truncation and normalization. As a first attempt\nto study CAUFC, we evaluate our method on the image classification task.\nExperimental results demonstrate that our method achieves superior\nrate-accuracy trade-offs compared to the architecture-specific baseline. This\nwork marks an initial step toward universal feature compression across\nheterogeneous model architectures."}
{"id": "2506.12738", "pdf": "https://arxiv.org/pdf/2506.12738", "abs": "https://arxiv.org/abs/2506.12738", "authors": ["Hang Xu", "Wei Yu", "Jiangtong Tan", "Zhen Zou", "Feng Zhao"], "title": "Adaptive Dropout: Unleashing Dropout across Layers for Generalizable Image Super-Resolution", "categories": ["cs.CV", "cs.AI"], "comment": "8 pages, 8 figures, CVPR2025", "summary": "Blind Super-Resolution (blind SR) aims to enhance the model's generalization\nability with unknown degradation, yet it still encounters severe overfitting\nissues. Some previous methods inspired by dropout, which enhances\ngeneralization by regularizing features, have shown promising results in blind\nSR. Nevertheless, these methods focus solely on regularizing features before\nthe final layer and overlook the need for generalization in features at\nintermediate layers. Without explicit regularization of features at\nintermediate layers, the blind SR network struggles to obtain well-generalized\nfeature representations. However, the key challenge is that directly applying\ndropout to intermediate layers leads to a significant performance drop, which\nwe attribute to the inconsistency in training-testing and across layers it\nintroduced. Therefore, we propose Adaptive Dropout, a new regularization method\nfor blind SR models, which mitigates the inconsistency and facilitates\napplication across intermediate layers of networks. Specifically, for\ntraining-testing inconsistency, we re-design the form of dropout and integrate\nthe features before and after dropout adaptively. For inconsistency in\ngeneralization requirements across different layers, we innovatively design an\nadaptive training strategy to strengthen feature propagation by layer-wise\nannealing. Experimental results show that our method outperforms all past\nregularization methods on both synthetic and real-world benchmark datasets,\nalso highly effective in other image restoration tasks. Code is available at\n\\href{https://github.com/xuhang07/Adpative-Dropout}{https://github.com/xuhang07/Adpative-Dropout}."}
{"id": "2506.12747", "pdf": "https://arxiv.org/pdf/2506.12747", "abs": "https://arxiv.org/abs/2506.12747", "authors": ["Rong Wu", "Ziqi Chen", "Liming Zhong", "Heng Li", "Hai Shu"], "title": "Unleashing Diffusion and State Space Models for Medical Image Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Existing segmentation models trained on a single medical imaging dataset\noften lack robustness when encountering unseen organs or tumors. Developing a\nrobust model capable of identifying rare or novel tumor categories not present\nduring training is crucial for advancing medical imaging applications. We\npropose DSM, a novel framework that leverages diffusion and state space models\nto segment unseen tumor categories beyond the training data. DSM utilizes two\nsets of object queries trained within modified attention decoders to enhance\nclassification accuracy. Initially, the model learns organ queries using an\nobject-aware feature grouping strategy to capture organ-level visual features.\nIt then refines tumor queries by focusing on diffusion-based visual prompts,\nenabling precise segmentation of previously unseen tumors. Furthermore, we\nincorporate diffusion-guided feature fusion to improve semantic segmentation\nperformance. By integrating CLIP text embeddings, DSM captures\ncategory-sensitive classes to improve linguistic transfer knowledge, thereby\nenhancing the model's robustness across diverse scenarios and multi-label\ntasks. Extensive experiments demonstrate the superior performance of DSM in\nvarious tumor segmentation tasks. Code is available at\nhttps://github.com/Rows21/KMax-Mamba."}
{"id": "2506.12766", "pdf": "https://arxiv.org/pdf/2506.12766", "abs": "https://arxiv.org/abs/2506.12766", "authors": ["Ruojing Li", "Wei An", "Xinyi Ying", "Yingqian Wang", "Yimian Dai", "Longguang Wang", "Miao Li", "Yulan Guo", "Li Liu"], "title": "Probing Deep into Temporal Profile Makes the Infrared Small Target Detector Much Better", "categories": ["cs.CV"], "comment": null, "summary": "Infrared small target (IRST) detection is challenging in simultaneously\nachieving precise, universal, robust and efficient performance due to extremely\ndim targets and strong interference. Current learning-based methods attempt to\nleverage ``more\" information from both the spatial and the short-term temporal\ndomains, but suffer from unreliable performance under complex conditions while\nincurring computational redundancy. In this paper, we explore the ``more\nessential\" information from a more crucial domain for the detection. Through\ntheoretical analysis, we reveal that the global temporal saliency and\ncorrelation information in the temporal profile demonstrate significant\nsuperiority in distinguishing target signals from other signals. To investigate\nwhether such superiority is preferentially leveraged by well-trained networks,\nwe built the first prediction attribution tool in this field and verified the\nimportance of the temporal profile information. Inspired by the above\nconclusions, we remodel the IRST detection task as a one-dimensional signal\nanomaly detection task, and propose an efficient deep temporal probe network\n(DeepPro) that only performs calculations in the time dimension for IRST\ndetection. We conducted extensive experiments to fully validate the\neffectiveness of our method. The experimental results are exciting, as our\nDeepPro outperforms existing state-of-the-art IRST detection methods on\nwidely-used benchmarks with extremely high efficiency, and achieves a\nsignificant improvement on dim targets and in complex scenarios. We provide a\nnew modeling domain, a new insight, a new method, and a new performance, which\ncan promote the development of IRST detection. Codes are available at\nhttps://github.com/TinaLRJ/DeepPro."}
{"id": "2506.12775", "pdf": "https://arxiv.org/pdf/2506.12775", "abs": "https://arxiv.org/abs/2506.12775", "authors": ["Han Ke", "Xiao Ke", "Ye Yan", "Rui Liu", "Jinpeng Yang", "Tianwen Zhang", "Xu Zhan", "Xiaowo Xu"], "title": "Scene-aware SAR ship detection guided by unsupervised sea-land segmentation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "DL based Synthetic Aperture Radar (SAR) ship detection has tremendous\nadvantages in numerous areas. However, it still faces some problems, such as\nthe lack of prior knowledge, which seriously affects detection accuracy. In\norder to solve this problem, we propose a scene-aware SAR ship detection method\nbased on unsupervised sea-land segmentation. This method follows a classical\ntwo-stage framework and is enhanced by two models: the unsupervised land and\nsea segmentation module (ULSM) and the land attention suppression module\n(LASM). ULSM and LASM can adaptively guide the network to reduce attention on\nland according to the type of scenes (inshore scene and offshore scene) and add\nprior knowledge (sea land segmentation information) to the network, thereby\nreducing the network's attention to land directly and enhancing offshore\ndetection performance relatively. This increases the accuracy of ship detection\nand enhances the interpretability of the model. Specifically, in consideration\nof the lack of land sea segmentation labels in existing deep learning-based SAR\nship detection datasets, ULSM uses an unsupervised approach to classify the\ninput data scene into inshore and offshore types and performs sea-land\nsegmentation for inshore scenes. LASM uses the sea-land segmentation\ninformation as prior knowledge to reduce the network's attention to land. We\nconducted our experiments using the publicly available SSDD dataset, which\ndemonstrated the effectiveness of our network."}
{"id": "2506.12776", "pdf": "https://arxiv.org/pdf/2506.12776", "abs": "https://arxiv.org/abs/2506.12776", "authors": ["Junbo Niu", "Yuanhong Zheng", "Ziyang Miao", "Hejun Dong", "Chunjiang Ge", "Hao Liang", "Ma Lu", "Bohan Zeng", "Qiahao Zheng", "Conghui He", "Wentao Zhang"], "title": "Native Visual Understanding: Resolving Resolution Dilemmas in Vision-Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Vision-Language Models (VLMs) face significant challenges when dealing with\nthe diverse resolutions and aspect ratios of real-world images, as most\nexisting models rely on fixed, low-resolution inputs. While recent studies have\nexplored integrating native resolution visual encoding to improve model\nperformance, such efforts remain fragmented and lack a systematic framework\nwithin the open-source community. Moreover, existing benchmarks fall short in\nevaluating VLMs under varied visual conditions, often neglecting resolution as\na critical factor. To address the \"Resolution Dilemma\" stemming from both model\ndesign and benchmark limitations, we introduce RC-Bench, a novel benchmark\nspecifically designed to systematically evaluate VLM capabilities under extreme\nvisual conditions, with an emphasis on resolution and aspect ratio variations.\nIn conjunction, we propose NativeRes-LLaVA, an open-source training framework\nthat empowers VLMs to effectively process images at their native resolutions\nand aspect ratios. Based on RC-Bench and NativeRes-LLaVA, we conduct\ncomprehensive experiments on existing visual encoding strategies. The results\nshow that Native Resolution Visual Encoding significantly improves the\nperformance of VLMs on RC-Bench as well as other resolution-centric benchmarks.\nCode is available at https://github.com/Niujunbo2002/NativeRes-LLaVA."}
{"id": "2506.12782", "pdf": "https://arxiv.org/pdf/2506.12782", "abs": "https://arxiv.org/abs/2506.12782", "authors": ["Szabolcs Velkei", "Csaba Goldschmidt", "Károly Vass"], "title": "A large-scale, physically-based synthetic dataset for satellite pose estimation", "categories": ["cs.CV", "68U10 (Primary), 68T45 (Secondary)", "I.4.8; I.2.10"], "comment": "8 pages, 6 figures", "summary": "The Deep Learning Visual Space Simulation System (DLVS3) introduces a novel\nsynthetic dataset generator and a simulation pipeline specifically designed for\ntraining and testing satellite pose estimation solutions. This work introduces\nthe DLVS3-HST-V1 dataset, which focuses on the Hubble Space Telescope (HST) as\na complex, articulated target. The dataset is generated using advanced\nreal-time and offline rendering technologies, integrating high-fidelity 3D\nmodels, dynamic lighting (including secondary sources like Earth reflection),\nand physically accurate material properties. The pipeline supports the creation\nof large-scale, richly annotated image sets with ground-truth 6-DoF pose and\nkeypoint data, semantic segmentation, depth, and normal maps. This enables the\ntraining and benchmarking of deep learning-based pose estimation solutions\nunder realistic, diverse, and challenging visual conditions. The paper details\nthe dataset generation process, the simulation architecture, and the\nintegration with deep learning frameworks, and positions DLVS3 as a significant\nstep toward closing the domain gap for autonomous spacecraft operations in\nproximity and servicing missions."}
{"id": "2506.12786", "pdf": "https://arxiv.org/pdf/2506.12786", "abs": "https://arxiv.org/abs/2506.12786", "authors": ["Chen Zhu", "Kang Liang", "Jianrong Bao", "Zhouxiang Zhao", "Zhaohui Yang", "Zhaoyang Zhang", "Mohammad Shikh-Bahaei"], "title": "Semantic-Aware Visual Information Transmission With Key Information Extraction Over Wireless Networks", "categories": ["cs.CV"], "comment": null, "summary": "The advent of 6G networks demands unprecedented levels of intelligence,\nadaptability, and efficiency to address challenges such as ultra-high-speed\ndata transmission, ultra-low latency, and massive connectivity in dynamic\nenvironments. Traditional wireless image transmission frameworks, reliant on\nstatic configurations and isolated source-channel coding, struggle to balance\ncomputational efficiency, robustness, and quality under fluctuating channel\nconditions. To bridge this gap, this paper proposes an AI-native deep joint\nsource-channel coding (JSCC) framework tailored for resource-constrained 6G\nnetworks. Our approach integrates key information extraction and adaptive\nbackground synthesis to enable intelligent, semantic-aware transmission.\nLeveraging AI-driven tools, Mediapipe for human pose detection and Rembg for\nbackground removal, the model dynamically isolates foreground features and\nmatches backgrounds from a pre-trained library, reducing data payloads while\npreserving visual fidelity. Experimental results demonstrate significant\nimprovements in peak signal-to-noise ratio (PSNR) compared with traditional\nJSCC method, especially under low-SNR conditions. This approach offers a\npractical solution for multimedia services in resource-constrained mobile\ncommunications."}
{"id": "2506.12787", "pdf": "https://arxiv.org/pdf/2506.12787", "abs": "https://arxiv.org/abs/2506.12787", "authors": ["Mufan Liu", "Cixiao Zhang", "Qi Yang", "Yujie Cao", "Yiling Xu", "Yin Xu", "Shu Sun", "Mingzeng Dai", "Yunfeng Guan"], "title": "Rasterizing Wireless Radiance Field via Deformable 2D Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "Modeling the wireless radiance field (WRF) is fundamental to modern\ncommunication systems, enabling key tasks such as localization, sensing, and\nchannel estimation. Traditional approaches, which rely on empirical formulas or\nphysical simulations, often suffer from limited accuracy or require strong\nscene priors. Recent neural radiance field (NeRF-based) methods improve\nreconstruction fidelity through differentiable volumetric rendering, but their\nreliance on computationally expensive multilayer perceptron (MLP) queries\nhinders real-time deployment. To overcome these challenges, we introduce\nGaussian splatting (GS) to the wireless domain, leveraging its efficiency in\nmodeling optical radiance fields to enable compact and accurate WRF\nreconstruction. Specifically, we propose SwiftWRF, a deformable 2D Gaussian\nsplatting framework that synthesizes WRF spectra at arbitrary positions under\nsingle-sided transceiver mobility. SwiftWRF employs CUDA-accelerated\nrasterization to render spectra at over 100000 fps and uses a lightweight MLP\nto model the deformation of 2D Gaussians, effectively capturing\nmobility-induced WRF variations. In addition to novel spectrum synthesis, the\nefficacy of SwiftWRF is further underscored in its applications in\nangle-of-arrival (AoA) and received signal strength indicator (RSSI)\nprediction. Experiments conducted on both real-world and synthetic indoor\nscenes demonstrate that SwiftWRF can reconstruct WRF spectra up to 500x faster\nthan existing state-of-the-art methods, while significantly enhancing its\nsignal quality. Code and datasets will be released."}
{"id": "2506.12793", "pdf": "https://arxiv.org/pdf/2506.12793", "abs": "https://arxiv.org/abs/2506.12793", "authors": ["Wenhao Shen", "Gangjian Zhang", "Jianfeng Zhang", "Yu Feng", "Nanjie Yao", "Xuanmeng Zhang", "Hao Wang"], "title": "SMPL Normal Map Is All You Need for Single-view Textured Human Reconstruction", "categories": ["cs.CV"], "comment": "Accepted to ICME 2025 (Oral)", "summary": "Single-view textured human reconstruction aims to reconstruct a clothed 3D\ndigital human by inputting a monocular 2D image. Existing approaches include\nfeed-forward methods, limited by scarce 3D human data, and diffusion-based\nmethods, prone to erroneous 2D hallucinations. To address these issues, we\npropose a novel SMPL normal map Equipped 3D Human Reconstruction (SEHR)\nframework, integrating a pretrained large 3D reconstruction model with human\ngeometry prior. SEHR performs single-view human reconstruction without using a\npreset diffusion model in one forward propagation. Concretely, SEHR consists of\ntwo key components: SMPL Normal Map Guidance (SNMG) and SMPL Normal Map\nConstraint (SNMC). SNMG incorporates SMPL normal maps into an auxiliary network\nto provide improved body shape guidance. SNMC enhances invisible body parts by\nconstraining the model to predict an extra SMPL normal Gaussians. Extensive\nexperiments on two benchmark datasets demonstrate that SEHR outperforms\nexisting state-of-the-art methods."}
{"id": "2506.12808", "pdf": "https://arxiv.org/pdf/2506.12808", "abs": "https://arxiv.org/abs/2506.12808", "authors": ["Afifa Khaled", "Mohammed Sabir", "Rizwan Qureshi", "Camillo Maria Caruso", "Valerio Guarrasi", "Suncheng Xiang", "S Kevin Zhou"], "title": "Leveraging MIMIC Datasets for Better Digital Health: A Review on Open Problems, Progress Highlights, and Future Promises", "categories": ["cs.CV"], "comment": null, "summary": "The Medical Information Mart for Intensive Care (MIMIC) datasets have become\nthe Kernel of Digital Health Research by providing freely accessible,\ndeidentified records from tens of thousands of critical care admissions,\nenabling a broad spectrum of applications in clinical decision support, outcome\nprediction, and healthcare analytics. Although numerous studies and surveys\nhave explored the predictive power and clinical utility of MIMIC based models,\ncritical challenges in data integration, representation, and interoperability\nremain underexplored. This paper presents a comprehensive survey that focuses\nuniquely on open problems. We identify persistent issues such as data\ngranularity, cardinality limitations, heterogeneous coding schemes, and ethical\nconstraints that hinder the generalizability and real-time implementation of\nmachine learning models. We highlight key progress in dimensionality reduction,\ntemporal modelling, causal inference, and privacy preserving analytics, while\nalso outlining promising directions including hybrid modelling, federated\nlearning, and standardized preprocessing pipelines. By critically examining\nthese structural limitations and their implications, this survey offers\nactionable insights to guide the next generation of MIMIC powered digital\nhealth innovations."}
{"id": "2506.12824", "pdf": "https://arxiv.org/pdf/2506.12824", "abs": "https://arxiv.org/abs/2506.12824", "authors": ["Haoyou Deng", "Zhiqiang Li", "Feng Zhang", "Qingbo Lu", "Zisheng Cao", "Yuanjie Shao", "Shuhang Gu", "Changxin Gao", "Nong Sang"], "title": "Learning Unpaired Image Dehazing with Physics-based Rehazy Generation", "categories": ["cs.CV"], "comment": null, "summary": "Overfitting to synthetic training pairs remains a critical challenge in image\ndehazing, leading to poor generalization capability to real-world scenarios. To\naddress this issue, existing approaches utilize unpaired realistic data for\ntraining, employing CycleGAN or contrastive learning frameworks. Despite their\nprogress, these methods often suffer from training instability, resulting in\nlimited dehazing performance. In this paper, we propose a novel training\nstrategy for unpaired image dehazing, termed Rehazy, to improve both dehazing\nperformance and training stability. This strategy explores the consistency of\nthe underlying clean images across hazy images and utilizes hazy-rehazy pairs\nfor effective learning of real haze characteristics. To favorably construct\nhazy-rehazy pairs, we develop a physics-based rehazy generation pipeline, which\nis theoretically validated to reliably produce high-quality rehazy images.\nAdditionally, leveraging the rehazy strategy, we introduce a dual-branch\nframework for dehazing network training, where a clean branch provides a basic\ndehazing capability in a synthetic manner, and a hazy branch enhances the\ngeneralization ability with hazy-rehazy pairs. Moreover, we design a new\ndehazing network within these branches to improve the efficiency, which\nprogressively restores clean scenes from coarse to fine. Extensive experiments\non four benchmarks demonstrate the superior performance of our approach,\nexceeding the previous state-of-the-art methods by 3.58 dB on the SOTS-Indoor\ndataset and by 1.85 dB on the SOTS-Outdoor dataset in PSNR. Our code will be\npublicly available."}
{"id": "2506.12826", "pdf": "https://arxiv.org/pdf/2506.12826", "abs": "https://arxiv.org/abs/2506.12826", "authors": ["Zhihan Zhang", "Xiang Pan", "Hongchen Wei", "Zhenzhong Chen"], "title": "LOP: Learning Optimal Pruning for Efficient On-Demand MLLMs Scaling", "categories": ["cs.CV"], "comment": null, "summary": "Structural pruning techniques are essential for deploying multimodal large\nlanguage models (MLLMs) across various hardware platforms, from edge devices to\ncloud servers. However, current pruning methods typically determine optimal\nstrategies through iterative search processes, resulting in substantial\ncomputational overhead for on-demand MLLMs adaptation. To address this\nchallenge, we propose LOP, an efficient neural pruning framework that learns\noptimal pruning strategies from the target pruning constraint, eliminating the\nneed for computationally expensive search-based methods. LOP approach trains\nautoregressive neural networks (NNs) to directly predict layer-wise pruning\nstrategies adaptive to the target pruning constraint, eliminating the\ntime-consuming iterative searches. Experimental results across multiple tasks\nshow that LOP outperforms state-of-the-art pruning methods in various metrics\nwhile achieving up to three orders of magnitude speedup."}
{"id": "2506.12830", "pdf": "https://arxiv.org/pdf/2506.12830", "abs": "https://arxiv.org/abs/2506.12830", "authors": ["Chenglin Wang", "Yucheng Zhou", "Qianning Wang", "Zhe Wang", "Kai Zhang"], "title": "ComplexBench-Edit: Benchmarking Complex Instruction-Driven Image Editing via Compositional Dependencies", "categories": ["cs.CV"], "comment": "7 Pages", "summary": "Text-driven image editing has achieved remarkable success in following single\ninstructions. However, real-world scenarios often involve complex, multi-step\ninstructions, particularly ``chain'' instructions where operations are\ninterdependent. Current models struggle with these intricate directives, and\nexisting benchmarks inadequately evaluate such capabilities. Specifically, they\noften overlook multi-instruction and chain-instruction complexities, and common\nconsistency metrics are flawed. To address this, we introduce\nComplexBench-Edit, a novel benchmark designed to systematically assess model\nperformance on complex, multi-instruction, and chain-dependent image editing\ntasks. ComplexBench-Edit also features a new vision consistency evaluation\nmethod that accurately assesses non-modified regions by excluding edited areas.\nFurthermore, we propose a simple yet powerful Chain-of-Thought (CoT)-based\napproach that significantly enhances the ability of existing models to follow\ncomplex instructions. Our extensive experiments demonstrate ComplexBench-Edit's\nefficacy in differentiating model capabilities and highlight the superior\nperformance of our CoT-based method in handling complex edits. The data and\ncode are released at https://github.com/llllly26/ComplexBench-Edit."}
{"id": "2506.12835", "pdf": "https://arxiv.org/pdf/2506.12835", "abs": "https://arxiv.org/abs/2506.12835", "authors": ["Di Kong", "Qianhui Wan"], "title": "DiffS-NOCS: 3D Point Cloud Reconstruction through Coloring Sketches to NOCS Maps Using Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "Reconstructing a 3D point cloud from a given conditional sketch is\nchallenging. Existing methods often work directly in 3D space, but domain\nvariability and difficulty in reconstructing accurate 3D structures from 2D\nsketches remain significant obstacles. Moreover, ideal models should also\naccept prompts for control, in addition with the sparse sketch, posing\nchallenges in multi-modal fusion. We propose DiffS-NOCS (Diffusion-based\nSketch-to-NOCS Map), which leverages ControlNet with a modified multi-view\ndecoder to generate NOCS maps with embedded 3D structure and position\ninformation in 2D space from sketches. The 3D point cloud is reconstructed by\ncombining multiple NOCS maps from different views. To enhance sketch\nunderstanding, we integrate a viewpoint encoder for extracting viewpoint\nfeatures. Additionally, we design a feature-level multi-view aggregation\nnetwork as the denoising module, facilitating cross-view information exchange\nand improving 3D consistency in NOCS map generation. Experiments on ShapeNet\ndemonstrate that DiffS-NOCS achieves controllable and fine-grained point cloud\nreconstruction aligned with sketches."}
{"id": "2506.12836", "pdf": "https://arxiv.org/pdf/2506.12836", "abs": "https://arxiv.org/abs/2506.12836", "authors": ["Mustansar Fiaz", "Mubashir Noman", "Hiyam Debary", "Kamran Ali", "Hisham Cholakkal"], "title": "HyRet-Change: A hybrid retentive network for remote sensing change detection", "categories": ["cs.CV"], "comment": "Accepted at IEEE IGARSS 2025", "summary": "Recently convolution and transformer-based change detection (CD) methods\nprovide promising performance. However, it remains unclear how the local and\nglobal dependencies interact to effectively alleviate the pseudo changes.\nMoreover, directly utilizing standard self-attention presents intrinsic\nlimitations including governing global feature representations limit to capture\nsubtle changes, quadratic complexity, and restricted training parallelism. To\naddress these limitations, we propose a Siamese-based framework, called\nHyRet-Change, which can seamlessly integrate the merits of convolution and\nretention mechanisms at multi-scale features to preserve critical information\nand enhance adaptability in complex scenes. Specifically, we introduce a novel\nfeature difference module to exploit both convolutions and multi-head retention\nmechanisms in a parallel manner to capture complementary information.\nFurthermore, we propose an adaptive local-global interactive context awareness\nmechanism that enables mutual learning and enhances discrimination capability\nthrough information exchange. We perform experiments on three challenging CD\ndatasets and achieve state-of-the-art performance compared to existing methods.\nOur source code is publicly available at\nhttps://github.com/mustansarfiaz/HyRect-Change."}
{"id": "2506.12848", "pdf": "https://arxiv.org/pdf/2506.12848", "abs": "https://arxiv.org/abs/2506.12848", "authors": ["Hao Xu", "Lechao Cheng", "Yaxiong Wang", "Shengeng Tang", "Zhun Zhong"], "title": "Towards Fine-Grained Emotion Understanding via Skeleton-Based Micro-Gesture Recognition", "categories": ["cs.CV"], "comment": "MiGA@IJCAI25: International IJCAI Workshop on 3rd Human Behavior\n  Analysis for Emotion Understanding, August 29, 2025, Guangzhou, China", "summary": "We present our solution to the MiGA Challenge at IJCAI 2025, which aims to\nrecognize micro-gestures (MGs) from skeleton sequences for the purpose of\nhidden emotion understanding. MGs are characterized by their subtlety, short\nduration, and low motion amplitude, making them particularly challenging to\nmodel and classify. We adopt PoseC3D as the baseline framework and introduce\nthree key enhancements: (1) a topology-aware skeleton representation\nspecifically designed for the iMiGUE dataset to better capture fine-grained\nmotion patterns; (2) an improved temporal processing strategy that facilitates\nsmoother and more temporally consistent motion modeling; and (3) the\nincorporation of semantic label embeddings as auxiliary supervision to improve\nthe model generalization. Our method achieves a Top-1 accuracy of 67.01\\% on\nthe iMiGUE test set. As a result of these contributions, our approach ranks\nthird on the official MiGA Challenge leaderboard. The source code is available\nat\n\\href{https://github.com/EGO-False-Sleep/Miga25_track1}{https://github.com/EGO-False-Sleep/Miga25\\_track1}."}
{"id": "2506.12849", "pdf": "https://arxiv.org/pdf/2506.12849", "abs": "https://arxiv.org/abs/2506.12849", "authors": ["Songtao Jiang", "Yuan Wang", "Ruizhe Chen", "Yan Zhang", "Ruilin Luo", "Bohan Lei", "Sibo Song", "Yang Feng", "Jimeng Sun", "Jian Wu", "Zuozhu Liu"], "title": "CAPO: Reinforcing Consistent Reasoning in Medical Decision-Making", "categories": ["cs.CV"], "comment": null, "summary": "In medical visual question answering (Med-VQA), achieving accurate responses\nrelies on three critical steps: precise perception of medical imaging data,\nlogical reasoning grounded in visual input and textual questions, and coherent\nanswer derivation from the reasoning process. Recent advances in general\nvision-language models (VLMs) show that large-scale reinforcement learning (RL)\ncould significantly enhance both reasoning capabilities and overall model\nperformance. However, their application in medical domains is hindered by two\nfundamental challenges: 1) misalignment between perceptual understanding and\nreasoning stages, and 2) inconsistency between reasoning pathways and answer\ngeneration, both compounded by the scarcity of high-quality medical datasets\nfor effective large-scale RL. In this paper, we first introduce Med-Zero-17K, a\ncurated dataset for pure RL-based training, encompassing over 30 medical image\nmodalities and 24 clinical tasks. Moreover, we propose a novel large-scale RL\nframework for Med-VLMs, Consistency-Aware Preference Optimization (CAPO), which\nintegrates rewards to ensure fidelity between perception and reasoning,\nconsistency in reasoning-to-answer derivation, and rule-based accuracy for\nfinal responses. Extensive experiments on both in-domain and out-of-domain\nscenarios demonstrate the superiority of our method over strong VLM baselines,\nshowcasing strong generalization capability to 3D Med-VQA benchmarks and\nR1-like training paradigms."}
{"id": "2506.12853", "pdf": "https://arxiv.org/pdf/2506.12853", "abs": "https://arxiv.org/abs/2506.12853", "authors": ["Jie Liu", "Zheng Hui"], "title": "EraserDiT: Fast Video Inpainting with Diffusion Transformer Model", "categories": ["cs.CV"], "comment": null, "summary": "Video object removal and inpainting are critical tasks in the fields of\ncomputer vision and multimedia processing, aimed at restoring missing or\ncorrupted regions in video sequences. Traditional methods predominantly rely on\nflow-based propagation and spatio-temporal Transformers, but these approaches\nface limitations in effectively leveraging long-term temporal features and\nensuring temporal consistency in the completion results, particularly when\ndealing with large masks. Consequently, performance on extensive masked areas\nremains suboptimal. To address these challenges, this paper introduces a novel\nvideo inpainting approach leveraging the Diffusion Transformer (DiT). DiT\nsynergistically combines the advantages of diffusion models and transformer\narchitectures to maintain long-term temporal consistency while ensuring\nhigh-quality inpainting results. We propose a Circular Position-Shift strategy\nto further enhance long-term temporal consistency during the inference stage.\nAdditionally, the proposed method automatically detects objects within videos,\ninteractively removes specified objects, and generates corresponding prompts.\nIn terms of processing speed, it takes only 180 seconds (testing on one NVIDIA\nA100 GPU) to complete a video with a resolution of $1080 \\times 1920$ with 121\nframes without any acceleration method. Experimental results indicate that the\nproposed method demonstrates superior performance in content fidelity, texture\nrestoration, and temporal consistency. Project page:\nhttps://jieliu95.github.io/EraserDiT_demo."}
{"id": "2506.12871", "pdf": "https://arxiv.org/pdf/2506.12871", "abs": "https://arxiv.org/abs/2506.12871", "authors": ["Rongxuan Peng", "Shunquan Tan", "Xianbo Mo", "Alex C. Kot", "Jiwu Huang"], "title": "Active Adversarial Noise Suppression for Image Forgery Localization", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in deep learning have significantly propelled the development\nof image forgery localization. However, existing models remain highly\nvulnerable to adversarial attacks: imperceptible noise added to forged images\ncan severely mislead these models. In this paper, we address this challenge\nwith an Adversarial Noise Suppression Module (ANSM) that generate a defensive\nperturbation to suppress the attack effect of adversarial noise. We observe\nthat forgery-relevant features extracted from adversarial and original forged\nimages exhibit distinct distributions. To bridge this gap, we introduce\nForgery-relevant Features Alignment (FFA) as a first-stage training strategy,\nwhich reduces distributional discrepancies by minimizing the channel-wise\nKullback-Leibler divergence between these features. To further refine the\ndefensive perturbation, we design a second-stage training strategy, termed\nMask-guided Refinement (MgR), which incorporates a dual-mask constraint. MgR\nensures that the perturbation remains effective for both adversarial and\noriginal forged images, recovering forgery localization accuracy to their\noriginal level. Extensive experiments across various attack algorithms\ndemonstrate that our method significantly restores the forgery localization\nmodel's performance on adversarial images. Notably, when ANSM is applied to\noriginal forged images, the performance remains nearly unaffected. To our best\nknowledge, this is the first report of adversarial defense in image forgery\nlocalization tasks. We have released the source code and anti-forensics\ndataset."}
{"id": "2506.12875", "pdf": "https://arxiv.org/pdf/2506.12875", "abs": "https://arxiv.org/abs/2506.12875", "authors": ["Lu Chen", "Han Yang", "Hu Wang", "Yuxin Cao", "Shaofeng Li", "Yuan Luo"], "title": "Intriguing Frequency Interpretation of Adversarial Robustness for CNNs and ViTs", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Adversarial examples have attracted significant attention over the years, yet\nunderstanding their frequency-based characteristics remains insufficient. In\nthis paper, we investigate the intriguing properties of adversarial examples in\nthe frequency domain for the image classification task, with the following key\nfindings. (1) As the high-frequency components increase, the performance gap\nbetween adversarial and natural examples becomes increasingly pronounced. (2)\nThe model performance against filtered adversarial examples initially increases\nto a peak and declines to its inherent robustness. (3) In Convolutional Neural\nNetworks, mid- and high-frequency components of adversarial examples exhibit\ntheir attack capabilities, while in Transformers, low- and mid-frequency\ncomponents of adversarial examples are particularly effective. These results\nsuggest that different network architectures have different frequency\npreferences and that differences in frequency components between adversarial\nand natural examples may directly influence model robustness. Based on our\nfindings, we further conclude with three useful proposals that serve as a\nvaluable reference to the AI model security community."}
{"id": "2506.12885", "pdf": "https://arxiv.org/pdf/2506.12885", "abs": "https://arxiv.org/abs/2506.12885", "authors": ["Mehmet Ozgur Turkoglu", "Selene Ledain", "Helge Aasen"], "title": "Model-Agnostic, Temperature-Informed Sampling Enhances Cross-Year Crop Mapping with Deep Learning", "categories": ["cs.CV", "eess.IV"], "comment": "under review", "summary": "Conventional benchmarks for crop type classification from optical satellite\ntime series typically assume access to labeled data from the same year and rely\non fixed calendar-day sampling. This limits generalization across seasons,\nwhere crop phenology shifts due to interannual climate variability, and\nprecludes real-time application when current-year labels are unavailable.\nFurthermore, uncertainty quantification is often neglected, making such\napproaches unreliable for crop monitoring applications. Inspired by\necophysiological principles of plant growth, we propose a simple,\nmodel-agnostic sampling strategy that leverages growing degree days (GDD),\nbased on daily average temperature, to replace calendar time with thermal time.\nBy uniformly subsampling time series in this biologically meaningful domain,\nthe method emphasizes phenologically active growth stages while reducing\ntemporal redundancy and noise. We evaluate the method on a multi-year\nSentinel-2 dataset spanning all of Switzerland, training on one growing season\nand testing on other seasons. Compared to state-of-the-art baselines, our\nmethod delivers substantial gains in classification accuracy and, critically,\nproduces more calibrated uncertainty estimates. Notably, our method excels in\nlow-data regimes and enables significantly more accurate early-season\nclassification. With only 10 percent of the training data, our method surpasses\nthe state-of-the-art baseline in both predictive accuracy and uncertainty\nestimation, and by the end of June, it achieves performance similar to a\nbaseline trained on the full season. These results demonstrate that leveraging\ntemperature data not only improves predictive performance across seasons but\nalso enhances the robustness and trustworthiness of crop-type mapping in\nreal-world applications."}
{"id": "2506.12896", "pdf": "https://arxiv.org/pdf/2506.12896", "abs": "https://arxiv.org/abs/2506.12896", "authors": ["Taiga Hayami", "Kakeru Koizumi", "Hiroshi Watanabe"], "title": "Efficient Neural Video Representation via Structure-Preseving Patch Decoding", "categories": ["cs.CV"], "comment": null, "summary": "Implicit Neural Representations (INRs) have attracted significant interest\nfor their ability to model complex signals by mapping spatial and temporal\ncoordinates to signal values. In the context of neural video representation,\nseveral decoding strategies have been explored to balance compactness and\nreconstruction quality, including pixel-wise, frame-wise, and patch-wise\nmethods. Patch-wise decoding aims to combine the flexibility of pixel-based\nmodels with the efficiency of frame-based approaches. However, conventional\nuniform patch division often leads to discontinuities at patch boundaries, as\nindependently reconstructed regions may fail to form a coherent global\nstructure. To address this limitation, we propose a neural video representation\nmethod based on Structure-Preserving Patches (SPPs). Our approach rearranges\neach frame into a set of spatially structured patch frames using a\nPixelUnshuffle-like operation. This rearrangement maintains the spatial\ncoherence of the original frame while enabling patch-level decoding. The\nnetwork learns to predict these rearranged patch frames, which supports a\nglobal-to-local fitting strategy and mitigates degradation caused by\nupsampling. Experiments on standard video datasets show that the proposed\nmethod improves reconstruction quality and compression performance compared to\nexisting INR-based video representation methods."}
{"id": "2506.12945", "pdf": "https://arxiv.org/pdf/2506.12945", "abs": "https://arxiv.org/abs/2506.12945", "authors": ["Hyunjin Kim", "Haebeom Jung", "Jaesik Park"], "title": "Metropolis-Hastings Sampling for 3D Gaussian Reconstruction", "categories": ["cs.CV"], "comment": "Project Page: https://hjhyunjinkim.github.io/MH-3DGS", "summary": "We propose an adaptive sampling framework for 3D Gaussian Splatting (3DGS)\nthat leverages comprehensive multi-view photometric error signals within a\nunified Metropolis-Hastings approach. Traditional 3DGS methods heavily rely on\nheuristic-based density-control mechanisms (e.g., cloning, splitting, and\npruning), which can lead to redundant computations or the premature removal of\nbeneficial Gaussians. Our framework overcomes these limitations by\nreformulating densification and pruning as a probabilistic sampling process,\ndynamically inserting and relocating Gaussians based on aggregated multi-view\nerrors and opacity scores. Guided by Bayesian acceptance tests derived from\nthese error-based importance scores, our method substantially reduces reliance\non heuristics, offers greater flexibility, and adaptively infers Gaussian\ndistributions without requiring predefined scene complexity. Experiments on\nbenchmark datasets, including Mip-NeRF360, Tanks and Temples, and Deep\nBlending, show that our approach reduces the number of Gaussians needed,\nenhancing computational efficiency while matching or modestly surpassing the\nview-synthesis quality of state-of-the-art models."}
{"id": "2506.12980", "pdf": "https://arxiv.org/pdf/2506.12980", "abs": "https://arxiv.org/abs/2506.12980", "authors": ["Nabil Hezil", "Suraj Singh", "Vita Vlasova", "Oleg Rogov", "Ahmed Bouridane", "Rifat Hamoudi"], "title": "Boundary-Aware Vision Transformer for Angiography Vascular Network Segmentation", "categories": ["cs.CV"], "comment": "5 pages, 2 figures, 2 tables; submitted to IPTA-2025", "summary": "Accurate segmentation of vascular structures in coronary angiography remains\na core challenge in medical image analysis due to the complexity of elongated,\nthin, and low-contrast vessels. Classical convolutional neural networks (CNNs)\noften fail to preserve topological continuity, while recent Vision Transformer\n(ViT)-based models, although strong in global context modeling, lack precise\nboundary awareness. In this work, we introduce BAVT, a Boundary-Aware Vision\nTransformer, a ViT-based architecture enhanced with an edge-aware loss that\nexplicitly guides the segmentation toward fine-grained vascular boundaries.\nUnlike hybrid transformer-CNN models, BAVT retains a minimal, scalable\nstructure that is fully compatible with large-scale vision foundation model\n(VFM) pretraining. We validate our approach on the DCA-1 coronary angiography\ndataset, where BAVT achieves superior performance across medical image\nsegmentation metrics outperforming both CNN and hybrid baselines. These results\ndemonstrate the effectiveness of combining plain ViT encoders with\nboundary-aware supervision for clinical-grade vascular segmentation."}
{"id": "2506.12982", "pdf": "https://arxiv.org/pdf/2506.12982", "abs": "https://arxiv.org/abs/2506.12982", "authors": ["Xiaoya Tang", "Bodong Zhang", "Man Minh Ho", "Beatrice S. Knudsen", "Tolga Tasdizen"], "title": "DuoFormer: Leveraging Hierarchical Representations by Local and Global Attention Vision Transformer", "categories": ["cs.CV"], "comment": null, "summary": "Despite the widespread adoption of transformers in medical applications, the\nexploration of multi-scale learning through transformers remains limited, while\nhierarchical representations are considered advantageous for computer-aided\nmedical diagnosis. We propose a novel hierarchical transformer model that\nadeptly integrates the feature extraction capabilities of Convolutional Neural\nNetworks (CNNs) with the advanced representational potential of Vision\nTransformers (ViTs). Addressing the lack of inductive biases and dependence on\nextensive training datasets in ViTs, our model employs a CNN backbone to\ngenerate hierarchical visual representations. These representations are adapted\nfor transformer input through an innovative patch tokenization process,\npreserving the inherited multi-scale inductive biases. We also introduce a\nscale-wise attention mechanism that directly captures intra-scale and\ninter-scale associations. This mechanism complements patch-wise attention by\nenhancing spatial understanding and preserving global perception, which we\nrefer to as local and global attention, respectively. Our model significantly\noutperforms baseline models in terms of classification accuracy, demonstrating\nits efficiency in bridging the gap between Convolutional Neural Networks (CNNs)\nand Vision Transformers (ViTs). The components are designed as plug-and-play\nfor different CNN architectures and can be adapted for multiple applications.\nThe code is available at https://github.com/xiaoyatang/DuoFormer.git."}
{"id": "2506.12992", "pdf": "https://arxiv.org/pdf/2506.12992", "abs": "https://arxiv.org/abs/2506.12992", "authors": ["Xinyi Zhao", "Congjing Zhang", "Pei Guo", "Wei Li", "Lin Chen", "Chaoyue Zhao", "Shuai Huang"], "title": "SmartHome-Bench: A Comprehensive Benchmark for Video Anomaly Detection in Smart Homes Using Multi-Modal Large Language Models", "categories": ["cs.CV"], "comment": "CVPR 2025 Workshop: VAND 3.0 - Visual Anomaly and Novelty Detection", "summary": "Video anomaly detection (VAD) is essential for enhancing safety and security\nby identifying unusual events across different environments. Existing VAD\nbenchmarks, however, are primarily designed for general-purpose scenarios,\nneglecting the specific characteristics of smart home applications. To bridge\nthis gap, we introduce SmartHome-Bench, the first comprehensive benchmark\nspecially designed for evaluating VAD in smart home scenarios, focusing on the\ncapabilities of multi-modal large language models (MLLMs). Our newly proposed\nbenchmark consists of 1,203 videos recorded by smart home cameras, organized\naccording to a novel anomaly taxonomy that includes seven categories, such as\nWildlife, Senior Care, and Baby Monitoring. Each video is meticulously\nannotated with anomaly tags, detailed descriptions, and reasoning. We further\ninvestigate adaptation methods for MLLMs in VAD, assessing state-of-the-art\nclosed-source and open-source models with various prompting techniques. Results\nreveal significant limitations in the current models' ability to detect video\nanomalies accurately. To address these limitations, we introduce the\nTaxonomy-Driven Reflective LLM Chain (TRLC), a new LLM chaining framework that\nachieves a notable 11.62% improvement in detection accuracy. The benchmark\ndataset and code are publicly available at\nhttps://github.com/Xinyi-0724/SmartHome-Bench-LLM."}
{"id": "2506.13027", "pdf": "https://arxiv.org/pdf/2506.13027", "abs": "https://arxiv.org/abs/2506.13027", "authors": ["Sebastian Janampa", "Marios Pattichis"], "title": "DETRPose: Real-time end-to-end transformer model for multi-person pose estimation", "categories": ["cs.CV"], "comment": null, "summary": "Multi-person pose estimation (MPPE) estimates keypoints for all individuals\npresent in an image. MPPE is a fundamental task for several applications in\ncomputer vision and virtual reality. Unfortunately, there are currently no\ntransformer-based models that can perform MPPE in real time. The paper presents\na family of transformer-based models capable of performing multi-person 2D pose\nestimation in real-time. Our approach utilizes a modified decoder architecture\nand keypoint similarity metrics to generate both positive and negative queries,\nthereby enhancing the quality of the selected queries within the architecture.\nCompared to state-of-the-art models, our proposed models train much faster,\nusing 5 to 10 times fewer epochs, with competitive inference times without\nrequiring quantization libraries to speed up the model. Furthermore, our\nproposed models provide competitive results or outperform alternative models,\noften using significantly fewer parameters."}
{"id": "2506.13030", "pdf": "https://arxiv.org/pdf/2506.13030", "abs": "https://arxiv.org/abs/2506.13030", "authors": ["Morris Alper", "David Novotny", "Filippos Kokkinos", "Hadar Averbuch-Elor", "Tom Monnier"], "title": "WildCAT3D: Appearance-Aware Multi-View Diffusion in the Wild", "categories": ["cs.CV"], "comment": "Project page: https://wildcat3d.github.io", "summary": "Despite recent advances in sparse novel view synthesis (NVS) applied to\nobject-centric scenes, scene-level NVS remains a challenge. A central issue is\nthe lack of available clean multi-view training data, beyond manually curated\ndatasets with limited diversity, camera variation, or licensing issues. On the\nother hand, an abundance of diverse and permissively-licensed data exists in\nthe wild, consisting of scenes with varying appearances (illuminations,\ntransient occlusions, etc.) from sources such as tourist photos. To this end,\nwe present WildCAT3D, a framework for generating novel views of scenes learned\nfrom diverse 2D scene image data captured in the wild. We unlock training on\nthese data sources by explicitly modeling global appearance conditions in\nimages, extending the state-of-the-art multi-view diffusion paradigm to learn\nfrom scene views of varying appearances. Our trained model generalizes to new\nscenes at inference time, enabling the generation of multiple consistent novel\nviews. WildCAT3D provides state-of-the-art results on single-view NVS in\nobject- and scene-level settings, while training on strictly less data sources\nthan prior methods. Additionally, it enables novel applications by providing\nglobal appearance control during generation."}
{"id": "2506.13032", "pdf": "https://arxiv.org/pdf/2506.13032", "abs": "https://arxiv.org/abs/2506.13032", "authors": ["Thanh Tran", "Son T. Luu", "Quan Bui", "Shoshin Nomura"], "title": "AS400-DET: Detection using Deep Learning Model for IBM i (AS/400)", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted at the IVSP 2025 conference", "summary": "This paper proposes a method for automatic GUI component detection for the\nIBM i system (formerly and still more commonly known as AS/400). We introduce a\nhuman-annotated dataset consisting of 1,050 system screen images, in which 381\nimages are screenshots of IBM i system screens in Japanese. Each image contains\nmultiple components, including text labels, text boxes, options, tables,\ninstructions, keyboards, and command lines. We then develop a detection system\nbased on state-of-the-art deep learning models and evaluate different\napproaches using our dataset. The experimental results demonstrate the\neffectiveness of our dataset in constructing a system for component detection\nfrom GUI screens. By automatically detecting GUI components from the screen,\nAS400-DET has the potential to perform automated testing on systems that\noperate via GUI screens."}
{"id": "2506.13038", "pdf": "https://arxiv.org/pdf/2506.13038", "abs": "https://arxiv.org/abs/2506.13038", "authors": ["Zijian Zhang", "Xuecheng Wu", "Danlei Huang", "Siyu Yan", "Chong Peng", "Xuezhi Cao"], "title": "HKD4VLM: A Progressive Hybrid Knowledge Distillation Framework for Robust Multimodal Hallucination and Factuality Detection in VLMs", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Driven by the rapid progress in vision-language models (VLMs), the\nresponsible behavior of large-scale multimodal models has become a prominent\nresearch area, particularly focusing on hallucination detection and factuality\nchecking. In this paper, we present the solution for the two tracks of\nResponsible AI challenge. Inspirations from the general domain demonstrate that\na smaller distilled VLM can often outperform a larger VLM that is directly\ntuned on downstream tasks, while achieving higher efficiency. We thus jointly\ntackle two tasks from the perspective of knowledge distillation and propose a\nprogressive hybrid knowledge distillation framework termed HKD4VLM.\nSpecifically, the overall framework can be decomposed into Pyramid-like\nProgressive Online Distillation and Ternary-Coupled Refinement Distillation,\nhierarchically moving from coarse-grained knowledge alignment to fine-grained\nrefinement. Besides, we further introduce the mapping shift-enhanced inference\nand diverse augmentation strategies to enhance model performance and\nrobustness. Extensive experimental results demonstrate the effectiveness of our\nHKD4VLM. Ablation studies provide insights into the critical design choices\ndriving performance gains."}
{"id": "2506.13039", "pdf": "https://arxiv.org/pdf/2506.13039", "abs": "https://arxiv.org/abs/2506.13039", "authors": ["Amran Bhuiyan", "Mizanur Rahman", "Md Tahmid Rahman Laskar", "Aijun An", "Jimmy Xiangji Huang"], "title": "Evolution of ReID: From Early Methods to LLM Integration", "categories": ["cs.CV"], "comment": null, "summary": "Person re-identification (ReID) has evolved from handcrafted feature-based\nmethods to deep learning approaches and, more recently, to models incorporating\nlarge language models (LLMs). Early methods struggled with variations in\nlighting, pose, and viewpoint, but deep learning addressed these issues by\nlearning robust visual features. Building on this, LLMs now enable ReID systems\nto integrate semantic and contextual information through natural language. This\nsurvey traces that full evolution and offers one of the first comprehensive\nreviews of ReID approaches that leverage LLMs, where textual descriptions are\nused as privileged information to improve visual matching. A key contribution\nis the use of dynamic, identity-specific prompts generated by GPT-4o, which\nenhance the alignment between images and text in vision-language ReID systems.\nExperimental results show that these descriptions improve accuracy, especially\nin complex or ambiguous cases. To support further research, we release a large\nset of GPT-4o-generated descriptions for standard ReID datasets. By bridging\ncomputer vision and natural language processing, this survey offers a unified\nperspective on the field's development and outlines key future directions such\nas better prompt design, cross-modal transfer learning, and real-world\nadaptability."}
{"id": "2506.13040", "pdf": "https://arxiv.org/pdf/2506.13040", "abs": "https://arxiv.org/abs/2506.13040", "authors": ["Hanz Cuevas-Velasquez", "Anastasios Yiannakidis", "Soyong Shin", "Giorgio Becherini", "Markus Höschle", "Joachim Tesch", "Taylor Obersat", "Tsvetelina Alexiadis", "Michael Black"], "title": "MAMMA: Markerless & Automatic Multi-Person Motion Action Capture", "categories": ["cs.CV"], "comment": null, "summary": "We present MAMMA, a markerless motion-capture pipeline that accurately\nrecovers SMPL-X parameters from multi-view video of two-person interaction\nsequences. Traditional motion-capture systems rely on physical markers.\nAlthough they offer high accuracy, their requirements of specialized hardware,\nmanual marker placement, and extensive post-processing make them costly and\ntime-consuming. Recent learning-based methods attempt to overcome these\nlimitations, but most are designed for single-person capture, rely on sparse\nkeypoints, or struggle with occlusions and physical interactions. In this work,\nwe introduce a method that predicts dense 2D surface landmarks conditioned on\nsegmentation masks, enabling person-specific correspondence estimation even\nunder heavy occlusion. We employ a novel architecture that exploits learnable\nqueries for each landmark. We demonstrate that our approach can handle complex\nperson--person interaction and offers greater accuracy than existing methods.\nTo train our network, we construct a large, synthetic multi-view dataset\ncombining human motions from diverse sources, including extreme poses, hand\nmotions, and close interactions. Our dataset yields high-variability synthetic\nsequences with rich body contact and occlusion, and includes SMPL-X\nground-truth annotations with dense 2D landmarks. The result is a system\ncapable of capturing human motion without the need for markers. Our approach\noffers competitive reconstruction quality compared to commercial marker-based\nmotion-capture solutions, without the extensive manual cleanup. Finally, we\naddress the absence of common benchmarks for dense-landmark prediction and\nmarkerless motion capture by introducing two evaluation settings built from\nreal multi-view sequences. We will release our dataset, benchmark, method,\ntraining code, and pre-trained model weights for research purposes."}
{"id": "2506.13043", "pdf": "https://arxiv.org/pdf/2506.13043", "abs": "https://arxiv.org/abs/2506.13043", "authors": ["Christian Hilaire", "Sima Didari"], "title": "ViewPCL: a point cloud based active learning method for multi-view segmentation", "categories": ["cs.CV"], "comment": null, "summary": "We propose a novel active learning framework for multi-view semantic\nsegmentation. This framework relies on a new score that measures the\ndiscrepancy between point cloud distributions generated from the extra\ngeometrical information derived from the model's prediction across different\nviews. Our approach results in a data efficient and explainable active learning\nmethod. The source code is available at https://github.com/chilai235/viewpclAL."}
{"id": "2506.13049", "pdf": "https://arxiv.org/pdf/2506.13049", "abs": "https://arxiv.org/abs/2506.13049", "authors": ["Adhrith Vutukuri", "Akash Awasthi", "David Yang", "Carol C. Wu", "Hien Van Nguyen"], "title": "Beyond the First Read: AI-Assisted Perceptual Error Detection in Chest Radiography Accounting for Interobserver Variability", "categories": ["cs.CV", "cs.AI"], "comment": "25 pages", "summary": "Chest radiography is widely used in diagnostic imaging. However, perceptual\nerrors -- especially overlooked but visible abnormalities -- remain common and\nclinically significant. Current workflows and AI systems provide limited\nsupport for detecting such errors after interpretation and often lack\nmeaningful human--AI collaboration. We introduce RADAR (Radiologist--AI\nDiagnostic Assistance and Review), a post-interpretation companion system.\nRADAR ingests finalized radiologist annotations and CXR images, then performs\nregional-level analysis to detect and refer potentially missed abnormal\nregions. The system supports a \"second-look\" workflow and offers suggested\nregions of interest (ROIs) rather than fixed labels to accommodate\ninter-observer variation. We evaluated RADAR on a simulated perceptual-error\ndataset derived from de-identified CXR cases, using F1 score and Intersection\nover Union (IoU) as primary metrics. RADAR achieved a recall of 0.78, precision\nof 0.44, and an F1 score of 0.56 in detecting missed abnormalities in the\nsimulated perceptual-error dataset. Although precision is moderate, this\nreduces over-reliance on AI by encouraging radiologist oversight in human--AI\ncollaboration. The median IoU was 0.78, with more than 90% of referrals\nexceeding 0.5 IoU, indicating accurate regional localization. RADAR effectively\ncomplements radiologist judgment, providing valuable post-read support for\nperceptual-error detection in CXR interpretation. Its flexible ROI suggestions\nand non-intrusive integration position it as a promising tool in real-world\nradiology workflows. To facilitate reproducibility and further evaluation, we\nrelease a fully open-source web implementation alongside a simulated error\ndataset. All code, data, demonstration videos, and the application are publicly\navailable at https://github.com/avutukuri01/RADAR."}
{"id": "2506.13051", "pdf": "https://arxiv.org/pdf/2506.13051", "abs": "https://arxiv.org/abs/2506.13051", "authors": ["Can Polat", "Hasan Kurban", "Erchin Serpedin", "Mustafa Kurban"], "title": "Stress-Testing Multimodal Foundation Models for Crystallographic Reasoning", "categories": ["cs.CV", "cond-mat.mtrl-sci", "cs.CL", "cs.LG"], "comment": null, "summary": "Evaluating foundation models for crystallographic reasoning requires\nbenchmarks that isolate generalization behavior while enforcing physical\nconstraints. This work introduces a multiscale multicrystal dataset with two\nphysically grounded evaluation protocols to stress-test multimodal generative\nmodels. The Spatial-Exclusion benchmark withholds all supercells of a given\nradius from a diverse dataset, enabling controlled assessments of spatial\ninterpolation and extrapolation. The Compositional-Exclusion benchmark omits\nall samples of a specific chemical composition, probing generalization across\nstoichiometries. Nine vision--language foundation models are prompted with\ncrystallographic images and textual context to generate structural annotations.\nResponses are evaluated via (i) relative errors in lattice parameters and\ndensity, (ii) a physics-consistency index penalizing volumetric violations, and\n(iii) a hallucination score capturing geometric outliers and invalid\nspace-group predictions. These benchmarks establish a reproducible, physically\ninformed framework for assessing generalization, consistency, and reliability\nin large-scale multimodal models. Dataset and code are available at\nhttps://github.com/KurbanIntelligenceLab/StressTestingMMFMinCR."}
{"id": "2506.13058", "pdf": "https://arxiv.org/pdf/2506.13058", "abs": "https://arxiv.org/abs/2506.13058", "authors": ["Hu Yu", "Hao Luo", "Fan Wang", "Feng Zhao"], "title": "DualFast: Dual-Speedup Framework for Fast Sampling of Diffusion Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Diffusion probabilistic models (DPMs) have achieved impressive success in\nvisual generation. While, they suffer from slow inference speed due to\niterative sampling. Employing fewer sampling steps is an intuitive solution,\nbut this will also introduces discretization error. Existing fast samplers make\ninspiring efforts to reduce discretization error through the adoption of\nhigh-order solvers, potentially reaching a plateau in terms of optimization.\nThis raises the question: can the sampling process be accelerated further? In\nthis paper, we re-examine the nature of sampling errors, discerning that they\ncomprise two distinct elements: the widely recognized discretization error and\nthe less explored approximation error. Our research elucidates the dynamics\nbetween these errors and the step by implementing a dual-error disentanglement\nstrategy. Building on these foundations, we introduce an unified and\ntraining-free acceleration framework, DualFast, designed to enhance the speed\nof DPM sampling by concurrently accounting for both error types, thereby\nminimizing the total sampling error. DualFast is seamlessly compatible with\nexisting samplers and significantly boost their sampling quality and speed,\nparticularly in extremely few sampling steps. We substantiate the effectiveness\nof our framework through comprehensive experiments, spanning both unconditional\nand conditional sampling domains, across both pixel-space and latent-space\nDPMs."}
{"id": "2506.13063", "pdf": "https://arxiv.org/pdf/2506.13063", "abs": "https://arxiv.org/abs/2506.13063", "authors": ["George Shaikovski", "Eugene Vorontsov", "Adam Casson", "Julian Viret", "Eric Zimmermann", "Neil Tenenholtz", "Yi Kan Wang", "Jan H. Bernhard", "Ran A. Godrich", "Juan A. Retamero", "Razik Yousfi", "Nicolo Fusi", "Thomas J. Fuchs", "Kristen Severson", "Siqi Liu"], "title": "PRISM2: Unlocking Multi-Modal General Pathology AI with Clinical Dialogue", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Recent pathology foundation models can provide rich tile-level\nrepresentations but fall short of delivering general-purpose clinical utility\nwithout further extensive model development. These models lack whole-slide\nimage (WSI) understanding and are not trained with large-scale diagnostic data,\nlimiting their performance on diverse downstream tasks. We introduce PRISM2, a\nmulti-modal slide-level foundation model trained via clinical dialogue to\nenable scalable, generalizable pathology AI. PRISM2 is trained on nearly\n700,000 specimens (2.3 million WSIs) paired with real-world clinical diagnostic\nreports in a two-stage process. In Stage 1, a vision-language model is trained\nusing contrastive and captioning objectives to align whole slide embeddings\nwith textual clinical diagnosis. In Stage 2, the language model is unfrozen to\nenable diagnostic conversation and extract more clinically meaningful\nrepresentations from hidden states. PRISM2 achieves strong performance on\ndiagnostic and biomarker prediction tasks, outperforming prior slide-level\nmodels including PRISM and TITAN. It also introduces a zero-shot yes/no\nclassification approach that surpasses CLIP-style methods without prompt tuning\nor class enumeration. By aligning visual features with clinical reasoning,\nPRISM2 improves generalization on both data-rich and low-sample tasks, offering\na scalable path forward for building general pathology AI agents capable of\nassisting diagnostic and prognostic decisions."}
{"id": "2506.13067", "pdf": "https://arxiv.org/pdf/2506.13067", "abs": "https://arxiv.org/abs/2506.13067", "authors": ["Xuhui Zhu", "Jing Xu", "Bingjie Wang", "Huikang Dai", "Hao Lu"], "title": "Video Individual Counting With Implicit One-to-Many Matching", "categories": ["cs.CV"], "comment": null, "summary": "Video Individual Counting (VIC) is a recently introduced task that aims to\nestimate pedestrian flux from a video. It extends conventional Video Crowd\nCounting (VCC) beyond the per-frame pedestrian count. In contrast to VCC that\nonly learns to count repeated pedestrian patterns across frames, the key\nproblem of VIC is how to identify co-existent pedestrians between frames, which\nturns out to be a correspondence problem. Existing VIC approaches, however,\nmainly follow a one-to-one (O2O) matching strategy where the same pedestrian\nmust be exactly matched between frames, leading to sensitivity to appearance\nvariations or missing detections. In this work, we show that the O2O matching\ncould be relaxed to a one-to-many (O2M) matching problem, which better fits the\nproblem nature of VIC and can leverage the social grouping behavior of walking\npedestrians. We therefore introduce OMAN, a simple but effective VIC model with\nimplicit One-to-Many mAtchiNg, featuring an implicit context generator and a\none-to-many pairwise matcher. Experiments on the SenseCrowd and CroHD\nbenchmarks show that OMAN achieves the state-of-the-art performance. Code is\navailable at \\href{https://github.com/tiny-smart/OMAN}{OMAN}."}
{"id": "2506.13073", "pdf": "https://arxiv.org/pdf/2506.13073", "abs": "https://arxiv.org/abs/2506.13073", "authors": ["Bingxi Liu", "Pengju Zhang", "Li He", "Hao Chen", "Shiyi Guo", "Yihong Wu", "Jinqiang Cui", "Hong Zhang"], "title": "SuperPlace: The Renaissance of Classical Feature Aggregation for Visual Place Recognition in the Era of Foundation Models", "categories": ["cs.CV"], "comment": "11 pages", "summary": "Recent visual place recognition (VPR) approaches have leveraged foundation\nmodels (FM) and introduced novel aggregation techniques. However, these methods\nhave failed to fully exploit key concepts of FM, such as the effective\nutilization of extensive training sets, and they have overlooked the potential\nof classical aggregation methods, such as GeM and NetVLAD. Building on these\ninsights, we revive classical feature aggregation methods and develop more\nfundamental VPR models, collectively termed SuperPlace. First, we introduce a\nsupervised label alignment method that enables training across various VPR\ndatasets within a unified framework. Second, we propose G$^2$M, a compact\nfeature aggregation method utilizing two GeMs, where one GeM learns the\nprincipal components of feature maps along the channel dimension and calibrates\nthe output of the other. Third, we propose the secondary fine-tuning (FT$^2$)\nstrategy for NetVLAD-Linear (NVL). NetVLAD first learns feature vectors in a\nhigh-dimensional space and then compresses them into a lower-dimensional space\nvia a single linear layer. Extensive experiments highlight our contributions\nand demonstrate the superiority of SuperPlace. Specifically, G$^2$M achieves\npromising results with only one-tenth of the feature dimensions compared to\nrecent methods. Moreover, NVL-FT$^2$ ranks first on the MSLS leaderboard."}
{"id": "2506.13089", "pdf": "https://arxiv.org/pdf/2506.13089", "abs": "https://arxiv.org/abs/2506.13089", "authors": ["Shahram Najam Syed", "Ishir Roongta", "Kavin Ravie", "Gangadhar Nageswar"], "title": "SuperPoint-SLAM3: Augmenting ORB-SLAM3 with Deep Features, Adaptive NMS, and Learning-Based Loop Closure", "categories": ["cs.CV", "cs.RO", "I.2.10; I.4.8; I.2.9"], "comment": "10 pages, 6 figures, code at\n  https://github.com/shahram95/SuperPointSLAM3", "summary": "Visual simultaneous localization and mapping (SLAM) must remain accurate\nunder extreme viewpoint, scale and illumination variations. The widely adopted\nORB-SLAM3 falters in these regimes because it relies on hand-crafted ORB\nkeypoints. We introduce SuperPoint-SLAM3, a drop-in upgrade that (i) replaces\nORB with the self-supervised SuperPoint detector--descriptor, (ii) enforces\nspatially uniform keypoints via adaptive non-maximal suppression (ANMS), and\n(iii) integrates a lightweight NetVLAD place-recognition head for\nlearning-based loop closure.\n  On the KITTI Odometry benchmark SuperPoint-SLAM3 reduces mean translational\nerror from 4.15% to 0.34% and mean rotational error from 0.0027 deg/m to 0.0010\ndeg/m. On the EuRoC MAV dataset it roughly halves both errors across every\nsequence (e.g., V2\\_03: 1.58% -> 0.79%). These gains confirm that fusing modern\ndeep features with a learned loop-closure module markedly improves ORB-SLAM3\naccuracy while preserving its real-time operation.\n  Implementation, pretrained weights and reproducibility scripts are available\nat https://github.com/shahram95/SuperPointSLAM3."}
{"id": "2506.13095", "pdf": "https://arxiv.org/pdf/2506.13095", "abs": "https://arxiv.org/abs/2506.13095", "authors": ["Yu Wang", "Shiwei Chen"], "title": "Learning Event Completeness for Weakly Supervised Video Anomaly Detection", "categories": ["cs.CV"], "comment": "Accepted by ICML", "summary": "Weakly supervised video anomaly detection (WS-VAD) is tasked with pinpointing\ntemporal intervals containing anomalous events within untrimmed videos,\nutilizing only video-level annotations. However, a significant challenge arises\ndue to the absence of dense frame-level annotations, often leading to\nincomplete localization in existing WS-VAD methods. To address this issue, we\npresent a novel LEC-VAD, Learning Event Completeness for Weakly Supervised\nVideo Anomaly Detection, which features a dual structure designed to encode\nboth category-aware and category-agnostic semantics between vision and\nlanguage. Within LEC-VAD, we devise semantic regularities that leverage an\nanomaly-aware Gaussian mixture to learn precise event boundaries, thereby\nyielding more complete event instances. Besides, we develop a novel memory\nbank-based prototype learning mechanism to enrich concise text descriptions\nassociated with anomaly-event categories. This innovation bolsters the text's\nexpressiveness, which is crucial for advancing WS-VAD. Our LEC-VAD demonstrates\nremarkable advancements over the current state-of-the-art methods on two\nbenchmark datasets XD-Violence and UCF-Crime."}
{"id": "2506.13097", "pdf": "https://arxiv.org/pdf/2506.13097", "abs": "https://arxiv.org/abs/2506.13097", "authors": ["Ziqing Zhou", "Binbin Gao", "Yuri Pan", "Lidong Wang", "Wenbing Zhu", "Yong Liu", "Jun Liu", "MIngmin Chi", "Dong Wu", "Bo Peng", "Chengjie Wang"], "title": "Pro-AD: Learning Comprehensive Prototypes with Prototype-based Constraint for Multi-class Unsupervised Anomaly Detection", "categories": ["cs.CV"], "comment": null, "summary": "Prototype-based reconstruction methods for unsupervised anomaly detection\nutilize a limited set of learnable prototypes which only aggregates\ninsufficient normal information, resulting in undesirable reconstruction.\nHowever, increasing the number of prototypes may lead to anomalies being well\nreconstructed through the attention mechanism, which we refer to as the \"Soft\nIdentity Mapping\" problem. In this paper, we propose Pro-AD to address these\nissues and fully utilize the prototypes to boost the performance of anomaly\ndetection. Specifically, we first introduce an expanded set of learnable\nprototypes to provide sufficient capacity for semantic information. Then we\nemploy a Dynamic Bidirectional Decoder which integrates the process of the\nnormal information aggregation and the target feature reconstruction via\nprototypes, with the aim of allowing the prototypes to aggregate more\ncomprehensive normal semantic information from different levels of the image\nfeatures and the target feature reconstruction to not only utilize its\ncontextual information but also dynamically leverage the learned comprehensive\nprototypes. Additionally, to prevent the anomalies from being well\nreconstructed using sufficient semantic information through the attention\nmechanism, Pro-AD introduces a Prototype-based Constraint that applied within\nthe target feature reconstruction process of the decoder, which further\nimproves the performance of our approach. Extensive experiments on multiple\nchallenging benchmarks demonstrate that our Pro-AD achieve state-of-the-art\nperformance, highlighting its superior robustness and practical effectiveness\nfor Multi-class Unsupervised Anomaly Detection task."}
{"id": "2506.13110", "pdf": "https://arxiv.org/pdf/2506.13110", "abs": "https://arxiv.org/abs/2506.13110", "authors": ["Jinguang Tong", "Xuesong li", "Fahira Afzal Maken", "Sundaram Muthu", "Lars Petersson", "Chuong Nguyen", "Hongdong Li"], "title": "GS-2DGS: Geometrically Supervised 2DGS for Reflective Object Reconstruction", "categories": ["cs.CV"], "comment": "Accepted by CVPR2025", "summary": "3D modeling of highly reflective objects remains challenging due to strong\nview-dependent appearances. While previous SDF-based methods can recover\nhigh-quality meshes, they are often time-consuming and tend to produce\nover-smoothed surfaces. In contrast, 3D Gaussian Splatting (3DGS) offers the\nadvantage of high speed and detailed real-time rendering, but extracting\nsurfaces from the Gaussians can be noisy due to the lack of geometric\nconstraints. To bridge the gap between these approaches, we propose a novel\nreconstruction method called GS-2DGS for reflective objects based on 2D\nGaussian Splatting (2DGS). Our approach combines the rapid rendering\ncapabilities of Gaussian Splatting with additional geometric information from\nfoundation models. Experimental results on synthetic and real datasets\ndemonstrate that our method significantly outperforms Gaussian-based techniques\nin terms of reconstruction and relighting and achieves performance comparable\nto SDF-based methods while being an order of magnitude faster. Code is\navailable at https://github.com/hirotong/GS2DGS"}
{"id": "2506.13130", "pdf": "https://arxiv.org/pdf/2506.13130", "abs": "https://arxiv.org/abs/2506.13130", "authors": ["Yuiga Wada", "Kazuki Matsuda", "Komei Sugiura", "Graham Neubig"], "title": "ZINA: Multimodal Fine-grained Hallucination Detection and Editing", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) often generate hallucinations, where\nthe output deviates from the visual content. Given that these hallucinations\ncan take diverse forms, detecting hallucinations at a fine-grained level is\nessential for comprehensive evaluation and analysis. To this end, we propose a\nnovel task of multimodal fine-grained hallucination detection and editing for\nMLLMs. Moreover, we propose ZINA, a novel method that identifies hallucinated\nspans at a fine-grained level, classifies their error types into six\ncategories, and suggests appropriate refinements. To train and evaluate models\nfor this task, we constructed VisionHall, a dataset comprising 6.9k outputs\nfrom twelve MLLMs manually annotated by 211 annotators, and 20k synthetic\nsamples generated using a graph-based method that captures dependencies among\nerror types. We demonstrated that ZINA outperformed existing methods, including\nGPT-4o and LLama-3.2, in both detection and editing tasks."}
{"id": "2506.13133", "pdf": "https://arxiv.org/pdf/2506.13133", "abs": "https://arxiv.org/abs/2506.13133", "authors": ["Bingxi Liu", "Hao Chen", "Shiyi Guo", "Yihong Wu", "Jinqiang Cui", "Hong Zhang"], "title": "EmbodiedPlace: Learning Mixture-of-Features with Embodied Constraints for Visual Place Recognition", "categories": ["cs.CV"], "comment": "17 Pages", "summary": "Visual Place Recognition (VPR) is a scene-oriented image retrieval problem in\ncomputer vision in which re-ranking based on local features is commonly\nemployed to improve performance. In robotics, VPR is also referred to as Loop\nClosure Detection, which emphasizes spatial-temporal verification within a\nsequence. However, designing local features specifically for VPR is\nimpractical, and relying on motion sequences imposes limitations. Inspired by\nthese observations, we propose a novel, simple re-ranking method that refines\nglobal features through a Mixture-of-Features (MoF) approach under embodied\nconstraints. First, we analyze the practical feasibility of embodied\nconstraints in VPR and categorize them according to existing datasets, which\ninclude GPS tags, sequential timestamps, local feature matching, and\nself-similarity matrices. We then propose a learning-based MoF\nweight-computation approach, utilizing a multi-metric loss function.\nExperiments demonstrate that our method improves the state-of-the-art (SOTA)\nperformance on public datasets with minimal additional computational overhead.\nFor instance, with only 25 KB of additional parameters and a processing time of\n10 microseconds per frame, our method achieves a 0.9\\% improvement over a\nDINOv2-based baseline performance on the Pitts-30k test set."}
{"id": "2506.13138", "pdf": "https://arxiv.org/pdf/2506.13138", "abs": "https://arxiv.org/abs/2506.13138", "authors": ["Jiamin Wang", "Yichen Yao", "Xiang Feng", "Hang Wu", "Yaming Wang", "Qingqiu Huang", "Yuexin Ma", "Xinge Zhu"], "title": "STAGE: A Stream-Centric Generative World Model for Long-Horizon Driving-Scene Simulation", "categories": ["cs.CV"], "comment": null, "summary": "The generation of temporally consistent, high-fidelity driving videos over\nextended horizons presents a fundamental challenge in autonomous driving world\nmodeling. Existing approaches often suffer from error accumulation and feature\nmisalignment due to inadequate decoupling of spatio-temporal dynamics and\nlimited cross-frame feature propagation mechanisms. To address these\nlimitations, we present STAGE (Streaming Temporal Attention Generative Engine),\na novel auto-regressive framework that pioneers hierarchical feature\ncoordination and multi-phase optimization for sustainable video synthesis. To\nachieve high-quality long-horizon driving video generation, we introduce\nHierarchical Temporal Feature Transfer (HTFT) and a novel multi-stage training\nstrategy. HTFT enhances temporal consistency between video frames throughout\nthe video generation process by modeling the temporal and denoising process\nseparately and transferring denoising features between frames. The multi-stage\ntraining strategy is to divide the training into three stages, through model\ndecoupling and auto-regressive inference process simulation, thereby\naccelerating model convergence and reducing error accumulation. Experiments on\nthe Nuscenes dataset show that STAGE has significantly surpassed existing\nmethods in the long-horizon driving video generation task. In addition, we also\nexplored STAGE's ability to generate unlimited-length driving videos. We\ngenerated 600 frames of high-quality driving videos on the Nuscenes dataset,\nwhich far exceeds the maximum length achievable by existing methods."}
{"id": "2506.13156", "pdf": "https://arxiv.org/pdf/2506.13156", "abs": "https://arxiv.org/abs/2506.13156", "authors": ["Jiashu He", "Jiayi He", "Shengeng Tang", "Huixia Ben", "Lechao Cheng", "Richang Hong"], "title": "StgcDiff: Spatial-Temporal Graph Condition Diffusion for Sign Language Transition Generation", "categories": ["cs.CV"], "comment": null, "summary": "Sign language transition generation seeks to convert discrete sign language\nsegments into continuous sign videos by synthesizing smooth transitions.\nHowever,most existing methods merely concatenate isolated signs, resulting in\npoor visual coherence and semantic accuracy in the generated videos. Unlike\ntextual languages,sign language is inherently rich in spatial-temporal cues,\nmaking it more complex to model. To address this,we propose StgcDiff, a\ngraph-based conditional diffusion framework that generates smooth transitions\nbetween discrete signs by capturing the unique spatial-temporal dependencies of\nsign language. Specifically, we first train an encoder-decoder architecture to\nlearn a structure-aware representation of spatial-temporal skeleton sequences.\nNext, we optimize a diffusion denoiser conditioned on the representations\nlearned by the pre-trained encoder, which is tasked with predicting transition\nframes from noise. Additionally, we design the Sign-GCN module as the key\ncomponent in our framework, which effectively models the spatial-temporal\nfeatures. Extensive experiments conducted on the PHOENIX14T, USTC-CSL100,and\nUSTC-SLR500 datasets demonstrate the superior performance of our method."}
{"id": "2506.13166", "pdf": "https://arxiv.org/pdf/2506.13166", "abs": "https://arxiv.org/abs/2506.13166", "authors": ["Ruiguang Pei", "Weiqing Sun", "Zhihui Fu", "Jun Wang"], "title": "GreedyPrune: Retenting Critical Visual Token Set for Large Vision Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Although Large Vision Language Models (LVLMs) have demonstrated remarkable\nperformance in image understanding tasks, their computational efficiency\nremains a significant challenge, particularly on resource-constrained devices\ndue to the high cost of processing large numbers of visual tokens. Recently,\ntraining-free visual token pruning methods have gained popularity as a low-cost\nsolution to this issue. However, existing approaches suffer from two key\nlimitations: semantic saliency-based strategies primarily focus on high\ncross-attention visual tokens, often neglecting visual diversity, whereas\nvisual diversity-based methods risk inadvertently discarding semantically\nimportant tokens, especially under high compression ratios. In this paper, we\nintroduce GreedyPrune, a training-free plug-and-play visual token pruning\nalgorithm designed to jointly optimize semantic saliency and visual diversity.\nWe formalize the token pruning process as a combinatorial optimization problem\nand demonstrate that greedy algorithms effectively balance computational\nefficiency with model accuracy. Extensive experiments validate the\neffectiveness of our approach, showing that GreedyPrune achieves\nstate-of-the-art accuracy across various multimodal tasks and models while\nsignificantly reducing end-to-end inference latency."}
{"id": "2506.13183", "pdf": "https://arxiv.org/pdf/2506.13183", "abs": "https://arxiv.org/abs/2506.13183", "authors": ["Bingxi Liu", "An Liu", "Hao Chen", "Jinqiang Cui", "Yiqun Wang", "Hong Zhang"], "title": "MT-PCR: A Hybrid Mamba-Transformer with Spatial Serialization for Hierarchical Point Cloud Registration", "categories": ["cs.CV"], "comment": "11 Pages", "summary": "Point cloud registration (PCR) is a fundamental task in 3D computer vision\nand robotics. Most existing learning-based PCR methods rely on Transformers,\nwhich suffer from quadratic computational complexity. This limitation restricts\nthe resolution of point clouds that can be processed, inevitably leading to\ninformation loss. In contrast, Mamba-a recently proposed model based on state\nspace models (SSMs)-achieves linear computational complexity while maintaining\nstrong long-range contextual modeling capabilities. However, directly applying\nMamba to PCR tasks yields suboptimal performance due to the unordered and\nirregular nature of point cloud data. To address this challenge, we propose\nMT-PCR, the first point cloud registration framework that integrates both Mamba\nand Transformer modules. Specifically, we serialize point cloud features using\nZ-order space-filling curves to enforce spatial locality, enabling Mamba to\nbetter model the geometric structure of the input. Additionally, we remove the\norder indicator module commonly used in Mamba-based sequence modeling, leads to\nimproved performance in our setting. The serialized features are then processed\nby an optimized Mamba encoder, followed by a Transformer refinement stage.\nExtensive experiments on multiple benchmarks demonstrate that MT-PCR\noutperforms Transformer-based and concurrent state-of-the-art methods in both\naccuracy and efficiency, significantly reducing while GPU memory usage and\nFLOPs."}
{"id": "2506.13201", "pdf": "https://arxiv.org/pdf/2506.13201", "abs": "https://arxiv.org/abs/2506.13201", "authors": ["Wenfeng Jia", "Bin Liang", "Yuxi Liu", "Muhammad Arif Khan", "Lihong Zheng"], "title": "A Comprehensive Survey on Deep Learning Solutions for 3D Flood Mapping", "categories": ["cs.CV"], "comment": null, "summary": "Flooding remains a major global challenge, worsened by climate change and\nurbanization, demanding advanced solutions for effective disaster management.\nWhile traditional 2D flood mapping techniques provide limited insights, 3D\nflood mapping, powered by deep learning (DL), offers enhanced capabilities by\nintegrating flood extent and depth. This paper presents a comprehensive survey\nof deep learning-based 3D flood mapping, emphasizing its advancements over 2D\nmaps by integrating flood extent and depth for effective disaster management\nand urban planning. The survey categorizes deep learning techniques into task\ndecomposition and end-to-end approaches, applicable to both static and dynamic\nflood features. We compare key DL architectures, highlighting their respective\nroles in enhancing prediction accuracy and computational efficiency.\nAdditionally, this work explores diverse data sources such as digital elevation\nmodels, satellite imagery, rainfall, and simulated data, outlining their roles\nin 3D flood mapping. The applications reviewed range from real-time flood\nprediction to long-term urban planning and risk assessment. However,\nsignificant challenges persist, including data scarcity, model\ninterpretability, and integration with traditional hydrodynamic models. This\nsurvey concludes by suggesting future directions to address these limitations,\nfocusing on enhanced datasets, improved models, and policy implications for\nflood management. This survey aims to guide researchers and practitioners in\nleveraging DL techniques for more robust and reliable 3D flood mapping,\nfostering improved flood management strategies."}
{"id": "2506.13215", "pdf": "https://arxiv.org/pdf/2506.13215", "abs": "https://arxiv.org/abs/2506.13215", "authors": ["Zhenlong Yuan", "Dapeng Zhang", "Zehao Li", "Chengxuan Qian", "Jianing Chen", "Yinda Chen", "Kehua Chen", "Tianlu Mao", "Zhaoxin Li", "Hao Jiang", "Zhaoqi Wang"], "title": "DVP-MVS++: Synergize Depth-Normal-Edge and Harmonized Visibility Prior for Multi-View Stereo", "categories": ["cs.CV"], "comment": null, "summary": "Recently, patch deformation-based methods have demonstrated significant\neffectiveness in multi-view stereo due to their incorporation of deformable and\nexpandable perception for reconstructing textureless areas. However, these\nmethods generally focus on identifying reliable pixel correlations to mitigate\nmatching ambiguity of patch deformation, while neglecting the deformation\ninstability caused by edge-skipping and visibility occlusions, which may cause\npotential estimation deviations. To address these issues, we propose DVP-MVS++,\nan innovative approach that synergizes both depth-normal-edge aligned and\nharmonized cross-view priors for robust and visibility-aware patch deformation.\nSpecifically, to avoid edge-skipping, we first apply DepthPro, Metric3Dv2 and\nRoberts operator to generate coarse depth maps, normal maps and edge maps,\nrespectively. These maps are then aligned via an erosion-dilation strategy to\nproduce fine-grained homogeneous boundaries for facilitating robust patch\ndeformation. Moreover, we reformulate view selection weights as visibility\nmaps, and then implement both an enhanced cross-view depth reprojection and an\narea-maximization strategy to help reliably restore visible areas and\neffectively balance deformed patch, thus acquiring harmonized cross-view priors\nfor visibility-aware patch deformation. Additionally, we obtain geometry\nconsistency by adopting both aggregated normals via view selection and\nprojection depth differences via epipolar lines, and then employ SHIQ for\nhighlight correction to enable geometry consistency with highlight-aware\nperception, thus improving reconstruction quality during propagation and\nrefinement stage. Evaluation results on ETH3D, Tanks & Temples and Strecha\ndatasets exhibit the state-of-the-art performance and robust generalization\ncapability of our proposed method."}
{"id": "2506.13224", "pdf": "https://arxiv.org/pdf/2506.13224", "abs": "https://arxiv.org/abs/2506.13224", "authors": ["Jinfeng Xu", "Xianzhi Li", "Yuan Tang", "Xu Han", "Qiao Yu", "Yixue Hao", "Long Hu", "Min Chen"], "title": "SASep: Saliency-Aware Structured Separation of Geometry and Feature for Open Set Learning on Point Clouds", "categories": ["cs.CV"], "comment": "10 pages, conference", "summary": "Recent advancements in deep learning have greatly enhanced 3D object\nrecognition, but most models are limited to closed-set scenarios, unable to\nhandle unknown samples in real-world applications. Open-set recognition (OSR)\naddresses this limitation by enabling models to both classify known classes and\nidentify novel classes. However, current OSR methods rely on global features to\ndifferentiate known and unknown classes, treating the entire object uniformly\nand overlooking the varying semantic importance of its different parts. To\naddress this gap, we propose Salience-Aware Structured Separation (SASep),\nwhich includes (i) a tunable semantic decomposition (TSD) module to\nsemantically decompose objects into important and unimportant parts, (ii) a\ngeometric synthesis strategy (GSS) to generate pseudo-unknown objects by\ncombining these unimportant parts, and (iii) a synth-aided margin separation\n(SMS) module to enhance feature-level separation by expanding the feature\ndistributions between classes. Together, these components improve both\ngeometric and feature representations, enhancing the model's ability to\neffectively distinguish known and unknown classes. Experimental results show\nthat SASep achieves superior performance in 3D OSR, outperforming existing\nstate-of-the-art methods."}
{"id": "2506.13233", "pdf": "https://arxiv.org/pdf/2506.13233", "abs": "https://arxiv.org/abs/2506.13233", "authors": ["Jiashu Dai", "Along Wang", "Binfan Ni", "Tao Cao"], "title": "High-Quality Facial Albedo Generation for 3D Face Reconstruction from a Single Image using a Coarse-to-Fine Approach", "categories": ["cs.CV"], "comment": null, "summary": "Facial texture generation is crucial for high-fidelity 3D face reconstruction\nfrom a single image. However, existing methods struggle to generate UV albedo\nmaps with high-frequency details. To address this challenge, we propose a novel\nend-to-end coarse-to-fine approach for UV albedo map generation. Our method\nfirst utilizes a UV Albedo Parametric Model (UVAPM), driven by low-dimensional\ncoefficients, to generate coarse albedo maps with skin tones and low-frequency\ntexture details. To capture high-frequency details, we train a detail generator\nusing a decoupled albedo map dataset, producing high-resolution albedo maps.\nExtensive experiments demonstrate that our method can generate high-fidelity\ntextures from a single image, outperforming existing methods in terms of\ntexture quality and realism. The code and pre-trained model are publicly\navailable at https://github.com/MVIC-DAI/UVAPM, facilitating reproducibility\nand further research."}
{"id": "2506.13260", "pdf": "https://arxiv.org/pdf/2506.13260", "abs": "https://arxiv.org/abs/2506.13260", "authors": ["Yining Shi", "Kun Jiang", "Qiang Meng", "Ke Wang", "Jiabao Wang", "Wenchao Sun", "Tuopu Wen", "Mengmeng Yang", "Diange Yang"], "title": "COME: Adding Scene-Centric Forecasting Control to Occupancy World Model", "categories": ["cs.CV"], "comment": null, "summary": "World models are critical for autonomous driving to simulate environmental\ndynamics and generate synthetic data. Existing methods struggle to disentangle\nego-vehicle motion (perspective shifts) from scene evolvement (agent\ninteractions), leading to suboptimal predictions. Instead, we propose to\nseparate environmental changes from ego-motion by leveraging the scene-centric\ncoordinate systems. In this paper, we introduce COME: a framework that\nintegrates scene-centric forecasting Control into the Occupancy world ModEl.\nSpecifically, COME first generates ego-irrelevant, spatially consistent future\nfeatures through a scene-centric prediction branch, which are then converted\ninto scene condition using a tailored ControlNet. These condition features are\nsubsequently injected into the occupancy world model, enabling more accurate\nand controllable future occupancy predictions. Experimental results on the\nnuScenes-Occ3D dataset show that COME achieves consistent and significant\nimprovements over state-of-the-art (SOTA) methods across diverse\nconfigurations, including different input sources (ground-truth, camera-based,\nfusion-based occupancy) and prediction horizons (3s and 8s). For example, under\nthe same settings, COME achieves 26.3% better mIoU metric than DOME and 23.7%\nbetter mIoU metric than UniScene. These results highlight the efficacy of\ndisentangled representation learning in enhancing spatio-temporal prediction\nfidelity for world models. Code and videos will be available at\nhttps://github.com/synsin0/COME."}
{"id": "2506.13265", "pdf": "https://arxiv.org/pdf/2506.13265", "abs": "https://arxiv.org/abs/2506.13265", "authors": ["Rohit Mohan", "Julia Hindel", "Florian Drews", "Claudius Gläser", "Daniele Cattaneo", "Abhinav Valada"], "title": "Open-Set LiDAR Panoptic Segmentation Guided by Uncertainty-Aware Learning", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": null, "summary": "Autonomous vehicles that navigate in open-world environments may encounter\npreviously unseen object classes. However, most existing LiDAR panoptic\nsegmentation models rely on closed-set assumptions, failing to detect unknown\nobject instances. In this work, we propose ULOPS, an uncertainty-guided\nopen-set panoptic segmentation framework that leverages Dirichlet-based\nevidential learning to model predictive uncertainty. Our architecture\nincorporates separate decoders for semantic segmentation with uncertainty\nestimation, embedding with prototype association, and instance center\nprediction. During inference, we leverage uncertainty estimates to identify and\nsegment unknown instances. To strengthen the model's ability to differentiate\nbetween known and unknown objects, we introduce three uncertainty-driven loss\nfunctions. Uniform Evidence Loss to encourage high uncertainty in unknown\nregions. Adaptive Uncertainty Separation Loss ensures a consistent difference\nin uncertainty estimates between known and unknown objects at a global scale.\nContrastive Uncertainty Loss refines this separation at the fine-grained level.\nTo evaluate open-set performance, we extend benchmark settings on KITTI-360 and\nintroduce a new open-set evaluation for nuScenes. Extensive experiments\ndemonstrate that ULOPS consistently outperforms existing open-set LiDAR\npanoptic segmentation methods."}
{"id": "2506.13282", "pdf": "https://arxiv.org/pdf/2506.13282", "abs": "https://arxiv.org/abs/2506.13282", "authors": ["Daichi Tanaka", "Takumi Karasawa", "Shu Takenouchi", "Rei Kawakami"], "title": "Anomaly Object Segmentation with Vision-Language Models for Steel Scrap Recycling", "categories": ["cs.CV"], "comment": null, "summary": "Recycling steel scrap can reduce carbon dioxide (CO2) emissions from the\nsteel industry. However, a significant challenge in steel scrap recycling is\nthe inclusion of impurities other than steel. To address this issue, we propose\nvision-language-model-based anomaly detection where a model is finetuned in a\nsupervised manner, enabling it to handle niche objects effectively. This model\nenables automated detection of anomalies at a fine-grained level within steel\nscrap. Specifically, we finetune the image encoder, equipped with multi-scale\nmechanism and text prompts aligned with both normal and anomaly images. The\nfinetuning process trains these modules using a multiclass classification as\nthe supervision."}
{"id": "2506.13292", "pdf": "https://arxiv.org/pdf/2506.13292", "abs": "https://arxiv.org/abs/2506.13292", "authors": ["Roman Flepp", "Leon Nissen", "Bastian Sigrist", "Arend Nieuwland", "Nicola Cavalcanti", "Philipp Fürnstahl", "Thomas Dreher", "Lilian Calvet"], "title": "Automatic Multi-View X-Ray/CT Registration Using Bone Substructure Contours", "categories": ["cs.CV", "cs.AI"], "comment": "This paper was accepted to IPCAI 2025", "summary": "Purpose: Accurate intraoperative X-ray/CT registration is essential for\nsurgical navigation in orthopedic procedures. However, existing methods\nstruggle with consistently achieving sub-millimeter accuracy, robustness under\nbroad initial pose estimates or need manual key-point annotations. This work\naims to address these challenges by proposing a novel multi-view X-ray/CT\nregistration method for intraoperative bone registration. Methods: The proposed\nregistration method consists of a multi-view, contour-based iterative closest\npoint (ICP) optimization. Unlike previous methods, which attempt to match bone\ncontours across the entire silhouette in both imaging modalities, we focus on\nmatching specific subcategories of contours corresponding to bone\nsubstructures. This leads to reduced ambiguity in the ICP matches, resulting in\na more robust and accurate registration solution. This approach requires only\ntwo X-ray images and operates fully automatically. Additionally, we contribute\na dataset of 5 cadaveric specimens, including real X-ray images, X-ray image\nposes and the corresponding CT scans. Results: The proposed registration method\nis evaluated on real X-ray images using mean reprojection error (mRPD). The\nmethod consistently achieves sub-millimeter accuracy with a mRPD 0.67mm\ncompared to 5.35mm by a commercial solution requiring manual intervention.\nFurthermore, the method offers improved practical applicability, being fully\nautomatic. Conclusion: Our method offers a practical, accurate, and efficient\nsolution for multi-view X-ray/CT registration in orthopedic surgeries, which\ncan be easily combined with tracking systems. By improving registration\naccuracy and minimizing manual intervention, it enhances intraoperative\nnavigation, contributing to more accurate and effective surgical outcomes in\ncomputer-assisted surgery (CAS)."}
{"id": "2506.13298", "pdf": "https://arxiv.org/pdf/2506.13298", "abs": "https://arxiv.org/abs/2506.13298", "authors": ["Jeonghoon Park", "Juyoung Lee", "Chaeyeon Chung", "Jaeseong Lee", "Jaegul Choo", "Jindong Gu"], "title": "Fair Generation without Unfair Distortions: Debiasing Text-to-Image Generation with Entanglement-Free Attention", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advancements in diffusion-based text-to-image (T2I) models have\nenabled the generation of high-quality and photorealistic images from text\ndescriptions. However, they often exhibit societal biases related to gender,\nrace, and socioeconomic status, thereby reinforcing harmful stereotypes and\nshaping public perception in unintended ways. While existing bias mitigation\nmethods demonstrate effectiveness, they often encounter attribute entanglement,\nwhere adjustments to attributes relevant to the bias (i.e., target attributes)\nunintentionally alter attributes unassociated with the bias (i.e., non-target\nattributes), causing undesirable distribution shifts. To address this\nchallenge, we introduce Entanglement-Free Attention (EFA), a method that\naccurately incorporates target attributes (e.g., White, Black, Asian, and\nIndian) while preserving non-target attributes (e.g., background details)\nduring bias mitigation. At inference time, EFA randomly samples a target\nattribute with equal probability and adjusts the cross-attention in selected\nlayers to incorporate the sampled attribute, achieving a fair distribution of\ntarget attributes. Extensive experiments demonstrate that EFA outperforms\nexisting methods in mitigating bias while preserving non-target attributes,\nthereby maintaining the output distribution and generation capability of the\noriginal model."}
{"id": "2506.13301", "pdf": "https://arxiv.org/pdf/2506.13301", "abs": "https://arxiv.org/abs/2506.13301", "authors": ["Biao Yang", "Muqi Huang", "Yuhui Zhang", "Yun Xiong", "Kun Zhou", "Xi Chen", "Shiyang Zhou", "Huishuai Bao", "Chuan Li", "Feng Shi", "Hualei Liu"], "title": "AttentionDrag: Exploiting Latent Correlation Knowledge in Pre-trained Diffusion Models for Image Editing", "categories": ["cs.CV"], "comment": null, "summary": "Traditional point-based image editing methods rely on iterative latent\noptimization or geometric transformations, which are either inefficient in\ntheir processing or fail to capture the semantic relationships within the\nimage. These methods often overlook the powerful yet underutilized image\nediting capabilities inherent in pre-trained diffusion models. In this work, we\npropose a novel one-step point-based image editing method, named AttentionDrag,\nwhich leverages the inherent latent knowledge and feature correlations within\npre-trained diffusion models for image editing tasks. This framework enables\nsemantic consistency and high-quality manipulation without the need for\nextensive re-optimization or retraining. Specifically, we reutilize the latent\ncorrelations knowledge learned by the self-attention mechanism in the U-Net\nmodule during the DDIM inversion process to automatically identify and adjust\nrelevant image regions, ensuring semantic validity and consistency.\nAdditionally, AttentionDrag adaptively generates masks to guide the editing\nprocess, enabling precise and context-aware modifications with friendly\ninteraction. Our results demonstrate a performance that surpasses most\nstate-of-the-art methods with significantly faster speeds, showing a more\nefficient and semantically coherent solution for point-based image editing\ntasks."}
{"id": "2506.13307", "pdf": "https://arxiv.org/pdf/2506.13307", "abs": "https://arxiv.org/abs/2506.13307", "authors": ["Solène Debuysère", "Nicolas Trouvé", "Nathan Letheule", "Olivier Lévêque", "Elise Colin"], "title": "Quantitative Comparison of Fine-Tuning Techniques for Pretrained Latent Diffusion Models in the Generation of Unseen SAR Image Concepts", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "This work investigates the adaptation of large pre-trained latent diffusion\nmodels to a radically new imaging domain: Synthetic Aperture Radar (SAR). While\nthese generative models, originally trained on natural images, demonstrate\nimpressive capabilities in text-to-image synthesis, they are not natively\nadapted to represent SAR data, which involves different physics, statistical\ndistributions, and visual characteristics. Using a sizeable SAR dataset (on the\norder of 100,000 to 1 million images), we address the fundamental question of\nfine-tuning such models for this unseen modality. We explore and compare\nmultiple fine-tuning strategies, including full model fine-tuning and\nparameter-efficient approaches like Low-Rank Adaptation (LoRA), focusing\nseparately on the UNet diffusion backbone and the text encoder components. To\nevaluate generative quality, we combine several metrics: statistical distance\nfrom real SAR distributions, textural similarity via GLCM descriptors, and\nsemantic alignment assessed with a CLIP model fine-tuned on SAR data. Our\nresults show that a hybrid tuning strategy yields the best performance: full\nfine-tuning of the UNet is better at capturing low-level SAR-specific patterns,\nwhile LoRA-based partial tuning of the text encoder, combined with embedding\nlearning of the <SAR> token, suffices to preserve prompt alignment. This work\nprovides a methodical strategy for adapting foundation models to unconventional\nimaging modalities beyond natural image domains."}
{"id": "2506.13320", "pdf": "https://arxiv.org/pdf/2506.13320", "abs": "https://arxiv.org/abs/2506.13320", "authors": ["Wenlong Wan", "Weiying Zheng", "Tianyi Xiang", "Guiqing Li", "Shengfeng He"], "title": "Action Dubber: Timing Audible Actions via Inflectional Flow", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted by ICML2025", "summary": "We introduce the task of Audible Action Temporal Localization, which aims to\nidentify the spatio-temporal coordinates of audible movements. Unlike\nconventional tasks such as action recognition and temporal action localization,\nwhich broadly analyze video content, our task focuses on the distinct kinematic\ndynamics of audible actions. It is based on the premise that key actions are\ndriven by inflectional movements; for example, collisions that produce sound\noften involve abrupt changes in motion. To capture this, we propose\n$TA^{2}Net$, a novel architecture that estimates inflectional flow using the\nsecond derivative of motion to determine collision timings without relying on\naudio input. $TA^{2}Net$ also integrates a self-supervised spatial localization\nstrategy during training, combining contrastive learning with spatial analysis.\nThis dual design improves temporal localization accuracy and simultaneously\nidentifies sound sources within video frames. To support this task, we\nintroduce a new benchmark dataset, $Audible623$, derived from Kinetics and\nUCF101 by removing non-essential vocalization subsets. Extensive experiments\nconfirm the effectiveness of our approach on $Audible623$ and show strong\ngeneralizability to other domains, such as repetitive counting and sound source\nlocalization. Code and dataset are available at\nhttps://github.com/WenlongWan/Audible623."}
{"id": "2506.13322", "pdf": "https://arxiv.org/pdf/2506.13322", "abs": "https://arxiv.org/abs/2506.13322", "authors": ["Weijia Feng", "Yichen Zhu", "Ruojia Zhang", "Chenyang Wang", "Fei Ma", "Xiaobao Wang", "Xiaobai Li"], "title": "Active Multimodal Distillation for Few-shot Action Recognition", "categories": ["cs.CV", "cs.AI"], "comment": "IJCAI 2025, the 34th International Joint Conference on Artificial\n  Intelligence", "summary": "Owing to its rapid progress and broad application prospects, few-shot action\nrecognition has attracted considerable interest. However, current methods are\npredominantly based on limited single-modal data, which does not fully exploit\nthe potential of multimodal information. This paper presents a novel framework\nthat actively identifies reliable modalities for each sample using\ntask-specific contextual cues, thus significantly improving recognition\nperformance. Our framework integrates an Active Sample Inference (ASI) module,\nwhich utilizes active inference to predict reliable modalities based on\nposterior distributions and subsequently organizes them accordingly. Unlike\nreinforcement learning, active inference replaces rewards with evidence-based\npreferences, making more stable predictions. Additionally, we introduce an\nactive mutual distillation module that enhances the representation learning of\nless reliable modalities by transferring knowledge from more reliable ones.\nAdaptive multimodal inference is employed during the meta-test to assign higher\nweights to reliable modalities. Extensive experiments across multiple\nbenchmarks demonstrate that our method significantly outperforms existing\napproaches."}
{"id": "2506.13326", "pdf": "https://arxiv.org/pdf/2506.13326", "abs": "https://arxiv.org/abs/2506.13326", "authors": ["Bo Pan", "Yixiao Fu", "Ke Wang", "Junyu Lu", "Lunke Pan", "Ziyang Qian", "Yuhan Chen", "Guoliang Wang", "Yitao Zhou", "Li Zheng", "Yinghao Tang", "Zhen Wen", "Yuchen Wu", "Junhua Lu", "Biao Zhu", "Minfeng Zhu", "Bo Zhang", "Wei Chen"], "title": "VIS-Shepherd: Constructing Critic for LLM-based Data Visualization Generation", "categories": ["cs.CV", "cs.HC"], "comment": null, "summary": "Data visualization generation using Large Language Models (LLMs) has shown\npromising results but often produces suboptimal visualizations that require\nhuman intervention for improvement. In this work, we introduce VIS-Shepherd, a\nspecialized Multimodal Large Language Model (MLLM)-based critic to evaluate and\nprovide feedback for LLM-generated data visualizations. At the core of our\napproach is a framework to construct a high-quality visualization critique\ndataset, where we collect human-created visualization instances, synthesize\ncorresponding LLM-generated instances, and construct high-quality critiques. We\nconduct both model-based automatic evaluation and human preference studies to\nevaluate the effectiveness of our approach. Our experiments show that even\nsmall (7B parameters) open-source MLLM models achieve substantial performance\ngains by leveraging our high-quality visualization critique dataset, reaching\nlevels comparable to much larger open-source or even proprietary models. Our\nwork demonstrates significant potential for MLLM-based automated visualization\ncritique and indicates promising directions for enhancing LLM-based data\nvisualization generation. Our project page:\nhttps://github.com/bopan3/VIS-Shepherd."}
{"id": "2506.13327", "pdf": "https://arxiv.org/pdf/2506.13327", "abs": "https://arxiv.org/abs/2506.13327", "authors": ["Andrea Bergamaschi", "Abhinav Verma", "Avik Bhattacharya", "Fabio Dell'Acqua"], "title": "Joint Analysis of Optical and SAR Vegetation Indices for Vineyard Monitoring: Assessing Biomass Dynamics and Phenological Stages over Po Valley, Italy", "categories": ["cs.CV"], "comment": null, "summary": "Multi-polarized Synthetic Aperture Radar (SAR) technology has gained\nincreasing attention in agriculture, offering unique capabilities for\nmonitoring vegetation dynamics thanks to its all-weather, day-and-night\noperation and high revisit frequency. This study presents, for the first time,\na comprehensive analysis combining dual-polarimetric radar vegetation index\n(DpRVI) with optical indices to characterize vineyard crops. Vineyards exhibit\ndistinct non-isotropic scattering behavior due to their pronounced row\norientation, making them particularly challenging and interesting targets for\nremote sensing. The research further investigates the relationship between\nDpRVI and optical vegetation indices, demonstrating the complementary nature of\ntheir information. We demonstrate that DpRVI and optical indices provide\ncomplementary information, with low correlation suggesting that they capture\ndistinct vineyard features. Key findings reveal a parabolic trend in DpRVI over\nthe growing season, potentially linked to biomass dynamics estimated via the\nWinkler Index. Unlike optical indices reflecting vegetation greenness, DpRVI\nappears more directly related to biomass growth, aligning with specific\nphenological phases. Preliminary results also highlight the potential of DpRVI\nfor distinguishing vineyards from other crops. This research aligns with the\nobjectives of the PNRR-NODES project, which promotes nature-based solutions\n(NbS) for sustainable vineyard management. The application of DpRVI for\nmonitoring vineyards is part of integrating remote sensing techniques into the\nbroader field of strategies for climate-related change adaptation and risk\nreduction, emphasizing the role of innovative SAR-based monitoring in\nsustainable agriculture."}
{"id": "2506.13335", "pdf": "https://arxiv.org/pdf/2506.13335", "abs": "https://arxiv.org/abs/2506.13335", "authors": ["Gabriel A. Carneiro", "Thierry J. Aubry", "António Cunha", "Petia Radeva", "Joaquim Sousa"], "title": "Advancing Image-Based Grapevine Variety Classification with a New Benchmark and Evaluation of Masked Autoencoders", "categories": ["cs.CV"], "comment": null, "summary": "Grapevine varieties are essential for the economies of many wine-producing\ncountries, influencing the production of wine, juice, and the consumption of\nfruits and leaves. Traditional identification methods, such as ampelography and\nmolecular analysis, have limitations: ampelography depends on expert knowledge\nand is inherently subjective, while molecular methods are costly and\ntime-intensive. To address these limitations, recent studies have applied deep\nlearning (DL) models to classify grapevine varieties using image data. However,\ndue to the small dataset sizes, these methods often depend on transfer learning\nfrom datasets from other domains, e.g., ImageNet1K (IN1K), which can lead to\nperformance degradation due to domain shift and supervision collapse. In this\ncontext, self-supervised learning (SSL) methods can be a good tool to avoid\nthis performance degradation, since they can learn directly from data, without\nexternal labels. This study presents an evaluation of Masked Autoencoders\n(MAEs) for identifying grapevine varieties based on field-acquired images. The\nmain contributions of this study include two benchmarks comprising 43 grapevine\nvarieties collected across different seasons, an analysis of MAE's application\nin the agricultural context, and a performance comparison of trained models\nacross seasons. Our results show that a ViT-B/16 model pre-trained with MAE and\nthe unlabeled dataset achieved an F1 score of 0.7956, outperforming all other\nmodels. Additionally, we observed that pre-trained models benefit from long\npre-training, perform well under low-data training regime, and that simple data\naugmentation methods are more effective than complex ones. The study also found\nthat the mask ratio in MAE impacts performance only marginally."}
{"id": "2506.13355", "pdf": "https://arxiv.org/pdf/2506.13355", "abs": "https://arxiv.org/abs/2506.13355", "authors": ["Yan Chen", "Hanlin Shang", "Ce Liu", "Yuxuan Chen", "Hui Li", "Weihao Yuan", "Hao Zhu", "Zilong Dong", "Siyu Zhu"], "title": "DicFace: Dirichlet-Constrained Variational Codebook Learning for Temporally Coherent Video Face Restoration", "categories": ["cs.CV"], "comment": null, "summary": "Video face restoration faces a critical challenge in maintaining temporal\nconsistency while recovering fine facial details from degraded inputs. This\npaper presents a novel approach that extends Vector-Quantized Variational\nAutoencoders (VQ-VAEs), pretrained on static high-quality portraits, into a\nvideo restoration framework through variational latent space modeling. Our key\ninnovation lies in reformulating discrete codebook representations as\nDirichlet-distributed continuous variables, enabling probabilistic transitions\nbetween facial features across frames. A spatio-temporal Transformer\narchitecture jointly models inter-frame dependencies and predicts latent\ndistributions, while a Laplacian-constrained reconstruction loss combined with\nperceptual (LPIPS) regularization enhances both pixel accuracy and visual\nquality. Comprehensive evaluations on blind face restoration, video inpainting,\nand facial colorization tasks demonstrate state-of-the-art performance. This\nwork establishes an effective paradigm for adapting intensive image priors,\npretrained on high-quality images, to video restoration while addressing the\ncritical challenge of flicker artifacts. The source code has been open-sourced\nand is available at https://github.com/fudan-generative-vision/DicFace."}
{"id": "2506.13387", "pdf": "https://arxiv.org/pdf/2506.13387", "abs": "https://arxiv.org/abs/2506.13387", "authors": ["Beilei Cui", "Yiming Huang", "Long Bai", "Hongliang Ren"], "title": "TR2M: Transferring Monocular Relative Depth to Metric Depth with Language Descriptions and Scale-Oriented Contrast", "categories": ["cs.CV"], "comment": null, "summary": "This work presents a generalizable framework to transfer relative depth to\nmetric depth. Current monocular depth estimation methods are mainly divided\ninto metric depth estimation (MMDE) and relative depth estimation (MRDE). MMDEs\nestimate depth in metric scale but are often limited to a specific domain.\nMRDEs generalize well across different domains, but with uncertain scales which\nhinders downstream applications. To this end, we aim to build up a framework to\nsolve scale uncertainty and transfer relative depth to metric depth. Previous\nmethods used language as input and estimated two factors for conducting\nrescaling. Our approach, TR2M, utilizes both text description and image as\ninputs and estimates two rescale maps to transfer relative depth to metric\ndepth at pixel level. Features from two modalities are fused with a\ncross-modality attention module to better capture scale information. A strategy\nis designed to construct and filter confident pseudo metric depth for more\ncomprehensive supervision. We also develop scale-oriented contrastive learning\nto utilize depth distribution as guidance to enforce the model learning about\nintrinsic knowledge aligning with the scale distribution. TR2M only exploits a\nsmall number of trainable parameters to train on datasets in various domains\nand experiments not only demonstrate TR2M's great performance in seen datasets\nbut also reveal superior zero-shot capabilities on five unseen datasets. We\nshow the huge potential in pixel-wise transferring relative depth to metric\ndepth with language assistance. (Code is available at:\nhttps://github.com/BeileiCui/TR2M)"}
{"id": "2506.13391", "pdf": "https://arxiv.org/pdf/2506.13391", "abs": "https://arxiv.org/abs/2506.13391", "authors": ["Zhen Wang", "Hongyi Liu", "Zhihui Wei"], "title": "Zero-Shot Solving of Imaging Inverse Problems via Noise-Refined Likelihood Guided Diffusion Models", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Diffusion models have achieved remarkable success in imaging inverse problems\nowing to their powerful generative capabilities. However, existing approaches\ntypically rely on models trained for specific degradation types, limiting their\ngeneralizability to various degradation scenarios. To address this limitation,\nwe propose a zero-shot framework capable of handling various imaging inverse\nproblems without model retraining. We introduce a likelihood-guided noise\nrefinement mechanism that derives a closed-form approximation of the likelihood\nscore, simplifying score estimation and avoiding expensive gradient\ncomputations. This estimated score is subsequently utilized to refine the\nmodel-predicted noise, thereby better aligning the restoration process with the\ngenerative framework of diffusion models. In addition, we integrate the\nDenoising Diffusion Implicit Models (DDIM) sampling strategy to further improve\ninference efficiency. The proposed mechanism can be applied to both\noptimization-based and sampling-based schemes, providing an effective and\nflexible zero-shot solution for imaging inverse problems. Extensive experiments\ndemonstrate that our method achieves superior performance across multiple\ninverse problems, particularly in compressive sensing, delivering high-quality\nreconstructions even at an extremely low sampling rate (5%)."}
{"id": "2506.13430", "pdf": "https://arxiv.org/pdf/2506.13430", "abs": "https://arxiv.org/abs/2506.13430", "authors": ["Tristan Kenneweg", "Philip Kenneweg", "Barbara Hammer"], "title": "Uncertainty-Aware Remaining Lifespan Prediction from Images", "categories": ["cs.CV"], "comment": "Submitted to IMPACT 2025", "summary": "Predicting mortality-related outcomes from images offers the prospect of\naccessible, noninvasive, and scalable health screening. We present a method\nthat leverages pretrained vision transformer foundation models to estimate\nremaining lifespan from facial and whole-body images, alongside robust\nuncertainty quantification. We show that predictive uncertainty varies\nsystematically with the true remaining lifespan, and that this uncertainty can\nbe effectively modeled by learning a Gaussian distribution for each sample. Our\napproach achieves state-of-the-art mean absolute error (MAE) of 7.48 years on\nan established Dataset, and further improves to 4.79 and 5.07 years MAE on two\nnew, higher-quality datasets curated and published in this work. Importantly,\nour models provide well-calibrated uncertainty estimates, as demonstrated by a\nbucketed expected calibration error of 0.62 years. While not intended for\nclinical deployment, these results highlight the potential of extracting\nmedically relevant signals from images. We make all code and datasets available\nto facilitate further research."}
{"id": "2506.13440", "pdf": "https://arxiv.org/pdf/2506.13440", "abs": "https://arxiv.org/abs/2506.13440", "authors": ["Shenqi Wang", "Yingfu Xu", "Amirreza Yousefzadeh", "Sherif Eissa", "Henk Corporaal", "Federico Corradi", "Guangzhi Tang"], "title": "Sparse Convolutional Recurrent Learning for Efficient Event-based Neuromorphic Object Detection", "categories": ["cs.CV", "cs.NE"], "comment": "Accepted by IJCNN 2025", "summary": "Leveraging the high temporal resolution and dynamic range, object detection\nwith event cameras can enhance the performance and safety of automotive and\nrobotics applications in real-world scenarios. However, processing sparse event\ndata requires compute-intensive convolutional recurrent units, complicating\ntheir integration into resource-constrained edge applications. Here, we propose\nthe Sparse Event-based Efficient Detector (SEED) for efficient event-based\nobject detection on neuromorphic processors. We introduce sparse convolutional\nrecurrent learning, which achieves over 92% activation sparsity in recurrent\nprocessing, vastly reducing the cost for spatiotemporal reasoning on sparse\nevent data. We validated our method on Prophesee's 1 Mpx and Gen1 event-based\nobject detection datasets. Notably, SEED sets a new benchmark in computational\nefficiency for event-based object detection which requires long-term temporal\nlearning. Compared to state-of-the-art methods, SEED significantly reduces\nsynaptic operations while delivering higher or same-level mAP. Our hardware\nsimulations showcase the critical role of SEED's hardware-aware design in\nachieving energy-efficient and low-latency neuromorphic processing."}
{"id": "2506.13444", "pdf": "https://arxiv.org/pdf/2506.13444", "abs": "https://arxiv.org/abs/2506.13444", "authors": ["Laiyan Ding", "Hualie Jiang", "Jiwei Chen", "Rui Huang"], "title": "Self-Supervised Enhancement for Depth from a Lightweight ToF Sensor with Monocular Images", "categories": ["cs.CV"], "comment": "accepted by IROS 2025", "summary": "Depth map enhancement using paired high-resolution RGB images offers a\ncost-effective solution for improving low-resolution depth data from\nlightweight ToF sensors. Nevertheless, naively adopting a depth estimation\npipeline to fuse the two modalities requires groundtruth depth maps for\nsupervision. To address this, we propose a self-supervised learning framework,\nSelfToF, which generates detailed and scale-aware depth maps. Starting from an\nimage-based self-supervised depth estimation pipeline, we add low-resolution\ndepth as inputs, design a new depth consistency loss, propose a scale-recovery\nmodule, and finally obtain a large performance boost. Furthermore, since the\nToF signal sparsity varies in real-world applications, we upgrade SelfToF to\nSelfToF* with submanifold convolution and guided feature fusion. Consequently,\nSelfToF* maintain robust performance across varying sparsity levels in ToF\ndata. Overall, our proposed method is both efficient and effective, as verified\nby extensive experiments on the NYU and ScanNet datasets. The code will be made\npublic."}
{"id": "2506.13445", "pdf": "https://arxiv.org/pdf/2506.13445", "abs": "https://arxiv.org/abs/2506.13445", "authors": ["Waqar Tanveer", "Laura Fernández-Robles", "Eduardo Fidalgo", "Víctor González-Castro", "Enrique Alegre"], "title": "Overcoming Occlusions in the Wild: A Multi-Task Age Head Approach to Age Estimation", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Facial age estimation has achieved considerable success under controlled\nconditions. However, in unconstrained real-world scenarios, which are often\nreferred to as 'in the wild', age estimation remains challenging, especially\nwhen faces are partially occluded, which may obscure their visibility. To\naddress this limitation, we propose a new approach integrating generative\nadversarial networks (GANs) and transformer architectures to enable robust age\nestimation from occluded faces. We employ an SN-Patch GAN to effectively remove\nocclusions, while an Attentive Residual Convolution Module (ARCM), paired with\na Swin Transformer, enhances feature representation. Additionally, we introduce\na Multi-Task Age Head (MTAH) that combines regression and distribution\nlearning, further improving age estimation under occlusion. Experimental\nresults on the FG-NET, UTKFace, and MORPH datasets demonstrate that our\nproposed approach surpasses existing state-of-the-art techniques for occluded\nfacial age estimation by achieving an MAE of $3.00$, $4.54$, and $2.53$ years,\nrespectively."}
{"id": "2506.13457", "pdf": "https://arxiv.org/pdf/2506.13457", "abs": "https://arxiv.org/abs/2506.13457", "authors": ["Momir Adžemović"], "title": "Deep Learning-Based Multi-Object Tracking: A Comprehensive Survey from Foundations to State-of-the-Art", "categories": ["cs.CV", "68T45, 94A08, 68W40, 62H35", "I.4.8; I.5.1; I.2.10; I.5.4"], "comment": "39 pages", "summary": "Multi-object tracking (MOT) is a core task in computer vision that involves\ndetecting objects in video frames and associating them across time. The rise of\ndeep learning has significantly advanced MOT, particularly within the\ntracking-by-detection paradigm, which remains the dominant approach.\nAdvancements in modern deep learning-based methods accelerated in 2022 with the\nintroduction of ByteTrack for tracking-by-detection and MOTR for end-to-end\ntracking. Our survey provides an in-depth analysis of deep learning-based MOT\nmethods, systematically categorizing tracking-by-detection approaches into five\ngroups: joint detection and embedding, heuristic-based, motion-based, affinity\nlearning, and offline methods. In addition, we examine end-to-end tracking\nmethods and compare them with existing alternative approaches. We evaluate the\nperformance of recent trackers across multiple benchmarks and specifically\nassess their generality by comparing results across different domains. Our\nfindings indicate that heuristic-based methods achieve state-of-the-art results\non densely populated datasets with linear object motion, while deep\nlearning-based association methods, in both tracking-by-detection and\nend-to-end approaches, excel in scenarios with complex motion patterns."}
{"id": "2506.13458", "pdf": "https://arxiv.org/pdf/2506.13458", "abs": "https://arxiv.org/abs/2506.13458", "authors": ["Cristina Mahanta", "Gagan Bhatia"], "title": "Leveraging Vision-Language Pre-training for Human Activity Recognition in Still Images", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Recognising human activity in a single photo enables indexing, safety and\nassistive applications, yet lacks motion cues. Using 285 MSCOCO images labelled\nas walking, running, sitting, and standing, scratch CNNs scored 41% accuracy.\nFine-tuning multimodal CLIP raised this to 76%, demonstrating that contrastive\nvision-language pre-training decisively improves still-image action recognition\nin real-world deployments."}
{"id": "2506.13465", "pdf": "https://arxiv.org/pdf/2506.13465", "abs": "https://arxiv.org/abs/2506.13465", "authors": ["Zerui Gong", "Zhonghua Wu", "Qingyi Tao", "Qinyue Li", "Chen Change Loy"], "title": "SA-LUT: Spatial Adaptive 4D Look-Up Table for Photorealistic Style Transfer", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Photorealistic style transfer (PST) enables real-world color grading by\nadapting reference image colors while preserving content structure. Existing\nmethods mainly follow either approaches: generation-based methods that\nprioritize stylistic fidelity at the cost of content integrity and efficiency,\nor global color transformation methods such as LUT, which preserve structure\nbut lack local adaptability. To bridge this gap, we propose Spatial Adaptive 4D\nLook-Up Table (SA-LUT), combining LUT efficiency with neural network\nadaptability. SA-LUT features: (1) a Style-guided 4D LUT Generator that\nextracts multi-scale features from the style image to predict a 4D LUT, and (2)\na Context Generator using content-style cross-attention to produce a context\nmap. This context map enables spatially-adaptive adjustments, allowing our 4D\nLUT to apply precise color transformations while preserving structural\nintegrity. To establish a rigorous evaluation framework for photorealistic\nstyle transfer, we introduce PST50, the first benchmark specifically designed\nfor PST assessment. Experiments demonstrate that SA-LUT substantially\noutperforms state-of-the-art methods, achieving a 66.7% reduction in LPIPS\nscore compared to 3D LUT approaches, while maintaining real-time performance at\n16 FPS for video stylization. Our code and benchmark are available at\nhttps://github.com/Ry3nG/SA-LUT"}
{"id": "2506.13476", "pdf": "https://arxiv.org/pdf/2506.13476", "abs": "https://arxiv.org/abs/2506.13476", "authors": ["Xiem HoangVan", "Dang Bui Dinh", "Thanh Nguyen Canh", "Van-Truong Nguyen"], "title": "ESRPCB: an Edge guided Super-Resolution model and Ensemble learning for tiny Printed Circuit Board Defect detection", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": "Published in Engineering Applications of Artificial Intelligence", "summary": "Printed Circuit Boards (PCBs) are critical components in modern electronics,\nwhich require stringent quality control to ensure proper functionality.\nHowever, the detection of defects in small-scale PCBs images poses significant\nchallenges as a result of the low resolution of the captured images, leading to\npotential confusion between defects and noise. To overcome these challenges,\nthis paper proposes a novel framework, named ESRPCB (edgeguided\nsuper-resolution for PCBs defect detection), which combines edgeguided\nsuper-resolution with ensemble learning to enhance PCBs defect detection. The\nframework leverages the edge information to guide the EDSR (Enhanced Deep\nSuper-Resolution) model with a novel ResCat (Residual Concatenation) structure,\nenabling it to reconstruct high-resolution images from small PCBs inputs. By\nincorporating edge features, the super-resolution process preserves critical\nstructural details, ensuring that tiny defects remain distinguishable in the\nenhanced image. Following this, a multi-modal defect detection model employs\nensemble learning to analyze the super-resolved"}
{"id": "2506.13484", "pdf": "https://arxiv.org/pdf/2506.13484", "abs": "https://arxiv.org/abs/2506.13484", "authors": ["Martina Pastorino", "Michael Alibani", "Nicola Acito", "Gabriele Moser"], "title": "Deep Diffusion Models and Unsupervised Hyperspectral Unmixing for Realistic Abundance Map Synthesis", "categories": ["cs.CV", "eess.IV"], "comment": "CVPRw2025", "summary": "This paper presents a novel methodology for generating realistic abundance\nmaps from hyperspectral imagery using an unsupervised, deep-learning-driven\napproach. Our framework integrates blind linear hyperspectral unmixing with\nstate-of-the-art diffusion models to enhance the realism and diversity of\nsynthetic abundance maps. First, we apply blind unmixing to extract endmembers\nand abundance maps directly from raw hyperspectral data. These abundance maps\nthen serve as inputs to a diffusion model, which acts as a generative engine to\nsynthesize highly realistic spatial distributions. Diffusion models have\nrecently revolutionized image synthesis by offering superior performance,\nflexibility, and stability, making them well-suited for high-dimensional\nspectral data. By leveraging this combination of physically interpretable\nunmixing and deep generative modeling, our approach enables the simulation of\nhyperspectral sensor outputs under diverse imaging conditions--critical for\ndata augmentation, algorithm benchmarking, and model evaluation in\nhyperspectral analysis. Notably, our method is entirely unsupervised, ensuring\nadaptability to different datasets without the need for labeled training data.\nWe validate our approach using real hyperspectral imagery from the PRISMA space\nmission for Earth observation, demonstrating its effectiveness in producing\nrealistic synthetic abundance maps that capture the spatial and spectral\ncharacteristics of natural scenes."}
{"id": "2506.13492", "pdf": "https://arxiv.org/pdf/2506.13492", "abs": "https://arxiv.org/abs/2506.13492", "authors": ["Chengrui Zhang", "Maizhen Ning", "Zihao Zhou", "Jie Sun", "Kaizhu Huang", "Qiufeng Wang"], "title": "GeoSDF: Plane Geometry Diagram Synthesis via Signed Distance Field", "categories": ["cs.CV"], "comment": null, "summary": "Plane Geometry Diagram Synthesis has been a crucial task in computer\ngraphics, with applications ranging from educational tools to AI-driven\nmathematical reasoning. Traditionally, we rely on computer tools (e.g.,\nMatplotlib and GeoGebra) to manually generate precise diagrams, but it usually\nrequires huge, complicated calculations cost. Recently, researchers start to\nwork on learning-based methods (e.g., Stable Diffusion and GPT4) to\nautomatically generate diagrams, saving operational cost but usually suffering\nfrom limited realism and insufficient accuracy. In this paper, we propose a\nnovel framework GeoSDF to automatically generate diagrams efficiently and\naccurately with Signed Distance Field (SDF). Specifically, we first represent\ngeometric elements in the SDF, then construct a series of constraint functions\nto represent geometric relationships, next we optimize such constraint\nfunctions to get an optimized field of both elements and constraints, finally\nby rendering the optimized field, we can obtain the synthesized diagram. In our\nGeoSDF, we define a symbolic language to easily represent geometric elements\nand those constraints, and our synthesized geometry diagrams can be\nself-verified in the SDF, ensuring both mathematical accuracy and visual\nplausibility. In experiments, our GeoSDF synthesized both normal high-school\nlevel and IMO-level geometry diagrams. Through both qualitative and\nquantitative analysis, we can see that synthesized diagrams are realistic and\naccurate, and our synthesizing process is simple and efficient. Furthermore, we\nobtain a very high accuracy of solving geometry problems (over 95\\% while the\ncurrent SOTA accuracy is around 75%) by leveraging our self-verification\nproperty. All of these demonstrate the advantage of GeoSDF, paving the way for\nmore sophisticated, accurate, and flexible generation of geometric diagrams for\na wide array of applications."}
{"id": "2506.13496", "pdf": "https://arxiv.org/pdf/2506.13496", "abs": "https://arxiv.org/abs/2506.13496", "authors": ["Kshitij Kavimandan", "Angelos Nalmpantis", "Emma Beauxis-Aussalet", "Robert-Jan Sips"], "title": "Hierarchical Multi-Positive Contrastive Learning for Patent Image Retrieval", "categories": ["cs.CV", "cs.IR", "cs.LG", "68T45, 68T07", "H.3.3; I.4.10; I.2.10"], "comment": "5 pages, 3 figures, Accepted as a short paper at the 6th Workshop on\n  Patent Text Mining and Semantic Technologies (PatentSemTech 2025), co-located\n  with SIGIR 2025", "summary": "Patent images are technical drawings that convey information about a patent's\ninnovation. Patent image retrieval systems aim to search in vast collections\nand retrieve the most relevant images. Despite recent advances in information\nretrieval, patent images still pose significant challenges due to their\ntechnical intricacies and complex semantic information, requiring efficient\nfine-tuning for domain adaptation. Current methods neglect patents'\nhierarchical relationships, such as those defined by the Locarno International\nClassification (LIC) system, which groups broad categories (e.g., \"furnishing\")\ninto subclasses (e.g., \"seats\" and \"beds\") and further into specific patent\ndesigns. In this work, we introduce a hierarchical multi-positive contrastive\nloss that leverages the LIC's taxonomy to induce such relations in the\nretrieval process. Our approach assigns multiple positive pairs to each patent\nimage within a batch, with varying similarity scores based on the hierarchical\ntaxonomy. Our experimental analysis with various vision and multimodal models\non the DeepPatent2 dataset shows that the proposed method enhances the\nretrieval results. Notably, our method is effective with low-parameter models,\nwhich require fewer computational resources and can be deployed on environments\nwith limited hardware."}
{"id": "2506.13501", "pdf": "https://arxiv.org/pdf/2506.13501", "abs": "https://arxiv.org/abs/2506.13501", "authors": ["Mingyuan Li", "Tong Jia", "Han Gu", "Hui Lu", "Hao Wang", "Bowen Ma", "Shuyang Lin", "Shiyi Guo", "Shizhuo Deng", "Dongyue Chen"], "title": "FOAM: A General Frequency-Optimized Anti-Overlapping Framework for Overlapping Object Perception", "categories": ["cs.CV"], "comment": null, "summary": "Overlapping object perception aims to decouple the randomly overlapping\nforeground-background features, extracting foreground features while\nsuppressing background features, which holds significant application value in\nfields such as security screening and medical auxiliary diagnosis. Despite some\nresearch efforts to tackle the challenge of overlapping object perception, most\nsolutions are confined to the spatial domain. Through frequency domain\nanalysis, we observe that the degradation of contours and textures due to the\noverlapping phenomenon can be intuitively reflected in the magnitude spectrum.\nBased on this observation, we propose a general Frequency-Optimized\nAnti-Overlapping Framework (FOAM) to assist the model in extracting more\ntexture and contour information, thereby enhancing the ability for\nanti-overlapping object perception. Specifically, we design the Frequency\nSpatial Transformer Block (FSTB), which can simultaneously extract features\nfrom both the frequency and spatial domains, helping the network capture more\ntexture features from the foreground. In addition, we introduce the\nHierarchical De-Corrupting (HDC) mechanism, which aligns adjacent features in\nthe separately constructed base branch and corruption branch using a specially\ndesigned consistent loss during the training phase. This mechanism suppresses\nthe response to irrelevant background features of FSTBs, thereby improving the\nperception of foreground contour. We conduct extensive experiments to validate\nthe effectiveness and generalization of the proposed FOAM, which further\nimproves the accuracy of state-of-the-art models on four datasets, specifically\nfor the three overlapping object perception tasks: Prohibited Item Detection,\nProhibited Item Segmentation, and Pneumonia Detection. The code will be open\nsource once the paper is accepted."}
{"id": "2506.13506", "pdf": "https://arxiv.org/pdf/2506.13506", "abs": "https://arxiv.org/abs/2506.13506", "authors": ["David W Arathorn", "Josephine C. D'Angelo", "Austin Roorda"], "title": "Stimulus Motion Perception Studies Imply Specific Neural Computations in Human Visual Stabilization", "categories": ["cs.CV", "q-bio.NC"], "comment": null, "summary": "Even during fixation the human eye is constantly in low amplitude motion,\njittering over small angles in random directions at up to 100Hz. This motion\nresults in all features of the image on the retina constantly traversing a\nnumber of cones, yet objects which are stable in the world are perceived to be\nstable, and any object which is moving in the world is perceived to be moving.\nA series of experiments carried out over a dozen years revealed the\npsychophysics of visual stabilization to be more nuanced than might be assumed,\nsay, from the mechanics of stabilization of camera images, or what might be\nassumed to be the simplest solution from an evolutionary perspective. The\npsychophysics revealed by the experiments strongly implies a specific set of\noperations on retinal signals resulting in the observed stabilization behavior.\nThe presentation is in two levels. First is a functional description of the\naction of the mechanism that is very likely responsible for the experimentally\nobserved behavior. Second is a more speculative proposal of circuit-level\nneural elements that might implement the functional behavior."}
{"id": "2506.13508", "pdf": "https://arxiv.org/pdf/2506.13508", "abs": "https://arxiv.org/abs/2506.13508", "authors": ["Jungeon Kim", "Geonsoo Park", "Seungyong Lee"], "title": "Multiview Geometric Regularization of Gaussian Splatting for Accurate Radiance Fields", "categories": ["cs.CV"], "comment": "Accepted to Computer Graphics Forum (EGSR 2025)", "summary": "Recent methods, such as 2D Gaussian Splatting and Gaussian Opacity Fields,\nhave aimed to address the geometric inaccuracies of 3D Gaussian Splatting while\nretaining its superior rendering quality. However, these approaches still\nstruggle to reconstruct smooth and reliable geometry, particularly in scenes\nwith significant color variation across viewpoints, due to their per-point\nappearance modeling and single-view optimization constraints. In this paper, we\npropose an effective multiview geometric regularization strategy that\nintegrates multiview stereo (MVS) depth, RGB, and normal constraints into\nGaussian Splatting initialization and optimization. Our key insight is the\ncomplementary relationship between MVS-derived depth points and Gaussian\nSplatting-optimized positions: MVS robustly estimates geometry in regions of\nhigh color variation through local patch-based matching and epipolar\nconstraints, whereas Gaussian Splatting provides more reliable and less noisy\ndepth estimates near object boundaries and regions with lower color variation.\nTo leverage this insight, we introduce a median depth-based multiview relative\ndepth loss with uncertainty estimation, effectively integrating MVS depth\ninformation into Gaussian Splatting optimization. We also propose an MVS-guided\nGaussian Splatting initialization to avoid Gaussians falling into suboptimal\npositions. Extensive experiments validate that our approach successfully\ncombines these strengths, enhancing both geometric accuracy and rendering\nquality across diverse indoor and outdoor scenes."}
{"id": "2506.13509", "pdf": "https://arxiv.org/pdf/2506.13509", "abs": "https://arxiv.org/abs/2506.13509", "authors": ["Xiaoyang Wei", "Camille Kurtz", "Florence Cloppet"], "title": "A Semantically-Aware Relevance Measure for Content-Based Medical Image Retrieval Evaluation", "categories": ["cs.CV"], "comment": "This paper has been accepted by the International Conference on Image\n  Analysis and Processing 2025", "summary": "Performance evaluation for Content-Based Image Retrieval (CBIR) remains a\ncrucial but unsolved problem today especially in the medical domain. Various\nevaluation metrics have been discussed in the literature to solve this problem.\nMost of the existing metrics (e.g., precision, recall) are adapted from\nclassification tasks which require manual labels as ground truth. However, such\nlabels are often expensive and unavailable in specific thematic domains.\nFurthermore, medical images are usually associated with (radiological) case\nreports or annotated with descriptive captions in literature figures, such text\ncontains information that can help to assess CBIR.Several researchers have\nargued that the medical concepts hidden in the text can serve as the basis for\nCBIR evaluation purpose. However, these works often consider these medical\nconcepts as independent and isolated labels while in fact the subtle\nrelationships between various concepts are neglected. In this work, we\nintroduce the use of knowledge graphs to measure the distance between various\nmedical concepts and propose a novel relevance measure for the evaluation of\nCBIR by defining an approximate matching-based relevance score between two sets\nof medical concepts which allows us to indirectly measure the similarity\nbetween medical images.We quantitatively demonstrate the effectiveness and\nfeasibility of our relevance measure using a public dataset."}
{"id": "2506.13516", "pdf": "https://arxiv.org/pdf/2506.13516", "abs": "https://arxiv.org/abs/2506.13516", "authors": ["Yihui Li", "Chengxin Lv", "Hongyu Yang", "Di Huang"], "title": "Micro-macro Gaussian Splatting with Enhanced Scalability for Unconstrained Scene Reconstruction", "categories": ["cs.CV"], "comment": null, "summary": "Reconstructing 3D scenes from unconstrained image collections poses\nsignificant challenges due to variations in appearance. In this paper, we\npropose Scalable Micro-macro Wavelet-based Gaussian Splatting (SMW-GS), a novel\nmethod that enhances 3D reconstruction across diverse scales by decomposing\nscene representations into global, refined, and intrinsic components. SMW-GS\nincorporates the following innovations: Micro-macro Projection, which enables\nGaussian points to sample multi-scale details with improved diversity; and\nWavelet-based Sampling, which refines feature representations using\nfrequency-domain information to better capture complex scene appearances. To\nachieve scalability, we further propose a large-scale scene promotion strategy,\nwhich optimally assigns camera views to scene partitions by maximizing their\ncontributions to Gaussian points, achieving consistent and high-quality\nreconstructions even in expansive environments. Extensive experiments\ndemonstrate that SMW-GS significantly outperforms existing methods in both\nreconstruction quality and scalability, particularly excelling in large-scale\nurban environments with challenging illumination variations. Project is\navailable at https://github.com/Kidleyh/SMW-GS."}
{"id": "2506.13542", "pdf": "https://arxiv.org/pdf/2506.13542", "abs": "https://arxiv.org/abs/2506.13542", "authors": ["Hugo Riffaud de Turckheim", "Sylvain Lobry", "Roberto Interdonato", "Diego Marcos"], "title": "Atomizer: Generalizing to new modalities by breaking satellite images down to a set of scalars", "categories": ["cs.CV"], "comment": null, "summary": "The growing number of Earth observation satellites has led to increasingly\ndiverse remote sensing data, with varying spatial, spectral, and temporal\nconfigurations. Most existing models rely on fixed input formats and\nmodality-specific encoders, which require retraining when new configurations\nare introduced, limiting their ability to generalize across modalities. We\nintroduce Atomizer, a flexible architecture that represents remote sensing\nimages as sets of scalars, each corresponding to a spectral band value of a\npixel. Each scalar is enriched with contextual metadata (acquisition time,\nspatial resolution, wavelength, and bandwidth), producing an atomic\nrepresentation that allows a single encoder to process arbitrary modalities\nwithout interpolation or resampling. Atomizer uses structured tokenization with\nFourier features and non-uniform radial basis functions to encode content and\ncontext, and maps tokens into a latent space via cross-attention. Under\nmodality-disjoint evaluations, Atomizer outperforms standard models and\ndemonstrates robust performance across varying resolutions and spatial sizes."}
{"id": "2506.13545", "pdf": "https://arxiv.org/pdf/2506.13545", "abs": "https://arxiv.org/abs/2506.13545", "authors": ["Yuan Gao", "Shaoyan Pan", "Mingzhe Hu", "Huiqiao Xie", "Jill Remick", "Chih-Wei Chang", "Justin Roper", "Zhen Tian", "Xiaofeng Yang"], "title": "Limited-Angle CBCT Reconstruction via Geometry-Integrated Cycle-domain Denoising Diffusion Probabilistic Models", "categories": ["cs.CV"], "comment": null, "summary": "Cone-beam CT (CBCT) is widely used in clinical radiotherapy for image-guided\ntreatment, improving setup accuracy, adaptive planning, and motion management.\nHowever, slow gantry rotation limits performance by introducing motion\nartifacts, blurring, and increased dose. This work aims to develop a clinically\nfeasible method for reconstructing high-quality CBCT volumes from consecutive\nlimited-angle acquisitions, addressing imaging challenges in time- or\ndose-constrained settings. We propose a limited-angle (LA) geometry-integrated\ncycle-domain (LA-GICD) framework for CBCT reconstruction, comprising two\ndenoising diffusion probabilistic models (DDPMs) connected via analytic\ncone-beam forward and back projectors. A Projection-DDPM completes missing\nprojections, followed by back-projection, and an Image-DDPM refines the volume.\nThis dual-domain design leverages complementary priors from projection and\nimage spaces to achieve high-quality reconstructions from limited-angle (<= 90\ndegrees) scans. Performance was evaluated against full-angle reconstruction.\nFour board-certified medical physicists conducted assessments. A total of 78\nplanning CTs in common CBCT geometries were used for training and evaluation.\nThe method achieved a mean absolute error of 35.5 HU, SSIM of 0.84, and PSNR of\n29.8 dB, with visibly reduced artifacts and improved soft-tissue clarity.\nLA-GICD's geometry-aware dual-domain learning, embedded in analytic\nforward/backward operators, enabled artifact-free, high-contrast\nreconstructions from a single 90-degree scan, reducing acquisition time and\ndose four-fold. LA-GICD improves limited-angle CBCT reconstruction with strong\ndata fidelity and anatomical realism. It offers a practical solution for\nshort-arc acquisitions, enhancing CBCT use in radiotherapy by providing\nclinically applicable images with reduced scan time and dose for more accurate,\npersonalized treatments."}
{"id": "2506.13552", "pdf": "https://arxiv.org/pdf/2506.13552", "abs": "https://arxiv.org/abs/2506.13552", "authors": ["Guohuan Xie", "Syed Ariff Syed Hesham", "Wenya Guo", "Bing Li", "Ming-Ming Cheng", "Guolei Sun", "Yun Liu"], "title": "A Comprehensive Survey on Video Scene Parsing:Advances, Challenges, and Prospects", "categories": ["cs.CV"], "comment": null, "summary": "Video Scene Parsing (VSP) has emerged as a cornerstone in computer vision,\nfacilitating the simultaneous segmentation, recognition, and tracking of\ndiverse visual entities in dynamic scenes. In this survey, we present a\nholistic review of recent advances in VSP, covering a wide array of vision\ntasks, including Video Semantic Segmentation (VSS), Video Instance Segmentation\n(VIS), Video Panoptic Segmentation (VPS), as well as Video Tracking and\nSegmentation (VTS), and Open-Vocabulary Video Segmentation (OVVS). We\nsystematically analyze the evolution from traditional hand-crafted features to\nmodern deep learning paradigms -- spanning from fully convolutional networks to\nthe latest transformer-based architectures -- and assess their effectiveness in\ncapturing both local and global temporal contexts. Furthermore, our review\ncritically discusses the technical challenges, ranging from maintaining\ntemporal consistency to handling complex scene dynamics, and offers a\ncomprehensive comparative study of datasets and evaluation metrics that have\nshaped current benchmarking standards. By distilling the key contributions and\nshortcomings of state-of-the-art methodologies, this survey highlights emerging\ntrends and prospective research directions that promise to further elevate the\nrobustness and adaptability of VSP in real-world applications."}
{"id": "2506.13553", "pdf": "https://arxiv.org/pdf/2506.13553", "abs": "https://arxiv.org/abs/2506.13553", "authors": ["Yueru Luo", "Changqing Zhou", "Yiming Yang", "Erlong Li", "Chao Zheng", "Shuqi Mei", "Shuguang Cui", "Zhen Li"], "title": "RelTopo: Enhancing Relational Modeling for Driving Scene Topology Reasoning", "categories": ["cs.CV"], "comment": "Preprint. Under review", "summary": "Accurate road topology reasoning is critical for autonomous driving, enabling\neffective navigation and adherence to traffic regulations. Central to this task\nare lane perception and topology reasoning. However, existing methods typically\nfocus on either lane detection or Lane-to-Lane (L2L) topology reasoning, often\n\\textit{neglecting} Lane-to-Traffic-element (L2T) relationships or\n\\textit{failing} to optimize these tasks jointly. Furthermore, most approaches\neither overlook relational modeling or apply it in a limited scope, despite the\ninherent spatial relationships among road elements. We argue that relational\nmodeling is beneficial for both perception and reasoning, as humans naturally\nleverage contextual relationships for road element recognition and their\nconnectivity inference. To this end, we introduce relational modeling into both\nperception and reasoning, \\textit{jointly} enhancing structural understanding.\nSpecifically, we propose: 1) a relation-aware lane detector, where our\ngeometry-biased self-attention and \\curve\\ cross-attention refine lane\nrepresentations by capturing relational dependencies; 2) relation-enhanced\ntopology heads, including a geometry-enhanced L2L head and a cross-view L2T\nhead, boosting reasoning with relational cues; and 3) a contrastive learning\nstrategy with InfoNCE loss to regularize relationship embeddings. Extensive\nexperiments on OpenLane-V2 demonstrate that our approach significantly improves\nboth detection and topology reasoning metrics, achieving +3.1 in DET$_l$, +5.3\nin TOP$_{ll}$, +4.9 in TOP$_{lt}$, and an overall +4.4 in OLS, setting a new\nstate-of-the-art. Code will be released."}
{"id": "2506.13558", "pdf": "https://arxiv.org/pdf/2506.13558", "abs": "https://arxiv.org/abs/2506.13558", "authors": ["Yu Yang", "Alan Liang", "Jianbiao Mei", "Yukai Ma", "Yong Liu", "Gim Hee Lee"], "title": "X-Scene: Large-Scale Driving Scene Generation with High Fidelity and Flexible Controllability", "categories": ["cs.CV"], "comment": "28 pages, 9 figures, Project page at https://x-scene.github.io/", "summary": "Diffusion models are advancing autonomous driving by enabling realistic data\nsynthesis, predictive end-to-end planning, and closed-loop simulation, with a\nprimary focus on temporally consistent generation. However, the generation of\nlarge-scale 3D scenes that require spatial coherence remains underexplored. In\nthis paper, we propose X-Scene, a novel framework for large-scale driving scene\ngeneration that achieves both geometric intricacy and appearance fidelity,\nwhile offering flexible controllability. Specifically, X-Scene supports\nmulti-granular control, including low-level conditions such as user-provided or\ntext-driven layout for detailed scene composition and high-level semantic\nguidance such as user-intent and LLM-enriched text prompts for efficient\ncustomization. To enhance geometrical and visual fidelity, we introduce a\nunified pipeline that sequentially generates 3D semantic occupancy and the\ncorresponding multiview images, while ensuring alignment between modalities.\nAdditionally, we extend the generated local region into a large-scale scene\nthrough consistency-aware scene outpainting, which extrapolates new occupancy\nand images conditioned on the previously generated area, enhancing spatial\ncontinuity and preserving visual coherence. The resulting scenes are lifted\ninto high-quality 3DGS representations, supporting diverse applications such as\nscene exploration. Comprehensive experiments demonstrate that X-Scene\nsignificantly advances controllability and fidelity for large-scale driving\nscene generation, empowering data generation and simulation for autonomous\ndriving."}
{"id": "2506.13564", "pdf": "https://arxiv.org/pdf/2506.13564", "abs": "https://arxiv.org/abs/2506.13564", "authors": ["Geewook Kim", "Minjoon Seo"], "title": "MambaMia: A State-Space-Model-Based Compression for Efficient Video Understanding in Large Multimodal Models", "categories": ["cs.CV"], "comment": "17 pages, 5 figures", "summary": "We propose an efficient framework to compress multiple video-frame features\nbefore feeding them into large multimodal models, thereby mitigating the severe\ntoken explosion arising from long or dense videos. Our design leverages a\nbidirectional state-space-based block equipped with a gated skip connection and\na learnable weighted-average pooling mechanism applied to periodically inserted\nlearned queries. This structure enables hierarchical downsampling across both\nspatial and temporal dimensions, preserving performance in a cost-effective\nmanner. Across challenging long and dense video understanding tasks, our\napproach demonstrates competitive results against state-of-the-art models,\nwhile significantly reducing overall token budget. Notably, replacing our\nproposed state-space block with a conventional Transformer results in\nsubstantial performance degradation, highlighting the advantages of state-space\nmodeling for effectively compressing multi-frame video data. Our framework\nemphasizes resource-conscious efficiency, making it practical for real-world\ndeployments. We validate its scalability and generality across multiple\nbenchmarks, achieving the dual objectives of efficient resource usage and\ncomprehensive video understanding."}
{"id": "2506.13573", "pdf": "https://arxiv.org/pdf/2506.13573", "abs": "https://arxiv.org/abs/2506.13573", "authors": ["Bowen Zheng"], "title": "Integrated Pipeline for Monocular 3D Reconstruction and Finite Element Simulation in Industrial Applications", "categories": ["cs.CV"], "comment": null, "summary": "To address the challenges of 3D modeling and structural simulation in\nindustrial environment, such as the difficulty of equipment deployment, and the\ndifficulty of balancing accuracy and real-time performance, this paper proposes\nan integrated workflow, which integrates high-fidelity 3D reconstruction based\non monocular video, finite element simulation analysis, and mixed reality\nvisual display, aiming to build an interactive digital twin system for\nindustrial inspection, equipment maintenance and other scenes. Firstly, the\nNeuralangelo algorithm based on deep learning is used to reconstruct the 3D\nmesh model with rich details from the surround-shot video. Then, the QuadRemesh\ntool of Rhino is used to optimize the initial triangular mesh and generate a\nstructured mesh suitable for finite element analysis. The optimized mesh is\nfurther discretized by HyperMesh, and the material parameter setting and stress\nsimulation are carried out in Abaqus to obtain high-precision stress and\ndeformation results. Finally, combined with Unity and Vuforia engine, the\nreal-time superposition and interactive operation of simulation results in the\naugmented reality environment are realized, which improves users 'intuitive\nunderstanding of structural response. Experiments show that the method has good\nsimulation efficiency and visualization effect while maintaining high geometric\naccuracy. It provides a practical solution for digital modeling, mechanical\nanalysis and interactive display in complex industrial scenes, and lays a\nfoundation for the deep integration of digital twin and mixed reality\ntechnology in industrial applications."}
{"id": "2506.13589", "pdf": "https://arxiv.org/pdf/2506.13589", "abs": "https://arxiv.org/abs/2506.13589", "authors": ["Zhucun Xue", "Jiangning Zhang", "Xurong Xie", "Yuxuan Cai", "Yong Liu", "Xiangtai Li", "Dacheng Tao"], "title": "Omni-AdaVideoRAG: Omni-Contextual Adaptive Retrieval-Augmented for Efficient Long Video Understanding", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) struggle with long videos due to\nfixed context windows and weak long-term dependency modeling. Existing\nRetrieval-Augmented Generation (RAG) methods for videos use static retrieval\nstrategies, leading to inefficiencies for simple queries and information loss\nfor complex tasks. To address this, we propose AdaVideoRAG, a novel framework\nthat dynamically adapts retrieval granularity based on query complexity using a\nlightweight intent classifier. Our framework employs an Omni-Knowledge Indexing\nmodule to build hierarchical databases from text (captions, ASR, OCR), visual\nfeatures, and semantic graphs, enabling optimal resource allocation across\ntasks. We also introduce the HiVU benchmark for comprehensive evaluation.\nExperiments demonstrate improved efficiency and accuracy for long-video\nunderstanding, with seamless integration into existing MLLMs. AdaVideoRAG\nestablishes a new paradigm for adaptive retrieval in video analysis. Codes will\nbe open-sourced at https://github.com/xzc-zju/AdaVideoRAG."}
{"id": "2506.13594", "pdf": "https://arxiv.org/pdf/2506.13594", "abs": "https://arxiv.org/abs/2506.13594", "authors": ["Weimin Bai", "Yubo Li", "Wenzheng Chen", "Weijian Luo", "He Sun"], "title": "Dive3D: Diverse Distillation-based Text-to-3D Generation via Score Implicit Matching", "categories": ["cs.CV"], "comment": null, "summary": "Distilling pre-trained 2D diffusion models into 3D assets has driven\nremarkable advances in text-to-3D synthesis. However, existing methods\ntypically rely on Score Distillation Sampling (SDS) loss, which involves\nasymmetric KL divergence--a formulation that inherently favors mode-seeking\nbehavior and limits generation diversity. In this paper, we introduce Dive3D, a\nnovel text-to-3D generation framework that replaces KL-based objectives with\nScore Implicit Matching (SIM) loss, a score-based objective that effectively\nmitigates mode collapse. Furthermore, Dive3D integrates both diffusion\ndistillation and reward-guided optimization under a unified divergence\nperspective. Such reformulation, together with SIM loss, yields significantly\nmore diverse 3D outputs while improving text alignment, human preference, and\noverall visual fidelity. We validate Dive3D across various 2D-to-3D prompts and\nfind that it consistently outperforms prior methods in qualitative assessments,\nincluding diversity, photorealism, and aesthetic appeal. We further evaluate\nits performance on the GPTEval3D benchmark, comparing against nine\nstate-of-the-art baselines. Dive3D also achieves strong results on quantitative\nmetrics, including text-asset alignment, 3D plausibility, text-geometry\nconsistency, texture quality, and geometric detail."}
{"id": "2506.13629", "pdf": "https://arxiv.org/pdf/2506.13629", "abs": "https://arxiv.org/abs/2506.13629", "authors": ["Chenlu Zhan", "Gaoang Wang", "Hongwei Wang"], "title": "FreeQ-Graph: Free-form Querying with Semantic Consistent Scene Graph for 3D Scene Understanding", "categories": ["cs.CV"], "comment": null, "summary": "Semantic querying in complex 3D scenes through free-form language presents a\nsignificant challenge. Existing 3D scene understanding methods use large-scale\ntraining data and CLIP to align text queries with 3D semantic features.\nHowever, their reliance on predefined vocabulary priors from training data\nhinders free-form semantic querying. Besides, recent advanced methods rely on\nLLMs for scene understanding but lack comprehensive 3D scene-level information\nand often overlook the potential inconsistencies in LLM-generated outputs. In\nour paper, we propose FreeQ-Graph, which enables Free-form Querying with a\nsemantic consistent scene Graph for 3D scene understanding. The core idea is to\nencode free-form queries from a complete and accurate 3D scene graph without\npredefined vocabularies, and to align them with 3D consistent semantic labels,\nwhich accomplished through three key steps. We initiate by constructing a\ncomplete and accurate 3D scene graph that maps free-form objects and their\nrelations through LLM and LVLM guidance, entirely free from training data or\npredefined priors. Most importantly, we align graph nodes with accurate\nsemantic labels by leveraging 3D semantic aligned features from merged\nsuperpoints, enhancing 3D semantic consistency. To enable free-form semantic\nquerying, we then design an LLM-based reasoning algorithm that combines\nscene-level and object-level information to intricate reasoning. We conducted\nextensive experiments on 3D semantic grounding, segmentation, and complex\nquerying tasks, while also validating the accuracy of graph generation.\nExperiments on 6 datasets show that our model excels in both complex free-form\nsemantic queries and intricate relational reasoning."}
{"id": "2506.13638", "pdf": "https://arxiv.org/pdf/2506.13638", "abs": "https://arxiv.org/abs/2506.13638", "authors": ["Zhiyi Shi", "Binjie Wang", "Chongjie Si", "Yichen Wu", "Junsik Kim", "Hanspeter Pfister"], "title": "DualEdit: Dual Editing for Knowledge Updating in Vision-Language Models", "categories": ["cs.CV", "cs.AI"], "comment": "Under Review", "summary": "Model editing aims to efficiently update a pre-trained model's knowledge\nwithout the need for time-consuming full retraining. While existing pioneering\nediting methods achieve promising results, they primarily focus on editing\nsingle-modal language models (LLMs). However, for vision-language models\n(VLMs), which involve multiple modalities, the role and impact of each modality\non editing performance remain largely unexplored. To address this gap, we\nexplore the impact of textual and visual modalities on model editing and find\nthat: (1) textual and visual representations reach peak sensitivity at\ndifferent layers, reflecting their varying importance; and (2) editing both\nmodalities can efficiently update knowledge, but this comes at the cost of\ncompromising the model's original capabilities. Based on our findings, we\npropose DualEdit, an editor that modifies both textual and visual modalities at\ntheir respective key layers. Additionally, we introduce a gating module within\nthe more sensitive textual modality, allowing DualEdit to efficiently update\nnew knowledge while preserving the model's original information. We evaluate\nDualEdit across multiple VLM backbones and benchmark datasets, demonstrating\nits superiority over state-of-the-art VLM editing baselines as well as adapted\nLLM editing methods on different evaluation metrics."}
{"id": "2506.13654", "pdf": "https://arxiv.org/pdf/2506.13654", "abs": "https://arxiv.org/abs/2506.13654", "authors": ["Shulin Tian", "Ruiqi Wang", "Hongming Guo", "Penghao Wu", "Yuhao Dong", "Xiuying Wang", "Jingkang Yang", "Hao Zhang", "Hongyuan Zhu", "Ziwei Liu"], "title": "Ego-R1: Chain-of-Tool-Thought for Ultra-Long Egocentric Video Reasoning", "categories": ["cs.CV", "cs.AI"], "comment": "Project page: https://egolife-ai.github.io/Ego-R1/", "summary": "We introduce Ego-R1, a novel framework for reasoning over ultra-long (i.e.,\nin days and weeks) egocentric videos, which leverages a structured\nChain-of-Tool-Thought (CoTT) process, orchestrated by an Ego-R1 Agent trained\nvia reinforcement learning (RL). Inspired by human problem-solving strategies,\nCoTT decomposes complex reasoning into modular steps, with the RL agent\ninvoking specific tools, one per step, to iteratively and collaboratively\nanswer sub-questions tackling such tasks as temporal retrieval and multi-modal\nunderstanding. We design a two-stage training paradigm involving supervised\nfinetuning (SFT) of a pretrained language model using CoTT data and RL to\nenable our agent to dynamically propose step-by-step tools for long-range\nreasoning. To facilitate training, we construct a dataset called Ego-R1 Data,\nwhich consists of Ego-CoTT-25K for SFT and Ego-QA-4.4K for RL. Furthermore, our\nEgo-R1 agent is evaluated on a newly curated week-long video QA benchmark,\nEgo-R1 Bench, which contains human-verified QA pairs from hybrid sources.\nExtensive results demonstrate that the dynamic, tool-augmented chain-of-thought\nreasoning by our Ego-R1 Agent can effectively tackle the unique challenges of\nunderstanding ultra-long egocentric videos, significantly extending the time\ncoverage from few hours to a week."}
{"id": "2506.13657", "pdf": "https://arxiv.org/pdf/2506.13657", "abs": "https://arxiv.org/abs/2506.13657", "authors": ["Dipayan Biswas", "Shishir Shah", "Jaspal Subhlok"], "title": "Lecture Video Visual Objects (LVVO) Dataset: A Benchmark for Visual Object Detection in Educational Videos", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "We introduce the Lecture Video Visual Objects (LVVO) dataset, a new benchmark\nfor visual object detection in educational video content. The dataset consists\nof 4,000 frames extracted from 245 lecture videos spanning biology, computer\nscience, and geosciences. A subset of 1,000 frames, referred to as LVVO_1k, has\nbeen manually annotated with bounding boxes for four visual categories: Table,\nChart-Graph, Photographic-image, and Visual-illustration. Each frame was\nlabeled independently by two annotators, resulting in an inter-annotator F1\nscore of 83.41%, indicating strong agreement. To ensure high-quality consensus\nannotations, a third expert reviewed and resolved all cases of disagreement\nthrough a conflict resolution process. To expand the dataset, a semi-supervised\napproach was employed to automatically annotate the remaining 3,000 frames,\nforming LVVO_3k. The complete dataset offers a valuable resource for developing\nand evaluating both supervised and semi-supervised methods for visual content\ndetection in educational videos. The LVVO dataset is publicly available to\nsupport further research in this domain."}
{"id": "2506.13691", "pdf": "https://arxiv.org/pdf/2506.13691", "abs": "https://arxiv.org/abs/2506.13691", "authors": ["Zhucun Xue", "Jiangning Zhang", "Teng Hu", "Haoyang He", "Yinan Chen", "Yuxuan Cai", "Yabiao Wang", "Chengjie Wang", "Yong Liu", "Xiangtai Li", "Dacheng Tao"], "title": "UltraVideo: High-Quality UHD Video Dataset with Comprehensive Captions", "categories": ["cs.CV"], "comment": null, "summary": "The quality of the video dataset (image quality, resolution, and fine-grained\ncaption) greatly influences the performance of the video generation model. The\ngrowing demand for video applications sets higher requirements for high-quality\nvideo generation models. For example, the generation of movie-level Ultra-High\nDefinition (UHD) videos and the creation of 4K short video content. However,\nthe existing public datasets cannot support related research and applications.\nIn this paper, we first propose a high-quality open-sourced UHD-4K (22.4\\% of\nwhich are 8K) text-to-video dataset named UltraVideo, which contains a wide\nrange of topics (more than 100 kinds), and each video has 9 structured captions\nwith one summarized caption (average of 824 words). Specifically, we carefully\ndesign a highly automated curation process with four stages to obtain the final\nhigh-quality dataset: \\textit{i)} collection of diverse and high-quality video\nclips. \\textit{ii)} statistical data filtering. \\textit{iii)} model-based data\npurification. \\textit{iv)} generation of comprehensive, structured captions. In\naddition, we expand Wan to UltraWan-1K/-4K, which can natively generate\nhigh-quality 1K/4K videos with more consistent text controllability,\ndemonstrating the effectiveness of our data curation.We believe that this work\ncan make a significant contribution to future research on UHD video generation.\nUltraVideo dataset and UltraWan models are available at\nhttps://xzc-zju.github.io/projects/UltraVideo."}
{"id": "2506.13697", "pdf": "https://arxiv.org/pdf/2506.13697", "abs": "https://arxiv.org/abs/2506.13697", "authors": ["Junyoung Seo", "Jisang Han", "Jaewoo Jung", "Siyoon Jin", "Joungbin Lee", "Takuya Narihira", "Kazumi Fukuda", "Takashi Shibuya", "Donghoon Ahn", "Shoukang Hu", "Seungryong Kim", "Yuki Mitsufuji"], "title": "Vid-CamEdit: Video Camera Trajectory Editing with Generative Rendering from Estimated Geometry", "categories": ["cs.CV"], "comment": "Our project page can be found at\n  https://cvlab-kaist.github.io/Vid-CamEdit/", "summary": "We introduce Vid-CamEdit, a novel framework for video camera trajectory\nediting, enabling the re-synthesis of monocular videos along user-defined\ncamera paths. This task is challenging due to its ill-posed nature and the\nlimited multi-view video data for training. Traditional reconstruction methods\nstruggle with extreme trajectory changes, and existing generative models for\ndynamic novel view synthesis cannot handle in-the-wild videos. Our approach\nconsists of two steps: estimating temporally consistent geometry, and\ngenerative rendering guided by this geometry. By integrating geometric priors,\nthe generative model focuses on synthesizing realistic details where the\nestimated geometry is uncertain. We eliminate the need for extensive 4D\ntraining data through a factorized fine-tuning framework that separately trains\nspatial and temporal components using multi-view image and video data. Our\nmethod outperforms baselines in producing plausible videos from novel camera\ntrajectories, especially in extreme extrapolation scenarios on real-world\nfootage."}
{"id": "2506.13722", "pdf": "https://arxiv.org/pdf/2506.13722", "abs": "https://arxiv.org/abs/2506.13722", "authors": ["Kaiyuan Tan", "Pavan Kumar B N", "Bharatesh Chakravarthi"], "title": "How Real is CARLAs Dynamic Vision Sensor? A Study on the Sim-to-Real Gap in Traffic Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "Event cameras are gaining traction in traffic monitoring applications due to\ntheir low latency, high temporal resolution, and energy efficiency, which makes\nthem well-suited for real-time object detection at traffic intersections.\nHowever, the development of robust event-based detection models is hindered by\nthe limited availability of annotated real-world datasets. To address this,\nseveral simulation tools have been developed to generate synthetic event data.\nAmong these, the CARLA driving simulator includes a built-in dynamic vision\nsensor (DVS) module that emulates event camera output. Despite its potential,\nthe sim-to-real gap for event-based object detection remains insufficiently\nstudied. In this work, we present a systematic evaluation of this gap by\ntraining a recurrent vision transformer model exclusively on synthetic data\ngenerated using CARLAs DVS and testing it on varying combinations of synthetic\nand real-world event streams. Our experiments show that models trained solely\non synthetic data perform well on synthetic-heavy test sets but suffer\nsignificant performance degradation as the proportion of real-world data\nincreases. In contrast, models trained on real-world data demonstrate stronger\ngeneralization across domains. This study offers the first quantifiable\nanalysis of the sim-to-real gap in event-based object detection using CARLAs\nDVS. Our findings highlight limitations in current DVS simulation fidelity and\nunderscore the need for improved domain adaptation techniques in neuromorphic\nvision for traffic monitoring."}
{"id": "2506.13723", "pdf": "https://arxiv.org/pdf/2506.13723", "abs": "https://arxiv.org/abs/2506.13723", "authors": ["Qiyu Xu", "Wenyang Chen", "Zhanxuan Hu", "Huafeng Li", "Yonghang Tai"], "title": "OTFusion: Bridging Vision-only and Vision-Language Models via Optimal Transport for Transductive Zero-Shot Learning", "categories": ["cs.CV"], "comment": null, "summary": "Transductive zero-shot learning (ZSL) aims to classify unseen categories by\nleveraging both semantic class descriptions and the distribution of unlabeled\ntest data. While Vision-Language Models (VLMs) such as CLIP excel at aligning\nvisual inputs with textual semantics, they often rely too heavily on\nclass-level priors and fail to capture fine-grained visual cues. In contrast,\nVision-only Foundation Models (VFMs) like DINOv2 provide rich perceptual\nfeatures but lack semantic alignment. To exploit the complementary strengths of\nthese models, we propose OTFusion, a simple yet effective training-free\nframework that bridges VLMs and VFMs via Optimal Transport. Specifically,\nOTFusion aims to learn a shared probabilistic representation that aligns visual\nand semantic information by minimizing the transport cost between their\nrespective distributions. This unified distribution enables coherent class\npredictions that are both semantically meaningful and visually grounded.\nExtensive experiments on 11 benchmark datasets demonstrate that OTFusion\nconsistently outperforms the original CLIP model, achieving an average accuracy\nimprovement of nearly $10\\%$, all without any fine-tuning or additional\nannotations. The code will be publicly released after the paper is accepted."}
{"id": "2506.13750", "pdf": "https://arxiv.org/pdf/2506.13750", "abs": "https://arxiv.org/abs/2506.13750", "authors": ["Yuheng Yuan", "Qiuhong Shen", "Shizun Wang", "Xingyi Yang", "Xinchao Wang"], "title": "Test3R: Learning to Reconstruct 3D at Test Time", "categories": ["cs.CV"], "comment": null, "summary": "Dense matching methods like DUSt3R regress pairwise pointmaps for 3D\nreconstruction. However, the reliance on pairwise prediction and the limited\ngeneralization capability inherently restrict the global geometric consistency.\nIn this work, we introduce Test3R, a surprisingly simple test-time learning\ntechnique that significantly boosts geometric accuracy. Using image triplets\n($I_1,I_2,I_3$), Test3R generates reconstructions from pairs ($I_1,I_2$) and\n($I_1,I_3$). The core idea is to optimize the network at test time via a\nself-supervised objective: maximizing the geometric consistency between these\ntwo reconstructions relative to the common image $I_1$. This ensures the model\nproduces cross-pair consistent outputs, regardless of the inputs. Extensive\nexperiments demonstrate that our technique significantly outperforms previous\nstate-of-the-art methods on the 3D reconstruction and multi-view depth\nestimation tasks. Moreover, it is universally applicable and nearly cost-free,\nmaking it easily applied to other models and implemented with minimal test-time\ntraining overhead and parameter footprint. Code is available at\nhttps://github.com/nopQAQ/Test3R."}
{"id": "2506.13757", "pdf": "https://arxiv.org/pdf/2506.13757", "abs": "https://arxiv.org/abs/2506.13757", "authors": ["Zewei Zhou", "Tianhui Cai", "Seth Z. Zhao", "Yun Zhang", "Zhiyu Huang", "Bolei Zhou", "Jiaqi Ma"], "title": "AutoVLA: A Vision-Language-Action Model for End-to-End Autonomous Driving with Adaptive Reasoning and Reinforcement Fine-Tuning", "categories": ["cs.CV"], "comment": "Website link:https://autovla.github.io/", "summary": "Recent advancements in Vision-Language-Action (VLA) models have shown promise\nfor end-to-end autonomous driving by leveraging world knowledge and reasoning\ncapabilities. However, current VLA models often struggle with physically\ninfeasible action outputs, complex model structures, or unnecessarily long\nreasoning. In this paper, we propose AutoVLA, a novel VLA model that unifies\nreasoning and action generation within a single autoregressive generation model\nfor end-to-end autonomous driving. AutoVLA performs semantic reasoning and\ntrajectory planning directly from raw visual inputs and language instructions.\nWe tokenize continuous trajectories into discrete, feasible actions, enabling\ndirect integration into the language model. For training, we employ supervised\nfine-tuning to equip the model with dual thinking modes: fast thinking\n(trajectory-only) and slow thinking (enhanced with chain-of-thought reasoning).\nTo further enhance planning performance and efficiency, we introduce a\nreinforcement fine-tuning method based on Group Relative Policy Optimization\n(GRPO), reducing unnecessary reasoning in straightforward scenarios. Extensive\nexperiments across real-world and simulated datasets and benchmarks, including\nnuPlan, nuScenes, Waymo, and CARLA, demonstrate the competitive performance of\nAutoVLA in both open-loop and closed-loop settings. Qualitative results\nshowcase the adaptive reasoning and accurate planning capabilities of AutoVLA\nin diverse scenarios."}
{"id": "2506.13766", "pdf": "https://arxiv.org/pdf/2506.13766", "abs": "https://arxiv.org/abs/2506.13766", "authors": ["Lingteng Qiu", "Peihao Li", "Qi Zuo", "Xiaodong Gu", "Yuan Dong", "Weihao Yuan", "Siyu Zhu", "Xiaoguang Han", "Guanying Chen", "Zilong Dong"], "title": "PF-LHM: 3D Animatable Avatar Reconstruction from Pose-free Articulated Human Images", "categories": ["cs.CV"], "comment": null, "summary": "Reconstructing an animatable 3D human from casually captured images of an\narticulated subject without camera or human pose information is a practical yet\nchallenging task due to view misalignment, occlusions, and the absence of\nstructural priors. While optimization-based methods can produce high-fidelity\nresults from monocular or multi-view videos, they require accurate pose\nestimation and slow iterative optimization, limiting scalability in\nunconstrained scenarios. Recent feed-forward approaches enable efficient\nsingle-image reconstruction but struggle to effectively leverage multiple input\nimages to reduce ambiguity and improve reconstruction accuracy. To address\nthese challenges, we propose PF-LHM, a large human reconstruction model that\ngenerates high-quality 3D avatars in seconds from one or multiple casually\ncaptured pose-free images. Our approach introduces an efficient Encoder-Decoder\nPoint-Image Transformer architecture, which fuses hierarchical geometric point\nfeatures and multi-view image features through multimodal attention. The fused\nfeatures are decoded to recover detailed geometry and appearance, represented\nusing 3D Gaussian splats. Extensive experiments on both real and synthetic\ndatasets demonstrate that our method unifies single- and multi-image 3D human\nreconstruction, achieving high-fidelity and animatable 3D human avatars without\nrequiring camera and human pose annotations. Code and models will be released\nto the public."}
{"id": "2506.12040", "pdf": "https://arxiv.org/pdf/2506.12040", "abs": "https://arxiv.org/abs/2506.12040", "authors": ["Hao Gu", "Lujun Li", "Zheyu Wang", "Bei Liu", "Qiyuan Zhu", "Sirui Han", "Yike Guo"], "title": "BTC-LLM: Efficient Sub-1-Bit LLM Quantization via Learnable Transformation and Binary Codebook", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Binary quantization represents the most extreme form of large language model\n(LLM) compression, reducing weights to $\\pm$1 for maximal memory and\ncomputational efficiency. While recent sparsity-aware binarization methods\nachieve sub-1-bit compression by pruning redundant binary weights, they suffer\nfrom three critical challenges: performance deterioration, computational\ncomplexity from sparse mask management, and limited hardware compatibility. In\nthis paper, we present BTC-LLM, a novel sub-1-bit LLM quantization framework\nthat leverages adaptive weight transformation and binary pattern clustering to\novercome these limitations, delivering both superior accuracy and efficiency.\nOur approach incorporates two key innovations: (1) a Learnable Transformation\nthat optimizes invertible scaling and rotation matrices to align binarized\nweights with full-precision distributions, enabling incoherence processing to\nenhance layer-wise representation quality; (2) a Flash and Accurate Binary\nCodebook that identifies recurring binary vector clusters, compressing them\ninto compact indices with tailored distance metrics and sign-based centroid\nupdates. This eliminates the need for sparse masks, enabling efficient\ninference on standard hardware. Our code is available at\nhttps://github.com/Chooovy/BTC-LLM."}
{"id": "2506.12041", "pdf": "https://arxiv.org/pdf/2506.12041", "abs": "https://arxiv.org/abs/2506.12041", "authors": ["Yewei Liu", "Xiyuan Wang", "Muhan Zhang"], "title": "Meta Pruning via Graph Metanetworks : A Meta Learning Framework for Network Pruning", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Network pruning, aimed at reducing network size while preserving accuracy,\nhas attracted significant research interest. Numerous pruning techniques have\nbeen proposed over time. They are becoming increasingly effective, but more\ncomplex and harder to interpret as well. Given the inherent complexity of\nneural networks, we argue that manually designing pruning criteria has reached\na bottleneck. To address this, we propose a novel approach in which we \"use a\nneural network to prune neural networks\". More specifically, we introduce the\nnewly developed idea of metanetwork from meta-learning into pruning. A\nmetanetwork is a network that takes another network as input and produces a\nmodified network as output. In this paper, we first establish a bijective\nmapping between neural networks and graphs, and then employ a graph neural\nnetwork as our metanetwork. We train a metanetwork that learns the pruning\nstrategy automatically which can transform a network that is hard to prune into\nanother network that is much easier to prune. Once the metanetwork is trained,\nour pruning needs nothing more than a feedforward through the metanetwork and\nthe standard finetuning to prune at state-of-the-art. Our method achieved\noutstanding results on many popular and representative pruning tasks (including\nResNet56 on CIFAR10, VGG19 on CIFAR100, ResNet50 on ImageNet). Our code is\navailable at https://github.com/Yewei-Liu/MetaPruning"}
{"id": "2506.12106", "pdf": "https://arxiv.org/pdf/2506.12106", "abs": "https://arxiv.org/abs/2506.12106", "authors": ["André Ferreira", "Kunpeng Xie", "Caroline Wilpert", "Gustavo Correia", "Felix Barajas Ordonez", "Tiago Gil Oliveira", "Maike Bode", "Robert Siepmann", "Frank Hölzle", "Rainer Röhrig", "Jens Kleesiek", "Daniel Truhn", "Jan Egger", "Victor Alves", "Behrus Puladi"], "title": "Enhancing Privacy: The Utility of Stand-Alone Synthetic CT and MRI for Tumor and Bone Segmentation", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "AI requires extensive datasets, while medical data is subject to high data\nprotection. Anonymization is essential, but poses a challenge for some regions,\nsuch as the head, as identifying structures overlap with regions of clinical\ninterest. Synthetic data offers a potential solution, but studies often lack\nrigorous evaluation of realism and utility. Therefore, we investigate to what\nextent synthetic data can replace real data in segmentation tasks. We employed\nhead and neck cancer CT scans and brain glioma MRI scans from two large\ndatasets. Synthetic data were generated using generative adversarial networks\nand diffusion models. We evaluated the quality of the synthetic data using MAE,\nMS-SSIM, Radiomics and a Visual Turing Test (VTT) performed by 5 radiologists\nand their usefulness in segmentation tasks using DSC. Radiomics indicates high\nfidelity of synthetic MRIs, but fall short in producing highly realistic CT\ntissue, with correlation coefficient of 0.8784 and 0.5461 for MRI and CT\ntumors, respectively. DSC results indicate limited utility of synthetic data:\ntumor segmentation achieved DSC=0.064 on CT and 0.834 on MRI, while bone\nsegmentation a mean DSC=0.841. Relation between DSC and correlation is\nobserved, but is limited by the complexity of the task. VTT results show\nsynthetic CTs' utility, but with limited educational applications. Synthetic\ndata can be used independently for the segmentation task, although limited by\nthe complexity of the structures to segment. Advancing generative models to\nbetter tolerate heterogeneous inputs and learn subtle details is essential for\nenhancing their realism and expanding their application potential."}
{"id": "2506.12116", "pdf": "https://arxiv.org/pdf/2506.12116", "abs": "https://arxiv.org/abs/2506.12116", "authors": ["Phillipe R. Sampaio", "Helene Maxcici"], "title": "Unsupervised Document and Template Clustering using Multimodal Embeddings", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "17 pages, 10 figures", "summary": "This paper investigates a novel approach to unsupervised document clustering\nby leveraging multimodal embeddings as input to traditional clustering\nalgorithms such as $k$-Means and DBSCAN. Our method aims to achieve a\nfiner-grained document understanding by not only grouping documents at the type\nlevel (e.g., invoices, purchase orders), but also distinguishing between\ndifferent templates within the same document category. This is achieved by\nusing embeddings that capture textual content, layout information, and visual\nfeatures of documents. We evaluated the effectiveness of this approach using\nembeddings generated by several state-of-the-art pretrained multimodal models,\nincluding SBERT, LayoutLMv1, LayoutLMv3, DiT, Donut, and ColPali. Our findings\ndemonstrate the potential of multimodal embeddings to significantly enhance\ndocument clustering, offering benefits for various applications in intelligent\ndocument processing, document layout analysis, and unsupervised document\nclassification. This work provides valuable insight into the advantages and\nlimitations of different multimodal models for this task and opens new avenues\nfor future research to understand and organize document collections."}
{"id": "2506.12156", "pdf": "https://arxiv.org/pdf/2506.12156", "abs": "https://arxiv.org/abs/2506.12156", "authors": ["Shehroz S. Khan", "Ali Abedi", "Charlene H. Chu"], "title": "Explaining Recovery Trajectories of Older Adults Post Lower-Limb Fracture Using Modality-wise Multiview Clustering and Large Language Models", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "15 pages, 2 figures, 3 tables", "summary": "Interpreting large volumes of high-dimensional, unlabeled data in a manner\nthat is comprehensible to humans remains a significant challenge across various\ndomains. In unsupervised healthcare data analysis, interpreting clustered data\ncan offer meaningful insights into patients' health outcomes, which hold direct\nimplications for healthcare providers. This paper addresses the problem of\ninterpreting clustered sensor data collected from older adult patients\nrecovering from lower-limb fractures in the community. A total of 560 days of\nmultimodal sensor data, including acceleration, step count, ambient motion, GPS\nlocation, heart rate, and sleep, alongside clinical scores, were remotely\ncollected from patients at home. Clustering was first carried out separately\nfor each data modality to assess the impact of feature sets extracted from each\nmodality on patients' recovery trajectories. Then, using context-aware\nprompting, a large language model was employed to infer meaningful cluster\nlabels for the clusters derived from each modality. The quality of these\nclusters and their corresponding labels was validated through rigorous\nstatistical testing and visualization against clinical scores collected\nalongside the multimodal sensor data. The results demonstrated the statistical\nsignificance of most modality-specific cluster labels generated by the large\nlanguage model with respect to clinical scores, confirming the efficacy of the\nproposed method for interpreting sensor data in an unsupervised manner. This\nunsupervised data analysis approach, relying solely on sensor data, enables\nclinicians to identify at-risk patients and take timely measures to improve\nhealth outcomes."}
{"id": "2506.12184", "pdf": "https://arxiv.org/pdf/2506.12184", "abs": "https://arxiv.org/abs/2506.12184", "authors": ["Stanley Lewis", "Vishal Chandra", "Tom Gao", "Odest Chadwicke Jenkins"], "title": "SPLATART: Articulated Gaussian Splatting with Estimated Object Structure", "categories": ["cs.RO", "cs.CV"], "comment": "7 pages, Accepted to the 2025 RSS Workshop on Gaussian\n  Representations for Robot Autonomy. Contact: Stanley Lewis, stanlew@umich.edu", "summary": "Representing articulated objects remains a difficult problem within the field\nof robotics. Objects such as pliers, clamps, or cabinets require\nrepresentations that capture not only geometry and color information, but also\npart seperation, connectivity, and joint parametrization. Furthermore, learning\nthese representations becomes even more difficult with each additional degree\nof freedom. Complex articulated objects such as robot arms may have seven or\nmore degrees of freedom, and the depth of their kinematic tree may be notably\ngreater than the tools, drawers, and cabinets that are the typical subjects of\narticulated object research. To address these concerns, we introduce SPLATART -\na pipeline for learning Gaussian splat representations of articulated objects\nfrom posed images, of which a subset contains image space part segmentations.\nSPLATART disentangles the part separation task from the articulation estimation\ntask, allowing for post-facto determination of joint estimation and\nrepresentation of articulated objects with deeper kinematic trees than\npreviously exhibited. In this work, we present data on the SPLATART pipeline as\napplied to the syntheic Paris dataset objects, and qualitative results on a\nreal-world object under spare segmentation supervision. We additionally present\non articulated serial chain manipulators to demonstrate usage on deeper\nkinematic tree structures."}
{"id": "2506.12186", "pdf": "https://arxiv.org/pdf/2506.12186", "abs": "https://arxiv.org/abs/2506.12186", "authors": ["Haoyu Dong", "Yuwen Chen", "Hanxue Gu", "Nicholas Konz", "Yaqian Chen", "Qihang Li", "Maciej A. Mazurowski"], "title": "MRI-CORE: A Foundation Model for Magnetic Resonance Imaging", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": "19 pages, 5 figures", "summary": "The widespread use of Magnetic Resonance Imaging (MRI) and the rise of deep\nlearning have enabled the development of powerful predictive models for a wide\nrange of diagnostic tasks in MRI, such as image classification or object\nsegmentation. However, training models for specific new tasks often requires\nlarge amounts of labeled data, which is difficult to obtain due to high\nannotation costs and data privacy concerns. To circumvent this issue, we\nintroduce MRI-CORE (MRI COmprehensive Representation Encoder), a vision\nfoundation model pre-trained using more than 6 million slices from over 110,000\nMRI volumes across 18 main body locations. Experiments on five diverse object\nsegmentation tasks in MRI demonstrate that MRI-CORE can significantly improve\nsegmentation performance in realistic scenarios with limited labeled data\navailability, achieving an average gain of 6.97% 3D Dice Coefficient using only\n10 annotated slices per task. We further demonstrate new model capabilities in\nMRI such as classification of image properties including body location,\nsequence type and institution, and zero-shot segmentation. These results\nhighlight the value of MRI-CORE as a generalist vision foundation model for\nMRI, potentially lowering the data annotation resource barriers for many\napplications."}
{"id": "2506.12239", "pdf": "https://arxiv.org/pdf/2506.12239", "abs": "https://arxiv.org/abs/2506.12239", "authors": ["Jayjun Lee", "Nima Fazeli"], "title": "ViTaSCOPE: Visuo-tactile Implicit Representation for In-hand Pose and Extrinsic Contact Estimation", "categories": ["cs.RO", "cs.CV"], "comment": "Accepted to RSS 2025 | Project page:\n  https://jayjunlee.github.io/vitascope/", "summary": "Mastering dexterous, contact-rich object manipulation demands precise\nestimation of both in-hand object poses and external contact\nlocations$\\unicode{x2013}$tasks particularly challenging due to partial and\nnoisy observations. We present ViTaSCOPE: Visuo-Tactile Simultaneous Contact\nand Object Pose Estimation, an object-centric neural implicit representation\nthat fuses vision and high-resolution tactile feedback. By representing objects\nas signed distance fields and distributed tactile feedback as neural shear\nfields, ViTaSCOPE accurately localizes objects and registers extrinsic contacts\nonto their 3D geometry as contact fields. Our method enables seamless reasoning\nover complementary visuo-tactile cues by leveraging simulation for scalable\ntraining and zero-shot transfers to the real-world by bridging the sim-to-real\ngap. We evaluate our method through comprehensive simulated and real-world\nexperiments, demonstrating its capabilities in dexterous manipulation\nscenarios."}
{"id": "2506.12269", "pdf": "https://arxiv.org/pdf/2506.12269", "abs": "https://arxiv.org/abs/2506.12269", "authors": ["Babak Naderi", "Ross Cutler", "Juhee Cho", "Nabakumar Khongbantabam", "Dejan Ivkovic"], "title": "ICME 2025 Grand Challenge on Video Super-Resolution for Video Conferencing", "categories": ["eess.IV", "cs.CV", "cs.MM"], "comment": null, "summary": "Super-Resolution (SR) is a critical task in computer vision, focusing on\nreconstructing high-resolution (HR) images from low-resolution (LR) inputs. The\nfield has seen significant progress through various challenges, particularly in\nsingle-image SR. Video Super-Resolution (VSR) extends this to the temporal\ndomain, aiming to enhance video quality using methods like local, uni-,\nbi-directional propagation, or traditional upscaling followed by restoration.\nThis challenge addresses VSR for conferencing, where LR videos are encoded with\nH.265 at fixed QPs. The goal is to upscale videos by a specific factor,\nproviding HR outputs with enhanced perceptual quality under a low-delay\nscenario using causal models. The challenge included three tracks:\ngeneral-purpose videos, talking head videos, and screen content videos, with\nseparate datasets provided by the organizers for training, validation, and\ntesting. We open-sourced a new screen content dataset for the SR task in this\nchallenge. Submissions were evaluated through subjective tests using a\ncrowdsourced implementation of the ITU-T Rec P.910."}
{"id": "2506.12344", "pdf": "https://arxiv.org/pdf/2506.12344", "abs": "https://arxiv.org/abs/2506.12344", "authors": ["Haoyu Zhai", "Shuo Wang", "Pirouz Naghavi", "Qingying Hao", "Gang Wang"], "title": "Restoring Gaussian Blurred Face Images for Deanonymization Attacks", "categories": ["cs.CR", "cs.CV"], "comment": "18 pages, 16 figures, IEEE Transaction format", "summary": "Gaussian blur is widely used to blur human faces in sensitive photos before\nthe photos are posted on the Internet. However, it is unclear to what extent\nthe blurred faces can be restored and used to re-identify the person,\nespecially under a high-blurring setting. In this paper, we explore this\nquestion by developing a deblurring method called Revelio. The key intuition is\nto leverage a generative model's memorization effect and approximate the\ninverse function of Gaussian blur for face restoration. Compared with existing\nmethods, we design the deblurring process to be identity-preserving. It uses a\nconditional Diffusion model for preliminary face restoration and then uses an\nidentity retrieval model to retrieve related images to further enhance\nfidelity. We evaluate Revelio with large public face image datasets and show\nthat it can effectively restore blurred faces, especially under a high-blurring\nsetting. It has a re-identification accuracy of 95.9%, outperforming existing\nsolutions. The result suggests that Gaussian blur should not be used for face\nanonymization purposes. We also demonstrate the robustness of this method\nagainst mismatched Gaussian kernel sizes and functions, and test preliminary\ncountermeasures and adaptive attacks to inspire future work."}
{"id": "2506.12348", "pdf": "https://arxiv.org/pdf/2506.12348", "abs": "https://arxiv.org/abs/2506.12348", "authors": ["Zaiqiang Wu", "I-Chao Shen", "Takeo Igarashi"], "title": "Real-Time Per-Garment Virtual Try-On with Temporal Consistency for Loose-Fitting Garments", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Per-garment virtual try-on methods collect garment-specific datasets and\ntrain networks tailored to each garment to achieve superior results. However,\nthese approaches often struggle with loose-fitting garments due to two key\nlimitations: (1) They rely on human body semantic maps to align garments with\nthe body, but these maps become unreliable when body contours are obscured by\nloose-fitting garments, resulting in degraded outcomes; (2) They train garment\nsynthesis networks on a per-frame basis without utilizing temporal information,\nleading to noticeable jittering artifacts. To address these challenges, we\npropose a two-stage approach for robust semantic map estimation. First, we\nextract a garment-invariant representation from the raw input image. This\nrepresentation is then passed through an auxiliary network to estimate the\nsemantic map. This enhances the robustness of semantic map estimation under\nloose-fitting garments during garment-specific dataset generation. Furthermore,\nwe introduce a recurrent garment synthesis framework that incorporates temporal\ndependencies to improve frame-to-frame coherence while maintaining real-time\nperformance. We conducted qualitative and quantitative evaluations to\ndemonstrate that our method outperforms existing approaches in both image\nquality and temporal coherence. Ablation studies further validate the\neffectiveness of the garment-invariant representation and the recurrent\nsynthesis framework."}
{"id": "2506.12364", "pdf": "https://arxiv.org/pdf/2506.12364", "abs": "https://arxiv.org/abs/2506.12364", "authors": ["Mingjun Xu", "Jinhan Dong", "Jue Hou", "Zehui Wang", "Sihang Li", "Zhifeng Gao", "Renxin Zhong", "Hengxing Cai"], "title": "MM-R5: MultiModal Reasoning-Enhanced ReRanker via Reinforcement Learning for Document Retrieval", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Multimodal document retrieval systems enable information access across text,\nimages, and layouts, benefiting various domains like document-based question\nanswering, report analysis, and interactive content summarization. Rerankers\nimprove retrieval precision by reordering retrieved candidates. However,\ncurrent multimodal reranking methods remain underexplored, with significant\nroom for improvement in both training strategies and overall effectiveness.\nMoreover, the lack of explicit reasoning makes it difficult to analyze and\noptimize these methods further. In this paper, We propose MM-R5, a MultiModal\nReasoning-Enhanced ReRanker via Reinforcement Learning for Document Retrieval,\naiming to provide a more effective and reliable solution for multimodal\nreranking tasks. MM-R5 is trained in two stages: supervised fine-tuning (SFT)\nand reinforcement learning (RL). In the SFT stage, we focus on improving\ninstruction-following and guiding the model to generate complete and\nhigh-quality reasoning chains. To support this, we introduce a novel data\nconstruction strategy that produces rich, high-quality reasoning data. In the\nRL stage, we design a task-specific reward framework, including a reranking\nreward tailored for multimodal candidates and a composite template-based reward\nto further refine reasoning quality. We conduct extensive experiments on\nMMDocIR, a challenging public benchmark spanning multiple domains. MM-R5\nachieves state-of-the-art performance on most metrics and delivers comparable\nresults to much larger models on the remaining ones. Moreover, compared to the\nbest retrieval-only method, MM-R5 improves recall@1 by over 4%. These results\nvalidate the effectiveness of our reasoning-enhanced training pipeline."}
{"id": "2506.12375", "pdf": "https://arxiv.org/pdf/2506.12375", "abs": "https://arxiv.org/abs/2506.12375", "authors": ["Stan Muñoz Gutiérrez", "Franz Wotawa"], "title": "Optimized Spectral Fault Receptive Fields for Diagnosis-Informed Prognosis", "categories": ["cs.NE", "cs.AI", "cs.CV", "cs.LG"], "comment": "Submitted to The 36th International Conference on Principles of\n  Diagnosis and Resilient Systems (DX'25)", "summary": "This paper introduces Spectral Fault Receptive Fields (SFRFs), a biologically\ninspired technique for degradation state assessment in bearing fault diagnosis\nand remaining useful life (RUL) estimation. Drawing on the center-surround\norganization of retinal ganglion cell receptive fields, we propose a\nfrequency-domain feature extraction algorithm that enhances the detection of\nfault signatures in vibration signals. SFRFs are designed as antagonistic\nspectral filters centered on characteristic fault frequencies, with inhibitory\nsurrounds that enable robust characterization of incipient faults under\nvariable operating conditions. A multi-objective evolutionary optimization\nstrategy based on NSGA-II algorithm is employed to tune the receptive field\nparameters by simultaneously minimizing RUL prediction error, maximizing\nfeature monotonicity, and promoting smooth degradation trajectories. The method\nis demonstrated on the XJTU-SY bearing run-to-failure dataset, confirming its\nsuitability for constructing condition indicators in health monitoring\napplications. Key contributions include: (i) the introduction of SFRFs,\ninspired by the biology of vision in the primate retina; (ii) an evolutionary\noptimization framework guided by condition monitoring and prognosis criteria;\nand (iii) experimental evidence supporting the detection of early-stage faults\nand their precursors. Furthermore, we confirm that our diagnosis-informed\nspectral representation achieves accurate RUL prediction using a bagging\nregressor. The results highlight the interpretability and principled design of\nSFRFs, bridging signal processing, biological sensing principles, and\ndata-driven prognostics in rotating machinery."}
{"id": "2506.12395", "pdf": "https://arxiv.org/pdf/2506.12395", "abs": "https://arxiv.org/abs/2506.12395", "authors": ["Minghui Zhang", "Yaoyu Liu", "Xin You", "Hanxiao Zhang", "Yun Gu"], "title": "Shape-aware Sampling Matters in the Modeling of Multi-Class Tubular Structures", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Accurate multi-class tubular modeling is critical for precise lesion\nlocalization and optimal treatment planning. Deep learning methods enable\nautomated shape modeling by prioritizing volumetric overlap accuracy. However,\nthe inherent complexity of fine-grained semantic tubular shapes is not fully\nemphasized by overlap accuracy, resulting in reduced topological preservation.\nTo address this, we propose the Shapeaware Sampling (SAS), which optimizes\npatchsize allocation for online sampling and extracts a topology-preserved\nskeletal representation for the objective function. Fractal Dimension-based\nPatchsize (FDPS) is first introduced to quantify semantic tubular shape\ncomplexity through axis-specific fractal dimension analysis. Axes with higher\nfractal complexity are then sampled with smaller patchsizes to capture\nfine-grained features and resolve structural intricacies. In addition, Minimum\nPath-Cost Skeletonization (MPC-Skel) is employed to sample topologically\nconsistent skeletal representations of semantic tubular shapes for\nskeleton-weighted objective functions. MPC-Skel reduces artifacts from\nconventional skeletonization methods and directs the focus to critical\ntopological regions, enhancing tubular topology preservation. SAS is\ncomputationally efficient and easily integrable into optimization pipelines.\nEvaluation on two semantic tubular datasets showed consistent improvements in\nboth volumetric overlap and topological integrity metrics."}
{"id": "2506.12411", "pdf": "https://arxiv.org/pdf/2506.12411", "abs": "https://arxiv.org/abs/2506.12411", "authors": ["Mengyuan Sun", "Yu Li", "Yuchen Liu", "Bo Du", "Yunjie Ge"], "title": "InverTune: Removing Backdoors from Multimodal Contrastive Learning Models via Trigger Inversion and Activation Tuning", "categories": ["cs.CR", "cs.CV"], "comment": null, "summary": "Multimodal contrastive learning models like CLIP have demonstrated remarkable\nvision-language alignment capabilities, yet their vulnerability to backdoor\nattacks poses critical security risks. Attackers can implant latent triggers\nthat persist through downstream tasks, enabling malicious control of model\nbehavior upon trigger presentation. Despite great success in recent defense\nmechanisms, they remain impractical due to strong assumptions about attacker\nknowledge or excessive clean data requirements. In this paper, we introduce\nInverTune, the first backdoor defense framework for multimodal models under\nminimal attacker assumptions, requiring neither prior knowledge of attack\ntargets nor access to the poisoned dataset. Unlike existing defense methods\nthat rely on the same dataset used in the poisoning stage, InverTune\neffectively identifies and removes backdoor artifacts through three key\ncomponents, achieving robust protection against backdoor attacks. Specifically,\nInverTune first exposes attack signatures through adversarial simulation,\nprobabilistically identifying the target label by analyzing model response\npatterns. Building on this, we develop a gradient inversion technique to\nreconstruct latent triggers through activation pattern analysis. Finally, a\nclustering-guided fine-tuning strategy is employed to erase the backdoor\nfunction with only a small amount of arbitrary clean data, while preserving the\noriginal model capabilities. Experimental results show that InverTune reduces\nthe average attack success rate (ASR) by 97.87% against the state-of-the-art\n(SOTA) attacks while limiting clean accuracy (CA) degradation to just 3.07%.\nThis work establishes a new paradigm for securing multimodal systems, advancing\nsecurity in foundation model deployment without compromising performance."}
{"id": "2506.12430", "pdf": "https://arxiv.org/pdf/2506.12430", "abs": "https://arxiv.org/abs/2506.12430", "authors": ["Zonghao Ying", "Siyang Wu", "Run Hao", "Peng Ying", "Shixuan Sun", "Pengyu Chen", "Junze Chen", "Hao Du", "Kaiwen Shen", "Shangkun Wu", "Jiwei Wei", "Shiyuan He", "Yang Yang", "Xiaohai Xu", "Ke Ma", "Qianqian Xu", "Qingming Huang", "Shi Lin", "Xun Wang", "Changting Lin", "Meng Han", "Yilei Jiang", "Siqi Lai", "Yaozhi Zheng", "Yifei Song", "Xiangyu Yue", "Zonglei Jing", "Tianyuan Zhang", "Zhilei Zhu", "Aishan Liu", "Jiakai Wang", "Siyuan Liang", "Xianglong Kong", "Hainan Li", "Junjie Mu", "Haotong Qin", "Yue Yu", "Lei Chen", "Felix Juefei-Xu", "Qing Guo", "Xinyun Chen", "Yew Soon Ong", "Xianglong Liu", "Dawn Song", "Alan Yuille", "Philip Torr", "Dacheng Tao"], "title": "Pushing the Limits of Safety: A Technical Report on the ATLAS Challenge 2025", "categories": ["cs.CR", "cs.CV"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have enabled transformative\nadvancements across diverse applications but remain susceptible to safety\nthreats, especially jailbreak attacks that induce harmful outputs. To\nsystematically evaluate and improve their safety, we organized the Adversarial\nTesting & Large-model Alignment Safety Grand Challenge (ATLAS) 2025}. This\ntechnical report presents findings from the competition, which involved 86\nteams testing MLLM vulnerabilities via adversarial image-text attacks in two\nphases: white-box and black-box evaluations. The competition results highlight\nongoing challenges in securing MLLMs and provide valuable guidance for\ndeveloping stronger defense mechanisms. The challenge establishes new\nbenchmarks for MLLM safety evaluation and lays groundwork for advancing safer\nmultimodal AI systems. The code and data for this challenge are openly\navailable at https://github.com/NY1024/ATLAS_Challenge_2025."}
{"id": "2506.12440", "pdf": "https://arxiv.org/pdf/2506.12440", "abs": "https://arxiv.org/abs/2506.12440", "authors": ["Federico Simonetta"], "title": "Style-based Composer Identification and Attribution of Symbolic Music Scores: a Systematic Survey", "categories": ["cs.SD", "cs.AI", "cs.CV", "cs.DL", "eess.AS"], "comment": "Accepted at the TISMIR", "summary": "This paper presents the first comprehensive systematic review of literature\non style-based composer identification and authorship attribution in symbolic\nmusic scores. Addressing the critical need for improved reliability and\nreproducibility in this field, the review rigorously analyzes 58 peer-reviewed\npapers published across various historical periods, with the search adapted to\nevolving terminology. The analysis critically assesses prevailing repertoires,\ncomputational approaches, and evaluation methodologies, highlighting\nsignificant challenges. It reveals that a substantial portion of existing\nresearch suffers from inadequate validation protocols and an over-reliance on\nsimple accuracy metrics for often imbalanced datasets, which can undermine the\ncredibility of attribution claims. The crucial role of robust metrics like\nBalanced Accuracy and rigorous cross-validation in ensuring trustworthy results\nis emphasized. The survey also details diverse feature representations and the\nevolution of machine learning models employed. Notable real-world authorship\nattribution cases, such as those involving works attributed to Bach, Josquin\nDesprez, and Lennon-McCartney, are specifically discussed, illustrating the\nopportunities and pitfalls of applying computational techniques to resolve\ndisputed musical provenance. Based on these insights, a set of actionable\nguidelines for future research are proposed. These recommendations are designed\nto significantly enhance the reliability, reproducibility, and musicological\nvalidity of composer identification and authorship attribution studies,\nfostering more robust and interpretable computational stylistic analysis."}
{"id": "2506.12471", "pdf": "https://arxiv.org/pdf/2506.12471", "abs": "https://arxiv.org/abs/2506.12471", "authors": ["Hyoung Suk Park", "Kiwan Jeon"], "title": "Adaptive Multi-resolution Hash-Encoding Framework for INR-based Dental CBCT Reconstruction with Truncated FOV", "categories": ["eess.IV", "cs.CV", "68Wxx"], "comment": "18 pages, 4 figures", "summary": "Implicit neural representation (INR), particularly in combination with hash\nencoding, has recently emerged as a promising approach for computed tomography\n(CT) image reconstruction. However, directly applying INR techniques to 3D\ndental cone-beam CT (CBCT) with a truncated field of view (FOV) is challenging.\nDuring the training process, if the FOV does not fully encompass the patient's\nhead, a discrepancy arises between the measured projections and the forward\nprojections computed within the truncated domain. This mismatch leads the\nnetwork to estimate attenuation values inaccurately, producing severe artifacts\nin the reconstructed images. In this study, we propose a computationally\nefficient INR-based reconstruction framework that leverages multi-resolution\nhash encoding for 3D dental CBCT with a truncated FOV. To mitigate truncation\nartifacts, we train the network over an expanded reconstruction domain that\nfully encompasses the patient's head. For computational efficiency, we adopt an\nadaptive training strategy that uses a multi-resolution grid: finer resolution\nlevels and denser sampling inside the truncated FOV, and coarser resolution\nlevels with sparser sampling outside. To maintain consistent input\ndimensionality of the network across spatially varying resolutions, we\nintroduce an adaptive hash encoder that selectively activates the lower-level\nfeatures of the hash hierarchy for points outside the truncated FOV. The\nproposed method with an extended FOV effectively mitigates truncation\nartifacts. Compared with a naive domain extension using fixed resolution levels\nand a fixed sampling rate, the adaptive strategy reduces computational time by\nover 60% for an image volume of 800x800x600, while preserving the PSNR within\nthe truncated FOV."}
{"id": "2506.12475", "pdf": "https://arxiv.org/pdf/2506.12475", "abs": "https://arxiv.org/abs/2506.12475", "authors": ["Fangwei Hao", "Ji Du", "Desheng Kong", "Jiesheng Wu", "Jing Xu", "Ping Li"], "title": "Efficient Star Distillation Attention Network for Lightweight Image Super-Resolution", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "In recent years, the performance of lightweight Single-Image Super-Resolution\n(SISR) has been improved significantly with the application of Convolutional\nNeural Networks (CNNs) and Large Kernel Attention (LKA). However, existing\ninformation distillation modules for lightweight SISR struggle to map inputs\ninto High-Dimensional Non-Linear (HDNL) feature spaces, limiting their\nrepresentation learning. And their LKA modules possess restricted ability to\ncapture the multi-shape multi-scale information for long-range dependencies\nwhile encountering a quadratic increase in the computational burden with\nincreasing convolutional kernel size of its depth-wise convolutional layer. To\naddress these issues, we firstly propose a Star Distillation Module (SDM) to\nenhance the discriminative representation learning via information distillation\nin the HDNL feature spaces. Besides, we present a Multi-shape Multi-scale Large\nKernel Attention (MM-LKA) module to learn representative long-range\ndependencies while incurring low computational and memory footprints, leading\nto improving the performance of CNN-based self-attention significantly.\nIntegrating SDM and MM-LKA, we develop a Residual Star Distillation Attention\nModule (RSDAM) and take it as the building block of the proposed efficient Star\nDistillation Attention Network (SDAN) which possesses high reconstruction\nefficiency to recover a higher-quality image from the corresponding\nlow-resolution (LR) counterpart. When compared with other lightweight\nstate-of-the-art SISR methods, extensive experiments show that our SDAN with\nlow model complexity yields superior performance quantitatively and visually."}
{"id": "2506.12479", "pdf": "https://arxiv.org/pdf/2506.12479", "abs": "https://arxiv.org/abs/2506.12479", "authors": ["Hongjun An", "Sida Huang", "Siqi Huang", "Ruanjun Li", "Yuanzhi Liang", "Jiawei Shao", "Zihan Wang", "Cheng Yuan", "Chi Zhang", "Hongyuan Zhang", "Wenhao Zhuang", "Xuelong Li"], "title": "AI Flow: Perspectives, Scenarios, and Approaches", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.DC", "eess.SP"], "comment": "Authors are with Institute of Artificial Intelligence (TeleAI), China\n  Telecom, China. Author names are listed alphabetically by surname. This work\n  was conducted at TeleAI, facilitated by Dr. Jiawei Shao (e-mail:\n  shaojw2@chinatelecom.cn) under the leadership of Prof. Xuelong Li. The\n  corresponding author is Prof. Xuelong Li (e-mail: xuelong li@ieee.org), the\n  CTO and Chief Scientist of China Telecom", "summary": "Pioneered by the foundational information theory by Claude Shannon and the\nvisionary framework of machine intelligence by Alan Turing, the convergent\nevolution of information and communication technologies (IT/CT) has created an\nunbroken wave of connectivity and computation. This synergy has sparked a\ntechnological revolution, now reaching its peak with large artificial\nintelligence (AI) models that are reshaping industries and redefining\nhuman-machine collaboration. However, the realization of ubiquitous\nintelligence faces considerable challenges due to substantial resource\nconsumption in large models and high communication bandwidth demands. To\naddress these challenges, AI Flow has been introduced as a multidisciplinary\nframework that integrates cutting-edge IT and CT advancements, with a\nparticular emphasis on the following three key points. First, device-edge-cloud\nframework serves as the foundation, which integrates end devices, edge servers,\nand cloud clusters to optimize scalability and efficiency for low-latency model\ninference. Second, we introduce the concept of familial models, which refers to\na series of different-sized models with aligned hidden features, enabling\neffective collaboration and the flexibility to adapt to varying resource\nconstraints and dynamic scenarios. Third, connectivity- and interaction-based\nintelligence emergence is a novel paradigm of AI Flow. By leveraging\ncommunication networks to enhance connectivity, the collaboration among AI\nmodels across heterogeneous nodes achieves emergent intelligence that surpasses\nthe capability of any single model. The innovations of AI Flow provide enhanced\nintelligence, timely responsiveness, and ubiquitous accessibility to AI\nservices, paving the way for the tighter fusion of AI techniques and\ncommunication systems."}
{"id": "2506.12541", "pdf": "https://arxiv.org/pdf/2506.12541", "abs": "https://arxiv.org/abs/2506.12541", "authors": ["Catalin E. Brita", "Hieu Nguyen", "Lohithsai Yadala Chanchu", "Domonkos Nagy", "Maksim Zhdanov"], "title": "BSA: Ball Sparse Attention for Large-scale Geometries", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "Long Context Foundation Models Workshop @ ICML 2025", "summary": "Self-attention scales quadratically with input size, limiting its use for\nlarge-scale physical systems. Although sparse attention mechanisms provide a\nviable alternative, they are primarily designed for regular structures such as\ntext or images, making them inapplicable for irregular geometries. In this\nwork, we present Ball Sparse Attention (BSA), which adapts Native Sparse\nAttention (NSA) (Yuan et al., 2025) to unordered point sets by imposing\nregularity using the Ball Tree structure from the Erwin Transformer (Zhdanov et\nal., 2025). We modify NSA's components to work with ball-based neighborhoods,\nyielding a global receptive field at sub-quadratic cost. On an airflow pressure\nprediction task, we achieve accuracy comparable to Full Attention while\nsignificantly reducing the theoretical computational complexity. Our\nimplementation is available at https://github.com/britacatalin/bsa."}
{"id": "2506.12542", "pdf": "https://arxiv.org/pdf/2506.12542", "abs": "https://arxiv.org/abs/2506.12542", "authors": ["Ejafa Bassam", "Dawei Zhu", "Kaigui Bian"], "title": "PLD: A Choice-Theoretic List-Wise Knowledge Distillation", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "comment": null, "summary": "Knowledge distillation is a model compression technique in which a compact\n\"student\" network is trained to replicate the predictive behavior of a larger\n\"teacher\" network. In logit-based knowledge distillation it has become the de\nfacto approach to augment cross-entropy with a distillation term. Typically\nthis term is either a KL divergence-matching marginal probabilities or a\ncorrelation-based loss capturing intra- and inter-class relationships but in\nevery case it sits as an add-on to cross-entropy with its own weight that must\nbe carefully tuned. In this paper we adopt a choice-theoretic perspective and\nrecast knowledge distillation under the Plackett-Luce model by interpreting\nteacher logits as \"worth\" scores. We introduce Plackett-Luce Distillation\n(PLD), a weighted list-wise ranking loss in which the teacher model transfers\nknowledge of its full ranking of classes, weighting each ranked choice by its\nown confidence. PLD directly optimizes a single teacher-optimal ranking of the\ntrue label first, followed by the remaining classes in descending teacher\nconfidence, yielding a convex, translation-invariant surrogate that subsumes\nweighted cross-entropy. Empirically on standard image classification\nbenchmarks, PLD improves Top-1 accuracy by an average of +0.42% over DIST\n(arXiv:2205.10536) and +1.04% over KD (arXiv:1503.02531) in homogeneous\nsettings and by +0.48% and +1.09% over DIST and KD, respectively, in\nheterogeneous settings."}
{"id": "2506.12678", "pdf": "https://arxiv.org/pdf/2506.12678", "abs": "https://arxiv.org/abs/2506.12678", "authors": ["Pranay Gupta", "Henny Admoni", "Andrea Bajcsy"], "title": "Adapting by Analogy: OOD Generalization of Visuomotor Policies via Functional Correspondence", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": "15 pages, 11 figures", "summary": "End-to-end visuomotor policies trained using behavior cloning have shown a\nremarkable ability to generate complex, multi-modal low-level robot behaviors.\nHowever, at deployment time, these policies still struggle to act reliably when\nfaced with out-of-distribution (OOD) visuals induced by objects, backgrounds,\nor environment changes. Prior works in interactive imitation learning solicit\ncorrective expert demonstrations under the OOD conditions -- but this can be\ncostly and inefficient. We observe that task success under OOD conditions does\nnot always warrant novel robot behaviors. In-distribution (ID) behaviors can\ndirectly be transferred to OOD conditions that share functional similarities\nwith ID conditions. For example, behaviors trained to interact with\nin-distribution (ID) pens can apply to interacting with a visually-OOD pencil.\nThe key challenge lies in disambiguating which ID observations functionally\ncorrespond to the OOD observation for the task at hand. We propose that an\nexpert can provide this OOD-to-ID functional correspondence. Thus, instead of\ncollecting new demonstrations and re-training at every OOD encounter, our\nmethod: (1) detects the need for feedback by first checking if current\nobservations are OOD and then identifying whether the most similar training\nobservations show divergent behaviors, (2) solicits functional correspondence\nfeedback to disambiguate between those behaviors, and (3) intervenes on the OOD\nobservations with the functionally corresponding ID observations to perform\ndeployment-time generalization. We validate our method across diverse\nreal-world robotic manipulation tasks with a Franka Panda robotic manipulator.\nOur results show that test-time functional correspondences can improve the\ngeneralization of a vision-based diffusion policy to OOD objects and\nenvironment conditions with low feedback."}
{"id": "2506.12693", "pdf": "https://arxiv.org/pdf/2506.12693", "abs": "https://arxiv.org/abs/2506.12693", "authors": ["Ali Zafari", "Xi Chen", "Shirin Jalali"], "title": "Zero-shot denoising via neural compression: Theoretical and algorithmic framework", "categories": ["eess.IV", "cs.CV", "cs.IT", "math.IT"], "comment": null, "summary": "Zero-shot denoising aims to denoise observations without access to training\nsamples or clean reference images. This setting is particularly relevant in\npractical imaging scenarios involving specialized domains such as medical\nimaging or biology. In this work, we propose the Zero-Shot Neural Compression\nDenoiser (ZS-NCD), a novel denoising framework based on neural compression.\nZS-NCD treats a neural compression network as an untrained model, optimized\ndirectly on patches extracted from a single noisy image. The final\nreconstruction is then obtained by aggregating the outputs of the trained model\nover overlapping patches. Thanks to the built-in entropy constraints of\ncompression architectures, our method naturally avoids overfitting and does not\nrequire manual regularization or early stopping. Through extensive experiments,\nwe show that ZS-NCD achieves state-of-the-art performance among zero-shot\ndenoisers for both Gaussian and Poisson noise, and generalizes well to both\nnatural and non-natural images. Additionally, we provide new finite-sample\ntheoretical results that characterize upper bounds on the achievable\nreconstruction error of general maximum-likelihood compression-based denoisers.\nThese results further establish the theoretical foundations of\ncompression-based denoising. Our code is available at:\ngithub.com/Computational-Imaging-RU/ZS-NCDenoiser."}
{"id": "2506.12719", "pdf": "https://arxiv.org/pdf/2506.12719", "abs": "https://arxiv.org/abs/2506.12719", "authors": ["Hu Xu", "Yang Jingling", "Jia Sihan", "Bi Yuda", "Calhoun Vince"], "title": "GM-LDM: Latent Diffusion Model for Brain Biomarker Identification through Functional Data-Driven Gray Matter Synthesis", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Generative models based on deep learning have shown significant potential in\nmedical imaging, particularly for modality transformation and multimodal fusion\nin MRI-based brain imaging. This study introduces GM-LDM, a novel framework\nthat leverages the latent diffusion model (LDM) to enhance the efficiency and\nprecision of MRI generation tasks. GM-LDM integrates a 3D autoencoder,\npre-trained on the large-scale ABCD MRI dataset, achieving statistical\nconsistency through KL divergence loss. We employ a Vision Transformer\n(ViT)-based encoder-decoder as the denoising network to optimize generation\nquality. The framework flexibly incorporates conditional data, such as\nfunctional network connectivity (FNC) data, enabling personalized brain\nimaging, biomarker identification, and functional-to-structural information\ntranslation for brain diseases like schizophrenia."}
{"id": "2506.12798", "pdf": "https://arxiv.org/pdf/2506.12798", "abs": "https://arxiv.org/abs/2506.12798", "authors": ["Garima Jain", "Ravi Kant Gupta", "Priyansh Jain", "Abhijeet Patil", "Ardhendu Sekhar", "Gajendra Smeeta", "Sanghamitra Pati", "Amit Sethi"], "title": "Predicting Genetic Mutations from Single-Cell Bone Marrow Images in Acute Myeloid Leukemia Using Noise-Robust Deep Learning Models", "categories": ["eess.IV", "cs.CV"], "comment": "2 figues", "summary": "In this study, we propose a robust methodology for identification of myeloid\nblasts followed by prediction of genetic mutation in single-cell images of\nblasts, tackling challenges associated with label accuracy and data noise. We\ntrained an initial binary classifier to distinguish between leukemic (blasts)\nand non-leukemic cells images, achieving 90 percent accuracy. To evaluate the\nmodels generalization, we applied this model to a separate large unlabeled\ndataset and validated the predictions with two haemato-pathologists, finding an\napproximate error rate of 20 percent in the leukemic and non-leukemic labels.\nAssuming this level of label noise, we further trained a four-class model on\nimages predicted as blasts to classify specific mutations. The mutation labels\nwere known for only a bag of cell images extracted from a single slide. Despite\nthe tumor label noise, our mutation classification model achieved 85 percent\naccuracy across four mutation classes, demonstrating resilience to label\ninconsistencies. This study highlights the capability of machine learning\nmodels to work with noisy labels effectively while providing accurate,\nclinically relevant mutation predictions, which is promising for diagnostic\napplications in areas such as haemato-pathology."}
{"id": "2506.12847", "pdf": "https://arxiv.org/pdf/2506.12847", "abs": "https://arxiv.org/abs/2506.12847", "authors": ["Zhelun Shen", "Chenming Wu", "Junsheng Zhou", "Chen Zhao", "Kaisiyuan Wang", "Hang Zhou", "Yingying Li", "Haocheng Feng", "Wei He", "Jingdong Wang"], "title": "iDiT-HOI: Inpainting-based Hand Object Interaction Reenactment via Video Diffusion Transformer", "categories": ["cs.GR", "cs.CV"], "comment": "Technical report, 12 pages", "summary": "Digital human video generation is gaining traction in fields like education\nand e-commerce, driven by advancements in head-body animation and lip-syncing\ntechnologies. However, realistic Hand-Object Interaction (HOI) - the complex\ndynamics between human hands and objects - continues to pose challenges.\nGenerating natural and believable HOI reenactments is difficult due to issues\nsuch as occlusion between hands and objects, variations in object shapes and\norientations, and the necessity for precise physical interactions, and\nimportantly, the ability to generalize to unseen humans and objects. This paper\npresents a novel framework iDiT-HOI that enables in-the-wild HOI reenactment\ngeneration. Specifically, we propose a unified inpainting-based token process\nmethod, called Inp-TPU, with a two-stage video diffusion transformer (DiT)\nmodel. The first stage generates a key frame by inserting the designated object\ninto the hand region, providing a reference for subsequent frames. The second\nstage ensures temporal coherence and fluidity in hand-object interactions. The\nkey contribution of our method is to reuse the pretrained model's context\nperception capabilities without introducing additional parameters, enabling\nstrong generalization to unseen objects and scenarios, and our proposed\nparadigm naturally supports long video generation. Comprehensive evaluations\ndemonstrate that our approach outperforms existing methods, particularly in\nchallenging real-world scenes, offering enhanced realism and more seamless\nhand-object interactions."}
{"id": "2506.13045", "pdf": "https://arxiv.org/pdf/2506.13045", "abs": "https://arxiv.org/abs/2506.13045", "authors": ["Haiyang Guo", "Fanhu Zeng", "Fei Zhu", "Jiayi Wang", "Xukai Wang", "Jingang Zhou", "Hongbo Zhao", "Wenzhuo Liu", "Shijie Ma", "Xu-Yao Zhang", "Cheng-Lin Liu"], "title": "A Comprehensive Survey on Continual Learning in Generative Models", "categories": ["cs.LG", "cs.CV"], "comment": "Preprint", "summary": "The rapid advancement of generative models has enabled modern AI systems to\ncomprehend and produce highly sophisticated content, even achieving human-level\nperformance in specific domains. However, these models remain fundamentally\nconstrained by catastrophic forgetting - a persistent challenge where adapting\nto new tasks typically leads to significant degradation in performance on\npreviously learned tasks. To address this practical limitation, numerous\napproaches have been proposed to enhance the adaptability and scalability of\ngenerative models in real-world applications. In this work, we present a\ncomprehensive survey of continual learning methods for mainstream generative\nmodels, including large language models, multimodal large language models,\nvision language action models, and diffusion models. Drawing inspiration from\nthe memory mechanisms of the human brain, we systematically categorize these\napproaches into three paradigms: architecture-based, regularization-based, and\nreplay-based methods, while elucidating their underlying methodologies and\nmotivations. We further analyze continual learning setups for different\ngenerative models, including training objectives, benchmarks, and core\nbackbones, offering deeper insights into the field. The project page of this\npaper is available at\nhttps://github.com/Ghy0501/Awesome-Continual-Learning-in-Generative-Models."}
{"id": "2506.13050", "pdf": "https://arxiv.org/pdf/2506.13050", "abs": "https://arxiv.org/abs/2506.13050", "authors": ["Pengfei Wang", "Qiujie Dong", "Fangtian Liang", "Hao Pan", "Lei Yang", "Congyi Zhang", "Guying Lin", "Caiming Zhang", "Yuanfeng Zhou", "Changhe Tu", "Shiqing Xin", "Alla Sheffer", "Xin Li", "Wenping Wang"], "title": "NeuVAS: Neural Implicit Surfaces for Variational Shape Modeling", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Neural implicit shape representation has drawn significant attention in\nrecent years due to its smoothness, differentiability, and topological\nflexibility. However, directly modeling the shape of a neural implicit surface,\nespecially as the zero-level set of a neural signed distance function (SDF),\nwith sparse geometric control is still a challenging task. Sparse input shape\ncontrol typically includes 3D curve networks or, more generally, 3D curve\nsketches, which are unstructured and cannot be connected to form a curve\nnetwork, and therefore more difficult to deal with. While 3D curve networks or\ncurve sketches provide intuitive shape control, their sparsity and varied\ntopology pose challenges in generating high-quality surfaces to meet such curve\nconstraints. In this paper, we propose NeuVAS, a variational approach to shape\nmodeling using neural implicit surfaces constrained under sparse input shape\ncontrol, including unstructured 3D curve sketches as well as connected 3D curve\nnetworks. Specifically, we introduce a smoothness term based on a functional of\nsurface curvatures to minimize shape variation of the zero-level set surface of\na neural SDF. We also develop a new technique to faithfully model G0 sharp\nfeature curves as specified in the input curve sketches. Comprehensive\ncomparisons with the state-of-the-art methods demonstrate the significant\nadvantages of our method."}
{"id": "2506.13056", "pdf": "https://arxiv.org/pdf/2506.13056", "abs": "https://arxiv.org/abs/2506.13056", "authors": ["Haibo Qiu", "Xiaohan Lan", "Fanfan Liu", "Xiaohu Sun", "Delian Ruan", "Peng Shi", "Lin Ma"], "title": "Metis-RISE: RL Incentivizes and SFT Enhances Multimodal Reasoning Model Learning", "categories": ["cs.AI", "cs.CV", "cs.LG"], "comment": "Project Page: https://github.com/MM-Thinking/Metis-RISE", "summary": "Recent advancements in large language models (LLMs) have witnessed a surge in\nthe development of advanced reasoning paradigms, which are now being integrated\ninto multimodal large language models (MLLMs). However, existing approaches\noften fall short: methods solely employing reinforcement learning (RL) can\nstruggle with sample inefficiency and activating entirely absent reasoning\ncapabilities, while conventional pipelines that initiate with a cold-start\nsupervised fine-tuning (SFT) phase before RL may restrict the model's\nexploratory capacity and face suboptimal convergence. In this work, we\nintroduce \\textbf{Metis-RISE} (\\textbf{R}L \\textbf{I}ncentivizes and\n\\textbf{S}FT \\textbf{E}nhances) for multimodal reasoning model learning. Unlike\nconventional approaches, Metis-RISE distinctively omits an initial SFT stage,\nbeginning instead with an RL phase (e.g., using a Group Relative Policy\nOptimization variant) to incentivize and activate the model's latent reasoning\ncapacity. Subsequently, the targeted SFT stage addresses two key challenges\nidentified during RL: (1) \\textit{inefficient trajectory sampling} for tasks\nwhere the model possesses but inconsistently applies correct reasoning, which\nwe tackle using self-distilled reasoning trajectories from the RL model itself;\nand (2) \\textit{fundamental capability absence}, which we address by injecting\nexpert-augmented knowledge for prompts where the model entirely fails. This\nstrategic application of RL for incentivization followed by SFT for enhancement\nforms the core of Metis-RISE, leading to two versions of our MLLMs (7B and 72B\nparameters). Evaluations on the OpenCompass Multimodal Reasoning Leaderboard\ndemonstrate that both models achieve state-of-the-art performance among\nsimilar-sized models, with the 72B version ranking fourth overall."}
{"id": "2506.13100", "pdf": "https://arxiv.org/pdf/2506.13100", "abs": "https://arxiv.org/abs/2506.13100", "authors": ["Zhanhua Xin", "Zhihao Wang", "Shenghao Zhang", "Wanchao Chi", "Yan Meng", "Shihan Kong", "Yan Xiong", "Chong Zhang", "Yuzhen Liu", "Junzhi Yu"], "title": "A Novel ViDAR Device With Visual Inertial Encoder Odometry and Reinforcement Learning-Based Active SLAM Method", "categories": ["cs.RO", "cs.CV", "93C85", "I.4"], "comment": "12 pages, 13 figures", "summary": "In the field of multi-sensor fusion for simultaneous localization and mapping\n(SLAM), monocular cameras and IMUs are widely used to build simple and\neffective visual-inertial systems. However, limited research has explored the\nintegration of motor-encoder devices to enhance SLAM performance. By\nincorporating such devices, it is possible to significantly improve active\ncapability and field of view (FOV) with minimal additional cost and structural\ncomplexity. This paper proposes a novel visual-inertial-encoder tightly coupled\nodometry (VIEO) based on a ViDAR (Video Detection and Ranging) device. A ViDAR\ncalibration method is introduced to ensure accurate initialization for VIEO. In\naddition, a platform motion decoupled active SLAM method based on deep\nreinforcement learning (DRL) is proposed. Experimental data demonstrate that\nthe proposed ViDAR and the VIEO algorithm significantly increase cross-frame\nco-visibility relationships compared to its corresponding visual-inertial\nodometry (VIO) algorithm, improving state estimation accuracy. Additionally,\nthe DRL-based active SLAM algorithm, with the ability to decouple from platform\nmotion, can increase the diversity weight of the feature points and further\nenhance the VIEO algorithm's performance. The proposed methodology sheds fresh\ninsights into both the updated platform design and decoupled approach of active\nSLAM systems in complex environments."}
{"id": "2506.13160", "pdf": "https://arxiv.org/pdf/2506.13160", "abs": "https://arxiv.org/abs/2506.13160", "authors": ["Ting Qiao", "Yiming Li", "Jianbin Li", "Yingjia Wang", "Leyi Qi", "Junfeng Guo", "Ruili Feng", "Dacheng Tao"], "title": "CertDW: Towards Certified Dataset Ownership Verification via Conformal Prediction", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.CV"], "comment": "The first two authors contributed equally to this work. 16 pages", "summary": "Deep neural networks (DNNs) rely heavily on high-quality open-source datasets\n(e.g., ImageNet) for their success, making dataset ownership verification (DOV)\ncrucial for protecting public dataset copyrights. In this paper, we find\nexisting DOV methods (implicitly) assume that the verification process is\nfaithful, where the suspicious model will directly verify ownership by using\nthe verification samples as input and returning their results. However, this\nassumption may not necessarily hold in practice and their performance may\ndegrade sharply when subjected to intentional or unintentional perturbations.\nTo address this limitation, we propose the first certified dataset watermark\n(i.e., CertDW) and CertDW-based certified dataset ownership verification method\nthat ensures reliable verification even under malicious attacks, under certain\nconditions (e.g., constrained pixel-level perturbation). Specifically, inspired\nby conformal prediction, we introduce two statistical measures, including\nprincipal probability (PP) and watermark robustness (WR), to assess model\nprediction stability on benign and watermarked samples under noise\nperturbations. We prove there exists a provable lower bound between PP and WR,\nenabling ownership verification when a suspicious model's WR value\nsignificantly exceeds the PP values of multiple benign models trained on\nwatermark-free datasets. If the number of PP values smaller than WR exceeds a\nthreshold, the suspicious model is regarded as having been trained on the\nprotected dataset. Extensive experiments on benchmark datasets verify the\neffectiveness of our CertDW method and its resistance to potential adaptive\nattacks. Our codes are at\n\\href{https://github.com/NcepuQiaoTing/CertDW}{GitHub}."}
{"id": "2506.13187", "pdf": "https://arxiv.org/pdf/2506.13187", "abs": "https://arxiv.org/abs/2506.13187", "authors": ["Yibo Yang", "Sihao Liu", "Chuan Rao", "Bang An", "Tiancheng Shen", "Philip H. S. Torr", "Ming-Hsuan Yang", "Bernard Ghanem"], "title": "Dynamic Context-oriented Decomposition for Task-aware Low-rank Adaptation with Less Forgetting and Faster Convergence", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Conventional low-rank adaptation methods build adapters without considering\ndata context, leading to sub-optimal fine-tuning performance and severe\nforgetting of inherent world knowledge. In this paper, we propose\ncontext-oriented decomposition adaptation (CorDA), a novel method that\ninitializes adapters in a task-aware manner. Concretely, we develop\ncontext-oriented singular value decomposition, where we collect covariance\nmatrices of input activations for each linear layer using sampled data from the\ntarget task, and apply SVD to the product of weight matrix and its\ncorresponding covariance matrix. By doing so, the task-specific capability is\ncompacted into the principal components. Thanks to the task awareness, our\nmethod enables two optional adaptation modes, knowledge-preserved mode (KPM)\nand instruction-previewed mode (IPM), providing flexibility to choose between\nfreezing the principal components to preserve their associated knowledge or\nadapting them to better learn a new task. We further develop CorDA++ by\nderiving a metric that reflects the compactness of task-specific principal\ncomponents, and then introducing dynamic covariance selection and dynamic rank\nallocation strategies based on the same metric. The two strategies provide each\nlayer with the most representative covariance matrix and a proper rank\nallocation. Experimental results show that CorDA++ outperforms CorDA by a\nsignificant margin. CorDA++ in KPM not only achieves better fine-tuning\nperformance than LoRA, but also mitigates the forgetting of pre-trained\nknowledge in both large language models and vision language models. For IPM,\nour method exhibits faster convergence, \\emph{e.g.,} 4.5x speedup over QLoRA,\nand improves adaptation performance in various scenarios, outperforming strong\nbaseline methods. Our method has been integrated into the PEFT library\ndeveloped by Hugging Face."}
{"id": "2506.13195", "pdf": "https://arxiv.org/pdf/2506.13195", "abs": "https://arxiv.org/abs/2506.13195", "authors": ["Bikram Keshari Parida", "Anusree P. Sunilkumar", "Abhijit Sen", "Wonsang You"], "title": "ViT-NeBLa: A Hybrid Vision Transformer and Neural Beer-Lambert Framework for Single-View 3D Reconstruction of Oral Anatomy from Panoramic Radiographs", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "10 figures, 19 pages", "summary": "Dental diagnosis relies on two primary imaging modalities: panoramic\nradiographs (PX) providing 2D oral cavity representations, and Cone-Beam\nComputed Tomography (CBCT) offering detailed 3D anatomical information. While\nPX images are cost-effective and accessible, their lack of depth information\nlimits diagnostic accuracy. CBCT addresses this but presents drawbacks\nincluding higher costs, increased radiation exposure, and limited\naccessibility. Existing reconstruction models further complicate the process by\nrequiring CBCT flattening or prior dental arch information, often unavailable\nclinically. We introduce ViT-NeBLa, a vision transformer-based Neural\nBeer-Lambert model enabling accurate 3D reconstruction directly from single PX.\nOur key innovations include: (1) enhancing the NeBLa framework with Vision\nTransformers for improved reconstruction capabilities without requiring CBCT\nflattening or prior dental arch information, (2) implementing a novel\nhorseshoe-shaped point sampling strategy with non-intersecting rays that\neliminates intermediate density aggregation required by existing models due to\nintersecting rays, reducing sampling point computations by $52 \\%$, (3)\nreplacing CNN-based U-Net with a hybrid ViT-CNN architecture for superior\nglobal and local feature extraction, and (4) implementing learnable hash\npositional encoding for better higher-dimensional representation of 3D sample\npoints compared to existing Fourier-based dense positional encoding.\nExperiments demonstrate that ViT-NeBLa significantly outperforms prior\nstate-of-the-art methods both quantitatively and qualitatively, offering a\ncost-effective, radiation-efficient alternative for enhanced dental\ndiagnostics."}
{"id": "2506.13277", "pdf": "https://arxiv.org/pdf/2506.13277", "abs": "https://arxiv.org/abs/2506.13277", "authors": ["Huyang Li", "Yahui Liu", "Hongyu Sun", "Deng Cai", "Leyang Cui", "Wei Bi", "Peilin Zhao", "Taro Watanabe"], "title": "SeqPE: Transformer with Sequential Position Encoding", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Since self-attention layers in Transformers are permutation invariant by\ndesign, positional encodings must be explicitly incorporated to enable spatial\nunderstanding. However, fixed-size lookup tables used in traditional learnable\nposition embeddings (PEs) limit extrapolation capabilities beyond pre-trained\nsequence lengths. Expert-designed methods such as ALiBi and RoPE, mitigate this\nlimitation but demand extensive modifications for adapting to new modalities,\nunderscoring fundamental challenges in adaptability and scalability. In this\nwork, we present SeqPE, a unified and fully learnable position encoding\nframework that represents each $n$-dimensional position index as a symbolic\nsequence and employs a lightweight sequential position encoder to learn their\nembeddings in an end-to-end manner. To regularize SeqPE's embedding space, we\nintroduce two complementary objectives: a contrastive objective that aligns\nembedding distances with a predefined position-distance function, and a\nknowledge distillation loss that anchors out-of-distribution position\nembeddings to in-distribution teacher representations, further enhancing\nextrapolation performance. Experiments across language modeling, long-context\nquestion answering, and 2D image classification demonstrate that SeqPE not only\nsurpasses strong baselines in perplexity, exact match (EM), and\naccuracy--particularly under context length extrapolation--but also enables\nseamless generalization to multi-dimensional inputs without requiring manual\narchitectural redesign. We release our code, data, and checkpoints at\nhttps://github.com/ghrua/seqpe."}
{"id": "2506.13306", "pdf": "https://arxiv.org/pdf/2506.13306", "abs": "https://arxiv.org/abs/2506.13306", "authors": ["Salah Ghamizi", "Georgia Kanli", "Yu Deng", "Magali Perquin", "Olivier Keunen"], "title": "Brain Imaging Foundation Models, Are We There Yet? A Systematic Review of Foundation Models for Brain Imaging and Biomedical Research", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Foundation models (FMs), large neural networks pretrained on extensive and\ndiverse datasets, have revolutionized artificial intelligence and shown\nsignificant promise in medical imaging by enabling robust performance with\nlimited labeled data. Although numerous surveys have reviewed the application\nof FM in healthcare care, brain imaging remains underrepresented, despite its\ncritical role in the diagnosis and treatment of neurological diseases using\nmodalities such as MRI, CT, and PET. Existing reviews either marginalize brain\nimaging or lack depth on the unique challenges and requirements of FM in this\ndomain, such as multimodal data integration, support for diverse clinical\ntasks, and handling of heterogeneous, fragmented datasets.\n  To address this gap, we present the first comprehensive and curated review of\nFMs for brain imaging. We systematically analyze 161 brain imaging datasets and\n86 FM architectures, providing information on key design choices, training\nparadigms, and optimizations driving recent advances. Our review highlights the\nleading models for various brain imaging tasks, summarizes their innovations,\nand critically examines current limitations and blind spots in the literature.\nWe conclude by outlining future research directions to advance FM applications\nin brain imaging, with the aim of fostering progress in both clinical and\nresearch settings."}
{"id": "2506.13348", "pdf": "https://arxiv.org/pdf/2506.13348", "abs": "https://arxiv.org/abs/2506.13348", "authors": ["Mae Younes", "Adnane Boukhayma"], "title": "TextureSplat: Per-Primitive Texture Mapping for Reflective Gaussian Splatting", "categories": ["cs.GR", "cs.CV"], "comment": "Code will be available at https://github.com/maeyounes/TextureSplat", "summary": "Gaussian Splatting have demonstrated remarkable novel view synthesis\nperformance at high rendering frame rates. Optimization-based inverse rendering\nwithin complex capture scenarios remains however a challenging problem. A\nparticular case is modelling complex surface light interactions for highly\nreflective scenes, which results in intricate high frequency specular radiance\ncomponents. We hypothesize that such challenging settings can benefit from\nincreased representation power. We hence propose a method that tackles this\nissue through a geometrically and physically grounded Gaussian Splatting borne\nradiance field, where normals and material properties are spatially variable in\nthe primitive's local space. Using per-primitive texture maps for this purpose,\nwe also propose to harness the GPU hardware to accelerate rendering at test\ntime via unified material texture atlas."}
{"id": "2506.13415", "pdf": "https://arxiv.org/pdf/2506.13415", "abs": "https://arxiv.org/abs/2506.13415", "authors": ["Xiang Yu", "Yayan Chen", "Guannan He", "Qing Zeng", "Yue Qin", "Meiling Liang", "Dandan Luo", "Yimei Liao", "Zeyu Ren", "Cheng Kang", "Delong Yang", "Bocheng Liang", "Bin Pu", "Ying Yuan", "Shengli Li"], "title": "Simple is what you need for efficient and accurate medical image segmentation", "categories": ["eess.IV", "cs.AI", "cs.CV", "I.4.6"], "comment": "15 pages, 11 figures", "summary": "While modern segmentation models often prioritize performance over\npracticality, we advocate a design philosophy prioritizing simplicity and\nefficiency, and attempted high performance segmentation model design. This\npaper presents SimpleUNet, a scalable ultra-lightweight medical image\nsegmentation model with three key innovations: (1) A partial feature selection\nmechanism in skip connections for redundancy reduction while enhancing\nsegmentation performance; (2) A fixed-width architecture that prevents\nexponential parameter growth across network stages; (3) An adaptive feature\nfusion module achieving enhanced representation with minimal computational\noverhead. With a record-breaking 16 KB parameter configuration, SimpleUNet\noutperforms LBUNet and other lightweight benchmarks across multiple public\ndatasets. The 0.67 MB variant achieves superior efficiency (8.60 GFLOPs) and\naccuracy, attaining a mean DSC/IoU of 85.76%/75.60% on multi-center breast\nlesion datasets, surpassing both U-Net and TransUNet. Evaluations on skin\nlesion datasets (ISIC 2017/2018: mDice 84.86%/88.77%) and endoscopic polyp\nsegmentation (KVASIR-SEG: 86.46%/76.48% mDice/mIoU) confirm consistent\ndominance over state-of-the-art models. This work demonstrates that extreme\nmodel compression need not compromise performance, providing new insights for\nefficient and accurate medical image segmentation. Codes can be found at\nhttps://github.com/Frankyu5666666/SimpleUNet."}
{"id": "2506.13419", "pdf": "https://arxiv.org/pdf/2506.13419", "abs": "https://arxiv.org/abs/2506.13419", "authors": ["Riku Takahashi", "Ryugo Morita", "Jinjia Zhou"], "title": "Audio-Visual Driven Compression for Low-Bitrate Talking Head Videos", "categories": ["eess.IV", "cs.CV"], "comment": "Accepted to ICMR2025", "summary": "Talking head video compression has advanced with neural rendering and\nkeypoint-based methods, but challenges remain, especially at low bit rates,\nincluding handling large head movements, suboptimal lip synchronization, and\ndistorted facial reconstructions. To address these problems, we propose a novel\naudio-visual driven video codec that integrates compact 3D motion features and\naudio signals. This approach robustly models significant head rotations and\naligns lip movements with speech, improving both compression efficiency and\nreconstruction quality. Experiments on the CelebV-HQ dataset show that our\nmethod reduces bitrate by 22% compared to VVC and by 8.5% over state-of-the-art\nlearning-based codec. Furthermore, it provides superior lip-sync accuracy and\nvisual fidelity at comparable bitrates, highlighting its effectiveness in\nbandwidth-constrained scenarios."}
{"id": "2506.13425", "pdf": "https://arxiv.org/pdf/2506.13425", "abs": "https://arxiv.org/abs/2506.13425", "authors": ["Sai Srinivas Jeevanandam", "Sandeep Inuganti", "Shreedhar Govil", "Didier Stricker", "Jason Rambach"], "title": "JENGA: Object selection and pose estimation for robotic grasping from a stack", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Vision-based robotic object grasping is typically investigated in the context\nof isolated objects or unstructured object sets in bin picking scenarios.\nHowever, there are several settings, such as construction or warehouse\nautomation, where a robot needs to interact with a structured object formation\nsuch as a stack. In this context, we define the problem of selecting suitable\nobjects for grasping along with estimating an accurate 6DoF pose of these\nobjects. To address this problem, we propose a camera-IMU based approach that\nprioritizes unobstructed objects on the higher layers of stacks and introduce a\ndataset for benchmarking and evaluation, along with a suitable evaluation\nmetric that combines object selection with pose accuracy. Experimental results\nshow that although our method can perform quite well, this is a challenging\nproblem if a completely error-free solution is needed. Finally, we show results\nfrom the deployment of our method for a brick-picking application in a\nconstruction scenario."}
{"id": "2506.13443", "pdf": "https://arxiv.org/pdf/2506.13443", "abs": "https://arxiv.org/abs/2506.13443", "authors": ["Kang Chen", "Bin Huang", "Xuebin Yang", "Junyan Zhang", "Qiegen Liu"], "title": "PRO: Projection Domain Synthesis for CT Imaging", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Synthesizing high quality CT images remains a signifi-cant challenge due to\nthe limited availability of annotat-ed data and the complex nature of CT\nimaging. In this work, we present PRO, a novel framework that, to the best of\nour knowledge, is the first to perform CT image synthesis in the projection\ndomain using latent diffusion models. Unlike previous approaches that operate\nin the image domain, PRO learns rich structural representa-tions from raw\nprojection data and leverages anatomi-cal text prompts for controllable\nsynthesis. This projec-tion domain strategy enables more faithful modeling of\nunderlying imaging physics and anatomical structures. Moreover, PRO functions\nas a foundation model, capa-ble of generalizing across diverse downstream tasks\nby adjusting its generative behavior via prompt inputs. Experimental results\ndemonstrated that incorporating our synthesized data significantly improves\nperfor-mance across multiple downstream tasks, including low-dose and\nsparse-view reconstruction, even with limited training data. These findings\nunderscore the versatility and scalability of PRO in data generation for\nvarious CT applications. These results highlight the potential of projection\ndomain synthesis as a powerful tool for data augmentation and robust CT\nimaging. Our source code is publicly available at:\nhttps://github.com/yqx7150/PRO."}
{"id": "2506.13477", "pdf": "https://arxiv.org/pdf/2506.13477", "abs": "https://arxiv.org/abs/2506.13477", "authors": ["Pegah Salehi", "Sajad Amouei Sheshkal", "Vajira Thambawita", "Pål Halvorsen"], "title": "From Flat to Feeling: A Feasibility and Impact Study on Dynamic Facial Emotions in AI-Generated Avatars", "categories": ["cs.HC", "cs.CV", "68T07, 68U99, 68T45, 91E45"], "comment": "15 pages, 4 figures, 4 tables", "summary": "Dynamic facial emotion is essential for believable AI-generated avatars;\nhowever, most systems remain visually inert, limiting their utility in\nhigh-stakes simulations such as virtual training for investigative interviews\nwith abused children. We introduce and evaluate a real-time architecture fusing\nUnreal Engine 5 MetaHuman rendering with NVIDIA Omniverse Audio2Face to\ntranslate vocal prosody into high-fidelity facial expressions on photorealistic\nchild avatars. We implemented a distributed two-PC setup that decouples\nlanguage processing and speech synthesis from GPU-intensive rendering, designed\nto support low-latency interaction in desktop and VR environments. A\nbetween-subjects study ($N=70$) using audio+visual and visual-only conditions\nassessed perceptual impacts as participants rated emotional clarity, facial\nrealism, and empathy for two avatars expressing joy, sadness, and anger.\n  Results demonstrate that avatars could express emotions recognizably, with\nsadness and joy achieving high identification rates. However, anger recognition\nsignificantly dropped without audio, highlighting the importance of congruent\nvocal cues for high-arousal emotions. Interestingly, removing audio boosted\nperceived facial realism, suggesting that audiovisual desynchrony remains a key\ndesign challenge. These findings confirm the technical feasibility of\ngenerating emotionally expressive avatars and provide guidance for improving\nnon-verbal communication in sensitive training simulations."}
{"id": "2506.13579", "pdf": "https://arxiv.org/pdf/2506.13579", "abs": "https://arxiv.org/abs/2506.13579", "authors": ["Andrew Zhang", "Anushka Sivakumar", "Chiawei Tang", "Chris Thomas"], "title": "Flexible-length Text Infilling for Discrete Diffusion Models", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Discrete diffusion models are a new class of text generators that offer\nadvantages such as bidirectional context use, parallelizable generation, and\nflexible prompting compared to autoregressive models. However, a critical\nlimitation of discrete diffusion models is their inability to perform\nflexible-length or flexible-position text infilling without access to\nground-truth positional data. We introduce \\textbf{DDOT} (\\textbf{D}iscrete\n\\textbf{D}iffusion with \\textbf{O}ptimal \\textbf{T}ransport Position Coupling),\nthe first discrete diffusion model to overcome this challenge. DDOT jointly\ndenoises token values and token positions, employing a novel sample-level\nOptimal Transport (OT) coupling. This coupling preserves relative token\nordering while dynamically adjusting the positions and length of infilled\nsegments, a capability previously missing in text diffusion. Our method is\northogonal to existing discrete text diffusion methods and is compatible with\nvarious pretrained text denoisers. Extensive experiments on text infilling\nbenchmarks such as One-Billion-Word and Yelp demonstrate that DDOT outperforms\nnaive diffusion baselines. Furthermore, DDOT achieves performance on par with\nstate-of-the-art non-autoregressive models and enables significant improvements\nin training efficiency and flexibility."}
{"id": "2506.13614", "pdf": "https://arxiv.org/pdf/2506.13614", "abs": "https://arxiv.org/abs/2506.13614", "authors": ["Gregory Bellchambers"], "title": "Exploiting the Exact Denoising Posterior Score in Training-Free Guidance of Diffusion Models", "categories": ["stat.ML", "cs.CV", "cs.LG"], "comment": null, "summary": "The success of diffusion models has driven interest in performing conditional\nsampling via training-free guidance of the denoising process to solve image\nrestoration and other inverse problems. A popular class of methods, based on\nDiffusion Posterior Sampling (DPS), attempts to approximate the intractable\nposterior score function directly. In this work, we present a novel expression\nfor the exact posterior score for purely denoising tasks that is tractable in\nterms of the unconditional score function. We leverage this result to analyze\nthe time-dependent error in the DPS score for denoising tasks and compute step\nsizes on the fly to minimize the error at each time step. We demonstrate that\nthese step sizes are transferable to related inverse problems such as\ncolorization, random inpainting, and super resolution. Despite its simplicity,\nthis approach is competitive with state-of-the-art techniques and enables\nsampling with fewer time steps than DPS."}
{"id": "2506.13642", "pdf": "https://arxiv.org/pdf/2506.13642", "abs": "https://arxiv.org/abs/2506.13642", "authors": ["Shaolei Zhang", "Shoutao Guo", "Qingkai Fang", "Yan Zhou", "Yang Feng"], "title": "Stream-Omni: Simultaneous Multimodal Interactions with Large Language-Vision-Speech Model", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.SD", "eess.AS"], "comment": "Code: https://github.com/ictnlp/Stream-Omni , Model:\n  https://huggingface.co/ICTNLP/stream-omni-8b", "summary": "The emergence of GPT-4o-like large multimodal models (LMMs) has raised the\nexploration of integrating text, vision, and speech modalities to support more\nflexible multimodal interaction. Existing LMMs typically concatenate\nrepresentation of modalities along the sequence dimension and feed them into a\nlarge language model (LLM) backbone. While sequence-dimension concatenation is\nstraightforward for modality integration, it often relies heavily on\nlarge-scale data to learn modality alignments. In this paper, we aim to model\nthe relationships between modalities more purposefully, thereby achieving more\nefficient and flexible modality alignments. To this end, we propose\nStream-Omni, a large language-vision-speech model with efficient modality\nalignments, which can simultaneously support interactions under various\nmodality combinations. Stream-Omni employs LLM as the backbone and aligns the\nvision and speech to the text based on their relationships. For vision that is\nsemantically complementary to text, Stream-Omni uses sequence-dimension\nconcatenation to achieve vision-text alignment. For speech that is semantically\nconsistent with text, Stream-Omni introduces a CTC-based layer-dimension\nmapping to achieve speech-text alignment. In this way, Stream-Omni can achieve\nmodality alignments with less data (especially speech), enabling the transfer\nof text capabilities to other modalities. Experiments on various benchmarks\ndemonstrate that Stream-Omni achieves strong performance on visual\nunderstanding, speech interaction, and vision-grounded speech interaction\ntasks. Owing to the layer-dimensional mapping, Stream-Omni can simultaneously\nprovide intermediate text outputs (such as ASR transcriptions and model\nresponses) during speech interaction, offering users a comprehensive multimodal\nexperience."}
{"id": "2506.13667", "pdf": "https://arxiv.org/pdf/2506.13667", "abs": "https://arxiv.org/abs/2506.13667", "authors": ["Bi Yuda", "Jia Sihan", "Gao Yutong", "Abrol Anees", "Fu Zening", "Calhoun Vince"], "title": "MultiViT2: A Data-augmented Multimodal Neuroimaging Prediction Framework via Latent Diffusion Model", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Multimodal medical imaging integrates diverse data types, such as structural\nand functional neuroimaging, to provide complementary insights that enhance\ndeep learning predictions and improve outcomes. This study focuses on a\nneuroimaging prediction framework based on both structural and functional\nneuroimaging data. We propose a next-generation prediction model,\n\\textbf{MultiViT2}, which combines a pretrained representative learning base\nmodel with a vision transformer backbone for prediction output. Additionally,\nwe developed a data augmentation module based on the latent diffusion model\nthat enriches input data by generating augmented neuroimaging samples, thereby\nenhancing predictive performance through reduced overfitting and improved\ngeneralizability. We show that MultiViT2 significantly outperforms the\nfirst-generation model in schizophrenia classification accuracy and\ndemonstrates strong scalability and portability."}
{"id": "2506.13679", "pdf": "https://arxiv.org/pdf/2506.13679", "abs": "https://arxiv.org/abs/2506.13679", "authors": ["Yuqing Wen", "Kefan Gu", "Haoxuan Liu", "Yucheng Zhao", "Tiancai Wang", "Haoqiang Fan", "Xiaoyan Sun"], "title": "ROSA: Harnessing Robot States for Vision-Language and Action Alignment", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "Vision-Language-Action (VLA) models have recently made significant advance in\nmulti-task, end-to-end robotic control, due to the strong generalization\ncapabilities of Vision-Language Models (VLMs). A fundamental challenge in\ndeveloping such models is effectively aligning the vision-language space with\nthe robotic action space. Existing approaches typically rely on directly\nfine-tuning VLMs using expert demonstrations. However, this strategy suffers\nfrom a spatio-temporal gap, resulting in considerable data inefficiency and\nheavy reliance on human labor. Spatially, VLMs operate within a high-level\nsemantic space, whereas robotic actions are grounded in low-level 3D physical\nspace; temporally, VLMs primarily interpret the present, while VLA models\nanticipate future actions. To overcome these challenges, we propose a novel\ntraining paradigm, ROSA, which leverages robot state estimation to improve\nalignment between vision-language and action spaces. By integrating robot state\nestimation data obtained via an automated process, ROSA enables the VLA model\nto gain enhanced spatial understanding and self-awareness, thereby boosting\nperformance and generalization. Extensive experiments in both simulated and\nreal-world environments demonstrate the effectiveness of ROSA, particularly in\nlow-data regimes."}
{"id": "2506.13754", "pdf": "https://arxiv.org/pdf/2506.13754", "abs": "https://arxiv.org/abs/2506.13754", "authors": ["Edward Li", "Zichen Wang", "Jiahe Huang", "Jeong Joon Park"], "title": "VideoPDE: Unified Generative PDE Solving via Video Inpainting Diffusion Models", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "Submitted to NeurIPS 2025. Project page: https://videopde.github.io/", "summary": "We present a unified framework for solving partial differential equations\n(PDEs) using video-inpainting diffusion transformer models. Unlike existing\nmethods that devise specialized strategies for either forward or inverse\nproblems under full or partial observation, our approach unifies these tasks\nunder a single, flexible generative framework. Specifically, we recast\nPDE-solving as a generalized inpainting problem, e.g., treating forward\nprediction as inferring missing spatiotemporal information of future states\nfrom initial conditions. To this end, we design a transformer-based\narchitecture that conditions on arbitrary patterns of known data to infer\nmissing values across time and space. Our method proposes pixel-space video\ndiffusion models for fine-grained, high-fidelity inpainting and conditioning,\nwhile enhancing computational efficiency through hierarchical modeling.\nExtensive experiments show that our video inpainting-based diffusion model\noffers an accurate and versatile solution across a wide range of PDEs and\nproblem setups, outperforming state-of-the-art baselines."}
{"id": "2506.13756", "pdf": "https://arxiv.org/pdf/2506.13756", "abs": "https://arxiv.org/abs/2506.13756", "authors": ["Jingwei Ma", "Vivek Jayaram", "Brian Curless", "Ira Kemelmacher-Shlizerman", "Steven M. Seitz"], "title": "UltraZoom: Generating Gigapixel Images from Regular Photos", "categories": ["cs.GR", "cs.CV"], "comment": "Project page: https://ultra-zoom.github.io/", "summary": "We present UltraZoom, a system for generating gigapixel-resolution images of\nobjects from casually captured inputs, such as handheld phone photos. Given a\nfull-shot image (global, low-detail) and one or more close-ups (local,\nhigh-detail), UltraZoom upscales the full image to match the fine detail and\nscale of the close-up examples. To achieve this, we construct a per-instance\npaired dataset from the close-ups and adapt a pretrained generative model to\nlearn object-specific low-to-high resolution mappings. At inference, we apply\nthe model in a sliding window fashion over the full image. Constructing these\npairs is non-trivial: it requires registering the close-ups within the full\nimage for scale estimation and degradation alignment. We introduce a simple,\nrobust method for getting registration on arbitrary materials in casual,\nin-the-wild captures. Together, these components form a system that enables\nseamless pan and zoom across the entire object, producing consistent,\nphotorealistic gigapixel imagery from minimal input."}
{"id": "2506.13762", "pdf": "https://arxiv.org/pdf/2506.13762", "abs": "https://arxiv.org/abs/2506.13762", "authors": ["Zifan Zhao", "Siddhant Haldar", "Jinda Cui", "Lerrel Pinto", "Raunaq Bhirangi"], "title": "Touch begins where vision ends: Generalizable policies for contact-rich manipulation", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Data-driven approaches struggle with precise manipulation; imitation learning\nrequires many hard-to-obtain demonstrations, while reinforcement learning\nyields brittle, non-generalizable policies. We introduce VisuoTactile Local\n(ViTaL) policy learning, a framework that solves fine-grained manipulation\ntasks by decomposing them into two phases: a reaching phase, where a\nvision-language model (VLM) enables scene-level reasoning to localize the\nobject of interest, and a local interaction phase, where a reusable,\nscene-agnostic ViTaL policy performs contact-rich manipulation using egocentric\nvision and tactile sensing. This approach is motivated by the observation that\nwhile scene context varies, the low-level interaction remains consistent across\ntask instances. By training local policies once in a canonical setting, they\ncan generalize via a localize-then-execute strategy. ViTaL achieves around 90%\nsuccess on contact-rich tasks in unseen environments and is robust to\ndistractors. ViTaL's effectiveness stems from three key insights: (1)\nfoundation models for segmentation enable training robust visual encoders via\nbehavior cloning; (2) these encoders improve the generalizability of policies\nlearned using residual RL; and (3) tactile sensing significantly boosts\nperformance in contact-rich tasks. Ablation studies validate each of these\ninsights, and we demonstrate that ViTaL integrates well with high-level VLMs,\nenabling robust, reusable low-level skills. Results and videos are available at\nhttps://vitalprecise.github.io."}
{"id": "2506.13763", "pdf": "https://arxiv.org/pdf/2506.13763", "abs": "https://arxiv.org/abs/2506.13763", "authors": ["Yixian Xu", "Shengjie Luo", "Liwei Wang", "Di He", "Chang Liu"], "title": "Diagnosing and Improving Diffusion Models by Estimating the Optimal Loss Value", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "comment": "29 pages, 8 figures, 3 tables. Preprint. Work in Progress", "summary": "Diffusion models have achieved remarkable success in generative modeling.\nDespite more stable training, the loss of diffusion models is not indicative of\nabsolute data-fitting quality, since its optimal value is typically not zero\nbut unknown, leading to confusion between large optimal loss and insufficient\nmodel capacity. In this work, we advocate the need to estimate the optimal loss\nvalue for diagnosing and improving diffusion models. We first derive the\noptimal loss in closed form under a unified formulation of diffusion models,\nand develop effective estimators for it, including a stochastic variant\nscalable to large datasets with proper control of variance and bias. With this\ntool, we unlock the inherent metric for diagnosing the training quality of\nmainstream diffusion model variants, and develop a more performant training\nschedule based on the optimal loss. Moreover, using models with 120M to 1.5B\nparameters, we find that the power law is better demonstrated after subtracting\nthe optimal loss from the actual training loss, suggesting a more principled\nsetting for investigating the scaling law for diffusion models."}
