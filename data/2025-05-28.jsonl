{"id": "2505.20381", "pdf": "https://arxiv.org/pdf/2505.20381", "abs": "https://arxiv.org/abs/2505.20381", "authors": ["Sijia Chen", "Yanqiu Yu", "En Yu", "Wenbing Tao"], "title": "ReaMOT: A Benchmark and Framework for Reasoning-based Multi-Object Tracking", "categories": ["cs.CV"], "comment": "19 pages, 11 figures, 6 tables", "summary": "Referring Multi-object tracking (RMOT) is an important research field in\ncomputer vision. Its task form is to guide the models to track the objects that\nconform to the language instruction. However, the RMOT task commonly requires\nclear language instructions, such methods often fail to work when complex\nlanguage instructions with reasoning characteristics appear. In this work, we\npropose a new task, called Reasoning-based Multi-Object Tracking (ReaMOT).\nReaMOT is a more challenging task that requires accurate reasoning about\nobjects that match the language instruction with reasoning characteristic and\ntracking the objects' trajectories. To advance the ReaMOT task and evaluate the\nreasoning capabilities of tracking models, we construct ReaMOT Challenge, a\nreasoning-based multi-object tracking benchmark built upon 12 datasets.\nSpecifically, it comprises 1,156 language instructions with reasoning\ncharacteristic, 423,359 image-language pairs, and 869 diverse scenes, which is\ndivided into three levels of reasoning difficulty. In addition, we propose a\nset of evaluation metrics tailored for the ReaMOT task. Furthermore, we propose\nReaTrack, a training-free framework for reasoning-based multi-object tracking\nbased on large vision-language models (LVLM) and SAM2, as a baseline for the\nReaMOT task. Extensive experiments on the ReaMOT Challenge benchmark\ndemonstrate the effectiveness of our ReaTrack framework."}
{"id": "2505.20405", "pdf": "https://arxiv.org/pdf/2505.20405", "abs": "https://arxiv.org/abs/2505.20405", "authors": ["Lorenzo Baraldi", "Davide Bucciarelli", "Federico Betti", "Marcella Cornia", "Lorenzo Baraldi", "Nicu Sebe", "Rita Cucchiara"], "title": "What Changed? Detecting and Evaluating Instruction-Guided Image Edits with Multimodal Large Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "comment": null, "summary": "Instruction-based image editing models offer increased personalization\nopportunities in generative tasks. However, properly evaluating their results\nis challenging, and most of the existing metrics lag in terms of alignment with\nhuman judgment and explainability. To tackle these issues, we introduce DICE\n(DIfference Coherence Estimator), a model designed to detect localized\ndifferences between the original and the edited image and to assess their\nrelevance to the given modification request. DICE consists of two key\ncomponents: a difference detector and a coherence estimator, both built on an\nautoregressive Multimodal Large Language Model (MLLM) and trained using a\nstrategy that leverages self-supervision, distillation from inpainting\nnetworks, and full supervision. Through extensive experiments, we evaluate each\nstage of our pipeline, comparing different MLLMs within the proposed framework.\nWe demonstrate that DICE effectively identifies coherent edits, effectively\nevaluating images generated by different editing models with a strong\ncorrelation with human judgment. We publicly release our source code, models,\nand data."}
{"id": "2505.20414", "pdf": "https://arxiv.org/pdf/2505.20414", "abs": "https://arxiv.org/abs/2505.20414", "authors": ["Royden Wagner", "Omer Sahin Tas", "Felix Hauser", "Marlon Steiner", "Dominik Strutz", "Abhishek Vivekanandan", "Carlos Fernandez", "Christoph Stiller"], "title": "RetroMotion: Retrocausal Motion Forecasting Models are Instructable", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": null, "summary": "Motion forecasts of road users (i.e., agents) vary in complexity as a\nfunction of scene constraints and interactive behavior. We address this with a\nmulti-task learning method for motion forecasting that includes a retrocausal\nflow of information. The corresponding tasks are to forecast (1) marginal\ntrajectory distributions for all modeled agents and (2) joint trajectory\ndistributions for interacting agents. Using a transformer model, we generate\nthe joint distributions by re-encoding marginal distributions followed by\npairwise modeling. This incorporates a retrocausal flow of information from\nlater points in marginal trajectories to earlier points in joint trajectories.\nPer trajectory point, we model positional uncertainty using compressed\nexponential power distributions. Notably, our method achieves state-of-the-art\nresults in the Waymo Interaction Prediction dataset and generalizes well to the\nArgoverse 2 dataset. Additionally, our method provides an interface for issuing\ninstructions through trajectory modifications. Our experiments show that\nregular training of motion forecasting leads to the ability to follow\ngoal-based instructions and to adapt basic directional instructions to the\nscene context. Code: https://github.com/kit-mrt/future-motion"}
{"id": "2505.20426", "pdf": "https://arxiv.org/pdf/2505.20426", "abs": "https://arxiv.org/abs/2505.20426", "authors": ["Yunlong Tang", "Pinxin Liu", "Mingqian Feng", "Zhangyun Tan", "Rui Mao", "Chao Huang", "Jing Bi", "Yunzhong Xiao", "Susan Liang", "Hang Hua", "Ali Vosoughi", "Luchuan Song", "Zeliang Zhang", "Chenliang Xu"], "title": "MMPerspective: Do MLLMs Understand Perspective? A Comprehensive Benchmark for Perspective Perception, Reasoning, and Robustness", "categories": ["cs.CV"], "comment": null, "summary": "Understanding perspective is fundamental to human visual perception, yet the\nextent to which multimodal large language models (MLLMs) internalize\nperspective geometry remains unclear. We introduce MMPerspective, the first\nbenchmark specifically designed to systematically evaluate MLLMs' understanding\nof perspective through 10 carefully crafted tasks across three complementary\ndimensions: Perspective Perception, Reasoning, and Robustness. Our benchmark\ncomprises 2,711 real-world and synthetic image instances with 5,083\nquestion-answer pairs that probe key capabilities, such as vanishing point\nperception and counting, perspective type reasoning, line relationship\nunderstanding in 3D space, invariance to perspective-preserving\ntransformations, etc. Through a comprehensive evaluation of 43 state-of-the-art\nMLLMs, we uncover significant limitations: while models demonstrate competence\non surface-level perceptual tasks, they struggle with compositional reasoning\nand maintaining spatial consistency under perturbations. Our analysis further\nreveals intriguing patterns between model architecture, scale, and perspective\ncapabilities, highlighting both robustness bottlenecks and the benefits of\nchain-of-thought prompting. MMPerspective establishes a valuable testbed for\ndiagnosing and advancing spatial understanding in vision-language systems.\nResources available at: https://yunlong10.github.io/MMPerspective/"}
{"id": "2505.20460", "pdf": "https://arxiv.org/pdf/2505.20460", "abs": "https://arxiv.org/abs/2505.20460", "authors": ["Ruqi Wu", "Xinjie Wang", "Liu Liu", "Chunle Guo", "Jiaxiong Qiu", "Chongyi Li", "Lichao Huang", "Zhizhong Su", "Ming-Ming Cheng"], "title": "DIPO: Dual-State Images Controlled Articulated Object Generation Powered by Diverse Data", "categories": ["cs.CV"], "comment": null, "summary": "We present DIPO, a novel framework for the controllable generation of\narticulated 3D objects from a pair of images: one depicting the object in a\nresting state and the other in an articulated state. Compared to the\nsingle-image approach, our dual-image input imposes only a modest overhead for\ndata collection, but at the same time provides important motion information,\nwhich is a reliable guide for predicting kinematic relationships between parts.\nSpecifically, we propose a dual-image diffusion model that captures\nrelationships between the image pair to generate part layouts and joint\nparameters. In addition, we introduce a Chain-of-Thought (CoT) based graph\nreasoner that explicitly infers part connectivity relationships. To further\nimprove robustness and generalization on complex articulated objects, we\ndevelop a fully automated dataset expansion pipeline, name LEGO-Art, that\nenriches the diversity and complexity of PartNet-Mobility dataset. We propose\nPM-X, a large-scale dataset of complex articulated 3D objects, accompanied by\nrendered images, URDF annotations, and textual descriptions. Extensive\nexperiments demonstrate that DIPO significantly outperforms existing baselines\nin both the resting state and the articulated state, while the proposed PM-X\ndataset further enhances generalization to diverse and structurally complex\narticulated objects. Our code and dataset will be released to the community\nupon publication."}
{"id": "2505.20469", "pdf": "https://arxiv.org/pdf/2505.20469", "abs": "https://arxiv.org/abs/2505.20469", "authors": ["Lei Tian", "Xiaomin Li", "Liqian Ma", "Hefei Huang", "Zirui Zheng", "Hao Yin", "Taiqing Li", "Huchuan Lu", "Xu Jia"], "title": "CCL-LGS: Contrastive Codebook Learning for 3D Language Gaussian Splatting", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advances in 3D reconstruction techniques and vision-language models\nhave fueled significant progress in 3D semantic understanding, a capability\ncritical to robotics, autonomous driving, and virtual/augmented reality.\nHowever, methods that rely on 2D priors are prone to a critical challenge:\ncross-view semantic inconsistencies induced by occlusion, image blur, and\nview-dependent variations. These inconsistencies, when propagated via\nprojection supervision, deteriorate the quality of 3D Gaussian semantic fields\nand introduce artifacts in the rendered outputs. To mitigate this limitation,\nwe propose CCL-LGS, a novel framework that enforces view-consistent semantic\nsupervision by integrating multi-view semantic cues. Specifically, our approach\nfirst employs a zero-shot tracker to align a set of SAM-generated 2D masks and\nreliably identify their corresponding categories. Next, we utilize CLIP to\nextract robust semantic encodings across views. Finally, our Contrastive\nCodebook Learning (CCL) module distills discriminative semantic features by\nenforcing intra-class compactness and inter-class distinctiveness. In contrast\nto previous methods that directly apply CLIP to imperfect masks, our framework\nexplicitly resolves semantic conflicts while preserving category\ndiscriminability. Extensive experiments demonstrate that CCL-LGS outperforms\nprevious state-of-the-art methods. Our project page is available at\nhttps://epsilontl.github.io/CCL-LGS/."}
{"id": "2505.20471", "pdf": "https://arxiv.org/pdf/2505.20471", "abs": "https://arxiv.org/abs/2505.20471", "authors": ["Chenghao Qian", "Wenjing Li", "Yuhu Guo", "Gustav Markkula"], "title": "WeatherEdit: Controllable Weather Editing with 4D Gaussian Field", "categories": ["cs.CV", "cs.AI", "cs.ET", "cs.LG", "cs.RO"], "comment": null, "summary": "In this work, we present WeatherEdit, a novel weather editing pipeline for\ngenerating realistic weather effects with controllable types and severity in 3D\nscenes. Our approach is structured into two key components: weather background\nediting and weather particle construction. For weather background editing, we\nintroduce an all-in-one adapter that integrates multiple weather styles into a\nsingle pretrained diffusion model, enabling the generation of diverse weather\neffects in 2D image backgrounds. During inference, we design a Temporal-View\n(TV-) attention mechanism that follows a specific order to aggregate temporal\nand spatial information, ensuring consistent editing across multi-frame and\nmulti-view images. To construct the weather particles, we first reconstruct a\n3D scene using the edited images and then introduce a dynamic 4D Gaussian field\nto generate snowflakes, raindrops and fog in the scene. The attributes and\ndynamics of these particles are precisely controlled through physical-based\nmodelling and simulation, ensuring realistic weather representation and\nflexible severity adjustments. Finally, we integrate the 4D Gaussian field with\nthe 3D scene to render consistent and highly realistic weather effects.\nExperiments on multiple driving datasets demonstrate that WeatherEdit can\ngenerate diverse weather effects with controllable condition severity,\nhighlighting its potential for autonomous driving simulation in adverse\nweather. See project page: https://jumponthemoon.github.io/w-edit"}
{"id": "2505.20498", "pdf": "https://arxiv.org/pdf/2505.20498", "abs": "https://arxiv.org/abs/2505.20498", "authors": ["Dongyu Luo", "Kelin Yu", "Amir-Hossein Shahidzadeh", "Cornelia Fermüller", "Yiannis Aloimonos"], "title": "ControlTac: Force- and Position-Controlled Tactile Data Augmentation with a Single Reference Image", "categories": ["cs.CV", "cs.LG", "cs.RO"], "comment": "22 pages, 11 figures, 7 tables", "summary": "Vision-based tactile sensing has been widely used in perception,\nreconstruction, and robotic manipulation. However, collecting large-scale\ntactile data remains costly due to the localized nature of sensor-object\ninteractions and inconsistencies across sensor instances. Existing approaches\nto scaling tactile data, such as simulation and free-form tactile generation,\noften suffer from unrealistic output and poor transferability to downstream\ntasks.To address this, we propose ControlTac, a two-stage controllable\nframework that generates realistic tactile images conditioned on a single\nreference tactile image, contact force, and contact position. With those\nphysical priors as control input, ControlTac generates physically plausible and\nvaried tactile images that can be used for effective data augmentation. Through\nexperiments on three downstream tasks, we demonstrate that ControlTac can\neffectively augment tactile datasets and lead to consistent gains. Our three\nreal-world experiments further validate the practical utility of our approach.\nProject page: https://dongyuluo.github.io/controltac."}
{"id": "2505.20507", "pdf": "https://arxiv.org/pdf/2505.20507", "abs": "https://arxiv.org/abs/2505.20507", "authors": ["Elias Arbash", "Ahmed Jamal Afifi", "Ymane Belahsen", "Margret Fuchs", "Pedram Ghamisi", "Paul Scheunders", "Richard Gloaguen"], "title": "Electrolyzers-HSI: Close-Range Multi-Scene Hyperspectral Imaging Benchmark Dataset", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The global challenge of sustainable recycling demands automated, fast, and\naccurate, state-of-the-art (SOTA) material detection systems that act as a\nbedrock for a circular economy. Democratizing access to these cutting-edge\nsolutions that enable real-time waste analysis is essential for scaling up\nrecycling efforts and fostering the Green Deal. In response, we introduce\n\\textbf{Electrolyzers-HSI}, a novel multimodal benchmark dataset designed to\naccelerate the recovery of critical raw materials through accurate electrolyzer\nmaterials classification. The dataset comprises 55 co-registered\nhigh-resolution RGB images and hyperspectral imaging (HSI) data cubes spanning\nthe 400--2500 nm spectral range, yielding over 4.2 million pixel vectors and\n424,169 labeled ones. This enables non-invasive spectral analysis of shredded\nelectrolyzer samples, supporting quantitative and qualitative material\nclassification and spectral properties investigation. We evaluate a suite of\nbaseline machine learning (ML) methods alongside SOTA transformer-based deep\nlearning (DL) architectures, including Vision Transformer, SpectralFormer, and\nthe Multimodal Fusion Transformer, to investigate architectural bottlenecks for\nfurther efficiency optimisation when deploying transformers in material\nidentification. We implement zero-shot detection techniques and majority voting\nacross pixel-level predictions to establish object-level classification\nrobustness. In adherence to the FAIR data principles, the electrolyzers-HSI\ndataset and accompanying codebase are openly available at\nhttps://github.com/hifexplo/Electrolyzers-HSI and\nhttps://rodare.hzdr.de/record/3668, supporting reproducible research and\nfacilitating the broader adoption of smart and sustainable e-waste recycling\nsolutions."}
{"id": "2505.20510", "pdf": "https://arxiv.org/pdf/2505.20510", "abs": "https://arxiv.org/abs/2505.20510", "authors": ["Yuxuan Sun", "Yixuan Si", "Chenglu Zhu", "Kai Zhang", "Zhongyi Shui", "Bowen Ding", "Tao Lin", "Lin Yang"], "title": "CPathAgent: An Agent-based Foundation Model for Interpretable High-Resolution Pathology Image Analysis Mimicking Pathologists' Diagnostic Logic", "categories": ["cs.CV"], "comment": "49 pages, 33 figures", "summary": "Recent advances in computational pathology have led to the emergence of\nnumerous foundation models. However, these approaches fail to replicate the\ndiagnostic process of pathologists, as they either simply rely on\ngeneral-purpose encoders with multi-instance learning for classification or\ndirectly apply multimodal models to generate reports from images. A significant\nlimitation is their inability to emulate the diagnostic logic employed by\npathologists, who systematically examine slides at low magnification for\noverview before progressively zooming in on suspicious regions to formulate\ncomprehensive diagnoses. To address this gap, we introduce CPathAgent, an\ninnovative agent-based model that mimics pathologists' reasoning processes by\nautonomously executing zoom-in/out and navigation operations across pathology\nimages based on observed visual features. To achieve this, we develop a\nmulti-stage training strategy unifying patch-level, region-level, and\nwhole-slide capabilities within a single model, which is essential for\nmimicking pathologists, who require understanding and reasoning capabilities\nacross all three scales. This approach generates substantially more detailed\nand interpretable diagnostic reports compared to existing methods, particularly\nfor huge region understanding. Additionally, we construct an expert-validated\nPathMMU-HR$^{2}$, the first benchmark for huge region analysis, a critical\nintermediate scale between patches and whole slides, as diagnosticians\ntypically examine several key regions rather than entire slides at once.\nExtensive experiments demonstrate that CPathAgent consistently outperforms\nexisting approaches across three scales of benchmarks, validating the\neffectiveness of our agent-based diagnostic approach and highlighting a\npromising direction for the future development of computational pathology."}
{"id": "2505.20512", "pdf": "https://arxiv.org/pdf/2505.20512", "abs": "https://arxiv.org/abs/2505.20512", "authors": ["Tangzheng Lian", "Oya Celiktutan"], "title": "A Feature-level Bias Evaluation Framework for Facial Expression Recognition Models", "categories": ["cs.CV"], "comment": "Submitted to IEEE Transactions on Affective Computing", "summary": "Recent studies on fairness have shown that Facial Expression Recognition\n(FER) models exhibit biases toward certain visually perceived demographic\ngroups. However, the limited availability of human-annotated demographic labels\nin public FER datasets has constrained the scope of such bias analysis. To\novercome this limitation, some prior works have resorted to pseudo-demographic\nlabels, which may distort bias evaluation results. Alternatively, in this\npaper, we propose a feature-level bias evaluation framework for evaluating\ndemographic biases in FER models under the setting where demographic labels are\nunavailable in the test set. Extensive experiments demonstrate that our method\nmore effectively evaluates demographic biases compared to existing approaches\nthat rely on pseudo-demographic labels. Furthermore, we observe that many\nexisting studies do not include statistical testing in their bias evaluations,\nraising concerns that some reported biases may not be statistically significant\nbut rather due to randomness. To address this issue, we introduce a\nplug-and-play statistical module to ensure the statistical significance of\nbiased evaluation results. A comprehensive bias analysis based on the proposed\nmodule is then conducted across three sensitive attributes (age, gender, and\nrace), seven facial expressions, and multiple network architectures on a\nlarge-scale dataset, revealing the prominent demographic biases in FER and\nproviding insights on selecting a fairer network architecture."}
{"id": "2505.20513", "pdf": "https://arxiv.org/pdf/2505.20513", "abs": "https://arxiv.org/abs/2505.20513", "authors": ["Wenhao Gu", "Li Gu", "Ching Yee Suen", "Yang Wang"], "title": "MetaWriter: Personalized Handwritten Text Recognition Using Meta-Learned Prompt Tuning", "categories": ["cs.CV"], "comment": "CVPR2025", "summary": "Recent advancements in handwritten text recognition (HTR) have enabled the\neffective conversion of handwritten text to digital formats. However, achieving\nrobust recognition across diverse writing styles remains challenging.\nTraditional HTR methods lack writer-specific personalization at test time due\nto limitations in model architecture and training strategies. Existing attempts\nto bridge this gap, through gradient-based meta-learning, still require labeled\nexamples and suffer from parameter-inefficient fine-tuning, leading to\nsubstantial computational and memory overhead. To overcome these challenges, we\npropose an efficient framework that formulates personalization as prompt\ntuning, incorporating an auxiliary image reconstruction task with a\nself-supervised loss to guide prompt adaptation with unlabeled test-time\nexamples. To ensure self-supervised loss effectively minimizes text recognition\nerror, we leverage meta-learning to learn the optimal initialization of the\nprompts. As a result, our method allows the model to efficiently capture unique\nwriting styles by updating less than 1% of its parameters and eliminating the\nneed for time-intensive annotation processes. We validate our approach on the\nRIMES and IAM Handwriting Database benchmarks, where it consistently\noutperforms previous state-of-the-art methods while using 20x fewer parameters.\nWe believe this represents a significant advancement in personalized\nhandwritten text recognition, paving the way for more reliable and practical\ndeployment in resource-constrained scenarios."}
{"id": "2505.20525", "pdf": "https://arxiv.org/pdf/2505.20525", "abs": "https://arxiv.org/abs/2505.20525", "authors": ["Aniket Roy", "Maitreya Suin", "Ketul Shah", "Rama Chellappa"], "title": "MultLFG: Training-free Multi-LoRA composition using Frequency-domain Guidance", "categories": ["cs.CV"], "comment": null, "summary": "Low-Rank Adaptation (LoRA) has gained prominence as a computationally\nefficient method for fine-tuning generative models, enabling distinct visual\nconcept synthesis with minimal overhead. However, current methods struggle to\neffectively merge multiple LoRA adapters without training, particularly in\ncomplex compositions involving diverse visual elements. We introduce MultLFG, a\nnovel framework for training-free multi-LoRA composition that utilizes\nfrequency-domain guidance to achieve adaptive fusion of multiple LoRAs. Unlike\nexisting methods that uniformly aggregate concept-specific LoRAs, MultLFG\nemploys a timestep and frequency subband adaptive fusion strategy, selectively\nactivating relevant LoRAs based on content relevance at specific timesteps and\nfrequency bands. This frequency-sensitive guidance not only improves spatial\ncoherence but also provides finer control over multi-LoRA composition, leading\nto more accurate and consistent results. Experimental evaluations on the\nComposLoRA benchmark reveal that MultLFG substantially enhances compositional\nfidelity and image quality across various styles and concept sets,\noutperforming state-of-the-art baselines in multi-concept generation tasks.\nCode will be released."}
{"id": "2505.20540", "pdf": "https://arxiv.org/pdf/2505.20540", "abs": "https://arxiv.org/abs/2505.20540", "authors": ["Md Rashidunnabi", "Kailash Hambarde", "Hugo Proença"], "title": "Causality and \"In-the-Wild\" Video-Based Person Re-ID: A Survey", "categories": ["cs.CV"], "comment": "30 pages, 9 figures", "summary": "Video-based person re-identification (Re-ID) remains brittle in real-world\ndeployments despite impressive benchmark performance. Most existing models rely\non superficial correlations such as clothing, background, or lighting that fail\nto generalize across domains, viewpoints, and temporal variations. This survey\nexamines the emerging role of causal reasoning as a principled alternative to\ntraditional correlation-based approaches in video-based Re-ID. We provide a\nstructured and critical analysis of methods that leverage structural causal\nmodels, interventions, and counterfactual reasoning to isolate\nidentity-specific features from confounding factors. The survey is organized\naround a novel taxonomy of causal Re-ID methods that spans generative\ndisentanglement, domain-invariant modeling, and causal transformers. We review\ncurrent evaluation metrics and introduce causal-specific robustness measures.\nIn addition, we assess practical challenges of scalability, fairness,\ninterpretability, and privacy that must be addressed for real-world adoption.\nFinally, we identify open problems and outline future research directions that\nintegrate causal modeling with efficient architectures and self-supervised\nlearning. This survey aims to establish a coherent foundation for causal\nvideo-based person Re-ID and to catalyze the next phase of research in this\nrapidly evolving domain."}
{"id": "2505.20569", "pdf": "https://arxiv.org/pdf/2505.20569", "abs": "https://arxiv.org/abs/2505.20569", "authors": ["Jihoon Lee", "Min Song"], "title": "Retrieval Visual Contrastive Decoding to Mitigate Object Hallucinations in Large Vision-Language Models", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "ACL Findings camera-ready version. Code is released at\n  https://github.com/JiHoonLee9898/RVCD", "summary": "Despite significant advancements in Large Vision-Language Models, Object\nHallucination (OH) remains a persistent challenge. Building upon prior studies\non contrastive decoding that address this issue without requiring additional\nmodel training, we introduce RVCD (Retrieval Visual Contrastive Decoding), an\nadvanced method to suppress OH. RVCD leverages both negative and positive\nimages at the logit level, explicitly referencing AI-generated images designed\nto represent a single concept. Our approach demonstrates substantial\nimprovements over existing decoding-based methods."}
{"id": "2505.20582", "pdf": "https://arxiv.org/pdf/2505.20582", "abs": "https://arxiv.org/abs/2505.20582", "authors": ["Yizhou Zhao", "Chunjiang Liu", "Haoyu Chen", "Bhiksha Raj", "Min Xu", "Tadas Baltrusaitis", "Mitch Rundle", "HsiangTao Wu", "Kamran Ghasedi"], "title": "Total-Editing: Head Avatar with Editable Appearance, Motion, and Lighting", "categories": ["cs.CV"], "comment": null, "summary": "Face reenactment and portrait relighting are essential tasks in portrait\nediting, yet they are typically addressed independently, without much synergy.\nMost face reenactment methods prioritize motion control and multiview\nconsistency, while portrait relighting focuses on adjusting shading effects. To\ntake advantage of both geometric consistency and illumination awareness, we\nintroduce Total-Editing, a unified portrait editing framework that enables\nprecise control over appearance, motion, and lighting. Specifically, we design\na neural radiance field decoder with intrinsic decomposition capabilities. This\nallows seamless integration of lighting information from portrait images or HDR\nenvironment maps into synthesized portraits. We also incorporate a moving least\nsquares based deformation field to enhance the spatiotemporal coherence of\navatar motion and shading effects. With these innovations, our unified\nframework significantly improves the quality and realism of portrait editing\nresults. Further, the multi-source nature of Total-Editing supports more\nflexible applications, such as illumination transfer from one portrait to\nanother, or portrait animation with customized backgrounds."}
{"id": "2505.20610", "pdf": "https://arxiv.org/pdf/2505.20610", "abs": "https://arxiv.org/abs/2505.20610", "authors": ["Xiaobao Wei", "Xiaoan Zhang", "Hao Wang", "Qingpo Wuwu", "Ming Lu", "Wenzhao Zheng", "Shanghang Zhang"], "title": "OmniIndoor3D: Comprehensive Indoor 3D Reconstruction", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "We propose a novel framework for comprehensive indoor 3D reconstruction using\nGaussian representations, called OmniIndoor3D. This framework enables accurate\nappearance, geometry, and panoptic reconstruction of diverse indoor scenes\ncaptured by a consumer-level RGB-D camera. Since 3DGS is primarily optimized\nfor photorealistic rendering, it lacks the precise geometry critical for\nhigh-quality panoptic reconstruction. Therefore, OmniIndoor3D first combines\nmultiple RGB-D images to create a coarse 3D reconstruction, which is then used\nto initialize the 3D Gaussians and guide the 3DGS training. To decouple the\noptimization conflict between appearance and geometry, we introduce a\nlightweight MLP that adjusts the geometric properties of 3D Gaussians. The\nintroduced lightweight MLP serves as a low-pass filter for geometry\nreconstruction and significantly reduces noise in indoor scenes. To improve the\ndistribution of Gaussian primitives, we propose a densification strategy guided\nby panoptic priors to encourage smoothness on planar surfaces. Through the\njoint optimization of appearance, geometry, and panoptic reconstruction,\nOmniIndoor3D provides comprehensive 3D indoor scene understanding, which\nfacilitates accurate and robust robotic navigation. We perform thorough\nevaluations across multiple datasets, and OmniIndoor3D achieves\nstate-of-the-art results in appearance, geometry, and panoptic reconstruction.\nWe believe our work bridges a critical gap in indoor 3D reconstruction. The\ncode will be released at: https://ucwxb.github.io/OmniIndoor3D/"}
{"id": "2505.20611", "pdf": "https://arxiv.org/pdf/2505.20611", "abs": "https://arxiv.org/abs/2505.20611", "authors": ["Zenghao Zheng", "Lianping Yang", "Jinshan Pan", "Hegui Zhu"], "title": "Mamba-Driven Topology Fusion for Monocular 3-D Human Pose Estimation", "categories": ["cs.CV"], "comment": null, "summary": "Transformer-based methods for 3-D human pose estimation face significant\ncomputational challenges due to the quadratic growth of self-attention\nmechanism complexity with sequence length. Recently, the Mamba model has\nsubstantially reduced computational overhead and demonstrated outstanding\nperformance in modeling long sequences by leveraging state space model (SSM).\nHowever, the ability of SSM to process sequential data is not suitable for 3-D\njoint sequences with topological structures, and the causal convolution\nstructure in Mamba also lacks insight into local joint relationships. To\naddress these issues, we propose the Mamba-Driven Topology Fusion framework in\nthis paper. Specifically, the proposed Bone Aware Module infers the direction\nand length of bone vectors in the spherical coordinate system, providing\neffective topological guidance for the Mamba model in processing joint\nsequences. Furthermore, we enhance the convolutional structure within the Mamba\nmodel by integrating forward and backward graph convolutional network, enabling\nit to better capture local joint dependencies. Finally, we design a\nSpatiotemporal Refinement Module to model both temporal and spatial\nrelationships within the sequence. Through the incorporation of skeletal\ntopology, our approach effectively alleviates Mamba's limitations in capturing\nhuman structural relationships. We conduct extensive experiments on the\nHuman3.6M and MPI-INF-3DHP datasets for testing and comparison, and the results\nshow that the proposed method greatly reduces computational cost while\nachieving higher accuracy. Ablation studies further demonstrate the\neffectiveness of each proposed module. The code and models will be released."}
{"id": "2505.20612", "pdf": "https://arxiv.org/pdf/2505.20612", "abs": "https://arxiv.org/abs/2505.20612", "authors": ["Peter Robicheaux", "Matvei Popov", "Anish Madan", "Isaac Robinson", "Joseph Nelson", "Deva Ramanan", "Neehar Peri"], "title": "Roboflow100-VL: A Multi-Domain Object Detection Benchmark for Vision-Language Models", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": "The first two authors contributed equally", "summary": "Vision-language models (VLMs) trained on internet-scale data achieve\nremarkable zero-shot detection performance on common objects like car, truck,\nand pedestrian. However, state-of-the-art models still struggle to generalize\nto out-of-distribution classes, tasks and imaging modalities not typically\nfound in their pre-training. Rather than simply re-training VLMs on more visual\ndata, we argue that one should align VLMs to new concepts with annotation\ninstructions containing a few visual examples and rich textual descriptions. To\nthis end, we introduce Roboflow100-VL, a large-scale collection of 100\nmulti-modal object detection datasets with diverse concepts not commonly found\nin VLM pre-training. We evaluate state-of-the-art models on our benchmark in\nzero-shot, few-shot, semi-supervised, and fully-supervised settings, allowing\nfor comparison across data regimes. Notably, we find that VLMs like\nGroundingDINO and Qwen2.5-VL achieve less than 2% zero-shot accuracy on\nchallenging medical imaging datasets within Roboflow100-VL, demonstrating the\nneed for few-shot concept alignment. Our code and dataset are available at\nhttps://github.com/roboflow/rf100-vl/ and\nhttps://universe.roboflow.com/rf100-vl/"}
{"id": "2505.20615", "pdf": "https://arxiv.org/pdf/2505.20615", "abs": "https://arxiv.org/abs/2505.20615", "authors": ["Omid Halimi Milani", "Ahmet Enis Cetin", "Bharati Prasad"], "title": "Intelligent Incident Hypertension Prediction in Obstructive Sleep Apnea", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted at EUSIPCO 2025. Camera-ready due June 20, 2025", "summary": "Obstructive sleep apnea (OSA) is a significant risk factor for hypertension,\nprimarily due to intermittent hypoxia and sleep fragmentation. Predicting\nwhether individuals with OSA will develop hypertension within five years\nremains a complex challenge. This study introduces a novel deep learning\napproach that integrates Discrete Cosine Transform (DCT)-based transfer\nlearning to enhance prediction accuracy. We are the first to incorporate all\npolysomnography signals together for hypertension prediction, leveraging their\ncollective information to improve model performance. Features were extracted\nfrom these signals and transformed into a 2D representation to utilize\npre-trained 2D neural networks such as MobileNet, EfficientNet, and ResNet\nvariants. To further improve feature learning, we introduced a DCT layer, which\ntransforms input features into a frequency-based representation, preserving\nessential spectral information, decorrelating features, and enhancing\nrobustness to noise. This frequency-domain approach, coupled with transfer\nlearning, is especially beneficial for limited medical datasets, as it\nleverages rich representations from pre-trained networks to improve\ngeneralization. By strategically placing the DCT layer at deeper truncation\ndepths within EfficientNet, our model achieved a best area under the curve\n(AUC) of 72.88%, demonstrating the effectiveness of frequency-domain feature\nextraction and transfer learning in predicting hypertension risk in OSA\npatients over a five-year period."}
{"id": "2505.20617", "pdf": "https://arxiv.org/pdf/2505.20617", "abs": "https://arxiv.org/abs/2505.20617", "authors": ["Naiyu Fang", "Zheyuan Zhou", "Fayao Liu", "Xulei Yang", "Jiacheng Wei", "Lemiao Qiu", "Guosheng Lin"], "title": "OccLE: Label-Efficient 3D Semantic Occupancy Prediction", "categories": ["cs.CV"], "comment": null, "summary": "3D semantic occupancy prediction offers an intuitive and efficient scene\nunderstanding and has attracted significant interest in autonomous driving\nperception. Existing approaches either rely on full supervision, which demands\ncostly voxel-level annotations, or on self-supervision, which provides limited\nguidance and yields suboptimal performance. To address these challenges, we\npropose OccLE, a Label-Efficient 3D Semantic Occupancy Prediction that takes\nimages and LiDAR as inputs and maintains high performance with limited voxel\nannotations. Our intuition is to decouple the semantic and geometric learning\ntasks and then fuse the learned feature grids from both tasks for the final\nsemantic occupancy prediction. Therefore, the semantic branch distills 2D\nfoundation model to provide aligned pseudo labels for 2D and 3D semantic\nlearning. The geometric branch integrates image and LiDAR inputs in cross-plane\nsynergy based on their inherency, employing semi-supervision to enhance\ngeometry learning. We fuse semantic-geometric feature grids through Dual Mamba\nand incorporate a scatter-accumulated projection to supervise unannotated\nprediction with aligned pseudo labels. Experiments show that OccLE achieves\ncompetitive performance with only 10% of voxel annotations, reaching a mIoU of\n16.59% on the SemanticKITTI validation set."}
{"id": "2505.20626", "pdf": "https://arxiv.org/pdf/2505.20626", "abs": "https://arxiv.org/abs/2505.20626", "authors": ["Yohai Mazuz", "Janna Bruner", "Lior Wolf"], "title": "ConsiStyle: Style Diversity in Training-Free Consistent T2I Generation", "categories": ["cs.CV"], "comment": null, "summary": "In text-to-image models, consistent character generation is the task of\nachieving text alignment while maintaining the subject's appearance across\ndifferent prompts. However, since style and appearance are often entangled, the\nexisting methods struggle to preserve consistent subject characteristics while\nadhering to varying style prompts. Current approaches for consistent\ntext-to-image generation typically rely on large-scale fine-tuning on curated\nimage sets or per-subject optimization, which either fail to generalize across\nprompts or do not align well with textual descriptions. Meanwhile,\ntraining-free methods often fail to maintain subject consistency across\ndifferent styles. In this work, we introduce a training-free method that\nachieves both style alignment and subject consistency. The attention matrices\nare manipulated such that Queries and Keys are obtained from the anchor\nimage(s) that are used to define the subject, while the Values are imported\nfrom a parallel copy that is not subject-anchored. Additionally, cross-image\ncomponents are added to the self-attention mechanism by expanding the Key and\nValue matrices. To do without shifting from the target style, we align the\nstatistics of the Value matrices. As is demonstrated in a comprehensive battery\nof qualitative and quantitative experiments, our method effectively decouples\nstyle from subject appearance and enables faithful generation of text-aligned\nimages with consistent characters across diverse styles."}
{"id": "2505.20629", "pdf": "https://arxiv.org/pdf/2505.20629", "abs": "https://arxiv.org/abs/2505.20629", "authors": ["Bolin Lai", "Sangmin Lee", "Xu Cao", "Xiang Li", "James M. Rehg"], "title": "Incorporating Flexible Image Conditioning into Text-to-Video Diffusion Models without Training", "categories": ["cs.CV", "cs.LG"], "comment": "21 pages, 11 figures, 4 tables", "summary": "Text-image-to-video (TI2V) generation is a critical problem for controllable\nvideo generation using both semantic and visual conditions. Most existing\nmethods typically add visual conditions to text-to-video (T2V) foundation\nmodels by finetuning, which is costly in resources and only limited to a few\npredefined conditioning settings. To tackle this issue, we introduce a unified\nformulation for TI2V generation with flexible visual conditioning. Furthermore,\nwe propose an innovative training-free approach, dubbed FlexTI2V, that can\ncondition T2V foundation models on an arbitrary amount of images at arbitrary\npositions. Specifically, we firstly invert the condition images to noisy\nrepresentation in a latent space. Then, in the denoising process of T2V models,\nour method uses a novel random patch swapping strategy to incorporate visual\nfeatures into video representations through local image patches. To balance\ncreativity and fidelity, we use a dynamic control mechanism to adjust the\nstrength of visual conditioning to each video frame. Extensive experiments\nvalidate that our method surpasses previous training-free image conditioning\nmethods by a notable margin. We also show more insights of our method by\ndetailed ablation study and analysis."}
{"id": "2505.20637", "pdf": "https://arxiv.org/pdf/2505.20637", "abs": "https://arxiv.org/abs/2505.20637", "authors": ["Ana M. Cabanas", "Alma Pedro", "Domingo Mery"], "title": "TrustSkin: A Fairness Pipeline for Trustworthy Facial Affect Analysis Across Skin Tone", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages", "summary": "Understanding how facial affect analysis (FAA) systems perform across\ndifferent demographic groups requires reliable measurement of sensitive\nattributes such as ancestry, often approximated by skin tone, which itself is\nhighly influenced by lighting conditions. This study compares two objective\nskin tone classification methods: the widely used Individual Typology Angle\n(ITA) and a perceptually grounded alternative based on Lightness ($L^*$) and\nHue ($H^*$). Using AffectNet and a MobileNet-based model, we assess fairness\nacross skin tone groups defined by each method. Results reveal a severe\nunderrepresentation of dark skin tones ($\\sim 2 \\%$), alongside fairness\ndisparities in F1-score (up to 0.08) and TPR (up to 0.11) across groups. While\nITA shows limitations due to its sensitivity to lighting, the $H^*$-$L^*$\nmethod yields more consistent subgrouping and enables clearer diagnostics\nthrough metrics such as Equal Opportunity. Grad-CAM analysis further highlights\ndifferences in model attention patterns by skin tone, suggesting variation in\nfeature encoding. To support future mitigation efforts, we also propose a\nmodular fairness-aware pipeline that integrates perceptual skin tone\nestimation, model interpretability, and fairness evaluation. These findings\nemphasize the relevance of skin tone measurement choices in fairness assessment\nand suggest that ITA-based evaluations may overlook disparities affecting\ndarker-skinned individuals."}
{"id": "2505.20639", "pdf": "https://arxiv.org/pdf/2505.20639", "abs": "https://arxiv.org/abs/2505.20639", "authors": ["Guiping Cao", "Tao Wang", "Wenjian Huang", "Xiangyuan Lan", "Jianguo Zhang", "Dongmei Jiang"], "title": "Open-Det: An Efficient Learning Framework for Open-Ended Detection", "categories": ["cs.CV"], "comment": "ICML 2025", "summary": "Open-Ended object Detection (OED) is a novel and challenging task that\ndetects objects and generates their category names in a free-form manner,\nwithout requiring additional vocabularies during inference. However, the\nexisting OED models, such as GenerateU, require large-scale datasets for\ntraining, suffer from slow convergence, and exhibit limited performance. To\naddress these issues, we present a novel and efficient Open-Det framework,\nconsisting of four collaborative parts. Specifically, Open-Det accelerates\nmodel training in both the bounding box and object name generation process by\nreconstructing the Object Detector and the Object Name Generator. To bridge the\nsemantic gap between Vision and Language modalities, we propose a\nVision-Language Aligner with V-to-L and L-to-V alignment mechanisms,\nincorporating with the Prompts Distiller to transfer knowledge from the VLM\ninto VL-prompts, enabling accurate object name generation for the LLM. In\naddition, we design a Masked Alignment Loss to eliminate contradictory\nsupervision and introduce a Joint Loss to enhance classification, resulting in\nmore efficient training. Compared to GenerateU, Open-Det, using only 1.5% of\nthe training data (0.077M vs. 5.077M), 20.8% of the training epochs (31 vs.\n149), and fewer GPU resources (4 V100 vs. 16 A100), achieves even higher\nperformance (+1.0% in APr). The source codes are available at:\nhttps://github.com/Med-Process/Open-Det."}
{"id": "2505.20640", "pdf": "https://arxiv.org/pdf/2505.20640", "abs": "https://arxiv.org/abs/2505.20640", "authors": ["Yifan Li", "Yuhang Chen", "Anh Dao", "Lichi Li", "Zhongyi Cai", "Zhen Tan", "Tianlong Chen", "Yu Kong"], "title": "IndustryEQA: Pushing the Frontiers of Embodied Question Answering in Industrial Scenarios", "categories": ["cs.CV"], "comment": "v1.0", "summary": "Existing Embodied Question Answering (EQA) benchmarks primarily focus on\nhousehold environments, often overlooking safety-critical aspects and reasoning\nprocesses pertinent to industrial settings. This drawback limits the evaluation\nof agent readiness for real-world industrial applications. To bridge this, we\nintroduce IndustryEQA, the first benchmark dedicated to evaluating embodied\nagent capabilities within safety-critical warehouse scenarios. Built upon the\nNVIDIA Isaac Sim platform, IndustryEQA provides high-fidelity episodic memory\nvideos featuring diverse industrial assets, dynamic human agents, and carefully\ndesigned hazardous situations inspired by real-world safety guidelines. The\nbenchmark includes rich annotations covering six categories: equipment safety,\nhuman safety, object recognition, attribute recognition, temporal\nunderstanding, and spatial understanding. Besides, it also provides extra\nreasoning evaluation based on these categories. Specifically, it comprises 971\nquestion-answer pairs generated from small warehouse and 373 pairs from large\nones, incorporating scenarios with and without human. We further propose a\ncomprehensive evaluation framework, including various baseline models, to\nassess their general perception and reasoning abilities in industrial\nenvironments. IndustryEQA aims to steer EQA research towards developing more\nrobust, safety-aware, and practically applicable embodied agents for complex\nindustrial environments. Benchmark and codes are available."}
{"id": "2505.20641", "pdf": "https://arxiv.org/pdf/2505.20641", "abs": "https://arxiv.org/abs/2505.20641", "authors": ["Yuan Wu", "Zhiqiang Yan", "Yigong Zhang", "Xiang Li", "ian Yang"], "title": "See through the Dark: Learning Illumination-affined Representations for Nighttime Occupancy Prediction", "categories": ["cs.CV"], "comment": null, "summary": "Occupancy prediction aims to estimate the 3D spatial distribution of occupied\nregions along with their corresponding semantic labels. Existing vision-based\nmethods perform well on daytime benchmarks but struggle in nighttime scenarios\ndue to limited visibility and challenging lighting conditions. To address these\nchallenges, we propose \\textbf{LIAR}, a novel framework that learns\nillumination-affined representations. LIAR first introduces Selective Low-light\nImage Enhancement (SLLIE), which leverages the illumination priors from daytime\nscenes to adaptively determine whether a nighttime image is genuinely dark or\nsufficiently well-lit, enabling more targeted global enhancement. Building on\nthe illumination maps generated by SLLIE, LIAR further incorporates two\nillumination-aware components: 2D Illumination-guided Sampling (2D-IGS) and 3D\nIllumination-driven Projection (3D-IDP), to respectively tackle local\nunderexposure and overexposure. Specifically, 2D-IGS modulates feature sampling\npositions according to illumination maps, assigning larger offsets to darker\nregions and smaller ones to brighter regions, thereby alleviating feature\ndegradation in underexposed areas. Subsequently, 3D-IDP enhances semantic\nunderstanding in overexposed regions by constructing illumination intensity\nfields and supplying refined residual queries to the BEV context refinement\nprocess. Extensive experiments on both real and synthetic datasets demonstrate\nthe superior performance of LIAR under challenging nighttime scenarios. The\nsource code and pretrained models are available\n\\href{https://github.com/yanzq95/LIAR}{here}."}
{"id": "2505.20644", "pdf": "https://arxiv.org/pdf/2505.20644", "abs": "https://arxiv.org/abs/2505.20644", "authors": ["Haoyu Zhang", "Yisen Feng", "Qiaohui Chu", "Meng Liu", "Weili Guan", "Yaowei Wang", "Liqiang Nie"], "title": "HCQA-1.5 @ Ego4D EgoSchema Challenge 2025", "categories": ["cs.CV", "cs.AI"], "comment": "The third-place solution for the Ego4D EgoSchema Challenge at the\n  CVPR EgoVis Workshop 2025", "summary": "In this report, we present the method that achieves third place for Ego4D\nEgoSchema Challenge in CVPR 2025. To improve the reliability of answer\nprediction in egocentric video question answering, we propose an effective\nextension to the previously proposed HCQA framework. Our approach introduces a\nmulti-source aggregation strategy to generate diverse predictions, followed by\na confidence-based filtering mechanism that selects high-confidence answers\ndirectly. For low-confidence cases, we incorporate a fine-grained reasoning\nmodule that performs additional visual and contextual analysis to refine the\npredictions. Evaluated on the EgoSchema blind test set, our method achieves 77%\naccuracy on over 5,000 human-curated multiple-choice questions, outperforming\nlast year's winning solution and the majority of participating teams. Our code\nwill be added at https://github.com/Hyu-Zhang/HCQA."}
{"id": "2505.20649", "pdf": "https://arxiv.org/pdf/2505.20649", "abs": "https://arxiv.org/abs/2505.20649", "authors": ["HsiaoYuan Hsu", "Yuxin Peng"], "title": "Scan-and-Print: Patch-level Data Summarization and Augmentation for Content-aware Layout Generation in Poster Design", "categories": ["cs.CV"], "comment": "Accepted to IJCAI 2025 (AI, Arts and Creativity). Project page is at\n  https://thekinsley.github.io/Scan-and-Print/", "summary": "In AI-empowered poster design, content-aware layout generation is crucial for\nthe on-image arrangement of visual-textual elements, e.g., logo, text, and\nunderlay. To perceive the background images, existing work demanded a high\nparameter count that far exceeds the size of available training data, which has\nimpeded the model's real-time performance and generalization ability. To\naddress these challenges, we proposed a patch-level data summarization and\naugmentation approach, vividly named Scan-and-Print. Specifically, the scan\nprocedure selects only the patches suitable for placing element vertices to\nperform fine-grained perception efficiently. Then, the print procedure mixes up\nthe patches and vertices across two image-layout pairs to synthesize over 100%\nnew samples in each epoch while preserving their plausibility. Besides, to\nfacilitate the vertex-level operations, a vertex-based layout representation is\nintroduced. Extensive experimental results on widely used benchmarks\ndemonstrated that Scan-and-Print can generate visually appealing layouts with\nstate-of-the-art quality while dramatically reducing computational bottleneck\nby 95.2%."}
{"id": "2505.20653", "pdf": "https://arxiv.org/pdf/2505.20653", "abs": "https://arxiv.org/abs/2505.20653", "authors": ["Lingyu Qiu", "Ke Jiang", "Xiaoyang Tan"], "title": "RoGA: Towards Generalizable Deepfake Detection through Robust Gradient Alignment", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to ICME2025", "summary": "Recent advancements in domain generalization for deepfake detection have\nattracted significant attention, with previous methods often incorporating\nadditional modules to prevent overfitting to domain-specific patterns. However,\nsuch regularization can hinder the optimization of the empirical risk\nminimization (ERM) objective, ultimately degrading model performance. In this\npaper, we propose a novel learning objective that aligns generalization\ngradient updates with ERM gradient updates. The key innovation is the\napplication of perturbations to model parameters, aligning the ascending points\nacross domains, which specifically enhances the robustness of deepfake\ndetection models to domain shifts. This approach effectively preserves\ndomain-invariant features while managing domain-specific characteristics,\nwithout introducing additional regularization. Experimental results on multiple\nchallenging deepfake detection datasets demonstrate that our gradient alignment\nstrategy outperforms state-of-the-art domain generalization techniques,\nconfirming the efficacy of our method. The code is available at\nhttps://github.com/Lynn0925/RoGA."}
{"id": "2505.20655", "pdf": "https://arxiv.org/pdf/2505.20655", "abs": "https://arxiv.org/abs/2505.20655", "authors": ["Lujian Yao", "Siming Zheng", "Xinbin Yuan", "Zhuoxuan Cai", "Pu Wu", "Jinwei Chen", "Bo Li", "Peng-Tao Jiang"], "title": "Photography Perspective Composition: Towards Aesthetic Perspective Recommendation", "categories": ["cs.CV"], "comment": null, "summary": "Traditional photography composition approaches are dominated by 2D\ncropping-based methods. However, these methods fall short when scenes contain\npoorly arranged subjects. Professional photographers often employ perspective\nadjustment as a form of 3D recomposition, modifying the projected 2D\nrelationships between subjects while maintaining their actual spatial positions\nto achieve better compositional balance. Inspired by this artistic practice, we\npropose photography perspective composition (PPC), extending beyond traditional\ncropping-based methods. However, implementing the PPC faces significant\nchallenges: the scarcity of perspective transformation datasets and undefined\nassessment criteria for perspective quality. To address these challenges, we\npresent three key contributions: (1) An automated framework for building PPC\ndatasets through expert photographs. (2) A video generation approach that\ndemonstrates the transformation process from suboptimal to optimal\nperspectives. (3) A perspective quality assessment (PQA) model constructed\nbased on human performance. Our approach is concise and requires no additional\nprompt instructions or camera trajectories, helping and guiding ordinary users\nto enhance their composition skills."}
{"id": "2505.20665", "pdf": "https://arxiv.org/pdf/2505.20665", "abs": "https://arxiv.org/abs/2505.20665", "authors": ["Muxi Diao", "Lele Yang", "Hongbo Yin", "Zhexu Wang", "Yejie Wang", "Daxin Tian", "Kongming Liang", "Zhanyu Ma"], "title": "DriveRX: A Vision-Language Reasoning Model for Cross-Task Autonomous Driving", "categories": ["cs.CV"], "comment": null, "summary": "Autonomous driving requires real-time, robust reasoning across perception,\nprediction, planning, and behavior. However, conventional end-to-end models\nfail to generalize in complex scenarios due to the lack of structured\nreasoning. Recent vision-language models (VLMs) have been applied to driving\ntasks, but they typically rely on isolated modules and static supervision,\nlimiting their ability to support multi-stage decision-making. We present\nAutoDriveRL, a unified training framework that formulates autonomous driving as\na structured reasoning process over four core tasks. Each task is independently\nmodeled as a vision-language question-answering problem and optimized using\ntask-specific reward models, enabling fine-grained reinforcement signals at\ndifferent reasoning stages. Within this framework, we train DriveRX, a\ncross-task reasoning VLM designed for real-time decision-making. DriveRX\nachieves strong performance on a public benchmark, outperforming GPT-4o in\nbehavior reasoning and demonstrating robustness under complex or corrupted\ndriving conditions. Our analysis further highlights the impact of vision\nencoder design and reward-guided reasoning compression. We will release the\nAutoDriveRL framework and the DriveRX model to support future research."}
{"id": "2505.20675", "pdf": "https://arxiv.org/pdf/2505.20675", "abs": "https://arxiv.org/abs/2505.20675", "authors": ["Lingyu Qiu", "Ke Jiang", "Xiaoyang Tan"], "title": "Contrastive Desensitization Learning for Cross Domain Face Forgery Detection", "categories": ["cs.CV"], "comment": null, "summary": "In this paper, we propose a new cross-domain face forgery detection method\nthat is insensitive to different and possibly unseen forgery methods while\nensuring an acceptable low false positive rate. Although existing face forgery\ndetection methods are applicable to multiple domains to some degree, they often\ncome with a high false positive rate, which can greatly disrupt the usability\nof the system. To address this issue, we propose an Contrastive Desensitization\nNetwork (CDN) based on a robust desensitization algorithm, which captures the\nessential domain characteristics through learning them from domain\ntransformation over pairs of genuine face images. One advantage of CDN lies in\nthat the learnt face representation is theoretical justified with regard to the\nits robustness against the domain changes. Extensive experiments over\nlarge-scale benchmark datasets demonstrate that our method achieves a much\nlower false alarm rate with improved detection accuracy compared to several\nstate-of-the-art methods."}
{"id": "2505.20676", "pdf": "https://arxiv.org/pdf/2505.20676", "abs": "https://arxiv.org/abs/2505.20676", "authors": ["Sadaf Safa", "Ali Abedi", "Shehroz S. Khan"], "title": "Supervised Contrastive Learning for Ordinal Engagement Measurement", "categories": ["cs.CV", "cs.HC"], "comment": "9 pages, 1 figure, 5 tables", "summary": "Student engagement plays a crucial role in the successful delivery of\neducational programs. Automated engagement measurement helps instructors\nmonitor student participation, identify disengagement, and adapt their teaching\nstrategies to enhance learning outcomes effectively. This paper identifies two\nkey challenges in this problem: class imbalance and incorporating order into\nengagement levels rather than treating it as mere categories. Then, a novel\napproach to video-based student engagement measurement in virtual learning\nenvironments is proposed that utilizes supervised contrastive learning for\nordinal classification of engagement. Various affective and behavioral features\nare extracted from video samples and utilized to train ordinal classifiers\nwithin a supervised contrastive learning framework (with a sequential\nclassifier as the encoder). A key step involves the application of diverse\ntime-series data augmentation techniques to these feature vectors, enhancing\nmodel training. The effectiveness of the proposed method was evaluated using a\npublicly available dataset for engagement measurement, DAiSEE, containing\nvideos of students who participated in virtual learning programs. The results\ndemonstrate the robust ability of the proposed method for the classification of\nthe engagement level. This approach promises a significant contribution to\nunderstanding and enhancing student engagement in virtual learning\nenvironments."}
{"id": "2505.20680", "pdf": "https://arxiv.org/pdf/2505.20680", "abs": "https://arxiv.org/abs/2505.20680", "authors": ["Haodong Lu", "Xinyu Zhang", "Kristen Moore", "Jason Xue", "Lina Yao", "Anton van den Hengel", "Dong Gong"], "title": "Continual Learning on CLIP via Incremental Prompt Tuning with Intrinsic Textual Anchors", "categories": ["cs.CV"], "comment": "Preprint", "summary": "Continual learning (CL) enables deep networks to acquire new knowledge while\navoiding catastrophic forgetting. The powerful generalization ability of\npre-trained models (PTMs), such as the Contrastive Language-Image Pre-training\n(CLIP) model, has inspired a range of CL methods targeting new and specialized\ntasks, providing rich multi-modal embeddings that support lightweight,\nincremental prompt tuning. Existing methods often rely on complex designs built\nupon specific assumptions, such as intricate regularization schemes for prompt\npools, specialized routing mechanisms, or multi-stage incrementations, that\nintroduce additional-and possibly unnecessary-complexity, underutilizing CLIP's\nintrinsic capabilities. In this paper, we propose a concise CL approach for\nCLIP based on incremental prompt tuning that fully exploits its multi-modal\nstructure and the stability of textual representations. Our method, Textual\nPrototype-guided Prompt Tuning (TPPT), introduces textual prototypes not merely\nas static classifiers, as in existing methods, but as stable anchors to guide\nthe learning of visual prompts, thereby shaping the embedding space (i.e.,\nTPPT-V). We show that our bidirectional supervision strategy enables more\neffective learning of new knowledge while reducing forgetting. To further close\nthe vision-language gap during CL, we jointly optimizes visual and textual\nprompts (i.e., TPPT-VT). We also introduce a relational diversity\nregularization on the textual anchors to prevent embedding space collapse and\nmitigate correlated forgetting. Extensive experiments and analyses demonstrate\nthe effectiveness of our proposed approach, highlighting the benefits of\nleveraging CLIP's intrinsic guidance for continual adaptation."}
{"id": "2505.20687", "pdf": "https://arxiv.org/pdf/2505.20687", "abs": "https://arxiv.org/abs/2505.20687", "authors": ["Mingxuan Sun", "Juntao Jiang", "Zhiqiang Yang", "Shenao Kong", "Jiamin Qi", "Jianru Shang", "Shuangling Luo", "Wanfa Sun", "Tianyi Wang", "Yanqi Wang", "Qixuan Wang", "Tingjian Dai", "Tianxiang Chen", "Jinming Zhang", "Xuerui Zhang", "Yuepeng He", "Pengcheng Fu", "Qiu Guan", "Shizheng Zhou", "Yanbo Yu", "Qigui Jiang", "Teng Zhou", "Liuyong Shi", "Hong Yan"], "title": "VisAlgae 2023: A Dataset and Challenge for Algae Detection in Microscopy Images", "categories": ["cs.CV"], "comment": null, "summary": "Microalgae, vital for ecological balance and economic sectors, present\nchallenges in detection due to their diverse sizes and conditions. This paper\nsummarizes the second \"Vision Meets Algae\" (VisAlgae 2023) Challenge, aiming to\nenhance high-throughput microalgae cell detection. The challenge, which\nattracted 369 participating teams, includes a dataset of 1000 images across six\nclasses, featuring microalgae of varying sizes and distinct features.\nParticipants faced tasks such as detecting small targets, handling motion blur,\nand complex backgrounds. The top 10 methods, outlined here, offer insights into\novercoming these challenges and maximizing detection accuracy. This\nintersection of algae research and computer vision offers promise for\necological understanding and technological advancement. The dataset can be\naccessed at: https://github.com/juntaoJianggavin/Visalgae2023/."}
{"id": "2505.20694", "pdf": "https://arxiv.org/pdf/2505.20694", "abs": "https://arxiv.org/abs/2505.20694", "authors": ["Xulin Gu", "Xinhao Zhong", "Zhixing Wei", "Yimin Zhou", "Shuoyang Sun", "Bin Chen", "Hongpeng Wang", "Yuan Luo"], "title": "Temporal Saliency-Guided Distillation: A Scalable Framework for Distilling Video Datasets", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Dataset distillation (DD) has emerged as a powerful paradigm for dataset\ncompression, enabling the synthesis of compact surrogate datasets that\napproximate the training utility of large-scale ones. While significant\nprogress has been achieved in distilling image datasets, extending DD to the\nvideo domain remains challenging due to the high dimensionality and temporal\ncomplexity inherent in video data. Existing video distillation (VD) methods\noften suffer from excessive computational costs and struggle to preserve\ntemporal dynamics, as na\\\"ive extensions of image-based approaches typically\nlead to degraded performance. In this paper, we propose a novel uni-level video\ndataset distillation framework that directly optimizes synthetic videos with\nrespect to a pre-trained model. To address temporal redundancy and enhance\nmotion preservation, we introduce a temporal saliency-guided filtering\nmechanism that leverages inter-frame differences to guide the distillation\nprocess, encouraging the retention of informative temporal cues while\nsuppressing frame-level redundancy. Extensive experiments on standard video\nbenchmarks demonstrate that our method achieves state-of-the-art performance,\nbridging the gap between real and distilled video data and offering a scalable\nsolution for video dataset compression."}
{"id": "2505.20704", "pdf": "https://arxiv.org/pdf/2505.20704", "abs": "https://arxiv.org/abs/2505.20704", "authors": ["Zixuan Hu", "Yichun Hu", "Xiaotong Li", "Shixiang Tang", "Ling-Yu Duan"], "title": "Beyond Entropy: Region Confidence Proxy for Wild Test-Time Adaptation", "categories": ["cs.CV"], "comment": "Accepted by ICML 2025", "summary": "Wild Test-Time Adaptation (WTTA) is proposed to adapt a source model to\nunseen domains under extreme data scarcity and multiple shifts. Previous\napproaches mainly focused on sample selection strategies, while overlooking the\nfundamental problem on underlying optimization. Initially, we critically\nanalyze the widely-adopted entropy minimization framework in WTTA and uncover\nits significant limitations in noisy optimization dynamics that substantially\nhinder adaptation efficiency. Through our analysis, we identify region\nconfidence as a superior alternative to traditional entropy, however, its\ndirect optimization remains computationally prohibitive for real-time\napplications. In this paper, we introduce a novel region-integrated method\nReCAP that bypasses the lengthy process. Specifically, we propose a\nprobabilistic region modeling scheme that flexibly captures semantic changes in\nembedding space. Subsequently, we develop a finite-to-infinite asymptotic\napproximation that transforms the intractable region confidence into a\ntractable and upper-bounded proxy. These innovations significantly unlock the\noverlooked potential dynamics in local region in a concise solution. Our\nextensive experiments demonstrate the consistent superiority of ReCAP over\nexisting methods across various datasets and wild scenarios."}
{"id": "2505.20710", "pdf": "https://arxiv.org/pdf/2505.20710", "abs": "https://arxiv.org/abs/2505.20710", "authors": ["Kui Wu", "Hao Chen", "Churan Wang", "Fakhri Karray", "Zhoujun Li", "Yizhou Wang", "Fangwei Zhong"], "title": "Hierarchical Instruction-aware Embodied Visual Tracking", "categories": ["cs.CV"], "comment": null, "summary": "User-Centric Embodied Visual Tracking (UC-EVT) presents a novel challenge for\nreinforcement learning-based models due to the substantial gap between\nhigh-level user instructions and low-level agent actions. While recent\nadvancements in language models (e.g., LLMs, VLMs, VLAs) have improved\ninstruction comprehension, these models face critical limitations in either\ninference speed (LLMs, VLMs) or generalizability (VLAs) for UC-EVT tasks. To\naddress these challenges, we propose \\textbf{Hierarchical Instruction-aware\nEmbodied Visual Tracking (HIEVT)} agent, which bridges instruction\ncomprehension and action generation using \\textit{spatial goals} as\nintermediaries. HIEVT first introduces \\textit{LLM-based Semantic-Spatial Goal\nAligner} to translate diverse human instructions into spatial goals that\ndirectly annotate the desired spatial position. Then the \\textit{RL-based\nAdaptive Goal-Aligned Policy}, a general offline policy, enables the tracker to\nposition the target as specified by the spatial goal. To benchmark UC-EVT\ntasks, we collect over ten million trajectories for training and evaluate\nacross one seen environment and nine unseen challenging environments. Extensive\nexperiments and real-world deployments demonstrate the robustness and\ngeneralizability of HIEVT across diverse environments, varying target dynamics,\nand complex instruction combinations. The complete project is available at\nhttps://sites.google.com/view/hievt."}
{"id": "2505.20715", "pdf": "https://arxiv.org/pdf/2505.20715", "abs": "https://arxiv.org/abs/2505.20715", "authors": ["Fuwen Luo", "Shengfeng Lou", "Chi Chen", "Ziyue Wang", "Chenliang Li", "Weizhou Shen", "Jiyue Guo", "Peng Li", "Ming Yan", "Ji Zhang", "Fei Huang", "Yang Liu"], "title": "MUSEG: Reinforcing Video Temporal Understanding via Timestamp-Aware Multi-Segment Grounding", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Video temporal understanding is crucial for multimodal large language models\n(MLLMs) to reason over events in videos. Despite recent advances in general\nvideo understanding, current MLLMs still struggle with fine-grained temporal\nreasoning. While reinforcement learning (RL) has been explored to address this\nissue recently, existing RL approaches remain limited in effectiveness. In this\nwork, we propose MUSEG, a novel RL-based method that enhances temporal\nunderstanding by introducing timestamp-aware multi-segment grounding. MUSEG\nenables MLLMs to align queries with multiple relevant video segments, promoting\nmore comprehensive temporal reasoning. To facilitate effective learning, we\ndesign a customized RL training recipe with phased rewards that progressively\nguides the model toward temporally grounded reasoning. Extensive experiments on\ntemporal grounding and time-sensitive video QA tasks demonstrate that MUSEG\nsignificantly outperforms existing methods and generalizes well across diverse\ntemporal understanding scenarios. View our project at\nhttps://github.com/THUNLP-MT/MUSEG."}
{"id": "2505.20718", "pdf": "https://arxiv.org/pdf/2505.20718", "abs": "https://arxiv.org/abs/2505.20718", "authors": ["Kui Wu", "Shuhang Xu", "Hao Chen", "Churan Wang", "Zhoujun Li", "Yizhou Wang", "Fangwei Zhong"], "title": "VLM Can Be a Good Assistant: Enhancing Embodied Visual Tracking with Self-Improving Visual-Language Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We introduce a novel self-improving framework that enhances Embodied Visual\nTracking (EVT) with Visual-Language Models (VLMs) to address the limitations of\ncurrent active visual tracking systems in recovering from tracking failure. Our\napproach combines the off-the-shelf active tracking methods with VLMs'\nreasoning capabilities, deploying a fast visual policy for normal tracking and\nactivating VLM reasoning only upon failure detection. The framework features a\nmemory-augmented self-reflection mechanism that enables the VLM to\nprogressively improve by learning from past experiences, effectively addressing\nVLMs' limitations in 3D spatial reasoning. Experimental results demonstrate\nsignificant performance improvements, with our framework boosting success rates\nby $72\\%$ with state-of-the-art RL-based approaches and $220\\%$ with PID-based\nmethods in challenging environments. This work represents the first integration\nof VLM-based reasoning to assist EVT agents in proactive failure recovery,\noffering substantial advances for real-world robotic applications that require\ncontinuous target monitoring in dynamic, unstructured environments. Project\nwebsite: https://sites.google.com/view/evt-recovery-assistant."}
{"id": "2505.20723", "pdf": "https://arxiv.org/pdf/2505.20723", "abs": "https://arxiv.org/abs/2505.20723", "authors": ["Pascal Zwick", "Nils Friederich", "Maximilian Beichter", "Lennart Hilbert", "Ralf Mikut", "Oliver Bringmann"], "title": "LeDiFlow: Learned Distribution-guided Flow Matching to Accelerate Image Generation", "categories": ["cs.CV", "cs.LG", "I.4; I.2"], "comment": null, "summary": "Enhancing the efficiency of high-quality image generation using Diffusion\nModels (DMs) is a significant challenge due to the iterative nature of the\nprocess. Flow Matching (FM) is emerging as a powerful generative modeling\nparadigm based on a simulation-free training objective instead of a score-based\none used in DMs. Typical FM approaches rely on a Gaussian distribution prior,\nwhich induces curved, conditional probability paths between the prior and\ntarget data distribution. These curved paths pose a challenge for the Ordinary\nDifferential Equation (ODE) solver, requiring a large number of inference calls\nto the flow prediction network. To address this issue, we present Learned\nDistribution-guided Flow Matching (LeDiFlow), a novel scalable method for\ntraining FM-based image generation models using a better-suited prior\ndistribution learned via a regression-based auxiliary model. By initializing\nthe ODE solver with a prior closer to the target data distribution, LeDiFlow\nenables the learning of more computationally tractable probability paths. These\npaths directly translate to fewer solver steps needed for high-quality image\ngeneration at inference time. Our method utilizes a State-Of-The-Art (SOTA)\ntransformer architecture combined with latent space sampling and can be trained\non a consumer workstation. We empirically demonstrate that LeDiFlow remarkably\noutperforms the respective FM baselines. For instance, when operating directly\non pixels, our model accelerates inference by up to 3.75x compared to the\ncorresponding pixel-space baseline. Simultaneously, our latent FM model\nenhances image quality on average by 1.32x in CLIP Maximum Mean Discrepancy\n(CMMD) metric against its respective baseline."}
{"id": "2505.20729", "pdf": "https://arxiv.org/pdf/2505.20729", "abs": "https://arxiv.org/abs/2505.20729", "authors": ["Xiangyu Sun", "Runnan Chen", "Mingming Gong", "Dong Xu", "Tongliang Liu"], "title": "Intern-GS: Vision Model Guided Sparse-View 3D Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "Sparse-view scene reconstruction often faces significant challenges due to\nthe constraints imposed by limited observational data. These limitations result\nin incomplete information, leading to suboptimal reconstructions using existing\nmethodologies. To address this, we present Intern-GS, a novel approach that\neffectively leverages rich prior knowledge from vision foundation models to\nenhance the process of sparse-view Gaussian Splatting, thereby enabling\nhigh-quality scene reconstruction. Specifically, Intern-GS utilizes vision\nfoundation models to guide both the initialization and the optimization process\nof 3D Gaussian splatting, effectively addressing the limitations of sparse\ninputs. In the initialization process, our method employs DUSt3R to generate a\ndense and non-redundant gaussian point cloud. This approach significantly\nalleviates the limitations encountered by traditional structure-from-motion\n(SfM) methods, which often struggle under sparse-view constraints. During the\noptimization process, vision foundation models predict depth and appearance for\nunobserved views, refining the 3D Gaussians to compensate for missing\ninformation in unseen regions. Extensive experiments demonstrate that Intern-GS\nachieves state-of-the-art rendering quality across diverse datasets, including\nboth forward-facing and large-scale scenes, such as LLFF, DTU, and Tanks and\nTemples."}
{"id": "2505.20744", "pdf": "https://arxiv.org/pdf/2505.20744", "abs": "https://arxiv.org/abs/2505.20744", "authors": ["Hao Zhang", "Zhan Zhuang", "Xuehao Wang", "Xiaodong Yang", "Yu Zhang"], "title": "MoPFormer: Motion-Primitive Transformer for Wearable-Sensor Activity Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Human Activity Recognition (HAR) with wearable sensors is challenged by\nlimited interpretability, which significantly impacts cross-dataset\ngeneralization. To address this challenge, we propose Motion-Primitive\nTransformer (MoPFormer), a novel self-supervised framework that enhances\ninterpretability by tokenizing inertial measurement unit signals into\nsemantically meaningful motion primitives and leverages a Transformer\narchitecture to learn rich temporal representations. MoPFormer comprises\ntwo-stages. first stage is to partition multi-channel sensor streams into short\nsegments and quantizing them into discrete \"motion primitive\" codewords, while\nthe second stage enriches those tokenized sequences through a context-aware\nembedding module and then processes them with a Transformer encoder. The\nproposed MoPFormer can be pre-trained using a masked motion-modeling objective\nthat reconstructs missing primitives, enabling it to develop robust\nrepresentations across diverse sensor configurations. Experiments on six HAR\nbenchmarks demonstrate that MoPFormer not only outperforms state-of-the-art\nmethods but also successfully generalizes across multiple datasets. Most\nimportantly, the learned motion primitives significantly enhance both\ninterpretability and cross-dataset performance by capturing fundamental\nmovement patterns that remain consistent across similar activities regardless\nof dataset origin."}
{"id": "2505.20753", "pdf": "https://arxiv.org/pdf/2505.20753", "abs": "https://arxiv.org/abs/2505.20753", "authors": ["Yufei Zhan", "Hongyin Zhao", "Yousong Zhu", "Shurong Zheng", "Fan Yang", "Ming Tang", "Jinqiao Wang"], "title": "Understand, Think, and Answer: Advancing Visual Reasoning with Large Multimodal Models", "categories": ["cs.CV", "cs.AI"], "comment": "Tech report", "summary": "Large Multimodal Models (LMMs) have recently demonstrated remarkable visual\nunderstanding performance on both vision-language and vision-centric tasks.\nHowever, they often fall short in integrating advanced, task-specific\ncapabilities for compositional reasoning, which hinders their progress toward\ntruly competent general vision models. To address this, we present a unified\nvisual reasoning mechanism that enables LMMs to solve complicated compositional\nproblems by leveraging their intrinsic capabilities (e.g. grounding and visual\nunderstanding capabilities). Different from the previous shortcut learning\nmechanism, our approach introduces a human-like\nunderstanding-thinking-answering process, allowing the model to complete all\nsteps in a single pass forwarding without the need for multiple inferences or\nexternal tools. This design bridges the gap between foundational visual\ncapabilities and general question answering, encouraging LMMs to generate\nfaithful and traceable responses for complex visual reasoning. Meanwhile, we\ncurate 334K visual instruction samples covering both general scenes and\ntext-rich scenes and involving multiple foundational visual capabilities. Our\ntrained model, Griffon-R, has the ability of end-to-end automatic\nunderstanding, self-thinking, and reasoning answers. Comprehensive experiments\nshow that Griffon-R not only achieves advancing performance on complex visual\nreasoning benchmarks including VSR and CLEVR, but also enhances multimodal\ncapabilities across various benchmarks like MMBench and ScienceQA. Data,\nmodels, and codes will be release at\nhttps://github.com/jefferyZhan/Griffon/tree/master/Griffon-R soon."}
{"id": "2505.20759", "pdf": "https://arxiv.org/pdf/2505.20759", "abs": "https://arxiv.org/abs/2505.20759", "authors": ["Ansel Blume", "Jeonghwan Kim", "Hyeonjeong Ha", "Elen Chatikyan", "Xiaomeng Jin", "Khanh Duy Nguyen", "Nanyun Peng", "Kai-Wei Chang", "Derek Hoiem", "Heng Ji"], "title": "PARTONOMY: Large Multimodal Models with Part-Level Visual Understanding", "categories": ["cs.CV", "cs.AI"], "comment": "18 pages", "summary": "Real-world objects are composed of distinctive, object-specific parts.\nIdentifying these parts is key to performing fine-grained, compositional\nreasoning-yet, large multimodal models (LMMs) struggle to perform this\nseemingly straightforward task. In this work, we introduce PARTONOMY, an LMM\nbenchmark designed for pixel-level part grounding. We construct PARTONOMY from\nexisting part datasets and our own rigorously annotated set of images,\nencompassing 862 part labels and 534 object labels for evaluation. Unlike\nexisting datasets that simply ask models to identify generic parts, PARTONOMY\nuses specialized concepts (e.g., agricultural airplane), and challenges models\nto compare objects' parts, consider part-whole relationships, and justify\ntextual predictions with visual segmentations. Our experiments demonstrate\nsignificant limitations in state-of-the-art LMMs (e.g., LISA-13B achieves only\n5.9% gIoU), highlighting a critical gap in their part grounding abilities. We\nnote that existing segmentation-enabled LMMs (segmenting LMMs) have two key\narchitectural shortcomings: they use special [SEG] tokens not seen during\npretraining which induce distribution shift, and they discard predicted\nsegmentations instead of using past predictions to guide future ones. To\naddress these deficiencies, we train several part-centric LMMs and propose\nPLUM, a novel segmenting LMM that uses span tagging instead of segmentation\ntokens and that conditions on prior predictions in a feedback loop. We find\nthat pretrained PLUM outperforms existing segmenting LMMs on reasoning\nsegmentation, VQA, and visual hallucination benchmarks. In addition, PLUM\nfinetuned on our proposed Explanatory Part Segmentation task is competitive\nwith segmenting LMMs trained on significantly more segmentation data. Our work\nopens up new avenues towards enabling fine-grained, grounded visual\nunderstanding in LMMs."}
{"id": "2505.20764", "pdf": "https://arxiv.org/pdf/2505.20764", "abs": "https://arxiv.org/abs/2505.20764", "authors": ["Eric Xing", "Pranavi Kolouju", "Robert Pless", "Abby Stylianou", "Nathan Jacobs"], "title": "ConText-CIR: Learning from Concepts in Text for Composed Image Retrieval", "categories": ["cs.CV", "cs.LG"], "comment": "15 pages, 8 figures, 6 tables. CVPR 2025", "summary": "Composed image retrieval (CIR) is the task of retrieving a target image\nspecified by a query image and a relative text that describes a semantic\nmodification to the query image. Existing methods in CIR struggle to accurately\nrepresent the image and the text modification, resulting in subpar performance.\nTo address this limitation, we introduce a CIR framework, ConText-CIR, trained\nwith a Text Concept-Consistency loss that encourages the representations of\nnoun phrases in the text modification to better attend to the relevant parts of\nthe query image. To support training with this loss function, we also propose a\nsynthetic data generation pipeline that creates training data from existing CIR\ndatasets or unlabeled images. We show that these components together enable\nstronger performance on CIR tasks, setting a new state-of-the-art in composed\nimage retrieval in both the supervised and zero-shot settings on multiple\nbenchmark datasets, including CIRR and CIRCO. Source code, model checkpoints,\nand our new datasets are available at https://github.com/mvrl/ConText-CIR."}
{"id": "2505.20772", "pdf": "https://arxiv.org/pdf/2505.20772", "abs": "https://arxiv.org/abs/2505.20772", "authors": ["Hongjia Liu", "Rongzhen Zhao", "Haohan Chen", "Joni Pajarinen"], "title": "MetaSlot: Break Through the Fixed Number of Slots in Object-Centric Learning", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Learning object-level, structured representations is widely regarded as a key\nto better generalization in vision and underpins the design of next-generation\nPre-trained Vision Models (PVMs). Mainstream Object-Centric Learning (OCL)\nmethods adopt Slot Attention or its variants to iteratively aggregate objects'\nsuper-pixels into a fixed set of query feature vectors, termed slots. However,\ntheir reliance on a static slot count leads to an object being represented as\nmultiple parts when the number of objects varies. We introduce MetaSlot, a\nplug-and-play Slot Attention variant that adapts to variable object counts.\nMetaSlot (i) maintains a codebook that holds prototypes of objects in a dataset\nby vector-quantizing the resulting slot representations; (ii) removes duplicate\nslots from the traditionally aggregated slots by quantizing them with the\ncodebook; and (iii) injects progressively weaker noise into the Slot Attention\niterations to accelerate and stabilize the aggregation. MetaSlot is a general\nSlot Attention variant that can be seamlessly integrated into existing OCL\narchitectures. Across multiple public datasets and tasks--including object\ndiscovery and recognition--models equipped with MetaSlot achieve significant\nperformance gains and markedly interpretable slot representations, compared\nwith existing Slot Attention variants."}
{"id": "2505.20777", "pdf": "https://arxiv.org/pdf/2505.20777", "abs": "https://arxiv.org/abs/2505.20777", "authors": ["Zhehan Kan", "Yanlin Liu", "Kun Yin", "Xinghua Jiang", "Xin Li", "Haoyu Cao", "Yinsong Liu", "Deqiang Jiang", "Xing Sun", "Qingmin Liao", "Wenming Yang"], "title": "TACO: Think-Answer Consistency for Optimized Long-Chain Reasoning and Efficient Data Learning via Reinforcement Learning in LVLMs", "categories": ["cs.CV"], "comment": null, "summary": "DeepSeek R1 has significantly advanced complex reasoning for large language\nmodels (LLMs). While recent methods have attempted to replicate R1's reasoning\ncapabilities in multimodal settings, they face limitations, including\ninconsistencies between reasoning and final answers, model instability and\ncrashes during long-chain exploration, and low data learning efficiency. To\naddress these challenges, we propose TACO, a novel reinforcement learning\nalgorithm for visual reasoning. Building on Generalized Reinforcement Policy\nOptimization (GRPO), TACO introduces Think-Answer Consistency, which tightly\ncouples reasoning with answer consistency to ensure answers are grounded in\nthoughtful reasoning. We also introduce the Rollback Resample Strategy, which\nadaptively removes problematic samples and reintroduces them to the sampler,\nenabling stable long-chain exploration and future learning opportunities.\nAdditionally, TACO employs an adaptive learning schedule that focuses on\nmoderate difficulty samples to optimize data efficiency. Furthermore, we\npropose the Test-Time-Resolution-Scaling scheme to address performance\ndegradation due to varying resolutions during reasoning while balancing\ncomputational overhead. Extensive experiments on in-distribution and\nout-of-distribution benchmarks for REC and VQA tasks show that fine-tuning\nLVLMs leads to significant performance improvements."}
{"id": "2505.20782", "pdf": "https://arxiv.org/pdf/2505.20782", "abs": "https://arxiv.org/abs/2505.20782", "authors": ["Taïga Gonçalves", "Tomo Miyazaki", "Shinichiro Omachi"], "title": "Breaking Dataset Boundaries: Class-Agnostic Targeted Adversarial Attacks", "categories": ["cs.CV"], "comment": null, "summary": "We present Cross-Domain Multi-Targeted Attack (CD-MTA), a method for\ngenerating adversarial examples that mislead image classifiers toward any\ntarget class, including those not seen during training. Traditional targeted\nattacks are limited to one class per model, requiring expensive retraining for\neach target. Multi-targeted attacks address this by introducing a perturbation\ngenerator with a conditional input to specify the target class. However,\nexisting methods are constrained to classes observed during training and\nrequire access to the black-box model's training data--introducing a form of\ndata leakage that undermines realistic evaluation in practical black-box\nscenarios. We identify overreliance on class embeddings as a key limitation,\nleading to overfitting and poor generalization to unseen classes. To address\nthis, CD-MTA replaces class-level supervision with an image-based conditional\ninput and introduces class-agnostic losses that align the perturbed and target\nimages in the feature space. This design removes dependence on class semantics,\nthereby enabling generalization to unseen classes across datasets. Experiments\non ImageNet and seven other datasets show that CD-MTA outperforms prior\nmulti-targeted attacks in both standard and cross-domain settings--without\naccessing the black-box model's training data."}
{"id": "2505.20789", "pdf": "https://arxiv.org/pdf/2505.20789", "abs": "https://arxiv.org/abs/2505.20789", "authors": ["Yang Zheng", "Wen Li", "Zhaoqiang Liu"], "title": "Integrating Intermediate Layer Optimization and Projected Gradient Descent for Solving Inverse Problems with Diffusion Models", "categories": ["cs.CV", "cs.LG"], "comment": "ICML 2025", "summary": "Inverse problems (IPs) involve reconstructing signals from noisy\nobservations. Traditional approaches often rely on handcrafted priors, which\ncan fail to capture the complexity of real-world data. The advent of\npre-trained generative models has introduced new paradigms, offering improved\nreconstructions by learning rich priors from data. Among these, diffusion\nmodels (DMs) have emerged as a powerful framework, achieving remarkable\nreconstruction performance across numerous IPs. However, existing DM-based\nmethods frequently encounter issues such as heavy computational demands and\nsuboptimal convergence. In this work, building upon the idea of the recent work\nDMPlug~\\cite{wang2024dmplug}, we propose two novel methods, DMILO and\nDMILO-PGD, to address these challenges. Our first method, DMILO, employs\nintermediate layer optimization (ILO) to alleviate the memory burden inherent\nin DMPlug. Additionally, by introducing sparse deviations, we expand the range\nof DMs, enabling the exploration of underlying signals that may lie outside the\nrange of the diffusion model. We further propose DMILO-PGD, which integrates\nILO with projected gradient descent (PGD), thereby reducing the risk of\nsuboptimal convergence. We provide an intuitive theoretical analysis of our\napproach under appropriate conditions and validate its superiority through\nextensive experiments on diverse image datasets, encompassing both linear and\nnonlinear IPs. Our results demonstrate significant performance gains over\nstate-of-the-art methods, highlighting the effectiveness of DMILO and DMILO-PGD\nin addressing common challenges in DM-based IP solvers."}
{"id": "2505.20793", "pdf": "https://arxiv.org/pdf/2505.20793", "abs": "https://arxiv.org/abs/2505.20793", "authors": ["Juan A. Rodriguez", "Haotian Zhang", "Abhay Puri", "Aarash Feizi", "Rishav Pramanik", "Pascal Wichmann", "Arnab Mondal", "Mohammad Reza Samsami", "Rabiul Awal", "Perouz Taslakian", "Spandana Gella", "Sai Rajeswar", "David Vazquez", "Christopher Pal", "Marco Pedersoli"], "title": "Rendering-Aware Reinforcement Learning for Vector Graphics Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Scalable Vector Graphics (SVG) offer a powerful format for representing\nvisual designs as interpretable code. Recent advances in vision-language models\n(VLMs) have enabled high-quality SVG generation by framing the problem as a\ncode generation task and leveraging large-scale pretraining. VLMs are\nparticularly suitable for this task as they capture both global semantics and\nfine-grained visual patterns, while transferring knowledge across vision,\nnatural language, and code domains. However, existing VLM approaches often\nstruggle to produce faithful and efficient SVGs because they never observe the\nrendered images during training. Although differentiable rendering for\nautoregressive SVG code generation remains unavailable, rendered outputs can\nstill be compared to original inputs, enabling evaluative feedback suitable for\nreinforcement learning (RL). We introduce RLRF(Reinforcement Learning from\nRendering Feedback), an RL method that enhances SVG generation in\nautoregressive VLMs by leveraging feedback from rendered SVG outputs. Given an\ninput image, the model generates SVG roll-outs that are rendered and compared\nto the original image to compute a reward. This visual fidelity feedback guides\nthe model toward producing more accurate, efficient, and semantically coherent\nSVGs. RLRF significantly outperforms supervised fine-tuning, addressing common\nfailure modes and enabling precise, high-quality SVG generation with strong\nstructural understanding and generalization."}
{"id": "2505.20808", "pdf": "https://arxiv.org/pdf/2505.20808", "abs": "https://arxiv.org/abs/2505.20808", "authors": ["Bo-Kai Ruan", "Zi-Xiang Ni", "Bo-Lun Huang", "Teng-Fang Hsiao", "Hong-Han Shuai"], "title": "Not All Thats Rare Is Lost: Causal Paths to Rare Concept Synthesis", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion models have shown strong capabilities in high-fidelity image\ngeneration but often falter when synthesizing rare concepts, i.e., prompts that\nare infrequently observed in the training distribution. In this paper, we\nintroduce RAP, a principled framework that treats rare concept generation as\nnavigating a latent causal path: a progressive, model-aligned trajectory\nthrough the generative space from frequent concepts to rare targets. Rather\nthan relying on heuristic prompt alternation, we theoretically justify that\nrare prompt guidance can be approximated by semantically related frequent\nprompts. We then formulate prompt switching as a dynamic process based on score\nsimilarity, enabling adaptive stage transitions. Furthermore, we reinterpret\nprompt alternation as a second-order denoising mechanism, promoting smooth\nsemantic progression and coherent visual synthesis. Through this causal lens,\nwe align input scheduling with the model's internal generative dynamics.\nExperiments across diverse diffusion backbones demonstrate that RAP\nconsistently enhances rare concept generation, outperforming strong baselines\nin both automated evaluations and human studies."}
{"id": "2505.20827", "pdf": "https://arxiv.org/pdf/2505.20827", "abs": "https://arxiv.org/abs/2505.20827", "authors": ["Guangcong Zheng", "Jianlong Yuan", "Bo Wang", "Haoyang Huang", "Guoqing Ma", "Nan Duan"], "title": "Frame-Level Captions for Long Video Generation with Complex Multi Scenes", "categories": ["cs.CV"], "comment": null, "summary": "Generating long videos that can show complex stories, like movie scenes from\nscripts, has great promise and offers much more than short clips. However,\ncurrent methods that use autoregression with diffusion models often struggle\nbecause their step-by-step process naturally leads to a serious error\naccumulation (drift). Also, many existing ways to make long videos focus on\nsingle, continuous scenes, making them less useful for stories with many events\nand changes. This paper introduces a new approach to solve these problems.\nFirst, we propose a novel way to annotate datasets at the frame-level,\nproviding detailed text guidance needed for making complex, multi-scene long\nvideos. This detailed guidance works with a Frame-Level Attention Mechanism to\nmake sure text and video match precisely. A key feature is that each part\n(frame) within these windows can be guided by its own distinct text prompt. Our\ntraining uses Diffusion Forcing to provide the model with the ability to handle\ntime flexibly. We tested our approach on difficult VBench 2.0 benchmarks\n(\"Complex Plots\" and \"Complex Landscapes\") based on the WanX2.1-T2V-1.3B model.\nThe results show our method is better at following instructions in complex,\nchanging scenes and creates high-quality long videos. We plan to share our\ndataset annotation methods and trained models with the research community.\nProject page: https://zgctroy.github.io/frame-level-captions ."}
{"id": "2505.20830", "pdf": "https://arxiv.org/pdf/2505.20830", "abs": "https://arxiv.org/abs/2505.20830", "authors": ["Linli Ma", "Suzhen Lin", "Jianchao Zeng", "Zanxia Jin", "Yanbo Wang", "Fengyuan Li", "Yubing Luo"], "title": "Causality-Driven Infrared and Visible Image Fusion", "categories": ["cs.CV"], "comment": null, "summary": "Image fusion aims to combine complementary information from multiple source\nimages to generate more comprehensive scene representations. Existing methods\nprimarily rely on the stacking and design of network architectures to enhance\nthe fusion performance, often ignoring the impact of dataset scene bias on\nmodel training. This oversight leads the model to learn spurious correlations\nbetween specific scenes and fusion weights under conventional likelihood\nestimation framework, thereby limiting fusion performance. To solve the above\nproblems, this paper first re-examines the image fusion task from the causality\nperspective, and disentangles the model from the impact of bias by constructing\na tailored causal graph to clarify the causalities among the variables in image\nfusion task. Then, the Back-door Adjustment based Feature Fusion Module (BAFFM)\nis proposed to eliminate confounder interference and enable the model to learn\nthe true causal effect. Finally, Extensive experiments on three standard\ndatasets prove that the proposed method significantly surpasses\nstate-of-the-art methods in infrared and visible image fusion."}
{"id": "2505.20834", "pdf": "https://arxiv.org/pdf/2505.20834", "abs": "https://arxiv.org/abs/2505.20834", "authors": ["Jingjun Yang", "Liangwei Fan", "Jinpu Zhang", "Xiangkai Lian", "Hui Shen", "Dewen Hu"], "title": "Fully Spiking Neural Networks for Unified Frame-Event Object Tracking", "categories": ["cs.CV", "cs.NE"], "comment": "13 pages,6 figures,4 tables", "summary": "The integration of image and event streams offers a promising approach for\nachieving robust visual object tracking in complex environments. However,\ncurrent fusion methods achieve high performance at the cost of significant\ncomputational overhead and struggle to efficiently extract the sparse,\nasynchronous information from event streams, failing to leverage the\nenergy-efficient advantages of event-driven spiking paradigms. To address this\nchallenge, we propose the first fully Spiking Frame-Event Tracking framework\ncalled SpikeFET. This network achieves synergistic integration of convolutional\nlocal feature extraction and Transformer-based global modeling within the\nspiking paradigm, effectively fusing frame and event data. To overcome the\ndegradation of translation invariance caused by convolutional padding, we\nintroduce a Random Patchwork Module (RPM) that eliminates positional bias\nthrough randomized spatial reorganization and learnable type encoding while\npreserving residual structures. Furthermore, we propose a Spatial-Temporal\nRegularization (STR) strategy that overcomes similarity metric degradation from\nasymmetric features by enforcing spatio-temporal consistency among temporal\ntemplate features in latent space. Extensive experiments across multiple\nbenchmarks demonstrate that the proposed framework achieves superior tracking\naccuracy over existing methods while significantly reducing power consumption,\nattaining an optimal balance between performance and efficiency. The code will\nbe released."}
{"id": "2505.20858", "pdf": "https://arxiv.org/pdf/2505.20858", "abs": "https://arxiv.org/abs/2505.20858", "authors": ["Jason Chui", "Daniel Cremers"], "title": "ProBA: Probabilistic Bundle Adjustment with the Bhattacharyya Coefficient", "categories": ["cs.CV"], "comment": "15 pages, 14 figures, 5 tables", "summary": "Classical Bundle Adjustment (BA) methods require accurate initial estimates\nfor convergence and typically assume known camera intrinsics, which limits\ntheir applicability when such information is uncertain or unavailable. We\npropose a novel probabilistic formulation of BA (ProBA) that explicitly models\nand propagates uncertainty in both the 2D observations and the 3D scene\nstructure, enabling optimization without any prior knowledge of camera poses or\nfocal length. Our method uses 3D Gaussians instead of point-like landmarks and\nwe introduce uncertainty-aware reprojection losses by projecting the 3D\nGaussians onto the 2D image space, and enforce geometric consistency across\nmultiple 3D Gaussians using the Bhattacharyya coefficient to encourage overlap\nbetween their corresponding Gaussian distributions. This probabilistic\nframework leads to more robust and reliable optimization, even in the presence\nof outliers in the correspondence set, reducing the likelihood of converging to\npoor local minima. Experimental results show that \\textit{ProBA} outperforms\ntraditional methods in challenging real-world conditions. By removing the need\nfor strong initialization and known intrinsics, ProBA enhances the practicality\nof SLAM systems deployed in unstructured environments."}
{"id": "2505.20861", "pdf": "https://arxiv.org/pdf/2505.20861", "abs": "https://arxiv.org/abs/2505.20861", "authors": ["Yifeng Ma", "Jinwei Qi", "Chaonan Ji", "Peng Zhang", "Bang Zhang", "Zhidong Deng", "Liefeng Bo"], "title": "Exploring Timeline Control for Facial Motion Generation", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025, Project Page:\n  https://humanaigc.github.io/facial-motion-timeline-control/", "summary": "This paper introduces a new control signal for facial motion generation:\ntimeline control. Compared to audio and text signals, timelines provide more\nfine-grained control, such as generating specific facial motions with precise\ntiming. Users can specify a multi-track timeline of facial actions arranged in\ntemporal intervals, allowing precise control over the timing of each action. To\nmodel the timeline control capability, We first annotate the time intervals of\nfacial actions in natural facial motion sequences at a frame-level granularity.\nThis process is facilitated by Toeplitz Inverse Covariance-based Clustering to\nminimize human labor. Based on the annotations, we propose a diffusion-based\ngeneration model capable of generating facial motions that are natural and\naccurately aligned with input timelines. Our method supports text-guided motion\ngeneration by using ChatGPT to convert text into timelines. Experimental\nresults show that our method can annotate facial action intervals with\nsatisfactory accuracy, and produces natural facial motions accurately aligned\nwith timelines."}
{"id": "2505.20862", "pdf": "https://arxiv.org/pdf/2505.20862", "abs": "https://arxiv.org/abs/2505.20862", "authors": ["Chaeyoung Jung", "Youngjoon Jang", "Joon Son Chung"], "title": "AVCD: Mitigating Hallucinations in Audio-Visual Large Language Models through Contrastive Decoding", "categories": ["cs.CV"], "comment": null, "summary": "Hallucination remains a major challenge in multimodal large language models\n(MLLMs). To address this, various contrastive decoding (CD) methods have been\nproposed that contrasts original logits with hallucinated logits generated from\nperturbed inputs. While CD has shown promise in vision-language models (VLMs),\nit is not well-suited for AV-LLMs, where hallucinations often emerge from both\nunimodal and cross-modal combinations involving audio, video, and language.\nThese intricate interactions call for a more adaptive and modality-aware\ndecoding strategy. In this paper, we propose Audio-Visual Contrastive Decoding\n(AVCD)-a novel, training-free decoding framework designed to model trimodal\ninteractions and suppress modality-induced hallucinations in AV-LLMs. Unlike\nprevious CD methods in VLMs that corrupt a fixed modality, AVCD leverages\nattention distributions to dynamically identify less dominant modalities and\napplies attentive masking to generate perturbed output logits. To support CD in\na trimodal setting, we also reformulate the original CD framework to jointly\nhandle audio, visual, and textual inputs. Finally, to improve efficiency, we\nintroduce entropy-guided adaptive decoding, which selectively skips unnecessary\ndecoding steps based on the model's confidence in its predictions. Extensive\nexperiments demonstrate that AVCD consistently outperforms existing decoding\nmethods. Especially, on the AVHBench dataset, it improves accuracy by 6% for\nVideoLLaMA2 and 11% for video-SALMONN, demonstrating strong robustness and\ngeneralizability."}
{"id": "2505.20872", "pdf": "https://arxiv.org/pdf/2505.20872", "abs": "https://arxiv.org/abs/2505.20872", "authors": ["Antony Zhao", "Alex Proshkin", "Fergal Hennessy", "Francesco Crivelli"], "title": "In Context Learning with Vision Transformers: Case Study", "categories": ["cs.CV", "cs.AI", "cs.LG", "I.2.6; I.2.10; I.4.8"], "comment": "12 pages, 16 figures. UC Berkeley research project", "summary": "Large transformer models have been shown to be capable of performing\nin-context learning. By using examples in a prompt as well as a query, they are\ncapable of performing tasks such as few-shot, one-shot, or zero-shot learning\nto output the corresponding answer to this query. One area of interest to us is\nthat these transformer models have been shown to be capable of learning the\ngeneral class of certain functions, such as linear functions and small 2-layer\nneural networks, on random data (Garg et al, 2023). We aim to extend this to\nthe image space to analyze their capability to in-context learn more complex\nfunctions on the image space, such as convolutional neural networks and other\nmethods."}
{"id": "2505.20873", "pdf": "https://arxiv.org/pdf/2505.20873", "abs": "https://arxiv.org/abs/2505.20873", "authors": ["Chaeyoung Jung", "Youngjoon Jang", "Jongmin Choi", "Joon Son Chung"], "title": "Fork-Merge Decoding: Enhancing Multimodal Understanding in Audio-Visual Large Language Models", "categories": ["cs.CV"], "comment": null, "summary": "The goal of this work is to enhance balanced multimodal understanding in\naudio-visual large language models (AV-LLMs) by addressing modality bias\nwithout requiring additional training. In current AV-LLMs, audio and video\nfeatures are typically processed jointly in the decoder. While this strategy\nfacilitates unified multimodal understanding, it may introduce modality bias,\nwhere the model tends to over-rely on one modality due to imbalanced training\nsignals. To mitigate this, we propose Fork-Merge Decoding (FMD), a simple yet\neffective inference-time strategy that requires no additional training or\narchitectural modifications. FMD first performs modality-specific reasoning by\nprocessing audio-only and video-only inputs through the early decoder layers (a\nfork phase), and then merges the resulting hidden states for joint reasoning in\nthe remaining layers (a merge phase). This approach promotes balanced modality\ncontributions and leverages complementary information across modalities. We\nevaluate our method on two representative AV-LLMs, VideoLLaMA2 and\nvideo-SALMONN, using three benchmark datasets. Experimental results demonstrate\nconsistent performance improvements on tasks focused on audio, video, and\ncombined audio-visual reasoning, demonstrating the effectiveness of\ninference-time interventions for robust multimodal understanding."}
{"id": "2505.20876", "pdf": "https://arxiv.org/pdf/2505.20876", "abs": "https://arxiv.org/abs/2505.20876", "authors": ["Tatsuya Sasayama", "Shintaro Ito", "Koichi Ito", "Takafumi Aoki"], "title": "Stereo Radargrammetry Using Deep Learning from Airborne SAR Images", "categories": ["cs.CV", "eess.IV"], "comment": "5 pages, 5 figures, conference IGARSS2025", "summary": "In this paper, we propose a stereo radargrammetry method using deep learning\nfrom airborne Synthetic Aperture Radar (SAR) images.Deep learning-based methods\nare considered to suffer less from geometric image modulation, while there is\nno public SAR image dataset used to train such methods.We create a SAR image\ndataset and perform fine-tuning of a deep learning-based image correspondence\nmethod.The proposed method suppresses the degradation of image quality by pixel\ninterpolation without ground projection of the SAR image and divides the SAR\nimage into patches for processing, which makes it possible to apply deep\nlearning.Through a set of experiments, we demonstrate that the proposed method\nexhibits a wider range and more accurate elevation measurements compared to\nconventional methods."}
{"id": "2505.20884", "pdf": "https://arxiv.org/pdf/2505.20884", "abs": "https://arxiv.org/abs/2505.20884", "authors": ["Weichao Pan", "Bohan Xu", "Xu Wang", "Chengze Lv", "Shuoyang Wang", "Zhenke Duan"], "title": "YOLO-FireAD: Efficient Fire Detection via Attention-Guided Inverted Residual Learning and Dual-Pooling Feature Preservation", "categories": ["cs.CV"], "comment": null, "summary": "Fire detection in dynamic environments faces continuous challenges, including\nthe interference of illumination changes, many false detections or missed\ndetections, and it is difficult to achieve both efficiency and accuracy. To\naddress the problem of feature extraction limitation and information loss in\nthe existing YOLO-based models, this study propose You Only Look Once for Fire\nDetection with Attention-guided Inverted Residual and Dual-pooling Downscale\nFusion (YOLO-FireAD) with two core innovations: (1) Attention-guided Inverted\nResidual Block (AIR) integrates hybrid channel-spatial attention with inverted\nresiduals to adaptively enhance fire features and suppress environmental noise;\n(2) Dual Pool Downscale Fusion Block (DPDF) preserves multi-scale fire patterns\nthrough learnable fusion of max-average pooling outputs, mitigating small-fire\ndetection failures. Extensive evaluation on two public datasets shows the\nefficient performance of our model. Our proposed model keeps the sum amount of\nparameters (1.45M, 51.8% lower than YOLOv8n) (4.6G, 43.2% lower than YOLOv8n),\nand mAP75 is higher than the mainstream real-time object detection models\nYOLOv8n, YOL-Ov9t, YOLOv10n, YOLO11n, YOLOv12n and other YOLOv8 variants\n1.3-5.5%."}
{"id": "2505.20890", "pdf": "https://arxiv.org/pdf/2505.20890", "abs": "https://arxiv.org/abs/2505.20890", "authors": ["Yoojin Kwon", "Hongjun Suh", "Wooseok Lee", "Taesik Gong", "Songyi Han", "Hyung-Sin Kim"], "title": "Frequency Composition for Compressed and Domain-Adaptive Neural Networks", "categories": ["cs.CV", "cs.AI"], "comment": "Work in progress", "summary": "Modern on-device neural network applications must operate under resource\nconstraints while adapting to unpredictable domain shifts. However, this\ncombined challenge-model compression and domain adaptation-remains largely\nunaddressed, as prior work has tackled each issue in isolation: compressed\nnetworks prioritize efficiency within a fixed domain, whereas large, capable\nmodels focus on handling domain shifts. In this work, we propose CoDA, a\nfrequency composition-based framework that unifies compression and domain\nadaptation. During training, CoDA employs quantization-aware training (QAT)\nwith low-frequency components, enabling a compressed model to selectively learn\nrobust, generalizable features. At test time, it refines the compact model in a\nsource-free manner (i.e., test-time adaptation, TTA), leveraging the\nfull-frequency information from incoming data to adapt to target domains while\ntreating high-frequency components as domain-specific cues. LFC are aligned\nwith the trained distribution, while HFC unique to the target distribution are\nsolely utilized for batch normalization. CoDA can be integrated synergistically\ninto existing QAT and TTA methods. CoDA is evaluated on widely used\ndomain-shift benchmarks, including CIFAR10-C and ImageNet-C, across various\nmodel architectures. With significant compression, it achieves accuracy\nimprovements of 7.96%p on CIFAR10-C and 5.37%p on ImageNet-C over the\nfull-precision TTA baseline."}
{"id": "2505.20897", "pdf": "https://arxiv.org/pdf/2505.20897", "abs": "https://arxiv.org/abs/2505.20897", "authors": ["Pingrui Zhang", "Yifei Su", "Pengyuan Wu", "Dong An", "Li Zhang", "Zhigang Wang", "Dong Wang", "Yan Ding", "Bin Zhao", "Xuelong Li"], "title": "Cross from Left to Right Brain: Adaptive Text Dreamer for Vision-and-Language Navigation", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.RO"], "comment": null, "summary": "Vision-and-Language Navigation (VLN) requires the agent to navigate by\nfollowing natural instructions under partial observability, making it difficult\nto align perception with language. Recent methods mitigate this by imagining\nfuture scenes, yet they rely on vision-based synthesis, leading to high\ncomputational cost and redundant details. To this end, we propose to adaptively\nimagine key environmental semantics via \\textit{language} form, enabling a more\nreliable and efficient strategy. Specifically, we introduce a novel Adaptive\nText Dreamer (ATD), a dual-branch self-guided imagination policy built upon a\nlarge language model (LLM). ATD is designed with a human-like left-right brain\narchitecture, where the left brain focuses on logical integration, and the\nright brain is responsible for imaginative prediction of future scenes. To\nachieve this, we fine-tune only the Q-former within both brains to efficiently\nactivate domain-specific knowledge in the LLM, enabling dynamic updates of\nlogical reasoning and imagination during navigation. Furthermore, we introduce\na cross-interaction mechanism to regularize the imagined outputs and inject\nthem into a navigation expert module, allowing ATD to jointly exploit both the\nreasoning capacity of the LLM and the expertise of the navigation model. We\nconduct extensive experiments on the R2R benchmark, where ATD achieves\nstate-of-the-art performance with fewer parameters. The code is\n\\href{https://github.com/zhangpingrui/Adaptive-Text-Dreamer}{here}."}
{"id": "2505.20904", "pdf": "https://arxiv.org/pdf/2505.20904", "abs": "https://arxiv.org/abs/2505.20904", "authors": ["Guanghu Xie", "Yonglong Zhang", "Zhiduo Jiang", "Yang Liu", "Zongwu Xie", "Baoshi Cao", "Hong Liu"], "title": "HTMNet: A Hybrid Network with Transformer-Mamba Bottleneck Multimodal Fusion for Transparent and Reflective Objects Depth Completion", "categories": ["cs.CV"], "comment": null, "summary": "Transparent and reflective objects pose significant challenges for depth\nsensors, resulting in incomplete depth information that adversely affects\ndownstream robotic perception and manipulation tasks. To address this issue, we\npropose HTMNet, a novel hybrid model integrating Transformer, CNN, and Mamba\narchitectures. The encoder is constructed based on a dual-branch\nTransformer-CNN framework, while the multi-scale fusion module leverages a\nTransformer-Mamba architecture, which also serves as the foundation for the\ndecoder design. We introduce a novel multimodal fusion module grounded in\nself-attention mechanisms and state space models, marking the first application\nof the Mamba architecture in the field of transparent object depth completion\nand revealing its promising potential. Additionally, we design an innovative\nmulti-scale fusion module for the decoder that combines channel attention,\nspatial attention, and multi-scale feature extraction techniques to effectively\nintegrate multi-scale features through a down-fusion strategy. Extensive\nevaluations on multiple public datasets demonstrate that our model achieves\nstate-of-the-art(SOTA) performance, validating the effectiveness of our\napproach."}
{"id": "2505.20909", "pdf": "https://arxiv.org/pdf/2505.20909", "abs": "https://arxiv.org/abs/2505.20909", "authors": ["Wei Li", "Hebei Li", "Yansong Peng", "Siying Wu", "Yueyi Zhang", "Xiaoyan Sun"], "title": "Create Anything Anywhere: Layout-Controllable Personalized Diffusion Model for Multiple Subjects", "categories": ["cs.CV"], "comment": "ICME 2025", "summary": "Diffusion models have significantly advanced text-to-image generation, laying\nthe foundation for the development of personalized generative frameworks.\nHowever, existing methods lack precise layout controllability and overlook the\npotential of dynamic features of reference subjects in improving fidelity. In\nthis work, we propose Layout-Controllable Personalized Diffusion\n(LCP-Diffusion) model, a novel framework that integrates subject identity\npreservation with flexible layout guidance in a tuning-free approach. Our model\nemploys a Dynamic-Static Complementary Visual Refining module to\ncomprehensively capture the intricate details of reference subjects, and\nintroduces a Dual Layout Control mechanism to enforce robust spatial control\nacross both training and inference stages. Extensive experiments validate that\nLCP-Diffusion excels in both identity preservation and layout controllability.\nTo the best of our knowledge, this is a pioneering work enabling users to\n\"create anything anywhere\"."}
{"id": "2505.20914", "pdf": "https://arxiv.org/pdf/2505.20914", "abs": "https://arxiv.org/abs/2505.20914", "authors": ["Jianman Lin", "Haojie Li", "Chunmei Qing", "Zhijing Yang", "Liang Lin", "Tianshui Chen"], "title": "Geometry-Editable and Appearance-Preserving Object Compositon", "categories": ["cs.CV"], "comment": null, "summary": "General object composition (GOC) aims to seamlessly integrate a target object\ninto a background scene with desired geometric properties, while simultaneously\npreserving its fine-grained appearance details. Recent approaches derive\nsemantic embeddings and integrate them into advanced diffusion models to enable\ngeometry-editable generation. However, these highly compact embeddings encode\nonly high-level semantic cues and inevitably discard fine-grained appearance\ndetails. We introduce a Disentangled Geometry-editable and\nAppearance-preserving Diffusion (DGAD) model that first leverages semantic\nembeddings to implicitly capture the desired geometric transformations and then\nemploys a cross-attention retrieval mechanism to align fine-grained appearance\nfeatures with the geometry-edited representation, facilitating both precise\ngeometry editing and faithful appearance preservation in object composition.\nSpecifically, DGAD builds on CLIP/DINO-derived and reference networks to\nextract semantic embeddings and appearance-preserving representations, which\nare then seamlessly integrated into the encoding and decoding pipelines in a\ndisentangled manner. We first integrate the semantic embeddings into\npre-trained diffusion models that exhibit strong spatial reasoning capabilities\nto implicitly capture object geometry, thereby facilitating flexible object\nmanipulation and ensuring effective editability. Then, we design a dense\ncross-attention mechanism that leverages the implicitly learned object geometry\nto retrieve and spatially align appearance features with their corresponding\nregions, ensuring faithful appearance consistency. Extensive experiments on\npublic benchmarks demonstrate the effectiveness of the proposed DGAD framework."}
{"id": "2505.20920", "pdf": "https://arxiv.org/pdf/2505.20920", "abs": "https://arxiv.org/abs/2505.20920", "authors": ["Qihang Fang", "Chengcheng Tang", "Bugra Tekin", "Shugao Ma", "Yanchao Yang"], "title": "HuMoCon: Concept Discovery for Human Motion Understanding", "categories": ["cs.CV", "68T07", "I.2.10; I.2.7"], "comment": "18 pages, 10 figures", "summary": "We present HuMoCon, a novel motion-video understanding framework designed for\nadvanced human behavior analysis. The core of our method is a human motion\nconcept discovery framework that efficiently trains multi-modal encoders to\nextract semantically meaningful and generalizable features. HuMoCon addresses\nkey challenges in motion concept discovery for understanding and reasoning,\nincluding the lack of explicit multi-modality feature alignment and the loss of\nhigh-frequency information in masked autoencoding frameworks. Our approach\nintegrates a feature alignment strategy that leverages video for contextual\nunderstanding and motion for fine-grained interaction modeling, further with a\nvelocity reconstruction mechanism to enhance high-frequency feature expression\nand mitigate temporal over-smoothing. Comprehensive experiments on standard\nbenchmarks demonstrate that HuMoCon enables effective motion concept discovery\nand significantly outperforms state-of-the-art methods in training large models\nfor human motion understanding. We will open-source the associated code with\nour paper."}
{"id": "2505.20928", "pdf": "https://arxiv.org/pdf/2505.20928", "abs": "https://arxiv.org/abs/2505.20928", "authors": ["Alexander Jaus", "Zdravko Marinov", "Constantin Seibold", "Simon Reiß", "Jens Kleesiek", "Rainer Stiefelhagen"], "title": "Good Enough: Is it Worth Improving your Label Quality?", "categories": ["cs.CV"], "comment": null, "summary": "Improving label quality in medical image segmentation is costly, but its\nbenefits remain unclear. We systematically evaluate its impact using multiple\npseudo-labeled versions of CT datasets, generated by models like nnU-Net,\nTotalSegmentator, and MedSAM. Our results show that while higher-quality labels\nimprove in-domain performance, gains remain unclear if below a small threshold.\nFor pre-training, label quality has minimal impact, suggesting that models\nrather transfer general concepts than detailed annotations. These findings\nprovide guidance on when improving label quality is worth the effort."}
{"id": "2505.20932", "pdf": "https://arxiv.org/pdf/2505.20932", "abs": "https://arxiv.org/abs/2505.20932", "authors": ["Ningyuan Tang", "Minghao Fu", "Hao Yu", "Jianxin Wu"], "title": "QwT-v2: Practical, Effective and Efficient Post-Training Quantization", "categories": ["cs.CV"], "comment": null, "summary": "Network quantization is arguably one of the most practical network\ncompression approaches for reducing the enormous resource consumption of modern\ndeep neural networks. They usually require diverse and subtle design choices\nfor specific architecture and tasks. Instead, the QwT method is a simple and\ngeneral approach which introduces lightweight additional structures to improve\nquantization. But QwT incurs extra parameters and latency. More importantly,\nQwT is not compatible with many hardware platforms. In this paper, we propose\nQwT-v2, which not only enjoys all advantages of but also resolves major defects\nof QwT. By adopting a very lightweight channel-wise affine compensation (CWAC)\nmodule, QwT-v2 introduces significantly less extra parameters and computations\ncompared to QwT, and at the same time matches or even outperforms QwT in\naccuracy. The compensation module of QwT-v2 can be integrated into quantization\ninference engines with little effort, which not only effectively removes the\nextra costs but also makes it compatible with most existing hardware platforms."}
{"id": "2505.20935", "pdf": "https://arxiv.org/pdf/2505.20935", "abs": "https://arxiv.org/abs/2505.20935", "authors": ["Sanghyun Jo", "Wooyeol Lee", "Ziseok Lee", "Kyungsu Kim"], "title": "ISAC: Training-Free Instance-to-Semantic Attention Control for Improving Multi-Instance Generation", "categories": ["cs.CV"], "comment": "34 pages", "summary": "Text-to-image diffusion models excel at generating single-instance scenes but\nstruggle with multi-instance scenarios, often merging or omitting objects.\nUnlike previous training-free approaches that rely solely on semantic-level\nguidance without addressing instance individuation, our training-free method,\nInstance-to-Semantic Attention Control (ISAC), explicitly resolves incomplete\ninstance formation and semantic entanglement through an instance-first modeling\napproach. This enables ISAC to effectively leverage a hierarchical,\ntree-structured prompt mechanism, disentangling multiple object instances and\nindividually aligning them with their corresponding semantic labels. Without\nemploying any external models, ISAC achieves up to 52% average multi-class\naccuracy and 83% average multi-instance accuracy by effectively forming\ndisentangled instances. The code will be made available upon publication."}
{"id": "2505.20941", "pdf": "https://arxiv.org/pdf/2505.20941", "abs": "https://arxiv.org/abs/2505.20941", "authors": ["Yaohua Zha", "Yanzi Wang", "Hang Guo", "Jinpeng Wang", "Tao Dai", "Bin Chen", "Zhihao Ouyang", "Xue Yuerong", "Ke Chen", "Shu-Tao Xia"], "title": "PMA: Towards Parameter-Efficient Point Cloud Understanding via Point Mamba Adapter", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Applying pre-trained models to assist point cloud understanding has recently\nbecome a mainstream paradigm in 3D perception. However, existing application\nstrategies are straightforward, utilizing only the final output of the\npre-trained model for various task heads. It neglects the rich complementary\ninformation in the intermediate layer, thereby failing to fully unlock the\npotential of pre-trained models. To overcome this limitation, we propose an\northogonal solution: Point Mamba Adapter (PMA), which constructs an ordered\nfeature sequence from all layers of the pre-trained model and leverages Mamba\nto fuse all complementary semantics, thereby promoting comprehensive point\ncloud understanding. Constructing this ordered sequence is non-trivial due to\nthe inherent isotropy of 3D space. Therefore, we further propose a\ngeometry-constrained gate prompt generator (G2PG) shared across different\nlayers, which applies shared geometric constraints to the output gates of the\nMamba and dynamically optimizes the spatial order, thus enabling more effective\nintegration of multi-layer information. Extensive experiments conducted on\nchallenging point cloud datasets across various tasks demonstrate that our PMA\nelevates the capability for point cloud understanding to a new level by fusing\ndiverse complementary intermediate features. Code is available at\nhttps://github.com/zyh16143998882/PMA."}
{"id": "2505.20951", "pdf": "https://arxiv.org/pdf/2505.20951", "abs": "https://arxiv.org/abs/2505.20951", "authors": ["Naiyu Fang", "Zheyuan Zhou", "Kang Wang", "Ruibo Li", "Lemiao Qiu", "Shuyou Zhang", "Zhe Wang", "Guosheng Lin"], "title": "DSOcc: Leveraging Depth Awareness and Semantic Aid to Boost Camera-Based 3D Semantic Occupancy Prediction", "categories": ["cs.CV"], "comment": null, "summary": "Camera-based 3D semantic occupancy prediction offers an efficient and\ncost-effective solution for perceiving surrounding scenes in autonomous\ndriving. However, existing works rely on explicit occupancy state inference,\nleading to numerous incorrect feature assignments, and insufficient samples\nrestrict the learning of occupancy class inference. To address these\nchallenges, we propose leveraging Depth awareness and Semantic aid to boost\ncamera-based 3D semantic Occupancy prediction (DSOcc). We jointly perform\noccupancy state and occupancy class inference, where soft occupancy confidence\nis calculated through non-learning method and multiplied with image features to\nmake the voxel representation aware of depth, enabling adaptive implicit\noccupancy state inference. Rather than focusing on improving feature learning,\nwe directly utilize well-trained image semantic segmentation and fuse multiple\nframes with their occupancy probabilities to aid occupancy class inference,\nthereby enhancing robustness. Experimental results demonstrate that DSOcc\nachieves state-of-the-art performance on the SemanticKITTI dataset among\ncamera-based methods."}
{"id": "2505.20958", "pdf": "https://arxiv.org/pdf/2505.20958", "abs": "https://arxiv.org/abs/2505.20958", "authors": ["Shubham Singh Paliwal", "Arushi Jain", "Monika Sharma", "Vikram Jamwal", "Lovekesh Vig"], "title": "OrienText: Surface Oriented Textual Image Generation", "categories": ["cs.CV"], "comment": "4 pages, SIGGRAPH Asia 2024 Technical Communications", "summary": "Textual content in images is crucial in e-commerce sectors, particularly in\nmarketing campaigns, product imaging, advertising, and the entertainment\nindustry. Current text-to-image (T2I) generation diffusion models, though\nproficient at producing high-quality images, often struggle to incorporate text\naccurately onto complex surfaces with varied perspectives, such as angled views\nof architectural elements like buildings, banners, or walls. In this paper, we\nintroduce the Surface Oriented Textual Image Generation (OrienText) method,\nwhich leverages region-specific surface normals as conditional input to T2I\ngeneration diffusion model. Our approach ensures accurate rendering and correct\norientation of the text within the image context. We demonstrate the\neffectiveness of the OrienText method on a self-curated dataset of images and\ncompare it against the existing textual image generation methods."}
{"id": "2505.20967", "pdf": "https://arxiv.org/pdf/2505.20967", "abs": "https://arxiv.org/abs/2505.20967", "authors": ["Jiarui Zhang", "Zhihao Li", "Chong Wang", "Bihan Wen"], "title": "RF4D:Neural Radar Fields for Novel View Synthesis in Outdoor Dynamic Scenes", "categories": ["cs.CV"], "comment": null, "summary": "Neural fields (NFs) have demonstrated remarkable performance in scene\nreconstruction, powering various tasks such as novel view synthesis. However,\nexisting NF methods relying on RGB or LiDAR inputs often exhibit severe\nfragility to adverse weather, particularly when applied in outdoor scenarios\nlike autonomous driving. In contrast, millimeter-wave radar is inherently\nrobust to environmental changes, while unfortunately, its integration with NFs\nremains largely underexplored. Besides, as outdoor driving scenarios frequently\ninvolve moving objects, making spatiotemporal modeling essential for temporally\nconsistent novel view synthesis. To this end, we introduce RF4D, a radar-based\nneural field framework specifically designed for novel view synthesis in\noutdoor dynamic scenes. RF4D explicitly incorporates temporal information into\nits representation, significantly enhancing its capability to model moving\nobjects. We further introduce a feature-level flow module that predicts latent\ntemporal offsets between adjacent frames, enforcing temporal coherence in\ndynamic scene modeling. Moreover, we propose a radar-specific power rendering\nformulation closely aligned with radar sensing physics, improving synthesis\naccuracy and interoperability. Extensive experiments on public radar datasets\ndemonstrate the superior performance of RF4D in terms of radar measurement\nsynthesis quality and occupancy estimation accuracy, achieving especially\npronounced improvements in dynamic outdoor scenarios."}
{"id": "2505.20975", "pdf": "https://arxiv.org/pdf/2505.20975", "abs": "https://arxiv.org/abs/2505.20975", "authors": ["Shamil Ayupov", "Maksim Nakhodnov", "Anastasia Yaschenko", "Andrey Kuznetsov", "Aibek Alanov"], "title": "DreamBoothDPO: Improving Personalized Generation using Direct Preference Optimization", "categories": ["cs.CV"], "comment": "The first two authors contributed equally. The source code can be\n  found at https://github.com/ControlGenAI/DreamBoothDPO", "summary": "Personalized diffusion models have shown remarkable success in Text-to-Image\n(T2I) generation by enabling the injection of user-defined concepts into\ndiverse contexts. However, balancing concept fidelity with contextual alignment\nremains a challenging open problem. In this work, we propose an RL-based\napproach that leverages the diverse outputs of T2I models to address this\nissue. Our method eliminates the need for human-annotated scores by generating\na synthetic paired dataset for DPO-like training using external quality\nmetrics. These better-worse pairs are specifically constructed to improve both\nconcept fidelity and prompt adherence. Moreover, our approach supports flexible\nadjustment of the trade-off between image fidelity and textual alignment.\nThrough multi-step training, our approach outperforms a naive baseline in\nconvergence speed and output quality. We conduct extensive qualitative and\nquantitative analysis, demonstrating the effectiveness of our method across\nvarious architectures and fine-tuning techniques. The source code can be found\nat https://github.com/ControlGenAI/DreamBoothDPO."}
{"id": "2505.20981", "pdf": "https://arxiv.org/pdf/2505.20981", "abs": "https://arxiv.org/abs/2505.20981", "authors": ["Cainan Davidson", "Deva Ramanan", "Neehar Peri"], "title": "RefAV: Towards Planning-Centric Scenario Mining", "categories": ["cs.CV", "cs.CL", "cs.RO"], "comment": null, "summary": "Autonomous Vehicles (AVs) collect and pseudo-label terabytes of multi-modal\ndata localized to HD maps during normal fleet testing. However, identifying\ninteresting and safety-critical scenarios from uncurated driving logs remains a\nsignificant challenge. Traditional scenario mining techniques are error-prone\nand prohibitively time-consuming, often relying on hand-crafted structured\nqueries. In this work, we revisit spatio-temporal scenario mining through the\nlens of recent vision-language models (VLMs) to detect whether a described\nscenario occurs in a driving log and, if so, precisely localize it in both time\nand space. To address this problem, we introduce RefAV, a large-scale dataset\nof 10,000 diverse natural language queries that describe complex multi-agent\ninteractions relevant to motion planning derived from 1000 driving logs in the\nArgoverse 2 Sensor dataset. We evaluate several referential multi-object\ntrackers and present an empirical analysis of our baselines. Notably, we find\nthat naively repurposing off-the-shelf VLMs yields poor performance, suggesting\nthat scenario mining presents unique challenges. Our code and dataset are\navailable at https://github.com/CainanD/RefAV/ and\nhttps://argoverse.github.io/user-guide/tasks/scenario_mining.html"}
{"id": "2505.20985", "pdf": "https://arxiv.org/pdf/2505.20985", "abs": "https://arxiv.org/abs/2505.20985", "authors": ["Mustafa İzzet Muştu", "Hazım Kemal Ekenel"], "title": "Assessing the Use of Face Swapping Methods as Face Anonymizers in Videos", "categories": ["cs.CV"], "comment": "Accepted to the 2025 25th International Conference on Digital Signal\n  Processing (DSP 2025)", "summary": "The increasing demand for large-scale visual data, coupled with strict\nprivacy regulations, has driven research into anonymization methods that hide\npersonal identities without seriously degrading data quality. In this paper, we\nexplore the potential of face swapping methods to preserve privacy in video\ndata. Through extensive evaluations focusing on temporal consistency, anonymity\nstrength, and visual fidelity, we find that face swapping techniques can\nproduce consistent facial transitions and effectively hide identities. These\nresults underscore the suitability of face swapping for privacy-preserving\nvideo applications and lay the groundwork for future advancements in\nanonymization focused face-swapping models."}
{"id": "2505.21002", "pdf": "https://arxiv.org/pdf/2505.21002", "abs": "https://arxiv.org/abs/2505.21002", "authors": ["Mustafa İzzet Muştu", "Hazım Kemal Ekenel"], "title": "Facial Attribute Based Text Guided Face Anonymization", "categories": ["cs.CV", "I.4.9; I.2.10; I.4.8"], "comment": "6 pages, 5 figures, published in the Proceedings of the Joint\n  visuAAL-GoodBrother Conference on Trustworthy Video- and Audio-Based\n  Assistive Technologies", "summary": "The increasing prevalence of computer vision applications necessitates\nhandling vast amounts of visual data, often containing personal information.\nWhile this technology offers significant benefits, it should not compromise\nprivacy. Data privacy regulations emphasize the need for individual consent for\nprocessing personal data, hindering researchers' ability to collect\nhigh-quality datasets containing the faces of the individuals. This paper\npresents a deep learning-based face anonymization pipeline to overcome this\nchallenge. Unlike most of the existing methods, our method leverages recent\nadvancements in diffusion-based inpainting models, eliminating the need for\ntraining Generative Adversarial Networks. The pipeline employs a three-stage\napproach: face detection with RetinaNet, feature extraction with VGG-Face, and\nrealistic face generation using the state-of-the-art BrushNet diffusion model.\nBrushNet utilizes the entire image, face masks, and text prompts specifying\ndesired facial attributes like age, ethnicity, gender, and expression. This\nenables the generation of natural-looking images with unrecognizable\nindividuals, facilitating the creation of privacy-compliant datasets for\ncomputer vision research."}
{"id": "2505.21010", "pdf": "https://arxiv.org/pdf/2505.21010", "abs": "https://arxiv.org/abs/2505.21010", "authors": ["Sabbir Ahmed", "Mamshad Nayeem Rizve", "Abdullah Al Arafat", "Jacqueline Liu", "Rahim Hossain", "Mohaiminul Al Nahian", "Adnan Siraj Rakin"], "title": "Unified Alignment Protocol: Making Sense of the Unlabeled Data in New Domains", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Semi-Supervised Federated Learning (SSFL) is gaining popularity over\nconventional Federated Learning in many real-world applications. Due to the\npractical limitation of limited labeled data on the client side, SSFL considers\nthat participating clients train with unlabeled data, and only the central\nserver has the necessary resources to access limited labeled data, making it an\nideal fit for real-world applications (e.g., healthcare). However, traditional\nSSFL assumes that the data distributions in the training phase and testing\nphase are the same. In practice, however, domain shifts frequently occur,\nmaking it essential for SSFL to incorporate generalization capabilities and\nenhance their practicality. The core challenge is improving model\ngeneralization to new, unseen domains while the client participate in SSFL.\nHowever, the decentralized setup of SSFL and unsupervised client training\nnecessitates innovation to achieve improved generalization across domains. To\nachieve this, we propose a novel framework called the Unified Alignment\nProtocol (UAP), which consists of an alternating two-stage training process.\nThe first stage involves training the server model to learn and align the\nfeatures with a parametric distribution, which is subsequently communicated to\nclients without additional communication overhead. The second stage proposes a\nnovel training algorithm that utilizes the server feature distribution to align\nclient features accordingly. Our extensive experiments on standard domain\ngeneralization benchmark datasets across multiple model architectures reveal\nthat proposed UAP successfully achieves SOTA generalization performance in SSFL\nsetting."}
{"id": "2505.21032", "pdf": "https://arxiv.org/pdf/2505.21032", "abs": "https://arxiv.org/abs/2505.21032", "authors": ["Nils Neukirch", "Johanna Vielhaben", "Nils Strodthoff"], "title": "FeatInv: Spatially resolved mapping from feature space to input space using conditional diffusion models", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "15 pages, 10 figures, code is available at\n  https://github.com/AI4HealthUOL/FeatInv", "summary": "Internal representations are crucial for understanding deep neural networks,\nsuch as their properties and reasoning patterns, but remain difficult to\ninterpret. While mapping from feature space to input space aids in interpreting\nthe former, existing approaches often rely on crude approximations. We propose\nusing a conditional diffusion model - a pretrained high-fidelity diffusion\nmodel conditioned on spatially resolved feature maps - to learn such a mapping\nin a probabilistic manner. We demonstrate the feasibility of this approach\nacross various pretrained image classifiers from CNNs to ViTs, showing\nexcellent reconstruction capabilities. Through qualitative comparisons and\nrobustness analysis, we validate our method and showcase possible applications,\nsuch as the visualization of concept steering in input space or investigations\nof the composite nature of the feature space. This approach has broad potential\nfor improving feature space understanding in computer vision models."}
{"id": "2505.21036", "pdf": "https://arxiv.org/pdf/2505.21036", "abs": "https://arxiv.org/abs/2505.21036", "authors": ["Aiyue Chen", "Bin Dong", "Jingru Li", "Jing Lin", "Yiwu Yao", "Gongyi Wang"], "title": "RainFusion: Adaptive Video Generation Acceleration via Multi-Dimensional Visual Redundancy", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Video generation using diffusion models is highly computationally intensive,\nwith 3D attention in Diffusion Transformer (DiT) models accounting for over\n80\\% of the total computational resources. In this work, we introduce {\\bf\nRainFusion}, a novel training-free sparse attention method that exploits\ninherent sparsity nature in visual data to accelerate attention computation\nwhile preserving video quality. Specifically, we identify three unique sparse\npatterns in video generation attention calculations--Spatial Pattern, Temporal\nPattern and Textural Pattern. The sparse pattern for each attention head is\ndetermined online with negligible overhead (\\textasciitilde\\,0.2\\%) with our\nproposed {\\bf ARM} (Adaptive Recognition Module) during inference. Our proposed\n{\\bf RainFusion} is a plug-and-play method, that can be seamlessly integrated\ninto state-of-the-art 3D-attention video generation models without additional\ntraining or calibration. We evaluate our method on leading open-sourced models\nincluding HunyuanVideo, OpenSoraPlan-1.2 and CogVideoX-5B, demonstrating its\nbroad applicability and effectiveness. Experimental results show that\nRainFusion achieves over {\\bf 2\\(\\times\\)} speedup in attention computation\nwhile maintaining video quality, with only a minimal impact on VBench scores\n(-0.2\\%)."}
{"id": "2505.21049", "pdf": "https://arxiv.org/pdf/2505.21049", "abs": "https://arxiv.org/abs/2505.21049", "authors": ["Dehao Wang", "Haohang Zhu", "Yiwen Xu", "Kaiqi Liu"], "title": "Robust Video-Based Pothole Detection and Area Estimation for Intelligent Vehicles with Depth Map and Kalman Smoothing", "categories": ["cs.CV"], "comment": null, "summary": "Road potholes pose a serious threat to driving safety and comfort, making\ntheir detection and assessment a critical task in fields such as autonomous\ndriving. When driving vehicles, the operators usually avoid large potholes and\napproach smaller ones at reduced speeds to ensure safety. Therefore, accurately\nestimating pothole area is of vital importance. Most existing vision-based\nmethods rely on distance priors to construct geometric models. However, their\nperformance is susceptible to variations in camera angles and typically relies\non the assumption of a flat road surface, potentially leading to significant\nerrors in complex real-world environments. To address these problems, a robust\npothole area estimation framework that integrates object detection and\nmonocular depth estimation in a video stream is proposed in this paper. First,\nto enhance pothole feature extraction and improve the detection of small\npotholes, ACSH-YOLOv8 is proposed with ACmix module and the small object\ndetection head. Then, the BoT-SORT algorithm is utilized for pothole tracking,\nwhile DepthAnything V2 generates depth maps for each frame. With the obtained\ndepth maps and potholes labels, a novel Minimum Bounding Triangulated Pixel\n(MBTP) method is proposed for pothole area estimation. Finally, Kalman Filter\nbased on Confidence and Distance (CDKF) is developed to maintain consistency of\nestimation results across consecutive frames. The results show that ACSH-YOLOv8\nmodel achieves an AP(50) of 76.6%, representing a 7.6% improvement over YOLOv8.\nThrough CDKF optimization across consecutive frames, pothole predictions become\nmore robust, thereby enhancing the method's practical applicability."}
{"id": "2505.21050", "pdf": "https://arxiv.org/pdf/2505.21050", "abs": "https://arxiv.org/abs/2505.21050", "authors": ["Xin Yang", "Jiantao Lin", "Yingjie Xu", "Haodong Li", "Yingcong Chen"], "title": "Advancing high-fidelity 3D and Texture Generation with 2.5D latents", "categories": ["cs.CV"], "comment": null, "summary": "Despite the availability of large-scale 3D datasets and advancements in 3D\ngenerative models, the complexity and uneven quality of 3D geometry and texture\ndata continue to hinder the performance of 3D generation techniques. In most\nexisting approaches, 3D geometry and texture are generated in separate stages\nusing different models and non-unified representations, frequently leading to\nunsatisfactory coherence between geometry and texture. To address these\nchallenges, we propose a novel framework for joint generation of 3D geometry\nand texture. Specifically, we focus in generate a versatile 2.5D\nrepresentations that can be seamlessly transformed between 2D and 3D. Our\napproach begins by integrating multiview RGB, normal, and coordinate images\ninto a unified representation, termed as 2.5D latents. Next, we adapt\npre-trained 2D foundation models for high-fidelity 2.5D generation, utilizing\nboth text and image conditions. Finally, we introduce a lightweight 2.5D-to-3D\nrefiner-decoder framework that efficiently generates detailed 3D\nrepresentations from 2.5D images. Extensive experiments demonstrate that our\nmodel not only excels in generating high-quality 3D objects with coherent\nstructure and color from text and image inputs but also significantly\noutperforms existing methods in geometry-conditioned texture generation."}
{"id": "2505.21060", "pdf": "https://arxiv.org/pdf/2505.21060", "abs": "https://arxiv.org/abs/2505.21060", "authors": ["Peng Wang", "Xiang Liu", "Peidong Liu"], "title": "Styl3R: Instant 3D Stylized Reconstruction for Arbitrary Scenes and Styles", "categories": ["cs.CV"], "comment": "Project page: https://nickisdope.github.io/Styl3R", "summary": "Stylizing 3D scenes instantly while maintaining multi-view consistency and\nfaithfully resembling a style image remains a significant challenge. Current\nstate-of-the-art 3D stylization methods typically involve computationally\nintensive test-time optimization to transfer artistic features into a\npretrained 3D representation, often requiring dense posed input images. In\ncontrast, leveraging recent advances in feed-forward reconstruction models, we\ndemonstrate a novel approach to achieve direct 3D stylization in less than a\nsecond using unposed sparse-view scene images and an arbitrary style image. To\naddress the inherent decoupling between reconstruction and stylization, we\nintroduce a branched architecture that separates structure modeling and\nappearance shading, effectively preventing stylistic transfer from distorting\nthe underlying 3D scene structure. Furthermore, we adapt an identity loss to\nfacilitate pre-training our stylization model through the novel view synthesis\ntask. This strategy also allows our model to retain its original reconstruction\ncapabilities while being fine-tuned for stylization. Comprehensive evaluations,\nusing both in-domain and out-of-domain datasets, demonstrate that our approach\nproduces high-quality stylized 3D content that achieve a superior blend of\nstyle and scene appearance, while also outperforming existing methods in terms\nof multi-view consistency and efficiency."}
{"id": "2505.21061", "pdf": "https://arxiv.org/pdf/2505.21061", "abs": "https://arxiv.org/abs/2505.21061", "authors": ["Fatemeh Pesaran Zadeh", "Yoojin Oh", "Gunhee Kim"], "title": "LPOI: Listwise Preference Optimization for Vision Language Models", "categories": ["cs.CV", "cs.AI"], "comment": "ACL 2025 Main. Code is released at\n  https://github.com/fatemehpesaran310/lpoi", "summary": "Aligning large VLMs with human preferences is a challenging task, as methods\nlike RLHF and DPO often overfit to textual information or exacerbate\nhallucinations. Although augmenting negative image samples partially addresses\nthese pitfalls, no prior work has employed listwise preference optimization for\nVLMs, due to the complexity and cost of constructing listwise image samples. In\nthis work, we propose LPOI, the first object-aware listwise preference\noptimization developed for reducing hallucinations in VLMs. LPOI identifies and\nmasks a critical object in the image, and then interpolates the masked region\nbetween the positive and negative images to form a sequence of incrementally\nmore complete images. The model is trained to rank these images in ascending\norder of object visibility, effectively reducing hallucinations while retaining\nvisual fidelity. LPOI requires no extra annotations beyond standard pairwise\npreference data, as it automatically constructs the ranked lists through object\nmasking and interpolation. Comprehensive experiments on MMHalBench, AMBER, and\nObject HalBench confirm that LPOI outperforms existing preference optimization\nmethods in reducing hallucinations and enhancing VLM performance. We make the\ncode available at https://github.com/fatemehpesaran310/lpoi."}
{"id": "2505.21062", "pdf": "https://arxiv.org/pdf/2505.21062", "abs": "https://arxiv.org/abs/2505.21062", "authors": ["Davide Lobba", "Fulvio Sanguigni", "Bin Ren", "Marcella Cornia", "Rita Cucchiara", "Nicu Sebe"], "title": "Inverse Virtual Try-On: Generating Multi-Category Product-Style Images from Clothed Individuals", "categories": ["cs.CV"], "comment": null, "summary": "While virtual try-on (VTON) systems aim to render a garment onto a target\nperson image, this paper tackles the novel task of virtual try-off (VTOFF),\nwhich addresses the inverse problem: generating standardized product images of\ngarments from real-world photos of clothed individuals. Unlike VTON, which must\nresolve diverse pose and style variations, VTOFF benefits from a consistent and\nwell-defined output format -- typically a flat, lay-down-style representation\nof the garment -- making it a promising tool for data generation and dataset\nenhancement. However, existing VTOFF approaches face two major limitations: (i)\ndifficulty in disentangling garment features from occlusions and complex poses,\noften leading to visual artifacts, and (ii) restricted applicability to\nsingle-category garments (e.g., upper-body clothes only), limiting\ngeneralization. To address these challenges, we present Text-Enhanced\nMUlti-category Virtual Try-Off (TEMU-VTOFF), a novel architecture featuring a\ndual DiT-based backbone with a modified multimodal attention mechanism for\nrobust garment feature extraction. Our architecture is designed to receive\ngarment information from multiple modalities like images, text, and masks to\nwork in a multi-category setting. Finally, we propose an additional alignment\nmodule to further refine the generated visual details. Experiments on VITON-HD\nand Dress Code datasets show that TEMU-VTOFF sets a new state-of-the-art on the\nVTOFF task, significantly improving both visual quality and fidelity to the\ntarget garments."}
{"id": "2505.21070", "pdf": "https://arxiv.org/pdf/2505.21070", "abs": "https://arxiv.org/abs/2505.21070", "authors": ["Zeqing Wang", "Bowen Zheng", "Xingyi Yang", "Yuecong Xu", "Xinchao Wang"], "title": "Minute-Long Videos with Dual Parallelisms", "categories": ["cs.CV"], "comment": "The code is available at\n  https://github.com/DualParal-Project/DualParal", "summary": "Diffusion Transformer (DiT)-based video diffusion models generate\nhigh-quality videos at scale but incur prohibitive processing latency and\nmemory costs for long videos. To address this, we propose a novel distributed\ninference strategy, termed DualParal. The core idea is that, instead of\ngenerating an entire video on a single GPU, we parallelize both temporal frames\nand model layers across GPUs. However, a naive implementation of this division\nfaces a key limitation: since diffusion models require synchronized noise\nlevels across frames, this implementation leads to the serialization of\noriginal parallelisms. We leverage a block-wise denoising scheme to handle\nthis. Namely, we process a sequence of frame blocks through the pipeline with\nprogressively decreasing noise levels. Each GPU handles a specific block and\nlayer subset while passing previous results to the next GPU, enabling\nasynchronous computation and communication. To further optimize performance, we\nincorporate two key enhancements. Firstly, a feature cache is implemented on\neach GPU to store and reuse features from the prior block as context,\nminimizing inter-GPU communication and redundant computation. Secondly, we\nemploy a coordinated noise initialization strategy, ensuring globally\nconsistent temporal dynamics by sharing initial noise patterns across GPUs\nwithout extra resource costs. Together, these enable fast, artifact-free, and\ninfinitely long video generation. Applied to the latest diffusion transformer\nvideo generator, our method efficiently produces 1,025-frame videos with up to\n6.54$\\times$ lower latency and 1.48$\\times$ lower memory cost on 8$\\times$RTX\n4090 GPUs."}
{"id": "2505.21076", "pdf": "https://arxiv.org/pdf/2505.21076", "abs": "https://arxiv.org/abs/2505.21076", "authors": ["Weihao Xuan", "Junjue Wang", "Heli Qi", "Zihang Chen", "Zhuo Zheng", "Yanfei Zhong", "Junshi Xia", "Naoto Yokoya"], "title": "DynamicVL: Benchmarking Multimodal Large Language Models for Dynamic City Understanding", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal large language models have demonstrated remarkable capabilities in\nvisual understanding, but their application to long-term Earth observation\nanalysis remains limited, primarily focusing on single-temporal or bi-temporal\nimagery. To address this gap, we introduce DVL-Suite, a comprehensive framework\nfor analyzing long-term urban dynamics through remote sensing imagery. Our\nsuite comprises 15,063 high-resolution (1.0m) multi-temporal images spanning 42\nmegacities in the U.S. from 2005 to 2023, organized into two components:\nDVL-Bench and DVL-Instruct. The DVL-Bench includes seven urban understanding\ntasks, from fundamental change detection (pixel-level) to quantitative analyses\n(regional-level) and comprehensive urban narratives (scene-level), capturing\ndiverse urban dynamics including expansion/transformation patterns, disaster\nassessment, and environmental challenges. We evaluate 17 state-of-the-art\nmultimodal large language models and reveal their limitations in long-term\ntemporal understanding and quantitative analysis. These challenges motivate the\ncreation of DVL-Instruct, a specialized instruction-tuning dataset designed to\nenhance models' capabilities in multi-temporal Earth observation. Building upon\nthis dataset, we develop DVLChat, a baseline model capable of both image-level\nquestion-answering and pixel-level segmentation, facilitating a comprehensive\nunderstanding of city dynamics through language interactions."}
{"id": "2505.21079", "pdf": "https://arxiv.org/pdf/2505.21079", "abs": "https://arxiv.org/abs/2505.21079", "authors": ["Yue Zhang", "Yingzhao Jian", "Hehe Fan", "Yi Yang", "Roger Zimmermann"], "title": "Uni3D-MoE: Scalable Multimodal 3D Scene Understanding via Mixture of Experts", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in multimodal large language models (MLLMs) have\ndemonstrated considerable potential for comprehensive 3D scene understanding.\nHowever, existing approaches typically utilize only one or a limited subset of\n3D modalities, resulting in incomplete representations of 3D scenes and reduced\ninterpretive accuracy. Furthermore, different types of queries inherently\ndepend on distinct modalities, indicating that uniform processing of all\nmodality tokens may fail to effectively capture query-specific context. To\naddress these challenges, we propose Uni3D-MoE, a sparse Mixture-of-Experts\n(MoE)-based 3D MLLM designed to enable adaptive 3D multimodal fusion.\nSpecifically, Uni3D-MoE integrates a comprehensive set of 3D modalities,\nincluding multi-view RGB and depth images, bird's-eye-view (BEV) maps, point\nclouds, and voxel representations. At its core, our framework employs a\nlearnable routing mechanism within the sparse MoE-based large language model,\ndynamically selecting appropriate experts at the token level. Each expert\nspecializes in processing multimodal tokens based on learned modality\npreferences, thus facilitating flexible collaboration tailored to diverse\ntask-specific requirements. Extensive evaluations on standard 3D scene\nunderstanding benchmarks and specialized datasets demonstrate the efficacy of\nUni3D-MoE."}
{"id": "2505.21089", "pdf": "https://arxiv.org/pdf/2505.21089", "abs": "https://arxiv.org/abs/2505.21089", "authors": ["Junjue Wang", "Weihao Xuan", "Heli Qi", "Zhihao Liu", "Kunyi Liu", "Yuhan Wu", "Hongruixuan Chen", "Jian Song", "Junshi Xia", "Zhuo Zheng", "Naoto Yokoya"], "title": "DisasterM3: A Remote Sensing Vision-Language Dataset for Disaster Damage Assessment and Response", "categories": ["cs.CV", "I.4.9"], "comment": "A multi-hazard, multi-sensor, and multi-task vision-language dataset\n  for global-scale disaster assessment and response", "summary": "Large vision-language models (VLMs) have made great achievements in Earth\nvision. However, complex disaster scenes with diverse disaster types,\ngeographic regions, and satellite sensors have posed new challenges for VLM\napplications. To fill this gap, we curate a remote sensing vision-language\ndataset (DisasterM3) for global-scale disaster assessment and response.\nDisasterM3 includes 26,988 bi-temporal satellite images and 123k instruction\npairs across 5 continents, with three characteristics: 1) Multi-hazard:\nDisasterM3 involves 36 historical disaster events with significant impacts,\nwhich are categorized into 10 common natural and man-made disasters.\n2)Multi-sensor: Extreme weather during disasters often hinders optical sensor\nimaging, making it necessary to combine Synthetic Aperture Radar (SAR) imagery\nfor post-disaster scenes. 3) Multi-task: Based on real-world scenarios,\nDisasterM3 includes 9 disaster-related visual perception and reasoning tasks,\nharnessing the full potential of VLM's reasoning ability with progressing from\ndisaster-bearing body recognition to structural damage assessment and object\nrelational reasoning, culminating in the generation of long-form disaster\nreports. We extensively evaluated 14 generic and remote sensing VLMs on our\nbenchmark, revealing that state-of-the-art models struggle with the disaster\ntasks, largely due to the lack of a disaster-specific corpus, cross-sensor gap,\nand damage object counting insensitivity. Focusing on these issues, we\nfine-tune four VLMs using our dataset and achieve stable improvements across\nall tasks, with robust cross-sensor and cross-disaster generalization\ncapabilities."}
{"id": "2505.21099", "pdf": "https://arxiv.org/pdf/2505.21099", "abs": "https://arxiv.org/abs/2505.21099", "authors": ["Tianhao Peng", "Ho Man Kwan", "Yuxuan Jiang", "Ge Gao", "Fan Zhang", "Xiaozhong Xu", "Shan Liu", "David Bull"], "title": "Instance Data Condensation for Image Super-Resolution", "categories": ["cs.CV"], "comment": null, "summary": "Deep learning based image Super-Resolution (ISR) relies on large training\ndatasets to optimize model generalization; this requires substantial\ncomputational and storage resources during training. While dataset condensation\nhas shown potential in improving data efficiency and privacy for high-level\ncomputer vision tasks, it has not yet been fully exploited for ISR. In this\npaper, we propose a novel Instance Data Condensation (IDC) framework\nspecifically for ISR, which achieves instance-level data condensation through\nRandom Local Fourier Feature Extraction and Multi-level Feature Distribution\nMatching. This aims to optimize feature distributions at both global and local\nlevels and obtain high-quality synthesized training content with fine detail.\nThis framework has been utilized to condense the most commonly used training\ndataset for ISR, DIV2K, with a 10% condensation rate. The resulting synthetic\ndataset offers comparable or (in certain cases) even better performance\ncompared to the original full dataset and excellent training stability when\nused to train various popular ISR models. To the best of our knowledge, this is\nthe first time that a condensed/synthetic dataset (with a 10% data volume) has\ndemonstrated such performance. The source code and the synthetic dataset have\nbeen made available at https://github.com/."}
{"id": "2505.21114", "pdf": "https://arxiv.org/pdf/2505.21114", "abs": "https://arxiv.org/abs/2505.21114", "authors": ["Shuai Wang", "Zexian Li", "Qipeng zhang", "Tianhui Song", "Xubin Li", "Tiezheng Ge", "Bo Zheng", "Limin Wang"], "title": "Differentiable Solver Search for Fast Diffusion Sampling", "categories": ["cs.CV"], "comment": "accpeted on ICML25", "summary": "Diffusion models have demonstrated remarkable generation quality but at the\ncost of numerous function evaluations. Recently, advanced ODE-based solvers\nhave been developed to mitigate the substantial computational demands of\nreverse-diffusion solving under limited sampling steps. However, these solvers,\nheavily inspired by Adams-like multistep methods, rely solely on t-related\nLagrange interpolation. We show that t-related Lagrange interpolation is\nsuboptimal for diffusion model and reveal a compact search space comprised of\ntime steps and solver coefficients. Building on our analysis, we propose a\nnovel differentiable solver search algorithm to identify more optimal solver.\nEquipped with the searched solver, rectified-flow models, e.g., SiT-XL/2 and\nFlowDCN-XL/2, achieve FID scores of 2.40 and 2.35, respectively, on ImageNet256\nwith only 10 steps. Meanwhile, DDPM model, DiT-XL/2, reaches a FID score of\n2.33 with only 10 steps. Notably, our searched solver outperforms traditional\nsolvers by a significant margin. Moreover, our searched solver demonstrates\ngenerality across various model architectures, resolutions, and model sizes."}
{"id": "2505.21117", "pdf": "https://arxiv.org/pdf/2505.21117", "abs": "https://arxiv.org/abs/2505.21117", "authors": ["Adeela Islam", "Stefano Fiorini", "Stuart James", "Pietro Morerio", "Alessio Del Bue"], "title": "ReassembleNet: Learnable Keypoints and Diffusion for 2D Fresco Reconstruction", "categories": ["cs.CV"], "comment": null, "summary": "The task of reassembly is a significant challenge across multiple domains,\nincluding archaeology, genomics, and molecular docking, requiring the precise\nplacement and orientation of elements to reconstruct an original structure. In\nthis work, we address key limitations in state-of-the-art Deep Learning methods\nfor reassembly, namely i) scalability; ii) multimodality; and iii) real-world\napplicability: beyond square or simple geometric shapes, realistic and complex\nerosion, or other real-world problems. We propose ReassembleNet, a method that\nreduces complexity by representing each input piece as a set of contour\nkeypoints and learning to select the most informative ones by Graph Neural\nNetworks pooling inspired techniques. ReassembleNet effectively lowers\ncomputational complexity while enabling the integration of features from\nmultiple modalities, including both geometric and texture data. Further\nenhanced through pretraining on a semi-synthetic dataset. We then apply\ndiffusion-based pose estimation to recover the original structure. We improve\non prior methods by 55% and 86% for RMSE Rotation and Translation,\nrespectively."}
{"id": "2505.21144", "pdf": "https://arxiv.org/pdf/2505.21144", "abs": "https://arxiv.org/abs/2505.21144", "authors": ["Sergey Karpukhin", "Vadim Titov", "Andrey Kuznetsov", "Aibek Alanov"], "title": "FastFace: Tuning Identity Preservation in Distilled Diffusion via Guidance and Attention", "categories": ["cs.CV"], "comment": "code available at https://github.com/shredder67/fastface", "summary": "In latest years plethora of identity-preserving adapters for a personalized\ngeneration with diffusion models have been released. Their main disadvantage is\nthat they are dominantly trained jointly with base diffusion models, which\nsuffer from slow multi-step inference. This work aims to tackle the challenge\nof training-free adaptation of pretrained ID-adapters to diffusion models\naccelerated via distillation - through careful re-design of classifier-free\nguidance for few-step stylistic generation and attention manipulation\nmechanisms in decoupled blocks to improve identity similarity and fidelity, we\npropose universal FastFace framework. Additionally, we develop a disentangled\npublic evaluation protocol for id-preserving adapters."}
{"id": "2505.21152", "pdf": "https://arxiv.org/pdf/2505.21152", "abs": "https://arxiv.org/abs/2505.21152", "authors": ["Xurui Li", "Zhonesheng Jiang", "Tingxuan Ai", "Yu Zhou"], "title": "RoBiS: Robust Binary Segmentation for High-Resolution Industrial Images", "categories": ["cs.CV"], "comment": null, "summary": "Robust unsupervised anomaly detection (AD) in real-world scenarios is an\nimportant task. Current methods exhibit severe performance degradation on the\nMVTec AD 2 benchmark due to its complex real-world challenges. To solve this\nproblem, we propose a robust framework RoBiS, which consists of three core\nmodules: (1) Swin-Cropping, a high-resolution image pre-processing strategy to\npreserve the information of small anomalies through overlapping window\ncropping. (2) The data augmentation of noise addition and lighting simulation\nis carried out on the training data to improve the robustness of AD model. We\nuse INP-Former as our baseline, which could generate better results on the\nvarious sub-images. (3) The traditional statistical-based binarization strategy\n(mean+3std) is combined with our previous work, MEBin (published in CVPR2025),\nfor joint adaptive binarization. Then, SAM is further employed to refine the\nsegmentation results. Compared with some methods reported by the MVTec AD 2,\nour RoBiS achieves a 29.2% SegF1 improvement (from 21.8% to 51.00%) on\nTest_private and 29.82% SegF1 gains (from 16.7% to 46.52%) on\nTest_private_mixed. Code is available at https://github.com/xrli-U/RoBiS."}
{"id": "2505.21179", "pdf": "https://arxiv.org/pdf/2505.21179", "abs": "https://arxiv.org/abs/2505.21179", "authors": ["Dar-Yen Chen", "Hmrishav Bandyopadhyay", "Kai Zou", "Yi-Zhe Song"], "title": "Normalized Attention Guidance: Universal Negative Guidance for Diffusion Model", "categories": ["cs.CV"], "comment": null, "summary": "Negative guidance -- explicitly suppressing unwanted attributes -- remains a\nfundamental challenge in diffusion models, particularly in few-step sampling\nregimes. While Classifier-Free Guidance (CFG) works well in standard settings,\nit fails under aggressive sampling step compression due to divergent\npredictions between positive and negative branches. We present Normalized\nAttention Guidance (NAG), an efficient, training-free mechanism that applies\nextrapolation in attention space with L1-based normalization and refinement.\nNAG restores effective negative guidance where CFG collapses while maintaining\nfidelity. Unlike existing approaches, NAG generalizes across architectures\n(UNet, DiT), sampling regimes (few-step, multi-step), and modalities (image,\nvideo), functioning as a \\textit{universal} plug-in with minimal computational\noverhead. Through extensive experimentation, we demonstrate consistent\nimprovements in text alignment (CLIP Score), fidelity (FID, PFID), and\nhuman-perceived quality (ImageReward). Our ablation studies validate each\ndesign component, while user studies confirm significant preference for\nNAG-guided outputs. As a model-agnostic inference-time approach requiring no\nretraining, NAG provides effortless negative guidance for all modern diffusion\nframeworks -- pseudocode in the Appendix!"}
{"id": "2505.21181", "pdf": "https://arxiv.org/pdf/2505.21181", "abs": "https://arxiv.org/abs/2505.21181", "authors": ["Yayin Zheng", "Chen Wan", "Zihong Guo", "Hailing Kuang", "Xiaohai Lu"], "title": "Boosting Adversarial Transferability via High-Frequency Augmentation and Hierarchical-Gradient Fusion", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Adversarial attacks have become a significant challenge in the security of\nmachine learning models, particularly in the context of black-box defense\nstrategies. Existing methods for enhancing adversarial transferability\nprimarily focus on the spatial domain. This paper presents Frequency-Space\nAttack (FSA), a new adversarial attack framework that effectively integrates\nfrequency-domain and spatial-domain transformations. FSA combines two key\ntechniques: (1) High-Frequency Augmentation, which applies Fourier transform\nwith frequency-selective amplification to diversify inputs and emphasize the\ncritical role of high-frequency components in adversarial attacks, and (2)\nHierarchical-Gradient Fusion, which merges multi-scale gradient decomposition\nand fusion to capture both global structures and fine-grained details,\nresulting in smoother perturbations. Our experiment demonstrates that FSA\nconsistently outperforms state-of-the-art methods across various black-box\nmodels. Notably, our proposed FSA achieves an average attack success rate\nincrease of 23.6% compared with BSR (CVPR 2024) on eight black-box defense\nmodels."}
{"id": "2505.21187", "pdf": "https://arxiv.org/pdf/2505.21187", "abs": "https://arxiv.org/abs/2505.21187", "authors": ["Hesam Araghi", "Jan van Gemert", "Nergis Tomen"], "title": "Making Every Event Count: Balancing Data Efficiency and Accuracy in Event Camera Subsampling", "categories": ["cs.CV"], "comment": null, "summary": "Event cameras offer high temporal resolution and power efficiency, making\nthem well-suited for edge AI applications. However, their high event rates\npresent challenges for data transmission and processing. Subsampling methods\nprovide a practical solution, but their effect on downstream visual tasks\nremains underexplored. In this work, we systematically evaluate six\nhardware-friendly subsampling methods using convolutional neural networks for\nevent video classification on various benchmark datasets. We hypothesize that\nevents from high-density regions carry more task-relevant information and are\ntherefore better suited for subsampling. To test this, we introduce a simple\ncausal density-based subsampling method, demonstrating improved classification\naccuracy in sparse regimes. Our analysis further highlights key factors\naffecting subsampling performance, including sensitivity to hyperparameters and\nfailure cases in scenarios with large event count variance. These findings\nprovide insights for utilization of hardware-efficient subsampling strategies\nthat balance data efficiency and task accuracy. The code for this paper will be\nreleased at: https://github.com/hesamaraghi/event-camera-subsampling-methods."}
{"id": "2505.21200", "pdf": "https://arxiv.org/pdf/2505.21200", "abs": "https://arxiv.org/abs/2505.21200", "authors": ["Xudong Tan", "Yaoxin Yang", "Peng Ye", "Jialin Zheng", "Bizhe Bai", "Xinyi Wang", "Jia Hao", "Tao Chen"], "title": "Think Twice, Act Once: Token-Aware Compression and Action Reuse for Efficient Inference in Vision-Language-Action Models", "categories": ["cs.CV"], "comment": null, "summary": "Vision-Language-Action (VLA) models have emerged as a powerful paradigm for\ngeneral-purpose robot control through natural language instructions. However,\ntheir high inference cost-stemming from large-scale token computation and\nautoregressive decoding-poses significant challenges for real-time deployment\nand edge applications. While prior work has primarily focused on architectural\noptimization, we take a different perspective by identifying a dual form of\nredundancy in VLA models: (i) high similarity across consecutive action steps,\nand (ii) substantial redundancy in visual tokens. Motivated by these\nobservations, we propose FlashVLA, the first training-free and plug-and-play\nacceleration framework that enables action reuse in VLA models. FlashVLA\nimproves inference efficiency through a token-aware action reuse mechanism that\navoids redundant decoding across stable action steps, and an information-guided\nvisual token selection strategy that prunes low-contribution tokens. Extensive\nexperiments on the LIBERO benchmark show that FlashVLA reduces FLOPs by 55.7%\nand latency by 36.0%, with only a 0.7% drop in task success rate. These results\ndemonstrate the effectiveness of FlashVLA in enabling lightweight, low-latency\nVLA inference without retraining."}
{"id": "2505.21205", "pdf": "https://arxiv.org/pdf/2505.21205", "abs": "https://arxiv.org/abs/2505.21205", "authors": ["Liuhan Chen", "Xiaodong Cun", "Xiaoyu Li", "Xianyi He", "Shenghai Yuan", "Jie Chen", "Ying Shan", "Li Yuan"], "title": "Sci-Fi: Symmetric Constraint for Frame Inbetweening", "categories": ["cs.CV"], "comment": "22 pages, 9 figures, submitted to NeurIPS2025, under reviewering", "summary": "Frame inbetweening aims to synthesize intermediate video sequences\nconditioned on the given start and end frames. Current state-of-the-art methods\nmainly extend large-scale pre-trained Image-to-Video Diffusion models (I2V-DMs)\nby incorporating end-frame constraints via directly fine-tuning or omitting\ntraining. We identify a critical limitation in their design: Their injections\nof the end-frame constraint usually utilize the same mechanism that originally\nimposed the start-frame (single image) constraint. However, since the original\nI2V-DMs are adequately trained for the start-frame condition in advance,\nnaively introducing the end-frame constraint by the same mechanism with much\nless (even zero) specialized training probably can't make the end frame have a\nstrong enough impact on the intermediate content like the start frame. This\nasymmetric control strength of the two frames over the intermediate content\nlikely leads to inconsistent motion or appearance collapse in generated frames.\nTo efficiently achieve symmetric constraints of start and end frames, we\npropose a novel framework, termed Sci-Fi, which applies a stronger injection\nfor the constraint of a smaller training scale. Specifically, it deals with the\nstart-frame constraint as before, while introducing the end-frame constraint by\nan improved mechanism. The new mechanism is based on a well-designed\nlightweight module, named EF-Net, which encodes only the end frame and expands\nit into temporally adaptive frame-wise features injected into the I2V-DM. This\nmakes the end-frame constraint as strong as the start-frame constraint,\nenabling our Sci-Fi to produce more harmonious transitions in various\nscenarios. Extensive experiments prove the superiority of our Sci-Fi compared\nwith other baselines."}
{"id": "2505.21228", "pdf": "https://arxiv.org/pdf/2505.21228", "abs": "https://arxiv.org/abs/2505.21228", "authors": ["Alvaro Gonzalez-Jimenez", "Simone Lionetti", "Ludovic Amruthalingam", "Philippe Gottfrois", "Fabian Gröger", "Marc Pouly", "Alexander A. Navarini"], "title": "Is Hyperbolic Space All You Need for Medical Anomaly Detection?", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Provisionally Accepted at MICCAI 2025", "summary": "Medical anomaly detection has emerged as a promising solution to challenges\nin data availability and labeling constraints. Traditional methods extract\nfeatures from different layers of pre-trained networks in Euclidean space;\nhowever, Euclidean representations fail to effectively capture the hierarchical\nrelationships within these features, leading to suboptimal anomaly detection\nperformance. We propose a novel yet simple approach that projects feature\nrepresentations into hyperbolic space, aggregates them based on confidence\nlevels, and classifies samples as healthy or anomalous. Our experiments\ndemonstrate that hyperbolic space consistently outperforms Euclidean-based\nframeworks, achieving higher AUROC scores at both image and pixel levels across\nmultiple medical benchmark datasets. Additionally, we show that hyperbolic\nspace exhibits resilience to parameter variations and excels in few-shot\nscenarios, where healthy images are scarce. These findings underscore the\npotential of hyperbolic space as a powerful alternative for medical anomaly\ndetection. The project website can be found at\nhttps://hyperbolic-anomalies.github.io"}
{"id": "2505.21231", "pdf": "https://arxiv.org/pdf/2505.21231", "abs": "https://arxiv.org/abs/2505.21231", "authors": ["Lintao Xu", "Yinghao Wang", "Chaohui Wang"], "title": "Occlusion Boundary and Depth: Mutual Enhancement via Multi-Task Learning", "categories": ["cs.CV"], "comment": "7 pages, 4 tables, 4 figures", "summary": "Occlusion Boundary Estimation (OBE) identifies boundaries arising from both\ninter-object occlusions and self-occlusion within individual objects,\ndistinguishing intrinsic object edges from occlusion-induced contours to\nimprove scene understanding and 3D reconstruction capacity. This is closely\nrelated to Monocular Depth Estimation (MDE), which infers depth from a single\nimage, as occlusion boundaries provide critical geometric cues for resolving\ndepth ambiguities, while depth priors can conversely refine occlusion reasoning\nin complex scenes. In this paper, we propose a novel network, MoDOT, that first\njointly estimates depth and OBs. We propose CASM, a cross-attention multi-scale\nstrip convolution module, leverages mid-level OB features to significantly\nenhance depth prediction. Additionally, we introduce an occlusion-aware loss\nfunction, OBDCL, which encourages sharper and more accurate depth boundaries.\nExtensive experiments on both real and synthetic datasets demonstrate the\nmutual benefits of jointly estimating depth and OB, and highlight the\neffectiveness of our model design. Our method achieves the state-of-the-art\n(SOTA) on both our proposed synthetic datasets and one popular real dataset,\nNYUD-v2, significantly outperforming multi-task baselines. Besides, without\ndomain adaptation, results on real-world depth transfer are comparable to the\ncompetitors, while preserving sharp occlusion boundaries for geometric\nfidelity. We will release our code, pre-trained models, and datasets to support\nfuture research in this direction."}
{"id": "2505.21233", "pdf": "https://arxiv.org/pdf/2505.21233", "abs": "https://arxiv.org/abs/2505.21233", "authors": ["Jiawei Guo", "Feifei Zhai", "Pu Jian", "Qianrun Wei", "Yu Zhou"], "title": "CROP: Contextual Region-Oriented Visual Token Pruning", "categories": ["cs.CV"], "comment": null, "summary": "Current VLM-based VQA methods often process entire images, leading to\nexcessive visual tokens that include redundant information irrelevant to the\nposed question. This abundance of unnecessary image details creates numerous\nvisual tokens, drastically increasing memory and computational requirements in\nVLMs. To address this, we propose Contextual Region-Oriented Visual Token\nPruning (CROP), a novel framework to compress visual tokens through a two-step\nprocess: Localization and Pruning. Specifically, CROP first employs an\nefficient model to identify the contextual region relevant to the input query.\nSubsequently, two distinct strategies are introduced for pruning: (1) Pre-LLM\nCompression (PLC), which adaptively compresses different image regions with\nvarying ratios, and (2) Inner-LLM Pruning (ILP), a training-free method that\nprunes tokens within early LLM layers guided by the identified contextual\nregion. Extensive experiments on a wide range of VQA tasks demonstrate that\nCROP significantly outperforms existing visual token pruning methods and\nachieves state-of-the-art performance. Our code and datasets will be made\navailable."}
{"id": "2505.21238", "pdf": "https://arxiv.org/pdf/2505.21238", "abs": "https://arxiv.org/abs/2505.21238", "authors": ["Jieyu Yuan", "Yujun Li", "Yuanlin Zhang", "Chunle Guo", "Xiongxin Tang", "Ruixing Wang", "Chongyi Li"], "title": "3D-UIR: 3D Gaussian for Underwater 3D Scene Reconstruction via Physics-Based Appearance-Medium Decouplin", "categories": ["cs.CV"], "comment": null, "summary": "Novel view synthesis for underwater scene reconstruction presents unique\nchallenges due to complex light-media interactions. Optical scattering and\nabsorption in water body bring inhomogeneous medium attenuation interference\nthat disrupts conventional volume rendering assumptions of uniform propagation\nmedium. While 3D Gaussian Splatting (3DGS) offers real-time rendering\ncapabilities, it struggles with underwater inhomogeneous environments where\nscattering media introduce artifacts and inconsistent appearance. In this\nstudy, we propose a physics-based framework that disentangles object appearance\nfrom water medium effects through tailored Gaussian modeling. Our approach\nintroduces appearance embeddings, which are explicit medium representations for\nbackscatter and attenuation, enhancing scene consistency. In addition, we\npropose a distance-guided optimization strategy that leverages pseudo-depth\nmaps as supervision with depth regularization and scale penalty terms to\nimprove geometric fidelity. By integrating the proposed appearance and medium\nmodeling components via an underwater imaging model, our approach achieves both\nhigh-quality novel view synthesis and physically accurate scene restoration.\nExperiments demonstrate our significant improvements in rendering quality and\nrestoration accuracy over existing methods. The project page is available at\n\\href{https://bilityniu.github.io/3D-UIR}{https://bilityniu.github.io/3D-UIR"}
{"id": "2505.21258", "pdf": "https://arxiv.org/pdf/2505.21258", "abs": "https://arxiv.org/abs/2505.21258", "authors": ["Changguanng Wu", "Jiangxin Dong", "Chengjian Li", "Jinhui Tang"], "title": "Plenodium: UnderWater 3D Scene Reconstruction with Plenoptic Medium Representation", "categories": ["cs.CV"], "comment": null, "summary": "We present Plenodium (plenoptic medium), an effective and efficient 3D\nrepresentation framework capable of jointly modeling both objects and\nparticipating media. In contrast to existing medium representations that rely\nsolely on view-dependent modeling, our novel plenoptic medium representation\nincorporates both directional and positional information through spherical\nharmonics encoding, enabling highly accurate underwater scene reconstruction.\nTo address the initialization challenge in degraded underwater environments, we\npropose the pseudo-depth Gaussian complementation to augment COLMAP-derived\npoint clouds with robust depth priors. In addition, a depth ranking regularized\nloss is developed to optimize the geometry of the scene and improve the ordinal\nconsistency of the depth maps. Extensive experiments on real-world underwater\ndatasets demonstrate that our method achieves significant improvements in 3D\nreconstruction. Furthermore, we conduct a simulated dataset with ground truth\nand the controllable scattering medium to demonstrate the restoration\ncapability of our method in underwater scenarios. Our code and dataset are\navailable at https://plenodium.github.io/."}
{"id": "2505.21262", "pdf": "https://arxiv.org/pdf/2505.21262", "abs": "https://arxiv.org/abs/2505.21262", "authors": ["M. Akin Yilmaz", "Ahmet Bilican", "A. Murat Tekalp"], "title": "DiMoSR: Feature Modulation via Multi-Branch Dilated Convolutions for Efficient Image Super-Resolution", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Balancing reconstruction quality versus model efficiency remains a critical\nchallenge in lightweight single image super-resolution (SISR). Despite the\nprevalence of attention mechanisms in recent state-of-the-art SISR approaches\nthat primarily emphasize or suppress feature maps, alternative architectural\nparadigms warrant further exploration. This paper introduces DiMoSR (Dilated\nModulation Super-Resolution), a novel architecture that enhances feature\nrepresentation through modulation to complement attention in lightweight SISR\nnetworks. The proposed approach leverages multi-branch dilated convolutions to\ncapture rich contextual information over a wider receptive field while\nmaintaining computational efficiency. Experimental results demonstrate that\nDiMoSR outperforms state-of-the-art lightweight methods across diverse\nbenchmark datasets, achieving superior PSNR and SSIM metrics with comparable or\nreduced computational complexity. Through comprehensive ablation studies, this\nwork not only validates the effectiveness of DiMoSR but also provides critical\ninsights into the interplay between attention mechanisms and feature modulation\nto guide future research in efficient network design. The code and model\nweights to reproduce our results are available at:\nhttps://github.com/makinyilmaz/DiMoSR"}
{"id": "2505.21269", "pdf": "https://arxiv.org/pdf/2505.21269", "abs": "https://arxiv.org/abs/2505.21269", "authors": ["Eva Gmelich Meijling", "Roberto Del Prete", "Arnoud Visser"], "title": "Supervised and self-supervised land-cover segmentation & classification of the Biesbosch wetlands", "categories": ["cs.CV", "eess.IV", "68", "I.4.6"], "comment": "12 pages, presented at the Netherlands Conference on Computer Vision\n  (NCCV), Utrecht, May 2025", "summary": "Accurate wetland land-cover classification is essential for environmental\nmonitoring, biodiversity assessment, and sustainable ecosystem management.\nHowever, the scarcity of annotated data, especially for high-resolution\nsatellite imagery, poses a significant challenge for supervised learning\napproaches. To tackle this issue, this study presents a methodology for wetland\nland-cover segmentation and classification that adopts both supervised and\nself-supervised learning (SSL). We train a U-Net model from scratch on\nSentinel-2 imagery across six wetland regions in the Netherlands, achieving a\nbaseline model accuracy of 85.26%.\n  Addressing the limited availability of labeled data, the results show that\nSSL pretraining with an autoencoder can improve accuracy, especially for the\nhigh-resolution imagery where it is more difficult to obtain labeled data,\nreaching an accuracy of 88.23%.\n  Furthermore, we introduce a framework to scale manually annotated\nhigh-resolution labels to medium-resolution inputs. While the quantitative\nperformance between resolutions is comparable, high-resolution imagery provides\nsignificantly sharper segmentation boundaries and finer spatial detail.\n  As part of this work, we also contribute a curated Sentinel-2 dataset with\nDynamic World labels, tailored for wetland classification tasks and made\npublicly available."}
{"id": "2505.21309", "pdf": "https://arxiv.org/pdf/2505.21309", "abs": "https://arxiv.org/abs/2505.21309", "authors": ["Zenghao Zheng", "Lianping Yang", "Hegui Zhu", "Mingrui Ye"], "title": "Spectral Compression Transformer with Line Pose Graph for Monocular 3D Human Pose Estimation", "categories": ["cs.CV"], "comment": null, "summary": "Transformer-based 3D human pose estimation methods suffer from high\ncomputational costs due to the quadratic complexity of self-attention with\nrespect to sequence length. Additionally, pose sequences often contain\nsignificant redundancy between frames. However, recent methods typically fail\nto improve model capacity while effectively eliminating sequence redundancy. In\nthis work, we introduce the Spectral Compression Transformer (SCT) to reduce\nsequence length and accelerate computation. The SCT encoder treats hidden\nfeatures between blocks as Temporal Feature Signals (TFS) and applies the\nDiscrete Cosine Transform, a Fourier transform-based technique, to determine\nthe spectral components to be retained. By filtering out certain high-frequency\nnoise components, SCT compresses the sequence length and reduces redundancy. To\nfurther enrich the input sequence with prior structural information, we propose\nthe Line Pose Graph (LPG) based on line graph theory. The LPG generates\nskeletal position information that complements the input 2D joint positions,\nthereby improving the model's performance. Finally, we design a dual-stream\nnetwork architecture to effectively model spatial joint relationships and the\ncompressed motion trajectory within the pose sequence. Extensive experiments on\ntwo benchmark datasets (i.e., Human3.6M and MPI-INF-3DHP) demonstrate that our\nmodel achieves state-of-the-art performance with improved computational\nefficiency. For example, on the Human3.6M dataset, our method achieves an MPJPE\nof 37.7mm while maintaining a low computational cost. Furthermore, we perform\nablation studies on each module to assess its effectiveness. The code and\nmodels will be released."}
{"id": "2505.21316", "pdf": "https://arxiv.org/pdf/2505.21316", "abs": "https://arxiv.org/abs/2505.21316", "authors": ["Enam Ahmed Taufik", "Antara Firoz Parsa", "Seraj Al Mahmud Mostafa"], "title": "Efficient Leaf Disease Classification and Segmentation using Midpoint Normalization Technique and Attention Mechanism", "categories": ["cs.CV", "eess.IV"], "comment": "Accepted in 2025 IEEE International Conference on Image Processing\n  (ICIP)", "summary": "Enhancing plant disease detection from leaf imagery remains a persistent\nchallenge due to scarce labeled data and complex contextual factors. We\nintroduce a transformative two-stage methodology, Mid Point Normalization (MPN)\nfor intelligent image preprocessing, coupled with sophisticated attention\nmechanisms that dynamically recalibrate feature representations. Our\nclassification pipeline, merging MPN with Squeeze-and-Excitation (SE) blocks,\nachieves remarkable 93% accuracy while maintaining exceptional class-wise\nbalance. The perfect F1 score attained for our target class exemplifies\nattention's power in adaptive feature refinement. For segmentation tasks, we\nseamlessly integrate identical attention blocks within U-Net architecture using\nMPN-enhanced inputs, delivering compelling performance gains with 72.44% Dice\nscore and 58.54% IoU, substantially outperforming baseline implementations.\nBeyond superior accuracy metrics, our approach yields computationally\nefficient, lightweight architectures perfectly suited for real-world computer\nvision applications."}
{"id": "2505.21325", "pdf": "https://arxiv.org/pdf/2505.21325", "abs": "https://arxiv.org/abs/2505.21325", "authors": ["Guangyuan Li", "Siming Zheng", "Hao Zhang", "Jinwei Chen", "Junsheng Luan", "Binkai Ou", "Lei Zhao", "Bo Li", "Peng-Tao Jiang"], "title": "MagicTryOn: Harnessing Diffusion Transformer for Garment-Preserving Video Virtual Try-on", "categories": ["cs.CV"], "comment": null, "summary": "Video Virtual Try-On (VVT) aims to simulate the natural appearance of\ngarments across consecutive video frames, capturing their dynamic variations\nand interactions with human body motion. However, current VVT methods still\nface challenges in terms of spatiotemporal consistency and garment content\npreservation. First, they use diffusion models based on the U-Net, which are\nlimited in their expressive capability and struggle to reconstruct complex\ndetails. Second, they adopt a separative modeling approach for spatial and\ntemporal attention, which hinders the effective capture of structural\nrelationships and dynamic consistency across frames. Third, their expression of\ngarment details remains insufficient, affecting the realism and stability of\nthe overall synthesized results, especially during human motion. To address the\nabove challenges, we propose MagicTryOn, a video virtual try-on framework built\nupon the large-scale video diffusion Transformer.We replace the U-Net\narchitecture with a diffusion Transformer and combine full self-attention to\njointly model the spatiotemporal consistency of videos. We design a\ncoarse-to-fine garment preservation strategy. The coarse strategy integrates\ngarment tokens during the embedding stage, while the fine strategy incorporates\nmultiple garment-based conditions, such as semantics, textures, and contour\nlines during the denoising stage. Moreover, we introduce a mask-aware loss to\nfurther optimize garment region fidelity. Extensive experiments on both image\nand video try-on datasets demonstrate that our method outperforms existing SOTA\nmethods in comprehensive evaluations and generalizes to in-the-wild scenarios."}
{"id": "2505.21333", "pdf": "https://arxiv.org/pdf/2505.21333", "abs": "https://arxiv.org/abs/2505.21333", "authors": ["Yang Shi", "Huanqian Wang", "Wulin Xie", "Huanyao Zhang", "Lijie Zhao", "Yi-Fan Zhang", "Xinfeng Li", "Chaoyou Fu", "Zhuoer Wen", "Wenting Liu", "Zhuoran Zhang", "Xinlong Chen", "Bohan Zeng", "Sihan Yang", "Yuanxing Zhang", "Pengfei Wan", "Haotian Wang", "Wenjing Yang"], "title": "MME-VideoOCR: Evaluating OCR-Based Capabilities of Multimodal LLMs in Video Scenarios", "categories": ["cs.CV"], "comment": "preprint", "summary": "Multimodal Large Language Models (MLLMs) have achieved considerable accuracy\nin Optical Character Recognition (OCR) from static images. However, their\nefficacy in video OCR is significantly diminished due to factors such as motion\nblur, temporal variations, and visual effects inherent in video content. To\nprovide clearer guidance for training practical MLLMs, we introduce the\nMME-VideoOCR benchmark, which encompasses a comprehensive range of video OCR\napplication scenarios. MME-VideoOCR features 10 task categories comprising 25\nindividual tasks and spans 44 diverse scenarios. These tasks extend beyond text\nrecognition to incorporate deeper comprehension and reasoning of textual\ncontent within videos. The benchmark consists of 1,464 videos with varying\nresolutions, aspect ratios, and durations, along with 2,000 meticulously\ncurated, manually annotated question-answer pairs. We evaluate 18\nstate-of-the-art MLLMs on MME-VideoOCR, revealing that even the best-performing\nmodel (Gemini-2.5 Pro) achieves an accuracy of only 73.7%. Fine-grained\nanalysis indicates that while existing MLLMs demonstrate strong performance on\ntasks where relevant texts are contained within a single or few frames, they\nexhibit limited capability in effectively handling tasks that demand holistic\nvideo comprehension. These limitations are especially evident in scenarios that\nrequire spatio-temporal reasoning, cross-frame information integration, or\nresistance to language prior bias. Our findings also highlight the importance\nof high-resolution visual input and sufficient temporal coverage for reliable\nOCR in dynamic video scenarios."}
{"id": "2505.21334", "pdf": "https://arxiv.org/pdf/2505.21334", "abs": "https://arxiv.org/abs/2505.21334", "authors": ["Kele Shao", "Keda Tao", "Can Qin", "Haoxuan You", "Yang Sui", "Huan Wang"], "title": "HoliTom: Holistic Token Merging for Fast Video Large Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Video large language models (video LLMs) excel at video comprehension but\nface significant computational inefficiency due to redundant video tokens.\nExisting token pruning methods offer solutions. However, approaches operating\nwithin the LLM (inner-LLM pruning), such as FastV, incur intrinsic\ncomputational overhead in shallow layers. In contrast, methods performing token\npruning before the LLM (outer-LLM pruning) primarily address spatial redundancy\nwithin individual frames or limited temporal windows, neglecting the crucial\nglobal temporal dynamics and correlations across longer video sequences. This\nleads to sub-optimal spatio-temporal reduction and does not leverage video\ncompressibility fully. Crucially, the synergistic potential and mutual\ninfluence of combining these strategies remain unexplored. To further reduce\nredundancy, we introduce HoliTom, a novel training-free holistic token merging\nframework. HoliTom employs outer-LLM pruning through global redundancy-aware\ntemporal segmentation, followed by spatial-temporal merging to reduce visual\ntokens by over 90%, significantly alleviating the LLM's computational burden.\nComplementing this, we introduce a robust inner-LLM token similarity-based\nmerging approach, designed for superior performance and compatibility with\nouter-LLM pruning. Evaluations demonstrate our method's promising\nefficiency-performance trade-off on LLaVA-OneVision-7B, reducing computational\ncosts to 6.9% of FLOPs while maintaining 99.1% of the original performance.\nFurthermore, we achieve a 2.28x reduction in Time-To-First-Token (TTFT) and a\n1.32x acceleration in decoding throughput, highlighting the practical benefits\nof our integrated pruning approach for efficient video LLMs inference."}
{"id": "2505.21338", "pdf": "https://arxiv.org/pdf/2505.21338", "abs": "https://arxiv.org/abs/2505.21338", "authors": ["Katarzyna Filus", "Mateusz Żarski"], "title": "Beyond Accuracy: Uncovering the Role of Similarity Perception and its Alignment with Semantics in Supervised Learning", "categories": ["cs.CV"], "comment": null, "summary": "Similarity manifests in various forms, including semantic similarity that is\nparticularly important, serving as an approximation of human object\ncategorization based on e.g. shared functionalities and evolutionary traits. It\nalso offers practical advantages in computational modeling via lexical\nstructures such as WordNet with constant and interpretable similarity. As in\nthe domain of deep vision, there is still not enough focus on the phenomena\nregarding the similarity perception emergence. We introduce Deep Similarity\nInspector (DSI) -- a systematic framework to inspect how deep vision networks\ndevelop their similarity perception and its alignment with semantic similarity.\nOur experiments show that both Convolutional Neural Networks' (CNNs) and Vision\nTransformers' (ViTs) develop a rich similarity perception during training with\n3 phases (initial similarity surge, refinement, stabilization), with clear\ndifferences between CNNs and ViTs. Besides the gradual mistakes elimination,\nthe mistakes refinement phenomenon can be observed."}
{"id": "2505.21357", "pdf": "https://arxiv.org/pdf/2505.21357", "abs": "https://arxiv.org/abs/2505.21357", "authors": ["Wenyuan Li", "Shunlin Liang", "Keyan Chen", "Yongzhe Chen", "Han Ma", "Jianglei Xu", "Yichuan Ma", "Shikang Guan", "Husheng Fang", "Zhenwei Shi"], "title": "AgriFM: A Multi-source Temporal Remote Sensing Foundation Model for Crop Mapping", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Accurate crop mapping fundamentally relies on modeling multi-scale\nspatiotemporal patterns, where spatial scales range from individual field\ntextures to landscape-level context, and temporal scales capture both\nshort-term phenological transitions and full growing-season dynamics.\nTransformer-based remote sensing foundation models (RSFMs) offer promising\npotential for crop mapping due to their innate ability for unified\nspatiotemporal processing. However, current RSFMs remain suboptimal for crop\nmapping: they either employ fixed spatiotemporal windows that ignore the\nmulti-scale nature of crop systems or completely disregard temporal information\nby focusing solely on spatial patterns. To bridge these gaps, we present\nAgriFM, a multi-source remote sensing foundation model specifically designed\nfor agricultural crop mapping. Our approach begins by establishing the\nnecessity of simultaneous hierarchical spatiotemporal feature extraction,\nleading to the development of a modified Video Swin Transformer architecture\nwhere temporal down-sampling is synchronized with spatial scaling operations.\nThis modified backbone enables efficient unified processing of long time-series\nsatellite inputs. AgriFM leverages temporally rich data streams from three\nsatellite sources including MODIS, Landsat-8/9 and Sentinel-2, and is\npre-trained on a global representative dataset comprising over 25 million image\nsamples supervised by land cover products. The resulting framework incorporates\na versatile decoder architecture that dynamically fuses these learned\nspatiotemporal representations, supporting diverse downstream tasks.\nComprehensive evaluations demonstrate AgriFM's superior performance over\nconventional deep learning approaches and state-of-the-art general-purpose\nRSFMs across all downstream tasks. Codes will be available at\nurlhttps://github.com/flyakon/AgriFM."}
{"id": "2505.21370", "pdf": "https://arxiv.org/pdf/2505.21370", "abs": "https://arxiv.org/abs/2505.21370", "authors": ["Xinyuan Wang", "Lian Peng", "Xiangcheng Li", "Yilin He", "KinTak U"], "title": "YOLO-SPCI: Enhancing Remote Sensing Object Detection via Selective-Perspective-Class Integration", "categories": ["cs.CV"], "comment": null, "summary": "Object detection in remote sensing imagery remains a challenging task due to\nextreme scale variation, dense object distributions, and cluttered backgrounds.\nWhile recent detectors such as YOLOv8 have shown promising results, their\nbackbone architectures lack explicit mechanisms to guide multi-scale feature\nrefinement, limiting performance on high-resolution aerial data. In this work,\nwe propose YOLO-SPCI, an attention-enhanced detection framework that introduces\na lightweight Selective-Perspective-Class Integration (SPCI) module to improve\nfeature representation. The SPCI module integrates three components: a\nSelective Stream Gate (SSG) for adaptive regulation of global feature flow, a\nPerspective Fusion Module (PFM) for context-aware multi-scale integration, and\na Class Discrimination Module (CDM) to enhance inter-class separability. We\nembed two SPCI blocks into the P3 and P5 stages of the YOLOv8 backbone,\nenabling effective refinement while preserving compatibility with the original\nneck and head. Experiments on the NWPU VHR-10 dataset demonstrate that\nYOLO-SPCI achieves superior performance compared to state-of-the-art detectors."}
{"id": "2505.21374", "pdf": "https://arxiv.org/pdf/2505.21374", "abs": "https://arxiv.org/abs/2505.21374", "authors": ["Junhao Cheng", "Yuying Ge", "Teng Wang", "Yixiao Ge", "Jing Liao", "Ying Shan"], "title": "Video-Holmes: Can MLLM Think Like Holmes for Complex Video Reasoning?", "categories": ["cs.CV"], "comment": "Homepage: https://github.com/TencentARC/Video-Holmes", "summary": "Recent advances in CoT reasoning and RL post-training have been reported to\nenhance video reasoning capabilities of MLLMs. This progress naturally raises a\nquestion: can these models perform complex video reasoning in a manner\ncomparable to human experts? However, existing video benchmarks primarily\nevaluate visual perception and grounding abilities, with questions that can be\nanswered based on explicit prompts or isolated visual cues. Such benchmarks do\nnot fully capture the intricacies of real-world reasoning, where humans must\nactively search for, integrate, and analyze multiple clues before reaching a\nconclusion. To address this issue, we present Video-Holmes, a benchmark\ninspired by the reasoning process of Sherlock Holmes, designed to evaluate the\ncomplex video reasoning capabilities of MLLMs. Video-Holmes consists of 1,837\nquestions derived from 270 manually annotated suspense short films, which spans\nseven carefully designed tasks. Each task is constructed by first identifying\nkey events and causal relationships within films, and then designing questions\nthat require models to actively locate and connect multiple relevant visual\nclues scattered across different video segments. Our comprehensive evaluation\nof state-of-the-art MLLMs reveals that, while these models generally excel at\nvisual perception, they encounter substantial difficulties with integrating\ninformation and often miss critical clues. For example, the best-performing\nmodel, Gemini-2.5-Pro, achieves an accuracy of only 45%, with most models\nscoring below 40%. We aim that Video-Holmes can serve as a \"Holmes-test\" for\nmultimodal reasoning, motivating models to reason more like humans and\nemphasizing the ongoing challenges in this field. The benchmark is released in\nhttps://github.com/TencentARC/Video-Holmes."}
{"id": "2505.21375", "pdf": "https://arxiv.org/pdf/2505.21375", "abs": "https://arxiv.org/abs/2505.21375", "authors": ["Fengxiang Wang", "Mingshuo Chen", "Yueying Li", "Di Wang", "Haotian Wang", "Zonghao Guo", "Zefan Wang", "Boqi Shan", "Long Lan", "Yulin Wang", "Hongzhen Wang", "Wenjing Yang", "Bo Du", "Jing Zhang"], "title": "GeoLLaVA-8K: Scaling Remote-Sensing Multimodal Large Language Models to 8K Resolution", "categories": ["cs.CV"], "comment": null, "summary": "Ultra-high-resolution (UHR) remote sensing (RS) imagery offers valuable data\nfor Earth observation but pose challenges for existing multimodal foundation\nmodels due to two key bottlenecks: (1) limited availability of UHR training\ndata, and (2) token explosion caused by the large image size. To address data\nscarcity, we introduce SuperRS-VQA (avg. 8,376$\\times$8,376) and HighRS-VQA\n(avg. 2,000$\\times$1,912), the highest-resolution vision-language datasets in\nRS to date, covering 22 real-world dialogue tasks. To mitigate token explosion,\nour pilot studies reveal significant redundancy in RS images: crucial\ninformation is concentrated in a small subset of object-centric tokens, while\npruning background tokens (e.g., ocean or forest) can even improve performance.\nMotivated by these findings, we propose two strategies: Background Token\nPruning and Anchored Token Selection, to reduce the memory footprint while\npreserving key semantics.Integrating these techniques, we introduce\nGeoLLaVA-8K, the first RS-focused multimodal large language model capable of\nhandling inputs up to 8K$\\times$8K resolution, built on the LLaVA framework.\nTrained on SuperRS-VQA and HighRS-VQA, GeoLLaVA-8K sets a new state-of-the-art\non the XLRS-Bench."}
{"id": "2505.21377", "pdf": "https://arxiv.org/pdf/2505.21377", "abs": "https://arxiv.org/abs/2505.21377", "authors": ["Yidi Li", "Jun Xiao", "Zhengda Lu", "Yiqun Wang", "Haiyong Jiang"], "title": "Empowering Vector Graphics with Consistently Arbitrary Viewing and View-dependent Visibility", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "This work presents a novel text-to-vector graphics generation approach,\nDream3DVG, allowing for arbitrary viewpoint viewing, progressive detail\noptimization, and view-dependent occlusion awareness. Our approach is a\ndual-branch optimization framework, consisting of an auxiliary 3D Gaussian\nSplatting optimization branch and a 3D vector graphics optimization branch. The\nintroduced 3DGS branch can bridge the domain gaps between text prompts and\nvector graphics with more consistent guidance. Moreover, 3DGS allows for\nprogressive detail control by scheduling classifier-free guidance, facilitating\nguiding vector graphics with coarse shapes at the initial stages and finer\ndetails at later stages. We also improve the view-dependent occlusions by\ndevising a visibility-awareness rendering module. Extensive results on 3D\nsketches and 3D iconographies, demonstrate the superiority of the method on\ndifferent abstraction levels of details, cross-view consistency, and\nocclusion-aware stroke culling."}
{"id": "2505.21381", "pdf": "https://arxiv.org/pdf/2505.21381", "abs": "https://arxiv.org/abs/2505.21381", "authors": ["Linshuang Diao", "Dayong Ren", "Sensen Song", "Yurong Qian"], "title": "ZigzagPointMamba: Spatial-Semantic Mamba for Point Cloud Understanding", "categories": ["cs.CV"], "comment": null, "summary": "State Space models (SSMs) such as PointMamba enable efficient feature\nextraction for point cloud self-supervised learning with linear complexity,\noutperforming Transformers in computational efficiency. However, existing\nPointMamba-based methods depend on complex token ordering and random masking,\nwhich disrupt spatial continuity and local semantic correlations. We propose\nZigzagPointMamba to tackle these challenges. The core of our approach is a\nsimple zigzag scan path that globally sequences point cloud tokens, enhancing\nspatial continuity by preserving the proximity of spatially adjacent point\ntokens. Nevertheless, random masking undermines local semantic modeling in\nself-supervised learning. To address this, we introduce a Semantic-Siamese\nMasking Strategy (SMS), which masks semantically similar tokens to facilitate\nreconstruction by integrating local features of original and similar tokens.\nThis overcomes the dependence on isolated local features and enables robust\nglobal semantic modeling. Our pre-trained ZigzagPointMamba weights\nsignificantly improve downstream tasks, achieving a 1.59% mIoU gain on\nShapeNetPart for part segmentation, a 0.4% higher accuracy on ModelNet40 for\nclassification, and 0.19%, 1.22%, and 0.72% higher accuracies respectively for\nthe classification tasks on the OBJ-BG, OBJ-ONLY, and PB-T50-RS subsets of\nScanObjectNN. The code is available at:\nhttps://anonymous.4open.science/r/ZigzagPointMamba-1800/"}
{"id": "2505.21387", "pdf": "https://arxiv.org/pdf/2505.21387", "abs": "https://arxiv.org/abs/2505.21387", "authors": ["Xihong Yang", "Siwei Wang", "Fangdi Wang", "Jiaqi Jin", "Suyuan Liu", "Yue Liu", "En Zhu", "Xinwang Liu", "Yueming Jin"], "title": "Automatically Identify and Rectify: Robust Deep Contrastive Multi-view Clustering in Noisy Scenarios", "categories": ["cs.CV"], "comment": null, "summary": "Leveraging the powerful representation learning capabilities, deep multi-view\nclustering methods have demonstrated reliable performance by effectively\nintegrating multi-source information from diverse views in recent years. Most\nexisting methods rely on the assumption of clean views. However, noise is\npervasive in real-world scenarios, leading to a significant degradation in\nperformance. To tackle this problem, we propose a novel multi-view clustering\nframework for the automatic identification and rectification of noisy data,\ntermed AIRMVC. Specifically, we reformulate noisy identification as an anomaly\nidentification problem using GMM. We then design a hybrid rectification\nstrategy to mitigate the adverse effects of noisy data based on the\nidentification results. Furthermore, we introduce a noise-robust contrastive\nmechanism to generate reliable representations. Additionally, we provide a\ntheoretical proof demonstrating that these representations can discard noisy\ninformation, thereby improving the performance of downstream tasks. Extensive\nexperiments on six benchmark datasets demonstrate that AIRMVC outperforms\nstate-of-the-art algorithms in terms of robustness in noisy scenarios. The code\nof AIRMVC are available at https://github.com/xihongyang1999/AIRMVC on Github."}
{"id": "2505.21420", "pdf": "https://arxiv.org/pdf/2505.21420", "abs": "https://arxiv.org/abs/2505.21420", "authors": ["Jinbao Wang", "Hanzhe Liang", "Can Gao", "Chenxi Hu", "Jie Zhou", "Yunkang Cao", "Linlin Shen", "Weiming Shen"], "title": "Mentor3AD: Feature Reconstruction-based 3D Anomaly Detection via Multi-modality Mentor Learning", "categories": ["cs.CV", "cs.AI"], "comment": "10 Pages, 6 Figures, 7 Tables", "summary": "Multimodal feature reconstruction is a promising approach for 3D anomaly\ndetection, leveraging the complementary information from dual modalities. We\nfurther advance this paradigm by utilizing multi-modal mentor learning, which\nfuses intermediate features to further distinguish normal from feature\ndifferences. To address these challenges, we propose a novel method called\nMentor3AD, which utilizes multi-modal mentor learning. By leveraging the shared\nfeatures of different modalities, Mentor3AD can extract more effective features\nand guide feature reconstruction, ultimately improving detection performance.\nSpecifically, Mentor3AD includes a Mentor of Fusion Module (MFM) that merges\nfeatures extracted from RGB and 3D modalities to create a mentor feature.\nAdditionally, we have designed a Mentor of Guidance Module (MGM) to facilitate\ncross-modal reconstruction, supported by the mentor feature. Lastly, we\nintroduce a Voting Module (VM) to more accurately generate the final anomaly\nscore. Extensive comparative and ablation studies on MVTec 3D-AD and Eyecandies\nhave verified the effectiveness of the proposed method."}
{"id": "2505.21448", "pdf": "https://arxiv.org/pdf/2505.21448", "abs": "https://arxiv.org/abs/2505.21448", "authors": ["Ziqiao Peng", "Jiwen Liu", "Haoxian Zhang", "Xiaoqiang Liu", "Songlin Tang", "Pengfei Wan", "Di Zhang", "Hongyan Liu", "Jun He"], "title": "OmniSync: Towards Universal Lip Synchronization via Diffusion Transformers", "categories": ["cs.CV"], "comment": "https://ziqiaopeng.github.io/OmniSync/", "summary": "Lip synchronization is the task of aligning a speaker's lip movements in\nvideo with corresponding speech audio, and it is essential for creating\nrealistic, expressive video content. However, existing methods often rely on\nreference frames and masked-frame inpainting, which limit their robustness to\nidentity consistency, pose variations, facial occlusions, and stylized content.\nIn addition, since audio signals provide weaker conditioning than visual cues,\nlip shape leakage from the original video will affect lip sync quality. In this\npaper, we present OmniSync, a universal lip synchronization framework for\ndiverse visual scenarios. Our approach introduces a mask-free training paradigm\nusing Diffusion Transformer models for direct frame editing without explicit\nmasks, enabling unlimited-duration inference while maintaining natural facial\ndynamics and preserving character identity. During inference, we propose a\nflow-matching-based progressive noise initialization to ensure pose and\nidentity consistency, while allowing precise mouth-region editing. To address\nthe weak conditioning signal of audio, we develop a Dynamic Spatiotemporal\nClassifier-Free Guidance (DS-CFG) mechanism that adaptively adjusts guidance\nstrength over time and space. We also establish the AIGC-LipSync Benchmark, the\nfirst evaluation suite for lip synchronization in diverse AI-generated videos.\nExtensive experiments demonstrate that OmniSync significantly outperforms prior\nmethods in both visual quality and lip sync accuracy, achieving superior\nresults in both real-world and AI-generated videos."}
{"id": "2505.21454", "pdf": "https://arxiv.org/pdf/2505.21454", "abs": "https://arxiv.org/abs/2505.21454", "authors": ["Yue Li Du", "Ben Alexander", "Mikhail Antonenka", "Rohan Mahadev", "Hao-yu Wu", "Dmitry Kislyuk"], "title": "Visual Product Graph: Bridging Visual Products And Composite Images For End-to-End Style Recommendations", "categories": ["cs.CV"], "comment": "10 pages, 10 figures", "summary": "Retrieving semantically similar but visually distinct contents has been a\ncritical capability in visual search systems. In this work, we aim to tackle\nthis problem with Visual Product Graph (VPG), leveraging high-performance\ninfrastructure for storage and state-of-the-art computer vision models for\nimage understanding. VPG is built to be an online real-time retrieval system\nthat enables navigation from individual products to composite scenes containing\nthose products, along with complementary recommendations. Our system not only\noffers contextual insights by showcasing how products can be styled in a\ncontext, but also provides recommendations for complementary products drawn\nfrom these inspirations. We discuss the essential components for building the\nVisual Product Graph, along with the core computer vision model improvements\nacross object detection, foundational visual embeddings, and other visual\nsignals. Our system achieves a 78.8% extremely similar@1 in end-to-end human\nrelevance evaluations, and a 6% module engagement rate. The \"Ways to Style It\"\nmodule, powered by the Visual Product Graph technology, is deployed in\nproduction at Pinterest."}
{"id": "2505.21457", "pdf": "https://arxiv.org/pdf/2505.21457", "abs": "https://arxiv.org/abs/2505.21457", "authors": ["Muzhi Zhu", "Hao Zhong", "Canyu Zhao", "Zongze Du", "Zheng Huang", "Mingyu Liu", "Hao Chen", "Cheng Zou", "Jingdong Chen", "Ming Yang", "Chunhua Shen"], "title": "Active-O3: Empowering Multimodal Large Language Models with Active Perception via GRPO", "categories": ["cs.CV", "cs.AI"], "comment": "Project Page: https://aim-uofa.github.io/ACTIVE-o3", "summary": "Active vision, also known as active perception, refers to the process of\nactively selecting where and how to look in order to gather task-relevant\ninformation. It is a critical component of efficient perception and\ndecision-making in humans and advanced embodied agents. Recently, the use of\nMultimodal Large Language Models (MLLMs) as central planning and\ndecision-making modules in robotic systems has gained extensive attention.\nHowever, despite the importance of active perception in embodied intelligence,\nthere is little to no exploration of how MLLMs can be equipped with or learn\nactive perception capabilities. In this paper, we first provide a systematic\ndefinition of MLLM-based active perception tasks. We point out that the\nrecently proposed GPT-o3 model's zoom-in search strategy can be regarded as a\nspecial case of active perception; however, it still suffers from low search\nefficiency and inaccurate region selection. To address these issues, we propose\nACTIVE-O3, a purely reinforcement learning based training framework built on\ntop of GRPO, designed to equip MLLMs with active perception capabilities. We\nfurther establish a comprehensive benchmark suite to evaluate ACTIVE-O3 across\nboth general open-world tasks, such as small-object and dense object grounding,\nand domain-specific scenarios, including small object detection in remote\nsensing and autonomous driving, as well as fine-grained interactive\nsegmentation. In addition, ACTIVE-O3 also demonstrates strong zero-shot\nreasoning abilities on the V* Benchmark, without relying on any explicit\nreasoning data. We hope that our work can provide a simple codebase and\nevaluation protocol to facilitate future research on active perception in\nMLLMs."}
{"id": "2505.21465", "pdf": "https://arxiv.org/pdf/2505.21465", "abs": "https://arxiv.org/abs/2505.21465", "authors": ["Bozhou Li", "Wentao Zhang"], "title": "ID-Align: RoPE-Conscious Position Remapping for Dynamic High-Resolution Adaptation in Vision-Language Models", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Currently, a prevalent approach for enhancing Vision-Language Models (VLMs)\nperformance is to encode both the high-resolution version and the thumbnail of\nan image simultaneously. While effective, this method generates a large number\nof image tokens. When combined with the widely used Rotary Position Embedding\n(RoPE), its long-term decay property hinders the interaction between\nhigh-resolution tokens and thumbnail tokens, as well as between text and image.\nTo address these issues, we propose ID-Align, which alleviates these problems\nby reordering position IDs. In this method, high-resolution tokens inherit IDs\nfrom their corresponding thumbnail token while constraining the overexpansion\nof positional indices. Our experiments conducted within the LLaVA-Next\nframework demonstrate that ID-Align achieves significant improvements,\nincluding a 6.09% enhancement on MMBench's relation reasoning tasks and notable\ngains across multiple benchmarks. Our code is available at the following link:\nhttps://github.com/zooblastlbz/ID-Align."}
{"id": "2505.21472", "pdf": "https://arxiv.org/pdf/2505.21472", "abs": "https://arxiv.org/abs/2505.21472", "authors": ["Mehrdad Fazli", "Bowen Wei", "Ziwei Zhu"], "title": "Mitigating Hallucination in Large Vision-Language Models via Adaptive Attention Calibration", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Large vision-language models (LVLMs) achieve impressive performance on\nmultimodal tasks but often suffer from hallucination, and confidently describe\nobjects or attributes not present in the image. Current inference-time\ninterventions, while training-free, struggle to maintain accuracy in open-ended\nand long-form generation scenarios. We introduce the Confidence-Aware Attention\nCalibration (CAAC) framework to address this challenge by targeting two key\nbiases: spatial perception bias, which distributes attention disproportionately\nacross image tokens, and modality bias, which shifts focus from visual to\ntextual inputs over time. CAAC employs a two-step approach: Visual-Token\nCalibration (VTC) to balance attention across visual tokens, and Adaptive\nAttention Re-Scaling (AAR) to reinforce visual grounding based on the model's\nconfidence. This confidence-driven adjustment ensures consistent visual\nalignment during generation. Experiments on CHAIR, AMBER, and POPE benchmarks\ndemonstrate that CAAC outperforms baselines, particularly in long-form\ngenerations, effectively reducing hallucination."}
{"id": "2505.21473", "pdf": "https://arxiv.org/pdf/2505.21473", "abs": "https://arxiv.org/abs/2505.21473", "authors": ["Yiheng Liu", "Liao Qu", "Huichao Zhang", "Xu Wang", "Yi Jiang", "Yiming Gao", "Hu Ye", "Xian Li", "Shuai Wang", "Daniel K. Du", "Shu Cheng", "Zehuan Yuan", "Xinglong Wu"], "title": "DetailFlow: 1D Coarse-to-Fine Autoregressive Image Generation via Next-Detail Prediction", "categories": ["cs.CV"], "comment": null, "summary": "This paper presents DetailFlow, a coarse-to-fine 1D autoregressive (AR) image\ngeneration method that models images through a novel next-detail prediction\nstrategy. By learning a resolution-aware token sequence supervised with\nprogressively degraded images, DetailFlow enables the generation process to\nstart from the global structure and incrementally refine details. This\ncoarse-to-fine 1D token sequence aligns well with the autoregressive inference\nmechanism, providing a more natural and efficient way for the AR model to\ngenerate complex visual content. Our compact 1D AR model achieves high-quality\nimage synthesis with significantly fewer tokens than previous approaches, i.e.\nVAR/VQGAN. We further propose a parallel inference mechanism with\nself-correction that accelerates generation speed by approximately 8x while\nreducing accumulation sampling error inherent in teacher-forcing supervision.\nOn the ImageNet 256x256 benchmark, our method achieves 2.96 gFID with 128\ntokens, outperforming VAR (3.3 FID) and FlexVAR (3.05 FID), which both require\n680 tokens in their AR models. Moreover, due to the significantly reduced token\ncount and parallel inference mechanism, our method runs nearly 2x faster\ninference speed compared to VAR and FlexVAR. Extensive experimental results\ndemonstrate DetailFlow's superior generation quality and efficiency compared to\nexisting state-of-the-art methods."}
{"id": "2505.21478", "pdf": "https://arxiv.org/pdf/2505.21478", "abs": "https://arxiv.org/abs/2505.21478", "authors": ["Uri Gadot", "Rinon Gal", "Yftah Ziser", "Gal Chechik", "Shie Mannor"], "title": "Policy Optimized Text-to-Image Pipeline Design", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Text-to-image generation has evolved beyond single monolithic models to\ncomplex multi-component pipelines. These combine fine-tuned generators,\nadapters, upscaling blocks and even editing steps, leading to significant\nimprovements in image quality. However, their effective design requires\nsubstantial expertise. Recent approaches have shown promise in automating this\nprocess through large language models (LLMs), but they suffer from two critical\nlimitations: extensive computational requirements from generating images with\nhundreds of predefined pipelines, and poor generalization beyond memorized\ntraining examples. We introduce a novel reinforcement learning-based framework\nthat addresses these inefficiencies. Our approach first trains an ensemble of\nreward models capable of predicting image quality scores directly from\nprompt-workflow combinations, eliminating the need for costly image generation\nduring training. We then implement a two-phase training strategy: initial\nworkflow vocabulary training followed by GRPO-based optimization that guides\nthe model toward higher-performing regions of the workflow space. Additionally,\nwe incorporate a classifier-free guidance based enhancement technique that\nextrapolates along the path between the initial and GRPO-tuned models, further\nimproving output quality. We validate our approach through a set of\ncomparisons, showing that it can successfully create new flows with greater\ndiversity and lead to superior image quality compared to existing baselines."}
{"id": "2505.21483", "pdf": "https://arxiv.org/pdf/2505.21483", "abs": "https://arxiv.org/abs/2505.21483", "authors": ["Kerui Ren", "Jiayang Bai", "Linning Xu", "Lihan Jiang", "Jiangmiao Pang", "Mulin Yu", "Bo Dai"], "title": "MV-CoLight: Efficient Object Compositing with Consistent Lighting and Shadow Generation", "categories": ["cs.CV"], "comment": null, "summary": "Object compositing offers significant promise for augmented reality (AR) and\nembodied intelligence applications. Existing approaches predominantly focus on\nsingle-image scenarios or intrinsic decomposition techniques, facing challenges\nwith multi-view consistency, complex scenes, and diverse lighting conditions.\nRecent inverse rendering advancements, such as 3D Gaussian and diffusion-based\nmethods, have enhanced consistency but are limited by scalability, heavy data\nrequirements, or prolonged reconstruction time per scene. To broaden its\napplicability, we introduce MV-CoLight, a two-stage framework for\nillumination-consistent object compositing in both 2D images and 3D scenes. Our\nnovel feed-forward architecture models lighting and shadows directly, avoiding\nthe iterative biases of diffusion-based methods. We employ a Hilbert\ncurve-based mapping to align 2D image inputs with 3D Gaussian scene\nrepresentations seamlessly. To facilitate training and evaluation, we further\nintroduce a large-scale 3D compositing dataset. Experiments demonstrate\nstate-of-the-art harmonized results across standard benchmarks and our dataset,\nas well as casually captured real-world scenes demonstrate the framework's\nrobustness and wide generalization."}
{"id": "2505.21488", "pdf": "https://arxiv.org/pdf/2505.21488", "abs": "https://arxiv.org/abs/2505.21488", "authors": ["Omer Dahary", "Yehonathan Cohen", "Or Patashnik", "Kfir Aberman", "Daniel Cohen-Or"], "title": "Be Decisive: Noise-Induced Layouts for Multi-Subject Generation", "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.LG"], "comment": "SIGGRAPH 2025. Project page: https://omer11a.github.io/be-decisive/", "summary": "Generating multiple distinct subjects remains a challenge for existing\ntext-to-image diffusion models. Complex prompts often lead to subject leakage,\ncausing inaccuracies in quantities, attributes, and visual features. Preventing\nleakage among subjects necessitates knowledge of each subject's spatial\nlocation. Recent methods provide these spatial locations via an external layout\ncontrol. However, enforcing such a prescribed layout often conflicts with the\ninnate layout dictated by the sampled initial noise, leading to misalignment\nwith the model's prior. In this work, we introduce a new approach that predicts\na spatial layout aligned with the prompt, derived from the initial noise, and\nrefines it throughout the denoising process. By relying on this noise-induced\nlayout, we avoid conflicts with externally imposed layouts and better preserve\nthe model's prior. Our method employs a small neural network to predict and\nrefine the evolving noise-induced layout at each denoising step, ensuring clear\nboundaries between subjects while maintaining consistency. Experimental results\nshow that this noise-aligned strategy achieves improved text-image alignment\nand more stable multi-subject generation compared to existing layout-guided\ntechniques, while preserving the rich diversity of the model's original\ndistribution."}
{"id": "2505.21491", "pdf": "https://arxiv.org/pdf/2505.21491", "abs": "https://arxiv.org/abs/2505.21491", "authors": ["Boyang Wang", "Xuweiyi Chen", "Matheus Gadelha", "Zezhou Cheng"], "title": "Frame In-N-Out: Unbounded Controllable Image-to-Video Generation", "categories": ["cs.CV"], "comment": null, "summary": "Controllability, temporal coherence, and detail synthesis remain the most\ncritical challenges in video generation. In this paper, we focus on a commonly\nused yet underexplored cinematic technique known as Frame In and Frame Out.\nSpecifically, starting from image-to-video generation, users can control the\nobjects in the image to naturally leave the scene or provide breaking new\nidentity references to enter the scene, guided by user-specified motion\ntrajectory. To support this task, we introduce a new dataset curated\nsemi-automatically, a comprehensive evaluation protocol targeting this setting,\nand an efficient identity-preserving motion-controllable video Diffusion\nTransformer architecture. Our evaluation shows that our proposed approach\nsignificantly outperforms existing baselines."}
{"id": "2505.21494", "pdf": "https://arxiv.org/pdf/2505.21494", "abs": "https://arxiv.org/abs/2505.21494", "authors": ["Xiaojun Jia", "Sensen Gao", "Simeng Qin", "Tianyu Pang", "Chao Du", "Yihao Huang", "Xinfeng Li", "Yiming Li", "Bo Li", "Yang Liu"], "title": "Adversarial Attacks against Closed-Source MLLMs via Feature Optimal Alignment", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal large language models (MLLMs) remain vulnerable to transferable\nadversarial examples. While existing methods typically achieve targeted attacks\nby aligning global features-such as CLIP's [CLS] token-between adversarial and\ntarget samples, they often overlook the rich local information encoded in patch\ntokens. This leads to suboptimal alignment and limited transferability,\nparticularly for closed-source models. To address this limitation, we propose a\ntargeted transferable adversarial attack method based on feature optimal\nalignment, called FOA-Attack, to improve adversarial transfer capability.\nSpecifically, at the global level, we introduce a global feature loss based on\ncosine similarity to align the coarse-grained features of adversarial samples\nwith those of target samples. At the local level, given the rich local\nrepresentations within Transformers, we leverage clustering techniques to\nextract compact local patterns to alleviate redundant local features. We then\nformulate local feature alignment between adversarial and target samples as an\noptimal transport (OT) problem and propose a local clustering optimal transport\nloss to refine fine-grained feature alignment. Additionally, we propose a\ndynamic ensemble model weighting strategy to adaptively balance the influence\nof multiple models during adversarial example generation, thereby further\nimproving transferability. Extensive experiments across various models\ndemonstrate the superiority of the proposed method, outperforming\nstate-of-the-art methods, especially in transferring to closed-source MLLMs.\nThe code is released at https://github.com/jiaxiaojunQAQ/FOA-Attack."}
{"id": "2505.21497", "pdf": "https://arxiv.org/pdf/2505.21497", "abs": "https://arxiv.org/abs/2505.21497", "authors": ["Wei Pang", "Kevin Qinghong Lin", "Xiangru Jian", "Xi He", "Philip Torr"], "title": "Paper2Poster: Towards Multimodal Poster Automation from Scientific Papers", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MA"], "comment": "Project Page: https://github.com/Paper2Poster/Paper2Poster", "summary": "Academic poster generation is a crucial yet challenging task in scientific\ncommunication, requiring the compression of long-context interleaved documents\ninto a single, visually coherent page. To address this challenge, we introduce\nthe first benchmark and metric suite for poster generation, which pairs recent\nconference papers with author-designed posters and evaluates outputs on\n(i)Visual Quality-semantic alignment with human posters, (ii)Textual\nCoherence-language fluency, (iii)Holistic Assessment-six fine-grained aesthetic\nand informational criteria scored by a VLM-as-judge, and notably\n(iv)PaperQuiz-the poster's ability to convey core paper content as measured by\nVLMs answering generated quizzes. Building on this benchmark, we propose\nPosterAgent, a top-down, visual-in-the-loop multi-agent pipeline: the (a)Parser\ndistills the paper into a structured asset library; the (b)Planner aligns\ntext-visual pairs into a binary-tree layout that preserves reading order and\nspatial balance; and the (c)Painter-Commenter loop refines each panel by\nexecuting rendering code and using VLM feedback to eliminate overflow and\nensure alignment. In our comprehensive evaluation, we find that GPT-4o\noutputs-though visually appealing at first glance-often exhibit noisy text and\npoor PaperQuiz scores, and we find that reader engagement is the primary\naesthetic bottleneck, as human-designed posters rely largely on visual\nsemantics to convey meaning. Our fully open-source variants (e.g. based on the\nQwen-2.5 series) outperform existing 4o-driven multi-agent systems across\nnearly all metrics, while using 87% fewer tokens. It transforms a 22-page paper\ninto a finalized yet editable .pptx poster - all for just $0.005. These\nfindings chart clear directions for the next generation of fully automated\nposter-generation models. The code and datasets are available at\nhttps://github.com/Paper2Poster/Paper2Poster."}
{"id": "2505.21500", "pdf": "https://arxiv.org/pdf/2505.21500", "abs": "https://arxiv.org/abs/2505.21500", "authors": ["Dingming Li", "Hongxing Li", "Zixuan Wang", "Yuchen Yan", "Hang Zhang", "Siqi Chen", "Guiyang Hou", "Shengpei Jiang", "Wenqi Zhang", "Yongliang Shen", "Weiming Lu", "Yueting Zhuang"], "title": "ViewSpatial-Bench: Evaluating Multi-perspective Spatial Localization in Vision-Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Project: https://zju-real.github.io/ViewSpatial-Page/", "summary": "Vision-language models (VLMs) have demonstrated remarkable capabilities in\nunderstanding and reasoning about visual content, but significant challenges\npersist in tasks requiring cross-viewpoint understanding and spatial reasoning.\nWe identify a critical limitation: current VLMs excel primarily at egocentric\nspatial reasoning (from the camera's perspective) but fail to generalize to\nallocentric viewpoints when required to adopt another entity's spatial frame of\nreference. We introduce ViewSpatial-Bench, the first comprehensive benchmark\ndesigned specifically for multi-viewpoint spatial localization recognition\nevaluation across five distinct task types, supported by an automated 3D\nannotation pipeline that generates precise directional labels. Comprehensive\nevaluation of diverse VLMs on ViewSpatial-Bench reveals a significant\nperformance disparity: models demonstrate reasonable performance on\ncamera-perspective tasks but exhibit reduced accuracy when reasoning from a\nhuman viewpoint. By fine-tuning VLMs on our multi-perspective spatial dataset,\nwe achieve an overall performance improvement of 46.24% across tasks,\nhighlighting the efficacy of our approach. Our work establishes a crucial\nbenchmark for spatial intelligence in embodied AI systems and provides\nempirical evidence that modeling 3D spatial relationships enhances VLMs'\ncorresponding spatial comprehension capabilities."}
{"id": "2505.21501", "pdf": "https://arxiv.org/pdf/2505.21501", "abs": "https://arxiv.org/abs/2505.21501", "authors": ["Yinjie Chen", "Zipeng Yan", "Chong Zhou", "Bo Dai", "Andrew F. Luo"], "title": "Vision Transformers with Self-Distilled Registers", "categories": ["cs.CV"], "comment": "27 pages, 14 figures", "summary": "Vision Transformers (ViTs) have emerged as the dominant architecture for\nvisual processing tasks, demonstrating excellent scalability with increased\ntraining data and model size. However, recent work has identified the emergence\nof artifact tokens in ViTs that are incongruous with the local semantics. These\nanomalous tokens degrade ViT performance in tasks that require fine-grained\nlocalization or structural coherence. An effective mitigation of this issue is\nto the addition of register tokens to ViTs, which implicitly \"absorb\" the\nartifact term during training. Given the availability of various large-scale\npre-trained ViTs, in this paper we aim at equipping them with such register\ntokens without the need of re-training them from scratch, which is infeasible\nconsidering their size. Specifically, we propose Post Hoc Registers (PH-Reg),\nan efficient self-distillation method that integrates registers into an\nexisting ViT without requiring additional labeled data and full retraining.\nPH-Reg initializes both teacher and student networks from the same pre-trained\nViT. The teacher remains frozen and unmodified, while the student is augmented\nwith randomly initialized register tokens. By applying test-time augmentation\nto the teacher's inputs, we generate denoised dense embeddings free of\nartifacts, which are then used to optimize only a small subset of unlocked\nstudent weights. We show that our approach can effectively reduce the number of\nartifact tokens, improving the segmentation and depth prediction of the student\nViT under zero-shot and linear probing."}
{"id": "2505.21502", "pdf": "https://arxiv.org/pdf/2505.21502", "abs": "https://arxiv.org/abs/2505.21502", "authors": ["Yipengjing Sun", "Chenyang Wang", "Shunyuan Zheng", "Zonglin Li", "Shengping Zhang", "Xiangyang Ji"], "title": "Generalizable and Relightable Gaussian Splatting for Human Novel View Synthesis", "categories": ["cs.CV"], "comment": "Project Webpage: https://sypj-98.github.io/grgs/", "summary": "We propose GRGS, a generalizable and relightable 3D Gaussian framework for\nhigh-fidelity human novel view synthesis under diverse lighting conditions.\nUnlike existing methods that rely on per-character optimization or ignore\nphysical constraints, GRGS adopts a feed-forward, fully supervised strategy\nthat projects geometry, material, and illumination cues from multi-view 2D\nobservations into 3D Gaussian representations. Specifically, to reconstruct\nlighting-invariant geometry, we introduce a Lighting-aware Geometry Refinement\n(LGR) module trained on synthetically relit data to predict accurate depth and\nsurface normals. Based on the high-quality geometry, a Physically Grounded\nNeural Rendering (PGNR) module is further proposed to integrate neural\nprediction with physics-based shading, supporting editable relighting with\nshadows and indirect illumination. Besides, we design a 2D-to-3D projection\ntraining scheme that leverages differentiable supervision from ambient\nocclusion, direct, and indirect lighting maps, which alleviates the\ncomputational cost of explicit ray tracing. Extensive experiments demonstrate\nthat GRGS achieves superior visual quality, geometric consistency, and\ngeneralization across characters and lighting conditions."}
{"id": "2505.20322", "pdf": "https://arxiv.org/pdf/2505.20322", "abs": "https://arxiv.org/abs/2505.20322", "authors": ["Mengru Wang", "Ziwen Xu", "Shengyu Mao", "Shumin Deng", "Zhaopeng Tu", "Huajun Chen", "Ningyu Zhang"], "title": "Beyond Prompt Engineering: Robust Behavior Control in LLMs via Steering Target Atoms", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.IR", "cs.LG"], "comment": null, "summary": "Precise control over language model generation is vital for ensuring both\nsafety and reliability. Although prompt engineering and steering are commonly\nused to intervene in model behaviors, the vast number of parameters in models\noften results in highly intertwined internal representations. This\ninterdependency can limit control precision and sometimes lead to unintended\nside effects. Recent research has explored the use of sparse autoencoders (SAE)\nto disentangle knowledge in high-dimensional spaces for steering. However,\nthese applications have been limited to toy tasks owing to the nontrivial issue\nof locating atomic knowledge components. In this paper, we propose Steering\nTarget Atoms (STA), a novel method that isolates and manipulates disentangled\nknowledge components to enhance safety. Comprehensive experiments demonstrate\nthe effectiveness of our approach. Further analysis reveals that steering\nexhibits superior robustness and flexibility, particularly in adversarial\nscenarios. We also apply the steering strategy to the large reasoning model,\nconfirming its effectiveness in precise reasoning control."}
{"id": "2505.20326", "pdf": "https://arxiv.org/pdf/2505.20326", "abs": "https://arxiv.org/abs/2505.20326", "authors": ["Avinash Madasu", "Vasudev Lal", "Phillip Howard"], "title": "Cultural Awareness in Vision-Language Models: A Cross-Country Exploration", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Vision-Language Models (VLMs) are increasingly deployed in diverse cultural\ncontexts, yet their internal biases remain poorly understood. In this work, we\npropose a novel framework to systematically evaluate how VLMs encode cultural\ndifferences and biases related to race, gender, and physical traits across\ncountries. We introduce three retrieval-based tasks: (1) Race to Country\nretrieval, which examines the association between individuals from specific\nracial groups (East Asian, White, Middle Eastern, Latino, South Asian, and\nBlack) and different countries; (2) Personal Traits to Country retrieval, where\nimages are paired with trait-based prompts (e.g., Smart, Honest, Criminal,\nViolent) to investigate potential stereotypical associations; and (3) Physical\nCharacteristics to Country retrieval, focusing on visual attributes like\nskinny, young, obese, and old to explore how physical appearances are\nculturally linked to nations. Our findings reveal persistent biases in VLMs,\nhighlighting how visual representations may inadvertently reinforce societal\nstereotypes."}
{"id": "2505.20353", "pdf": "https://arxiv.org/pdf/2505.20353", "abs": "https://arxiv.org/abs/2505.20353", "authors": ["Dong Liu", "Jiayi Zhang", "Yifan Li", "Yanxuan Yu", "Ben Lengerich", "Ying Nian Wu"], "title": "FastCache: Fast Caching for Diffusion Transformer Through Learnable Linear Approximation", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.MM", "cs.PF"], "comment": null, "summary": "Diffusion Transformers (DiT) are powerful generative models but remain\ncomputationally intensive due to their iterative structure and deep transformer\nstacks. To alleviate this inefficiency, we propose FastCache, a\nhidden-state-level caching and compression framework that accelerates DiT\ninference by exploiting redundancy within the model's internal representations.\nFastCache introduces a dual strategy: (1) a spatial-aware token selection\nmechanism that adaptively filters redundant tokens based on hidden state\nsaliency, and (2) a transformer-level cache that reuses latent activations\nacross timesteps when changes are statistically insignificant. These modules\nwork jointly to reduce unnecessary computation while preserving generation\nfidelity through learnable linear approximation. Theoretical analysis shows\nthat FastCache maintains bounded approximation error under a\nhypothesis-testing-based decision rule. Empirical evaluations across multiple\nDiT variants demonstrate substantial reductions in latency and memory usage,\nwith best generation output quality compared to other cache methods, as\nmeasured by FID and t-FID. Code implementation of FastCache is available on\nGitHub at https://github.com/NoakLiu/FastCache-xDiT."}
{"id": "2505.20423", "pdf": "https://arxiv.org/pdf/2505.20423", "abs": "https://arxiv.org/abs/2505.20423", "authors": ["Julio de la Torre-Vanegas", "Miguel Soriano-Garcia", "Israel Becerra", "Diego Mercado-Ravell"], "title": "Vision-Based Risk Aware Emergency Landing for UAVs in Complex Urban Environments", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "Landing safely in crowded urban environments remains an essential yet\nchallenging endeavor for Unmanned Aerial Vehicles (UAVs), especially in\nemergency situations. In this work, we propose a risk-aware approach that\nharnesses semantic segmentation to continuously evaluate potential hazards in\nthe drone's field of view. By using a specialized deep neural network to assign\npixel-level risk values and applying an algorithm based on risk maps, our\nmethod adaptively identifies a stable Safe Landing Zone (SLZ) despite moving\ncritical obstacles such as vehicles, people, etc., and other visual challenges\nlike shifting illumination. A control system then guides the UAV toward this\nlow-risk region, employing altitude-dependent safety thresholds and temporal\nlanding point stabilization to ensure robust descent trajectories. Experimental\nvalidation in diverse urban environments demonstrates the effectiveness of our\napproach, achieving over 90% landing success rates in very challenging real\nscenarios, showing significant improvements in various risk metrics. Our\nfindings suggest that risk-oriented vision methods can effectively help reduce\nthe risk of accidents in emergency landing situations, particularly in complex,\nunstructured, urban scenarios, densely populated with moving risky obstacles,\nwhile potentiating the true capabilities of UAVs in complex urban operations."}
{"id": "2505.20429", "pdf": "https://arxiv.org/pdf/2505.20429", "abs": "https://arxiv.org/abs/2505.20429", "authors": ["Shuhao Guan", "Moule Lin", "Cheng Xu", "Xinyi Liu", "Jinman Zhao", "Jiexin Fan", "Qi Xu", "Derek Greene"], "title": "PreP-OCR: A Complete Pipeline for Document Image Restoration and Enhanced OCR Accuracy", "categories": ["cs.CL", "cs.CV"], "comment": "ACL 2025 main", "summary": "This paper introduces PreP-OCR, a two-stage pipeline that combines document\nimage restoration with semantic-aware post-OCR correction to improve text\nextraction from degraded historical documents. Our key innovation lies in\njointly optimizing image clarity and linguistic consistency. First, we generate\nsynthetic image pairs with randomized text fonts, layouts, and degradations. An\nimage restoration model is trained on this synthetic data, using\nmulti-directional patch extraction and fusion to process large images. Second,\na ByT5 post-corrector, fine-tuned on synthetic historical text training pairs,\naddresses any remaining OCR errors. Detailed experiments on 13,831 pages of\nreal historical documents in English, French, and Spanish show that PreP-OCR\npipeline reduces character error rates by 63.9-70.3\\% compared to OCR on raw\nimages. Our pipeline demonstrates the potential of integrating image\nrestoration with linguistic error correction for digitizing historical\narchives."}
{"id": "2505.20431", "pdf": "https://arxiv.org/pdf/2505.20431", "abs": "https://arxiv.org/abs/2505.20431", "authors": ["Qimin Chen", "Yuezhi Yang", "Yifang Wang", "Vladimir G. Kim", "Siddhartha Chaudhuri", "Hao Zhang", "Zhiqin Chen"], "title": "ART-DECO: Arbitrary Text Guidance for 3D Detailizer Construction", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "We introduce a 3D detailizer, a neural model which can instantaneously (in\n<1s) transform a coarse 3D shape proxy into a high-quality asset with detailed\ngeometry and texture as guided by an input text prompt. Our model is trained\nusing the text prompt, which defines the shape class and characterizes the\nappearance and fine-grained style of the generated details. The coarse 3D\nproxy, which can be easily varied and adjusted (e.g., via user editing),\nprovides structure control over the final shape. Importantly, our detailizer is\nnot optimized for a single shape; it is the result of distilling a generative\nmodel, so that it can be reused, without retraining, to generate any number of\nshapes, with varied structures, whose local details all share a consistent\nstyle and appearance. Our detailizer training utilizes a pretrained multi-view\nimage diffusion model, with text conditioning, to distill the foundational\nknowledge therein into our detailizer via Score Distillation Sampling (SDS). To\nimprove SDS and enable our detailizer architecture to learn generalizable\nfeatures over complex structures, we train our model in two training stages to\ngenerate shapes with increasing structural complexity. Through extensive\nexperiments, we show that our method generates shapes of superior quality and\ndetails compared to existing text-to-3D models under varied structure control.\nOur detailizer can refine a coarse shape in less than a second, making it\npossible to interactively author and adjust 3D shapes. Furthermore, the\nuser-imposed structure control can lead to creative, and hence\nout-of-distribution, 3D asset generations that are beyond the current\ncapabilities of leading text-to-3D generative models. We demonstrate an\ninteractive 3D modeling workflow our method enables, and its strong\ngeneralizability over styles, structures, and object categories."}
{"id": "2505.20444", "pdf": "https://arxiv.org/pdf/2505.20444", "abs": "https://arxiv.org/abs/2505.20444", "authors": ["Haoran Li", "Yingjie Qin", "Baoyuan Ou", "Lai Xu", "Ruiwen Xu"], "title": "HoPE: Hybrid of Position Embedding for Length Generalization in Vision-Language Models", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Vision-Language Models (VLMs) have made significant progress in multimodal\ntasks. However, their performance often deteriorates in long-context scenarios,\nparticularly long videos. While Rotary Position Embedding (RoPE) has been\nwidely adopted for length generalization in Large Language Models (LLMs),\nextending vanilla RoPE to capture the intricate spatial-temporal dependencies\nin videos remains an unsolved challenge. Existing methods typically allocate\ndifferent frequencies within RoPE to encode 3D positional information. However,\nthese allocation strategies mainly rely on heuristics, lacking in-depth\ntheoretical analysis. In this paper, we first study how different allocation\nstrategies impact the long-context capabilities of VLMs. Our analysis reveals\nthat current multimodal RoPEs fail to reliably capture semantic similarities\nover extended contexts. To address this issue, we propose HoPE, a Hybrid of\nPosition Embedding designed to improve the long-context capabilities of VLMs.\nHoPE introduces a hybrid frequency allocation strategy for reliable semantic\nmodeling over arbitrarily long context, and a dynamic temporal scaling\nmechanism to facilitate robust learning and flexible inference across diverse\ncontext lengths. Extensive experiments across four video benchmarks on long\nvideo understanding and retrieval tasks demonstrate that HoPE consistently\noutperforms existing methods, confirming its effectiveness. Code is available\nat https://github.com/hrlics/HoPE."}
{"id": "2505.20473", "pdf": "https://arxiv.org/pdf/2505.20473", "abs": "https://arxiv.org/abs/2505.20473", "authors": ["Selena Ling", "Merlin Nimier-David", "Alec Jacobson", "Nicholas Sharp"], "title": "Stochastic Preconditioning for Neural Field Optimization", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": "15 pages, 11 figures, SIGGRAPH 2025 (Journal track)", "summary": "Neural fields are a highly effective representation across visual computing.\nThis work observes that fitting these fields is greatly improved by\nincorporating spatial stochasticity during training, and that this simple\ntechnique can replace or even outperform custom-designed hierarchies and\nfrequency space constructions. The approach is formalized as implicitly\noperating on a blurred version of the field, evaluated in-expectation by\nsampling with Gaussian-distributed offsets. Querying the blurred field during\noptimization greatly improves convergence and robustness, akin to the role of\npreconditioners in numerical linear algebra. This implicit, sampling-based\nperspective fits naturally into the neural field paradigm, comes at no\nadditional cost, and is extremely simple to implement. We describe the basic\ntheory of this technique, including details such as handling boundary\nconditions, and extending to a spatially-varying blur. Experiments demonstrate\nthis approach on representations including coordinate MLPs, neural hashgrids,\ntriplanes, and more, across tasks including surface reconstruction and radiance\nfields. In settings where custom-designed hierarchies have already been\ndeveloped, stochastic preconditioning nearly matches or improves their\nperformance with a simple and unified approach; in settings without existing\nhierarchies it provides an immediate boost to quality and robustness."}
{"id": "2505.20485", "pdf": "https://arxiv.org/pdf/2505.20485", "abs": "https://arxiv.org/abs/2505.20485", "authors": ["Abhijit Chunduru", "Majid Morafah", "Mahdi Morafah", "Vishnu Pandi Chellapandi", "Ang Li"], "title": "Avoid Forgetting by Preserving Global Knowledge Gradients in Federated Learning with Non-IID Data", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.DC", "cs.PF"], "comment": null, "summary": "The inevitable presence of data heterogeneity has made federated learning\nvery challenging. There are numerous methods to deal with this issue, such as\nlocal regularization, better model fusion techniques, and data sharing. Though\neffective, they lack a deep understanding of how data heterogeneity can affect\nthe global decision boundary. In this paper, we bridge this gap by performing\nan experimental analysis of the learned decision boundary using a toy example.\nOur observations are surprising: (1) we find that the existing methods suffer\nfrom forgetting and clients forget the global decision boundary and only learn\nthe perfect local one, and (2) this happens regardless of the initial weights,\nand clients forget the global decision boundary even starting from pre-trained\noptimal weights. In this paper, we present FedProj, a federated learning\nframework that robustly learns the global decision boundary and avoids its\nforgetting during local training. To achieve better ensemble knowledge fusion,\nwe design a novel server-side ensemble knowledge transfer loss to further\ncalibrate the learned global decision boundary. To alleviate the issue of\nlearned global decision boundary forgetting, we further propose leveraging an\nepisodic memory of average ensemble logits on a public unlabeled dataset to\nregulate the gradient updates at each step of local training. Experimental\nresults demonstrate that FedProj outperforms state-of-the-art methods by a\nlarge margin."}
{"id": "2505.20503", "pdf": "https://arxiv.org/pdf/2505.20503", "abs": "https://arxiv.org/abs/2505.20503", "authors": ["Matthew Lisondra", "Beno Benhabib", "Goldie Nejat"], "title": "Embodied AI with Foundation Models for Mobile Service Robots: A Systematic Review", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV", "cs.LG"], "comment": null, "summary": "Rapid advancements in foundation models, including Large Language Models,\nVision-Language Models, Multimodal Large Language Models, and\nVision-Language-Action Models have opened new avenues for embodied AI in mobile\nservice robotics. By combining foundation models with the principles of\nembodied AI, where intelligent systems perceive, reason, and act through\nphysical interactions, robots can improve understanding, adapt to, and execute\ncomplex tasks in dynamic real-world environments. However, embodied AI in\nmobile service robots continues to face key challenges, including multimodal\nsensor fusion, real-time decision-making under uncertainty, task\ngeneralization, and effective human-robot interactions (HRI). In this paper, we\npresent the first systematic review of the integration of foundation models in\nmobile service robotics, identifying key open challenges in embodied AI and\nexamining how foundation models can address them. Namely, we explore the role\nof such models in enabling real-time sensor fusion, language-conditioned\ncontrol, and adaptive task execution. Furthermore, we discuss real-world\napplications in the domestic assistance, healthcare, and service automation\nsectors, demonstrating the transformative impact of foundation models on\nservice robotics. We also include potential future research directions,\nemphasizing the need for predictive scaling laws, autonomous long-term\nadaptation, and cross-embodiment generalization to enable scalable, efficient,\nand robust deployment of foundation models in human-centric robotic systems."}
{"id": "2505.20563", "pdf": "https://arxiv.org/pdf/2505.20563", "abs": "https://arxiv.org/abs/2505.20563", "authors": ["Jingjing Liu", "Xiansen Ju", "Xianchao Xiu", "Wanquan Liu"], "title": "Bi-Level Unsupervised Feature Selection", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Unsupervised feature selection (UFS) is an important task in data\nengineering. However, most UFS methods construct models from a single\nperspective and often fail to simultaneously evaluate feature importance and\npreserve their inherent data structure, thus limiting their performance. To\naddress this challenge, we propose a novel bi-level unsupervised feature\nselection (BLUFS) method, including a clustering level and a feature level.\nSpecifically, at the clustering level, spectral clustering is used to generate\npseudo-labels for representing the data structure, while a continuous linear\nregression model is developed to learn the projection matrix. At the feature\nlevel, the $\\ell_{2,0}$-norm constraint is imposed on the projection matrix for\nmore effectively selecting features. To the best of our knowledge, this is the\nfirst work to combine a bi-level framework with the $\\ell_{2,0}$-norm. To solve\nthe proposed bi-level model, we design an efficient proximal alternating\nminimization (PAM) algorithm, whose subproblems either have explicit solutions\nor can be computed by fast solvers. Furthermore, we establish the convergence\nresult and computational complexity. Finally, extensive experiments on two\nsynthetic datasets and eight real datasets demonstrate the superiority of BLUFS\nin clustering and classification tasks."}
{"id": "2505.20638", "pdf": "https://arxiv.org/pdf/2505.20638", "abs": "https://arxiv.org/abs/2505.20638", "authors": ["Wenhao You", "Xingjian Diao", "Chunhui Zhang", "Keyi Kong", "Weiyi Wu", "Zhongyu Ouyang", "Chiyu Ma", "Tingxuan Wu", "Noah Wei", "Zong Ke", "Ming Cheng", "Soroush Vosoughi", "Jiang Gui"], "title": "Music's Multimodal Complexity in AVQA: Why We Need More than General Multimodal LLMs", "categories": ["cs.SD", "cs.CV", "cs.MM", "eess.AS"], "comment": null, "summary": "While recent Multimodal Large Language Models exhibit impressive capabilities\nfor general multimodal tasks, specialized domains like music necessitate\ntailored approaches. Music Audio-Visual Question Answering (Music AVQA)\nparticularly underscores this, presenting unique challenges with its\ncontinuous, densely layered audio-visual content, intricate temporal dynamics,\nand the critical need for domain-specific knowledge. Through a systematic\nanalysis of Music AVQA datasets and methods, this position paper identifies\nthat specialized input processing, architectures incorporating dedicated\nspatial-temporal designs, and music-specific modeling strategies are critical\nfor success in this domain. Our study provides valuable insights for\nresearchers by highlighting effective design patterns empirically linked to\nstrong performance, proposing concrete future directions for incorporating\nmusical priors, and aiming to establish a robust foundation for advancing\nmultimodal musical understanding. This work is intended to inspire broader\nattention and further research, supported by a continuously updated anonymous\nGitHub repository of relevant papers:\nhttps://github.com/xid32/Survey4MusicAVQA."}
{"id": "2505.20688", "pdf": "https://arxiv.org/pdf/2505.20688", "abs": "https://arxiv.org/abs/2505.20688", "authors": ["Taehyo Kim", "Qiran Jia", "Mony J. de Leon", "Hai Shu"], "title": "A False Discovery Rate Control Method Using a Fully Connected Hidden Markov Random Field for Neuroimaging Data", "categories": ["stat.ML", "cs.CV", "cs.LG", "stat.ME"], "comment": null, "summary": "False discovery rate (FDR) control methods are essential for voxel-wise\nmultiple testing in neuroimaging data analysis, where hundreds of thousands or\neven millions of tests are conducted to detect brain regions associated with\ndisease-related changes. Classical FDR control methods (e.g., BH, q-value, and\nLocalFDR) assume independence among tests and often lead to high false\nnon-discovery rates (FNR). Although various spatial FDR control methods have\nbeen developed to improve power, they still fall short in jointly addressing\nthree major challenges in neuroimaging applications: capturing complex spatial\ndependencies, maintaining low variability in both false discovery proportion\n(FDP) and false non-discovery proportion (FNP) across replications, and\nachieving computational scalability for high-resolution data. To address these\nchallenges, we propose fcHMRF-LIS, a powerful, stable, and scalable spatial FDR\ncontrol method for voxel-wise multiple testing. It integrates the local index\nof significance (LIS)-based testing procedure with a novel fully connected\nhidden Markov random field (fcHMRF) designed to model complex spatial\nstructures using a parsimonious parameterization. We develop an efficient\nexpectation-maximization algorithm incorporating mean-field approximation, the\nConditional Random Fields as Recurrent Neural Networks (CRF-RNN) technique, and\npermutohedral lattice filtering, reducing the computational complexity from\nquadratic to linear in the number of tests. Extensive simulations demonstrate\nthat fcHMRF-LIS achieves accurate FDR control, lower FNR, reduced variability\nin FDP and FNP, and a higher number of true positives compared to existing\nmethods. Applied to an FDG-PET dataset from the Alzheimer's Disease\nNeuroimaging Initiative, fcHMRF-LIS identifies neurobiologically relevant brain\nregions and offers notable advantages in computational efficiency."}
{"id": "2505.20739", "pdf": "https://arxiv.org/pdf/2505.20739", "abs": "https://arxiv.org/abs/2505.20739", "authors": ["Kunpeng Zhao", "Asahi Miyazaki", "Tsuyoshi Okita"], "title": "Detecting Informative Channels: ActionFormer", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Human Activity Recognition (HAR) has recently witnessed advancements with\nTransformer-based models. Especially, ActionFormer shows us a new perspectives\nfor HAR in the sense that this approach gives us additional outputs which\ndetect the border of the activities as well as the activity labels.\nActionFormer was originally proposed with its input as image/video. However,\nthis was converted to with its input as sensor signals as well. We analyze this\nextensively in terms of deep learning architectures. Based on the report of\nhigh temporal dynamics which limits the model's ability to capture subtle\nchanges effectively and of the interdependencies between the spatial and\ntemporal features. We propose the modified ActionFormer which will decrease\nthese defects for sensor signals. The key to our approach lies in accordance\nwith the Sequence-and-Excitation strategy to minimize the increase in\nadditional parameters and opt for the swish activation function to retain the\ninformation about direction in the negative range. Experiments on the WEAR\ndataset show that our method achieves substantial improvement of a 16.01\\% in\nterms of average mAP for inertial data."}
{"id": "2505.20746", "pdf": "https://arxiv.org/pdf/2505.20746", "abs": "https://arxiv.org/abs/2505.20746", "authors": ["Nikola Andrejic", "Milica Spasic", "Igor Mihajlovic", "Petra Milosavljevic", "Djordje Pavlovic", "Filip Milisavljevic", "Uros Milivojevic", "Danilo Delibasic", "Ivana Mikic", "Sinisa Todorovic"], "title": "Unpaired Image-to-Image Translation for Segmentation and Signal Unmixing", "categories": ["eess.IV", "cs.CV"], "comment": "submitted to NeurIPs 2025", "summary": "This work introduces Ui2i, a novel model for unpaired image-to-image\ntranslation, trained on content-wise unpaired datasets to enable style transfer\nacross domains while preserving content. Building on CycleGAN, Ui2i\nincorporates key modifications to better disentangle content and style\nfeatures, and preserve content integrity. Specifically, Ui2i employs\nU-Net-based generators with skip connections to propagate localized shallow\nfeatures deep into the generator. Ui2i removes feature-based normalization\nlayers from all modules and replaces them with approximate bidirectional\nspectral normalization -- a parameter-based alternative that enhances training\nstability. To further support content preservation, channel and spatial\nattention mechanisms are integrated into the generators. Training is\nfacilitated through image scale augmentation. Evaluation on two biomedical\ntasks -- domain adaptation for nuclear segmentation in immunohistochemistry\n(IHC) images and unmixing of biological structures superimposed in\nsingle-channel immunofluorescence (IF) images -- demonstrates Ui2i's ability to\npreserve content fidelity in settings that demand more accurate structural\npreservation than typical translation tasks. To the best of our knowledge, Ui2i\nis the first approach capable of separating superimposed signals in IF images\nusing real, unpaired training data."}
{"id": "2505.20755", "pdf": "https://arxiv.org/pdf/2505.20755", "abs": "https://arxiv.org/abs/2505.20755", "authors": ["Yifei Wang", "Weimin Bai", "Colin Zhang", "Debing Zhang", "Weijian Luo", "He Sun"], "title": "Uni-Instruct: One-step Diffusion Model through Unified Diffusion Divergence Instruction", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "In this paper, we unify more than 10 existing one-step diffusion distillation\napproaches, such as Diff-Instruct, DMD, SIM, SiD, $f$-distill, etc, inside a\ntheory-driven framework which we name the \\textbf{\\emph{Uni-Instruct}}.\nUni-Instruct is motivated by our proposed diffusion expansion theory of the\n$f$-divergence family. Then we introduce key theories that overcome the\nintractability issue of the original expanded $f$-divergence, resulting in an\nequivalent yet tractable loss that effectively trains one-step diffusion models\nby minimizing the expanded $f$-divergence family. The novel unification\nintroduced by Uni-Instruct not only offers new theoretical contributions that\nhelp understand existing approaches from a high-level perspective but also\nleads to state-of-the-art one-step diffusion generation performances. On the\nCIFAR10 generation benchmark, Uni-Instruct achieves record-breaking Frechet\nInception Distance (FID) values of \\textbf{\\emph{1.46}} for unconditional\ngeneration and \\textbf{\\emph{1.38}} for conditional generation. On the\nImageNet-$64\\times 64$ generation benchmark, Uni-Instruct achieves a new SoTA\none-step generation FID of \\textbf{\\emph{1.02}}, which outperforms its 79-step\nteacher diffusion with a significant improvement margin of 1.33 (1.02 vs 2.35).\nWe also apply Uni-Instruct on broader tasks like text-to-3D generation. For\ntext-to-3D generation, Uni-Instruct gives decent results, which slightly\noutperforms previous methods, such as SDS and VSD, in terms of both generation\nquality and diversity. Both the solid theoretical and empirical contributions\nof Uni-Instruct will potentially help future studies on one-step diffusion\ndistillation and knowledge transferring of diffusion models."}
{"id": "2505.20802", "pdf": "https://arxiv.org/pdf/2505.20802", "abs": "https://arxiv.org/abs/2505.20802", "authors": ["Hemanth Saratchandran", "Damien Teney", "Simon Lucey"], "title": "Leaner Transformers: More Heads, Less Depth", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Transformers have reshaped machine learning by utilizing attention mechanisms\nto capture complex patterns in large datasets, leading to significant\nimprovements in performance. This success has contributed to the belief that\n\"bigger means better\", leading to ever-increasing model sizes. This paper\nchallenge this ideology by showing that many existing transformers might be\nunnecessarily oversized. We discover a theoretical principle that redefines the\nrole of multi-head attention. An important benefit of the multiple heads is in\nimproving the conditioning of the attention block. We exploit this theoretical\ninsight and redesign popular architectures with an increased number of heads.\nThe improvement in the conditioning proves so significant in practice that\nmodel depth can be decreased, reducing the parameter count by up to 30-50%\nwhile maintaining accuracy. We obtain consistent benefits across a variety of\ntransformer-based architectures of various scales, on tasks in computer vision\n(ImageNet-1k) as well as language and sequence modeling (GLUE benchmark,\nTinyStories, and the Long-Range Arena benchmark)."}
{"id": "2505.20810", "pdf": "https://arxiv.org/pdf/2505.20810", "abs": "https://arxiv.org/abs/2505.20810", "authors": ["Tariq M Khan", "Toufique Ahmed Soomro", "Imran Razzak"], "title": "The Role of AI in Early Detection of Life-Threatening Diseases: A Retinal Imaging Perspective", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Retinal imaging has emerged as a powerful, non-invasive modality for\ndetecting and quantifying biomarkers of systemic diseases-ranging from diabetes\nand hypertension to Alzheimer's disease and cardiovascular disorders but\ncurrent insights remain dispersed across platforms and specialties. Recent\ntechnological advances in optical coherence tomography (OCT/OCTA) and adaptive\noptics (AO) now deliver ultra-high-resolution scans (down to 5 {\\mu}m ) with\nsuperior contrast and spatial integration, allowing early identification of\nmicrovascular abnormalities and neurodegenerative changes. At the same time,\nAI-driven and machine learning (ML) algorithms have revolutionized the analysis\nof large-scale retinal datasets, increasing sensitivity and specificity; for\nexample, deep learning models achieve > 90 \\% sensitivity for diabetic\nretinopathy and AUC = 0.89 for the prediction of cardiovascular risk from\nfundus photographs. The proliferation of mobile health technologies and\ntelemedicine platforms further extends access, reduces costs, and facilitates\ncommunity-based screening and longitudinal monitoring. Despite these\nbreakthroughs, translation into routine practice is hindered by heterogeneous\nimaging protocols, limited external validation of AI models, and integration\nchallenges within clinical workflows. In this review, we systematically\nsynthesize the latest OCT/OCT and AO developments, AI/ML approaches, and\nmHealth/Tele-ophthalmology initiatives and quantify their diagnostic\nperformance across disease domains. Finally, we propose a roadmap for\nmulticenter protocol standardization, prospective validation trials, and\nseamless incorporation of retinal screening into primary and specialty care\npathways-paving the way for precision prevention, early intervention, and\nongoing treatment of life-threatening systemic diseases."}
{"id": "2505.20814", "pdf": "https://arxiv.org/pdf/2505.20814", "abs": "https://arxiv.org/abs/2505.20814", "authors": ["Yiqi Huang", "Travis Davies", "Jiahuan Yan", "Jiankai Sun", "Xiang Chen", "Luhui Hu"], "title": "Spatial RoboGrasp: Generalized Robotic Grasping Control Policy", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Achieving generalizable and precise robotic manipulation across diverse\nenvironments remains a critical challenge, largely due to limitations in\nspatial perception. While prior imitation-learning approaches have made\nprogress, their reliance on raw RGB inputs and handcrafted features often leads\nto overfitting and poor 3D reasoning under varied lighting, occlusion, and\nobject conditions. In this paper, we propose a unified framework that couples\nrobust multimodal perception with reliable grasp prediction. Our architecture\nfuses domain-randomized augmentation, monocular depth estimation, and a\ndepth-aware 6-DoF Grasp Prompt into a single spatial representation for\ndownstream action planning. Conditioned on this encoding and a high-level task\nprompt, our diffusion-based policy yields precise action sequences, achieving\nup to 40% improvement in grasp success and 45% higher task success rates under\nenvironmental variation. These results demonstrate that spatially grounded\nperception, paired with diffusion-based imitation learning, offers a scalable\nand robust solution for general-purpose robotic grasping."}
{"id": "2505.20902", "pdf": "https://arxiv.org/pdf/2505.20902", "abs": "https://arxiv.org/abs/2505.20902", "authors": ["Ruiying Li", "Bin Pan", "Lan Ma", "Xia Xu", "Zhenwei Shi"], "title": "Multitemporal Latent Dynamical Framework for Hyperspectral Images Unmixing", "categories": ["eess.IV", "cs.CV", "68T07", "I.4.10"], "comment": "11 Pages,8 figures", "summary": "Multitemporal hyperspectral unmixing can capture dynamical evolution of\nmaterials. Despite its capability, current methods emphasize variability of\nendmembers while neglecting dynamics of abundances, which motivates our\nadoption of neural ordinary differential equations to model abundances\ntemporally. However, this motivation is hindered by two challenges: the\ninherent complexity in defining, modeling and solving problem, and the absence\nof theoretical support. To address above challenges, in this paper, we propose\na multitemporal latent dynamical (MiLD) unmixing framework by capturing\ndynamical evolution of materials with theoretical validation. For addressing\nmultitemporal hyperspectral unmixing, MiLD consists of problem definition,\nmathematical modeling, solution algorithm and theoretical support. We formulate\nmultitemporal unmixing problem definition by conducting ordinary differential\nequations and developing latent variables. We transfer multitemporal unmixing\nto mathematical model by dynamical discretization approaches, which describe\nthe discreteness of observed sequence images with mathematical expansions. We\npropose algorithm to solve problem and capture dynamics of materials, which\napproximates abundance evolution by neural networks. Furthermore, we provide\ntheoretical support by validating the crucial properties, which verifies\nconsistency, convergence and stability theorems. The major contributions of\nMiLD include defining problem by ordinary differential equations, modeling\nproblem by dynamical discretization approach, solving problem by multitemporal\nunmixing algorithm, and presenting theoretical support. Our experiments on both\nsynthetic and real datasets have validated the utility of our work"}
{"id": "2505.20962", "pdf": "https://arxiv.org/pdf/2505.20962", "abs": "https://arxiv.org/abs/2505.20962", "authors": ["Nikos Giannakakis", "Argyris Manetas", "Panagiotis P. Filntisis", "Petros Maragos", "George Retsinas"], "title": "Object-Centric Action-Enhanced Representations for Robot Visuo-Motor Policy Learning", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Learning visual representations from observing actions to benefit robot\nvisuo-motor policy generation is a promising direction that closely resembles\nhuman cognitive function and perception. Motivated by this, and further\ninspired by psychological theories suggesting that humans process scenes in an\nobject-based fashion, we propose an object-centric encoder that performs\nsemantic segmentation and visual representation generation in a coupled manner,\nunlike other works, which treat these as separate processes. To achieve this,\nwe leverage the Slot Attention mechanism and use the SOLV model, pretrained in\nlarge out-of-domain datasets, to bootstrap fine-tuning on human action video\ndata. Through simulated robotic tasks, we demonstrate that visual\nrepresentations can enhance reinforcement and imitation learning training,\nhighlighting the effectiveness of our integrated approach for semantic\nsegmentation and encoding. Furthermore, we show that exploiting models\npretrained on out-of-domain datasets can benefit this process, and that\nfine-tuning on datasets depicting human actions -- although still out-of-domain\n-- , can significantly improve performance due to close alignment with robotic\ntasks. These findings show the capability to reduce reliance on annotated or\nrobot-specific action datasets and the potential to build on existing visual\nencoders to accelerate training and improve generalizability."}
{"id": "2505.20984", "pdf": "https://arxiv.org/pdf/2505.20984", "abs": "https://arxiv.org/abs/2505.20984", "authors": ["Minghao Han", "Weiyi You", "Jinhua Zhang", "Leheng Zhang", "Ce Zhu", "Shuhang Gu"], "title": "Generative Image Compression by Estimating Gradients of the Rate-variable Feature Distribution", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "While learned image compression (LIC) focuses on efficient data transmission,\ngenerative image compression (GIC) extends this framework by integrating\ngenerative modeling to produce photo-realistic reconstructed images. In this\npaper, we propose a novel diffusion-based generative modeling framework\ntailored for generative image compression. Unlike prior diffusion-based\napproaches that indirectly exploit diffusion modeling, we reinterpret the\ncompression process itself as a forward diffusion path governed by stochastic\ndifferential equations (SDEs). A reverse neural network is trained to\nreconstruct images by reversing the compression process directly, without\nrequiring Gaussian noise initialization. This approach achieves smooth rate\nadjustment and photo-realistic reconstructions with only a minimal number of\nsampling steps. Extensive experiments on benchmark datasets demonstrate that\nour method outperforms existing generative image compression approaches across\na range of metrics, including perceptual distortion, statistical fidelity, and\nno-reference quality assessments."}
{"id": "2505.21041", "pdf": "https://arxiv.org/pdf/2505.21041", "abs": "https://arxiv.org/abs/2505.21041", "authors": ["Weihang Liu", "Yuhui Zhong", "Yuke Li", "Xi Chen", "Jiadi Cui", "Honglong Zhang", "Lan Xu", "Xin Lou", "Yujiao Shi", "Jingyi Yu", "Yingliang Zhang"], "title": "CityGo: Lightweight Urban Modeling and Rendering with Proxy Buildings and Residual Gaussians", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Accurate and efficient modeling of large-scale urban scenes is critical for\napplications such as AR navigation, UAV based inspection, and smart city\ndigital twins. While aerial imagery offers broad coverage and complements\nlimitations of ground-based data, reconstructing city-scale environments from\nsuch views remains challenging due to occlusions, incomplete geometry, and high\nmemory demands. Recent advances like 3D Gaussian Splatting (3DGS) improve\nscalability and visual quality but remain limited by dense primitive usage,\nlong training times, and poor suit ability for edge devices. We propose CityGo,\na hybrid framework that combines textured proxy geometry with residual and\nsurrounding 3D Gaussians for lightweight, photorealistic rendering of urban\nscenes from aerial perspectives. Our approach first extracts compact building\nproxy meshes from MVS point clouds, then uses zero order SH Gaussians to\ngenerate occlusion-free textures via image-based rendering and back-projection.\nTo capture high-frequency details, we introduce residual Gaussians placed based\non proxy-photo discrepancies and guided by depth priors. Broader urban context\nis represented by surrounding Gaussians, with importance-aware downsampling\napplied to non-critical regions to reduce redundancy. A tailored optimization\nstrategy jointly refines proxy textures and Gaussian parameters, enabling\nreal-time rendering of complex urban scenes on mobile GPUs with significantly\nreduced training and memory requirements. Extensive experiments on real-world\naerial datasets demonstrate that our hybrid representation significantly\nreduces training time, achieving on average 1.4x speedup, while delivering\ncomparable visual fidelity to pure 3D Gaussian Splatting approaches.\nFurthermore, CityGo enables real-time rendering of large-scale urban scenes on\nmobile consumer GPUs, with substantially reduced memory usage and energy\nconsumption."}
{"id": "2505.21068", "pdf": "https://arxiv.org/pdf/2505.21068", "abs": "https://arxiv.org/abs/2505.21068", "authors": ["Anil Batra", "Laura Sevilla-Lara", "Marcus Rohrbach", "Frank Keller"], "title": "Predicting Implicit Arguments in Procedural Video Instructions", "categories": ["cs.CL", "cs.CV"], "comment": "ACL 2025 Main", "summary": "Procedural texts help AI enhance reasoning about context and action\nsequences. Transforming these into Semantic Role Labeling (SRL) improves\nunderstanding of individual steps by identifying predicate-argument structure\nlike {verb,what,where/with}. Procedural instructions are highly elliptic, for\ninstance, (i) add cucumber to the bowl and (ii) add sliced tomatoes, the second\nstep's where argument is inferred from the context, referring to where the\ncucumber was placed. Prior SRL benchmarks often miss implicit arguments,\nleading to incomplete understanding. To address this, we introduce\nImplicit-VidSRL, a dataset that necessitates inferring implicit and explicit\narguments from contextual information in multimodal cooking procedures. Our\nproposed dataset benchmarks multimodal models' contextual reasoning, requiring\nentity tracking through visual changes in recipes. We study recent multimodal\nLLMs and reveal that they struggle to predict implicit arguments of what and\nwhere/with from multi-modal procedural data given the verb. Lastly, we propose\niSRL-Qwen2-VL, which achieves a 17% relative improvement in F1-score for\nwhat-implicit and a 14.7% for where/with-implicit semantic roles over GPT-4o."}
{"id": "2505.21074", "pdf": "https://arxiv.org/pdf/2505.21074", "abs": "https://arxiv.org/abs/2505.21074", "authors": ["Yichuan Cao", "Yibo Miao", "Xiao-Shan Gao", "Yinpeng Dong"], "title": "Red-Teaming Text-to-Image Systems by Rule-based Preference Modeling", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.CV", "stat.ML"], "comment": null, "summary": "Text-to-image (T2I) models raise ethical and safety concerns due to their\npotential to generate inappropriate or harmful images. Evaluating these models'\nsecurity through red-teaming is vital, yet white-box approaches are limited by\ntheir need for internal access, complicating their use with closed-source\nmodels. Moreover, existing black-box methods often assume knowledge about the\nmodel's specific defense mechanisms, limiting their utility in real-world\ncommercial API scenarios. A significant challenge is how to evade unknown and\ndiverse defense mechanisms. To overcome this difficulty, we propose a novel\nRule-based Preference modeling Guided Red-Teaming (RPG-RT), which iteratively\nemploys LLM to modify prompts to query and leverages feedback from T2I systems\nfor fine-tuning the LLM. RPG-RT treats the feedback from each iteration as a\nprior, enabling the LLM to dynamically adapt to unknown defense mechanisms.\nGiven that the feedback is often labeled and coarse-grained, making it\ndifficult to utilize directly, we further propose rule-based preference\nmodeling, which employs a set of rules to evaluate desired or undesired\nfeedback, facilitating finer-grained control over the LLM's dynamic adaptation\nprocess. Extensive experiments on nineteen T2I systems with varied safety\nmechanisms, three online commercial API services, and T2V models verify the\nsuperiority and practicality of our approach."}
{"id": "2505.21135", "pdf": "https://arxiv.org/pdf/2505.21135", "abs": "https://arxiv.org/abs/2505.21135", "authors": ["Anqi Tang", "Youming Chen", "Shuchen Xue", "Zhaoqiang Liu"], "title": "Learning Single Index Models with Diffusion Priors", "categories": ["cs.LG", "cs.CV", "stat.ML"], "comment": "ICML 2025", "summary": "Diffusion models (DMs) have demonstrated remarkable ability to generate\ndiverse and high-quality images by efficiently modeling complex data\ndistributions. They have also been explored as powerful generative priors for\nsignal recovery, resulting in a substantial improvement in the quality of\nreconstructed signals. However, existing research on signal recovery with\ndiffusion models either focuses on specific reconstruction problems or is\nunable to handle nonlinear measurement models with discontinuous or unknown\nlink functions. In this work, we focus on using DMs to achieve accurate\nrecovery from semi-parametric single index models, which encompass a variety of\npopular nonlinear models that may have {\\em discontinuous} and {\\em unknown}\nlink functions. We propose an efficient reconstruction method that only\nrequires one round of unconditional sampling and (partial) inversion of DMs.\nTheoretical analysis on the effectiveness of the proposed methods has been\nestablished under appropriate conditions. We perform numerical experiments on\nimage datasets for different nonlinear measurement models. We observe that\ncompared to competing methods, our approach can yield more accurate\nreconstructions while utilizing significantly fewer neural function\nevaluations."}
{"id": "2505.21136", "pdf": "https://arxiv.org/pdf/2505.21136", "abs": "https://arxiv.org/abs/2505.21136", "authors": ["Jintao Zhang", "Xiaoming Xu", "Jia Wei", "Haofeng Huang", "Pengle Zhang", "Chendong Xiang", "Jun Zhu", "Jianfei Chen"], "title": "SageAttention2++: A More Efficient Implementation of SageAttention2", "categories": ["cs.LG", "cs.AI", "cs.AR", "cs.CV"], "comment": null, "summary": "The efficiency of attention is critical because its time complexity grows\nquadratically with sequence length. SageAttention2 addresses this by utilizing\nquantization to accelerate matrix multiplications (Matmul) in attention. To\nfurther accelerate SageAttention2, we propose to utilize the faster instruction\nof FP8 Matmul accumulated in FP16. The instruction is 2x faster than the FP8\nMatmul used in SageAttention2. Our experiments show that SageAttention2++\nachieves a 3.9x speedup over FlashAttention while maintaining the same\nattention accuracy as SageAttention2. This means SageAttention2++ effectively\naccelerates various models, including those for language, image, and video\ngeneration, with negligible end-to-end metrics loss. The code will be available\nat https://github.com/thu-ml/SageAttention."}
{"id": "2505.21146", "pdf": "https://arxiv.org/pdf/2505.21146", "abs": "https://arxiv.org/abs/2505.21146", "authors": ["Yang Zhao", "Yan Zhang", "Xubo Yang"], "title": "IKMo: Image-Keyframed Motion Generation with Trajectory-Pose Conditioned Motion Diffusion Model", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Existing human motion generation methods with trajectory and pose inputs\noperate global processing on both modalities, leading to suboptimal outputs. In\nthis paper, we propose IKMo, an image-keyframed motion generation method based\non the diffusion model with trajectory and pose being decoupled. The trajectory\nand pose inputs go through a two-stage conditioning framework. In the first\nstage, the dedicated optimization module is applied to refine inputs. In the\nsecond stage, trajectory and pose are encoded via a Trajectory Encoder and a\nPose Encoder in parallel. Then, motion with high spatial and semantic fidelity\nis guided by a motion ControlNet, which processes the fused trajectory and pose\ndata. Experiment results based on HumanML3D and KIT-ML datasets demonstrate\nthat the proposed method outperforms state-of-the-art on all metrics under\ntrajectory-keyframe constraints. In addition, MLLM-based agents are implemented\nto pre-process model inputs. Given texts and keyframe images from users, the\nagents extract motion descriptions, keyframe poses, and trajectories as the\noptimized inputs into the motion generation model. We conducts a user study\nwith 10 participants. The experiment results prove that the MLLM-based agents\npre-processing makes generated motion more in line with users' expectation. We\nbelieve that the proposed method improves both the fidelity and controllability\nof motion generation by the diffusion model."}
{"id": "2505.21173", "pdf": "https://arxiv.org/pdf/2505.21173", "abs": "https://arxiv.org/abs/2505.21173", "authors": ["Zhiwang Yu"], "title": "Topological Deep Learning for Speech Data", "categories": ["cs.LG", "cs.CV", "cs.SD", "eess.AS"], "comment": "21 pages, 15 figures", "summary": "Topological data analysis (TDA) offers novel mathematical tools for deep\nlearning. Inspired by Carlsson et al., this study designs topology-aware\nconvolutional kernels that significantly improve speech recognition networks.\nTheoretically, by investigating orthogonal group actions on kernels, we\nestablish a fiber-bundle decomposition of matrix spaces, enabling new filter\ngeneration methods. Practically, our proposed Orthogonal Feature (OF) layer\nachieves superior performance in phoneme recognition, particularly in low-noise\nscenarios, while demonstrating cross-domain adaptability. This work reveals\nTDA's potential in neural network optimization, opening new avenues for\nmathematics-deep learning interdisciplinary studies."}
{"id": "2505.21196", "pdf": "https://arxiv.org/pdf/2505.21196", "abs": "https://arxiv.org/abs/2505.21196", "authors": ["Ibrahim Shoer", "Engin Erzin"], "title": "Learning Annotation Consensus for Continuous Emotion Recognition", "categories": ["cs.HC", "cs.CV"], "comment": null, "summary": "In affective computing, datasets often contain multiple annotations from\ndifferent annotators, which may lack full agreement. Typically, these\nannotations are merged into a single gold standard label, potentially losing\nvaluable inter-rater variability. We propose a multi-annotator training\napproach for continuous emotion recognition (CER) that seeks a consensus across\nall annotators rather than relying on a single reference label. Our method\nemploys a consensus network to aggregate annotations into a unified\nrepresentation, guiding the main arousal-valence predictor to better reflect\ncollective inputs. Tested on the RECOLA and COGNIMUSE datasets, our approach\noutperforms traditional methods that unify annotations into a single label.\nThis underscores the benefits of fully leveraging multi-annotator data in\nemotion recognition and highlights its applicability across various fields\nwhere annotations are abundant yet inconsistent."}
{"id": "2505.21319", "pdf": "https://arxiv.org/pdf/2505.21319", "abs": "https://arxiv.org/abs/2505.21319", "authors": ["Biao Zhang", "Peter Wonka"], "title": "efunc: An Efficient Function Representation without Neural Networks", "categories": ["cs.GR", "cs.CV"], "comment": "Project website: https://efunc.github.io/efunc/", "summary": "Function fitting/approximation plays a fundamental role in computer graphics\nand other engineering applications. While recent advances have explored neural\nnetworks to address this task, these methods often rely on architectures with\nmany parameters, limiting their practical applicability. In contrast, we pursue\nhigh-quality function approximation using parameter-efficient representations\nthat eliminate the dependency on neural networks entirely. We first propose a\nnovel framework for continuous function modeling. Most existing works can be\nformulated using this framework. We then introduce a compact function\nrepresentation, which is based on polynomials interpolated using radial basis\nfunctions, bypassing both neural networks and complex/hierarchical data\nstructures. We also develop memory-efficient CUDA-optimized algorithms that\nreduce computational time and memory consumption to less than 10% compared to\nconventional automatic differentiation frameworks. Finally, we validate our\nrepresentation and optimization pipeline through extensive experiments on 3D\nsigned distance functions (SDFs). The proposed representation achieves\ncomparable or superior performance to state-of-the-art techniques (e.g.,\noctree/hash-grid techniques) with significantly fewer parameters."}
{"id": "2505.21327", "pdf": "https://arxiv.org/pdf/2505.21327", "abs": "https://arxiv.org/abs/2505.21327", "authors": ["Jiakang Yuan", "Tianshuo Peng", "Yilei Jiang", "Yiting Lu", "Renrui Zhang", "Kaituo Feng", "Chaoyou Fu", "Tao Chen", "Lei Bai", "Bo Zhang", "Xiangyu Yue"], "title": "MME-Reasoning: A Comprehensive Benchmark for Logical Reasoning in MLLMs", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "Logical reasoning is a fundamental aspect of human intelligence and an\nessential capability for multimodal large language models (MLLMs). Despite the\nsignificant advancement in multimodal reasoning, existing benchmarks fail to\ncomprehensively evaluate their reasoning abilities due to the lack of explicit\ncategorization for logical reasoning types and an unclear understanding of\nreasoning. To address these issues, we introduce MME-Reasoning, a comprehensive\nbenchmark designed to evaluate the reasoning ability of MLLMs, which covers all\nthree types of reasoning (i.e., inductive, deductive, and abductive) in its\nquestions. We carefully curate the data to ensure that each question\neffectively evaluates reasoning ability rather than perceptual skills or\nknowledge breadth, and extend the evaluation protocols to cover the evaluation\nof diverse questions. Our evaluation reveals substantial limitations of\nstate-of-the-art MLLMs when subjected to holistic assessments of logical\nreasoning capabilities. Even the most advanced MLLMs show limited performance\nin comprehensive logical reasoning, with notable performance imbalances across\nreasoning types. In addition, we conducted an in-depth analysis of approaches\nsuch as ``thinking mode'' and Rule-based RL, which are commonly believed to\nenhance reasoning abilities. These findings highlight the critical limitations\nand performance imbalances of current MLLMs in diverse logical reasoning\nscenarios, providing comprehensive and systematic insights into the\nunderstanding and evaluation of reasoning capabilities."}
{"id": "2505.21335", "pdf": "https://arxiv.org/pdf/2505.21335", "abs": "https://arxiv.org/abs/2505.21335", "authors": ["Takuhiro Kaneko"], "title": "Structure from Collision", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.LG", "cs.RO"], "comment": "Accepted to CVPR 2025 (Highlight). Project page:\n  https://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/sfc/", "summary": "Recent advancements in neural 3D representations, such as neural radiance\nfields (NeRF) and 3D Gaussian splatting (3DGS), have enabled the accurate\nestimation of 3D structures from multiview images. However, this capability is\nlimited to estimating the visible external structure, and identifying the\ninvisible internal structure hidden behind the surface is difficult. To\novercome this limitation, we address a new task called Structure from Collision\n(SfC), which aims to estimate the structure (including the invisible internal\nstructure) of an object from appearance changes during collision. To solve this\nproblem, we propose a novel model called SfC-NeRF that optimizes the invisible\ninternal structure of an object through a video sequence under physical,\nappearance (i.e., visible external structure)-preserving, and keyframe\nconstraints. In particular, to avoid falling into undesirable local optima\nowing to its ill-posed nature, we propose volume annealing; that is, searching\nfor global optima by repeatedly reducing and expanding the volume. Extensive\nexperiments on 115 objects involving diverse structures (i.e., various cavity\nshapes, locations, and sizes) and material properties revealed the properties\nof SfC and demonstrated the effectiveness of the proposed SfC-NeRF."}
{"id": "2505.21355", "pdf": "https://arxiv.org/pdf/2505.21355", "abs": "https://arxiv.org/abs/2505.21355", "authors": ["Muhammad Imran", "Wayne G. Brisbane", "Li-Ming Su", "Jason P. Joseph", "Wei Shao"], "title": "Prostate Cancer Screening with Artificial Intelligence-Enhanced Micro-Ultrasound: A Comparative Study with Traditional Methods", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Background and objective: Micro-ultrasound (micro-US) is a novel imaging\nmodality with diagnostic accuracy comparable to MRI for detecting clinically\nsignificant prostate cancer (csPCa). We investigated whether artificial\nintelligence (AI) interpretation of micro-US can outperform clinical screening\nmethods using PSA and digital rectal examination (DRE). Methods: We\nretrospectively studied 145 men who underwent micro-US guided biopsy (79 with\ncsPCa, 66 without). A self-supervised convolutional autoencoder was used to\nextract deep image features from 2D micro-US slices. Random forest classifiers\nwere trained using five-fold cross-validation to predict csPCa at the slice\nlevel. Patients were classified as csPCa-positive if 88 or more consecutive\nslices were predicted positive. Model performance was compared with a\nclassifier using PSA, DRE, prostate volume, and age. Key findings and\nlimitations: The AI-based micro-US model and clinical screening model achieved\nAUROCs of 0.871 and 0.753, respectively. At a fixed threshold, the micro-US\nmodel achieved 92.5% sensitivity and 68.1% specificity, while the clinical\nmodel showed 96.2% sensitivity but only 27.3% specificity. Limitations include\na retrospective single-center design and lack of external validation.\nConclusions and clinical implications: AI-interpreted micro-US improves\nspecificity while maintaining high sensitivity for csPCa detection. This method\nmay reduce unnecessary biopsies and serve as a low-cost alternative to\nPSA-based screening. Patient summary: We developed an AI system to analyze\nprostate micro-ultrasound images. It outperformed PSA and DRE in detecting\naggressive cancer and may help avoid unnecessary biopsies."}
{"id": "2505.21437", "pdf": "https://arxiv.org/pdf/2505.21437", "abs": "https://arxiv.org/abs/2505.21437", "authors": ["Huaijin Pi", "Zhi Cen", "Zhiyang Dou", "Taku Komura"], "title": "CoDA: Coordinated Diffusion Noise Optimization for Whole-Body Manipulation of Articulated Objects", "categories": ["cs.GR", "cs.CV", "cs.RO"], "comment": "Project page: https://phj128.github.io/page/CoDA/index.html", "summary": "Synthesizing whole-body manipulation of articulated objects, including body\nmotion, hand motion, and object motion, is a critical yet challenging task with\nbroad applications in virtual humans and robotics. The core challenges are\ntwofold. First, achieving realistic whole-body motion requires tight\ncoordination between the hands and the rest of the body, as their movements are\ninterdependent during manipulation. Second, articulated object manipulation\ntypically involves high degrees of freedom and demands higher precision, often\nrequiring the fingers to be placed at specific regions to actuate movable\nparts. To address these challenges, we propose a novel coordinated diffusion\nnoise optimization framework. Specifically, we perform noise-space optimization\nover three specialized diffusion models for the body, left hand, and right\nhand, each trained on its own motion dataset to improve generalization.\nCoordination naturally emerges through gradient flow along the human kinematic\nchain, allowing the global body posture to adapt in response to hand motion\nobjectives with high fidelity. To further enhance precision in hand-object\ninteraction, we adopt a unified representation based on basis point sets (BPS),\nwhere end-effector positions are encoded as distances to the same BPS used for\nobject geometry. This unified representation captures fine-grained spatial\nrelationships between the hand and articulated object parts, and the resulting\ntrajectories serve as targets to guide the optimization of diffusion noise,\nproducing highly accurate interaction motion. We conduct extensive experiments\ndemonstrating that our method outperforms existing approaches in motion quality\nand physical plausibility, and enables various capabilities such as object pose\ncontrol, simultaneous walking and manipulation, and whole-body generation from\nhand-only data."}
{"id": "2505.21445", "pdf": "https://arxiv.org/pdf/2505.21445", "abs": "https://arxiv.org/abs/2505.21445", "authors": ["Zhiqi Ai", "Meixuan Bao", "Zhiyong Chen", "Zhi Yang", "Xinnuo Li", "Shugong Xu"], "title": "VoxAging: Continuously Tracking Speaker Aging with a Large-Scale Longitudinal Dataset in English and Mandarin", "categories": ["cs.SD", "cs.AI", "cs.CV", "cs.MM"], "comment": "5 pages, 4 figures, Accepted by Interspeech 2025", "summary": "The performance of speaker verification systems is adversely affected by\nspeaker aging. However, due to challenges in data collection, particularly the\nlack of sustained and large-scale longitudinal data for individuals, research\non speaker aging remains difficult. In this paper, we present VoxAging, a\nlarge-scale longitudinal dataset collected from 293 speakers (226 English\nspeakers and 67 Mandarin speakers) over several years, with the longest time\nspan reaching 17 years (approximately 900 weeks). For each speaker, the data\nwere recorded at weekly intervals. We studied the phenomenon of speaker aging\nand its effects on advanced speaker verification systems, analyzed individual\nspeaker aging processes, and explored the impact of factors such as age group\nand gender on speaker aging research."}
{"id": "2505.21459", "pdf": "https://arxiv.org/pdf/2505.21459", "abs": "https://arxiv.org/abs/2505.21459", "authors": ["Xiangru Jian", "Wei Pang", "Zhengyuan Dong", "Chao Zhang", "M. Tamer Özsu"], "title": "LazyVLM: Neuro-Symbolic Approach to Video Analytics", "categories": ["cs.DB", "cs.AI", "cs.CV", "cs.IR", "cs.MM"], "comment": "5 pages, 2 figures, Working paper", "summary": "Current video analytics approaches face a fundamental trade-off between\nflexibility and efficiency. End-to-end Vision Language Models (VLMs) often\nstruggle with long-context processing and incur high computational costs, while\nneural-symbolic methods depend heavily on manual labeling and rigid rule\ndesign. In this paper, we introduce LazyVLM, a neuro-symbolic video analytics\nsystem that provides a user-friendly query interface similar to VLMs, while\naddressing their scalability limitation. LazyVLM enables users to effortlessly\ndrop in video data and specify complex multi-frame video queries using a\nsemi-structured text interface for video analytics. To address the scalability\nlimitations of VLMs, LazyVLM decomposes multi-frame video queries into\nfine-grained operations and offloads the bulk of the processing to efficient\nrelational query execution and vector similarity search. We demonstrate that\nLazyVLM provides a robust, efficient, and user-friendly solution for querying\nopen-domain video data at scale."}
{"id": "2505.21496", "pdf": "https://arxiv.org/pdf/2505.21496", "abs": "https://arxiv.org/abs/2505.21496", "authors": ["Han Xiao", "Guozhi Wang", "Yuxiang Chai", "Zimu Lu", "Weifeng Lin", "Hao He", "Lue Fan", "Liuyang Bian", "Rui Hu", "Liang Liu", "Shuai Ren", "Yafei Wen", "Xiaoxin Chen", "Aojun Zhou", "Hongsheng Li"], "title": "UI-Genie: A Self-Improving Approach for Iteratively Boosting MLLM-based Mobile GUI Agents", "categories": ["cs.CL", "cs.CV", "cs.LG"], "comment": "https://github.com/Euphoria16/UI-Genie", "summary": "In this paper, we introduce UI-Genie, a self-improving framework addressing\ntwo key challenges in GUI agents: verification of trajectory outcome is\nchallenging and high-quality training data are not scalable. These challenges\nare addressed by a reward model and a self-improving pipeline, respectively.\nThe reward model, UI-Genie-RM, features an image-text interleaved architecture\nthat efficiently pro- cesses historical context and unifies action-level and\ntask-level rewards. To sup- port the training of UI-Genie-RM, we develop\ndeliberately-designed data genera- tion strategies including rule-based\nverification, controlled trajectory corruption, and hard negative mining. To\naddress the second challenge, a self-improvement pipeline progressively expands\nsolvable complex GUI tasks by enhancing both the agent and reward models\nthrough reward-guided exploration and outcome verification in dynamic\nenvironments. For training the model, we generate UI- Genie-RM-517k and\nUI-Genie-Agent-16k, establishing the first reward-specific dataset for GUI\nagents while demonstrating high-quality synthetic trajectory gen- eration\nwithout manual annotation. Experimental results show that UI-Genie achieves\nstate-of-the-art performance across multiple GUI agent benchmarks with three\ngenerations of data-model self-improvement. We open-source our complete\nframework implementation and generated datasets to facilitate further research\nin https://github.com/Euphoria16/UI-Genie."}
