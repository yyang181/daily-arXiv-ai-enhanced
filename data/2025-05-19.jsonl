{"id": "2505.10575", "pdf": "https://arxiv.org/pdf/2505.10575", "abs": "https://arxiv.org/abs/2505.10575", "authors": ["Adnan Ahmad", "Bahareh Nakisa", "Mohammad Naim Rastgoo"], "title": "Robust Emotion Recognition via Bi-Level Self-Supervised Continual Learning", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Emotion recognition through physiological signals such as\nelectroencephalogram (EEG) has become an essential aspect of affective\ncomputing and provides an objective way to capture human emotions. However,\nphysiological data characterized by cross-subject variability and noisy labels\nhinder the performance of emotion recognition models. Existing domain\nadaptation and continual learning methods struggle to address these issues,\nespecially under realistic conditions where data is continuously streamed and\nunlabeled. To overcome these limitations, we propose a novel bi-level\nself-supervised continual learning framework, SSOCL, based on a dynamic memory\nbuffer. This bi-level architecture iteratively refines the dynamic buffer and\npseudo-label assignments to effectively retain representative samples, enabling\ngeneralization from continuous, unlabeled physiological data streams for\nemotion recognition. The assigned pseudo-labels are subsequently leveraged for\naccurate emotion prediction. Key components of the framework, including a fast\nadaptation module and a cluster-mapping module, enable robust learning and\neffective handling of evolving data streams. Experimental validation on two\nmainstream EEG tasks demonstrates the framework's ability to adapt to\ncontinuous data streams while maintaining strong generalization across\nsubjects, outperforming existing approaches."}
{"id": "2505.10579", "pdf": "https://arxiv.org/pdf/2505.10579", "abs": "https://arxiv.org/abs/2505.10579", "authors": ["Germani Elodie", "Selin Türk Ilayda", "Zeineddine Fatima", "Mourad Charbel", "Albarqouni Shadi"], "title": "Bias and Generalizability of Foundation Models across Datasets in Breast Mammography", "categories": ["cs.CV"], "comment": "Accepted at the International Conference on Medical Image Computing\n  and Computer-Assisted Intervention (MICCAI) 2025", "summary": "Over the past decades, computer-aided diagnosis tools for breast cancer have\nbeen developed to enhance screening procedures, yet their clinical adoption\nremains challenged by data variability and inherent biases. Although foundation\nmodels (FMs) have recently demonstrated impressive generalizability and\ntransfer learning capabilities by leveraging vast and diverse datasets, their\nperformance can be undermined by spurious correlations that arise from\nvariations in image quality, labeling uncertainty, and sensitive patient\nattributes. In this work, we explore the fairness and bias of FMs for breast\nmammography classification by leveraging a large pool of datasets from diverse\nsources-including data from underrepresented regions and an in-house dataset.\nOur extensive experiments show that while modality-specific pre-training of FMs\nenhances performance, classifiers trained on features from individual datasets\nfail to generalize across domains. Aggregating datasets improves overall\nperformance, yet does not fully mitigate biases, leading to significant\ndisparities across under-represented subgroups such as extreme breast densities\nand age groups. Furthermore, while domain-adaptation strategies can reduce\nthese disparities, they often incur a performance trade-off. In contrast,\nfairness-aware techniques yield more stable and equitable performance across\nsubgroups. These findings underscore the necessity of incorporating rigorous\nfairness evaluations and mitigation strategies into FM-based models to foster\ninclusive and generalizable AI."}
{"id": "2505.10583", "pdf": "https://arxiv.org/pdf/2505.10583", "abs": "https://arxiv.org/abs/2505.10583", "authors": ["Diogo Freitas", "Brigt Håvardstun", "Cèsar Ferri", "Darío Garigliotti", "Jan Arne Telle", "José Hernández-Orallo"], "title": "Relative Drawing Identification Complexity is Invariant to Modality in Vision-Language Models", "categories": ["cs.CV", "cs.CL"], "comment": "54 pages (42 pages of appendix)", "summary": "Large language models have become multimodal, and many of them are said to\nintegrate their modalities using common representations. If this were true, a\ndrawing of a car as an image, for instance, should map to the similar area in\nthe latent space as a textual description of the strokes that conform the\ndrawing. To explore this in a black-box access regime to these models, we\npropose the use of machine teaching, a theory that studies the minimal set of\nexamples a teacher needs to choose so that the learner captures the concept. In\nthis paper we evaluate the complexity of teaching visual-language models a\nsubset of objects in the Quick, Draw! dataset using two presentations: raw\nimages as bitmaps and trace coordinates in TikZ format. The results indicate\nthat image-based representations generally require fewer segments and achieve\nhigher accuracy than coordinate-based representations. But, surprisingly, the\nteaching size usually ranks concepts similarly across both modalities, even\nwhen controlling for (a human proxy of) concept priors, suggesting that the\nsimplicity of concepts may be an inherent property that transcends modality\nrepresentations."}
{"id": "2505.10584", "pdf": "https://arxiv.org/pdf/2505.10584", "abs": "https://arxiv.org/abs/2505.10584", "authors": ["Huafeng Shi", "Jianzhong Liang", "Rongchang Xie", "Xian Wu", "Cheng Chen", "Chang Liu"], "title": "Aquarius: A Family of Industry-Level Video Generation Models for Marketing Scenarios", "categories": ["cs.CV"], "comment": null, "summary": "This report introduces Aquarius, a family of industry-level video generation\nmodels for marketing scenarios designed for thousands-xPU clusters and models\nwith hundreds of billions of parameters. Leveraging efficient engineering\narchitecture and algorithmic innovation, Aquarius demonstrates exceptional\nperformance in high-fidelity, multi-aspect-ratio, and long-duration video\nsynthesis. By disclosing the framework's design details, we aim to demystify\nindustrial-scale video generation systems and catalyze advancements in the\ngenerative video community. The Aquarius framework consists of five components:\nDistributed Graph and Video Data Processing Pipeline: Manages tens of thousands\nof CPUs and thousands of xPUs via automated task distribution, enabling\nefficient video data processing. Additionally, we are about to open-source the\nentire data processing framework named \"Aquarius-Datapipe\". Model Architectures\nfor Different Scales: Include a Single-DiT architecture for 2B models and a\nMultimodal-DiT architecture for 13.4B models, supporting multi-aspect ratios,\nmulti-resolution, and multi-duration video generation. High-Performance\ninfrastructure designed for video generation model training: Incorporating\nhybrid parallelism and fine-grained memory optimization strategies, this\ninfrastructure achieves 36% MFU at large scale. Multi-xPU Parallel Inference\nAcceleration: Utilizes diffusion cache and attention optimization to achieve a\n2.35x inference speedup. Multiple marketing-scenarios applications: Including\nimage-to-video, text-to-video (avatar), video inpainting and video\npersonalization, among others. More downstream applications and\nmulti-dimensional evaluation metrics will be added in the upcoming version\nupdates."}
{"id": "2505.10585", "pdf": "https://arxiv.org/pdf/2505.10585", "abs": "https://arxiv.org/abs/2505.10585", "authors": ["Azim Akhtarshenas", "Ramin Toosi", "David López-Pérez", "Tohid Alizadeh", "Alireza Hosseini"], "title": "Efficient Malicious UAV Detection Using Autoencoder-TSMamba Integration", "categories": ["cs.CV", "cs.CR"], "comment": "12 pages, 6 figures and 3 tables, accepted in IbPRIA 2025,\n  https://www.ibpria.org/2025/?page=dates", "summary": "Malicious Unmanned Aerial Vehicles (UAVs) present a significant threat to\nnext-generation networks (NGNs), posing risks such as unauthorized\nsurveillance, data theft, and the delivery of hazardous materials. This paper\nproposes an integrated (AE)-classifier system to detect malicious UAVs. The\nproposed AE, based on a 4-layer Tri-orientated Spatial Mamba (TSMamba)\narchitecture, effectively captures complex spatial relationships crucial for\nidentifying malicious UAV activities. The first phase involves generating\nresidual values through the AE, which are subsequently processed by a\nResNet-based classifier. This classifier leverages the residual values to\nachieve lower complexity and higher accuracy. Our experiments demonstrate\nsignificant improvements in both binary and multi-class classification\nscenarios, achieving up to 99.8 % recall compared to 96.7 % in the benchmark.\nAdditionally, our method reduces computational complexity, making it more\nsuitable for large-scale deployment. These results highlight the robustness and\nscalability of our approach, offering an effective solution for malicious UAV\ndetection in NGN environments."}
{"id": "2505.10589", "pdf": "https://arxiv.org/pdf/2505.10589", "abs": "https://arxiv.org/abs/2505.10589", "authors": ["Kağan ÇETİN"], "title": "Super-Resolution Generative Adversarial Networks based Video Enhancement", "categories": ["cs.CV", "cs.AI", "eess.IV", "I.4.3"], "comment": null, "summary": "This study introduces an enhanced approach to video super-resolution by\nextending ordinary Single-Image Super-Resolution (SISR) Super-Resolution\nGenerative Adversarial Network (SRGAN) structure to handle spatio-temporal\ndata. While SRGAN has proven effective for single-image enhancement, its design\ndoes not account for the temporal continuity required in video processing. To\naddress this, a modified framework that incorporates 3D Non-Local Blocks is\nproposed, which is enabling the model to capture relationships across both\nspatial and temporal dimensions. An experimental training pipeline is\ndeveloped, based on patch-wise learning and advanced data degradation\ntechniques, to simulate real-world video conditions and learn from both local\nand global structures and details. This helps the model generalize better and\nmaintain stability across varying video content while maintaining the general\nstructure besides the pixel-wise correctness. Two model variants-one larger and\none more lightweight-are presented to explore the trade-offs between\nperformance and efficiency. The results demonstrate improved temporal\ncoherence, sharper textures, and fewer visual artifacts compared to traditional\nsingle-image methods. This work contributes to the development of practical,\nlearning-based solutions for video enhancement tasks, with potential\napplications in streaming, gaming, and digital restoration."}
{"id": "2505.10595", "pdf": "https://arxiv.org/pdf/2505.10595", "abs": "https://arxiv.org/abs/2505.10595", "authors": ["Xingye Cui", "Junhai Luo", "Jiakun Deng", "Kexuan Li", "Xiangyu Qiu", "Zhenming Peng"], "title": "ARFC-WAHNet: Adaptive Receptive Field Convolution and Wavelet-Attentive Hierarchical Network for Infrared Small Target Detection", "categories": ["cs.CV"], "comment": null, "summary": "Infrared small target detection (ISTD) is critical in both civilian and\nmilitary applications. However, the limited texture and structural information\nin infrared images makes accurate detection particularly challenging. Although\nrecent deep learning-based methods have improved performance, their use of\nconventional convolution kernels limits adaptability to complex scenes and\ndiverse targets. Moreover, pooling operations often cause feature loss and\ninsufficient exploitation of image information. To address these issues, we\npropose an adaptive receptive field convolution and wavelet-attentive\nhierarchical network for infrared small target detection (ARFC-WAHNet). This\nnetwork incorporates a multi-receptive field feature interaction convolution\n(MRFFIConv) module to adaptively extract discriminative features by integrating\nmultiple convolutional branches with a gated unit. A wavelet frequency\nenhancement downsampling (WFED) module leverages Haar wavelet transform and\nfrequency-domain reconstruction to enhance target features and suppress\nbackground noise. Additionally, we introduce a high-low feature fusion (HLFF)\nmodule for integrating low-level details with high-level semantics, and a\nglobal median enhancement attention (GMEA) module to improve feature diversity\nand expressiveness via global attention. Experiments on public datasets SIRST,\nNUDT-SIRST, and IRSTD-1k demonstrate that ARFC-WAHNet outperforms recent\nstate-of-the-art methods in both detection accuracy and robustness,\nparticularly under complex backgrounds. The code is available at\nhttps://github.com/Leaf2001/ARFC-WAHNet."}
{"id": "2505.10601", "pdf": "https://arxiv.org/pdf/2505.10601", "abs": "https://arxiv.org/abs/2505.10601", "authors": ["Chuang Chen", "Wenyi Ge"], "title": "SRMamba: Mamba for Super-Resolution of LiDAR Point Clouds", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "In recent years, range-view-based LiDAR point cloud super-resolution\ntechniques attract significant attention as a low-cost method for generating\nhigher-resolution point cloud data. However, due to the sparsity and irregular\nstructure of LiDAR point clouds, the point cloud super-resolution problem\nremains a challenging topic, especially for point cloud upsampling under novel\nviews. In this paper, we propose SRMamba, a novel method for super-resolution\nof LiDAR point clouds in sparse scenes, addressing the key challenge of\nrecovering the 3D spatial structure of point clouds from novel views.\nSpecifically, we implement projection technique based on Hough Voting and Hole\nCompensation strategy to eliminate horizontally linear holes in range image. To\nimprove the establishment of long-distance dependencies and to focus on\npotential geometric features in vertical 3D space, we employ Visual State Space\nmodel and Multi-Directional Scanning mechanism to mitigate the loss of 3D\nspatial structural information due to the range image. Additionally, an\nasymmetric U-Net network adapts to the input characteristics of LiDARs with\ndifferent beam counts, enabling super-resolution reconstruction for multi-beam\npoint clouds. We conduct a series of experiments on multiple challenging public\nLiDAR datasets (SemanticKITTI and nuScenes), and SRMamba demonstrates\nsignificant superiority over other algorithms in both qualitative and\nquantitative evaluations."}
{"id": "2505.10604", "pdf": "https://arxiv.org/pdf/2505.10604", "abs": "https://arxiv.org/abs/2505.10604", "authors": ["Chonghan Liu", "Haoran Wang", "Felix Henry", "Pu Miao", "Yajie Zhang", "Yu Zhao", "Peiran Wu"], "title": "MIRAGE: A Multi-modal Benchmark for Spatial Perception, Reasoning, and Intelligence", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Spatial perception and reasoning are core components of human cognition,\nencompassing object recognition, spatial relational understanding, and dynamic\nreasoning. Despite progress in computer vision, existing benchmarks reveal\nsignificant gaps in models' abilities to accurately recognize object attributes\nand reason about spatial relationships, both essential for dynamic reasoning.\nTo address these limitations, we propose MIRAGE, a multi-modal benchmark\ndesigned to evaluate models' capabilities in Counting (object attribute\nrecognition), Relation (spatial relational reasoning), and Counting with\nRelation. Through diverse and complex scenarios requiring fine-grained\nrecognition and reasoning, MIRAGE highlights critical limitations in\nstate-of-the-art models, underscoring the need for improved representations and\nreasoning frameworks. By targeting these foundational abilities, MIRAGE\nprovides a pathway toward spatiotemporal reasoning in future research."}
{"id": "2505.10610", "pdf": "https://arxiv.org/pdf/2505.10610", "abs": "https://arxiv.org/abs/2505.10610", "authors": ["Zhaowei Wang", "Wenhao Yu", "Xiyu Ren", "Jipeng Zhang", "Yu Zhao", "Rohit Saxena", "Liang Cheng", "Ginny Wong", "Simon See", "Pasquale Minervini", "Yangqiu Song", "Mark Steedman"], "title": "MMLongBench: Benchmarking Long-Context Vision-Language Models Effectively and Thoroughly", "categories": ["cs.CV", "cs.CL"], "comment": "Work in progress", "summary": "The rapid extension of context windows in large vision-language models has\ngiven rise to long-context vision-language models (LCVLMs), which are capable\nof handling hundreds of images with interleaved text tokens in a single forward\npass. In this work, we introduce MMLongBench, the first benchmark covering a\ndiverse set of long-context vision-language tasks, to evaluate LCVLMs\neffectively and thoroughly. MMLongBench is composed of 13,331 examples spanning\nfive different categories of downstream tasks, such as Visual RAG and Many-Shot\nICL. It also provides broad coverage of image types, including various natural\nand synthetic images. To assess the robustness of the models to different input\nlengths, all examples are delivered at five standardized input lengths (8K-128K\ntokens) via a cross-modal tokenization scheme that combines vision patches and\ntext tokens. Through a thorough benchmarking of 46 closed-source and\nopen-source LCVLMs, we provide a comprehensive analysis of the current models'\nvision-language long-context ability. Our results show that: i) performance on\na single task is a weak proxy for overall long-context capability; ii) both\nclosed-source and open-source models face challenges in long-context\nvision-language tasks, indicating substantial room for future improvement; iii)\nmodels with stronger reasoning ability tend to exhibit better long-context\nperformance. By offering wide task coverage, various image types, and rigorous\nlength control, MMLongBench provides the missing foundation for diagnosing and\nadvancing the next generation of LCVLMs."}
{"id": "2505.10634", "pdf": "https://arxiv.org/pdf/2505.10634", "abs": "https://arxiv.org/abs/2505.10634", "authors": ["Jianfei Zhao", "Feng Zhang", "Xin Sun", "Chong Feng"], "title": "Mitigate Language Priors in Large Vision-Language Models by Cross-Images Contrastive Decoding", "categories": ["cs.CV"], "comment": null, "summary": "Language priors constitute one of the primary causes of hallucinations in\nLarge Vision-Language Models (LVLMs), driving the models to generate\nlinguistically plausible yet visually inconsistent content. The language priors\nin LVLMs originate from the linguistic knowledge inherited from their\npre-trained Large Language Model (LLM) backbone. Consequently, this\ncharacteristic is an intrinsic property of the model that remains independent\nof visual inputs. Inspired by the finding that language priors are consistent\nacross images, we propose Cross-Image Contrastive Decoding (CICD), a simple yet\neffective training-free method to alleviate language priors in LVLMs. CICD\nfirst identifies essential and detrimental priors, and then employs contrastive\ndecoding to eliminate the detrimental ones. This approach simultaneously\nprevents LVLMs from generating hallucinated content while maintaining textual\nfluency and coherence. Furthermore, the limited information overlap between\nimages helps prevent visual information loss during contrastive decoding. We\nvalidate the effectiveness of CICD on four benchmarks with six LVLMs. Our\nexperiments demonstrate that CICD performs remarkably well in mitigating\nlanguage priors, especially in the image captioning task, where such priors are\nmost pronounced. Code will be released once accepted."}
{"id": "2505.10649", "pdf": "https://arxiv.org/pdf/2505.10649", "abs": "https://arxiv.org/abs/2505.10649", "authors": ["Xianrui Li", "Yufei Cui", "Jun Li", "Antoni B. Chan"], "title": "Advancing Multiple Instance Learning with Continual Learning for Whole Slide Imaging", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Advances in medical imaging and deep learning have propelled progress in\nwhole slide image (WSI) analysis, with multiple instance learning (MIL) showing\npromise for efficient and accurate diagnostics. However, conventional MIL\nmodels often lack adaptability to evolving datasets, as they rely on static\ntraining that cannot incorporate new information without extensive retraining.\nApplying continual learning (CL) to MIL models is a possible solution, but\noften sees limited improvements. In this paper, we analyze CL in the context of\nattention MIL models and find that the model forgetting is mainly concentrated\nin the attention layers of the MIL model. Using the results of this analysis we\npropose two components for improving CL on MIL: Attention Knowledge\nDistillation (AKD) and the Pseudo-Bag Memory Pool (PMP). AKD mitigates\ncatastrophic forgetting by focusing on retaining attention layer knowledge\nbetween learning sessions, while PMP reduces the memory footprint by\nselectively storing only the most informative patches, or ``pseudo-bags'' from\nWSIs. Experimental evaluations demonstrate that our method significantly\nimproves both accuracy and memory efficiency on diverse WSI datasets,\noutperforming current state-of-the-art CL methods. This work provides a\nfoundation for CL in large-scale, weakly annotated clinical datasets, paving\nthe way for more adaptable and resilient diagnostic models."}
{"id": "2505.10664", "pdf": "https://arxiv.org/pdf/2505.10664", "abs": "https://arxiv.org/abs/2505.10664", "authors": ["Ziyang Ou"], "title": "CLIP Embeddings for AI-Generated Image Detection: A Few-Shot Study with Lightweight Classifier", "categories": ["cs.CV", "cs.AI", "I.2.10"], "comment": "8 pages, 5 figures, not submitted to any conference", "summary": "Verifying the authenticity of AI-generated images presents a growing\nchallenge on social media platforms these days. While vision-language models\n(VLMs) like CLIP outdo in multimodal representation, their capacity for\nAI-generated image classification is underexplored due to the absence of such\nlabels during the pre-training process. This work investigates whether CLIP\nembeddings inherently contain information indicative of AI generation. A\nproposed pipeline extracts visual embeddings using a frozen CLIP model, feeds\nits embeddings to lightweight networks, and fine-tunes only the final\nclassifier. Experiments on the public CIFAKE benchmark show the performance\nreaches 95% accuracy without language reasoning. Few-shot adaptation to curated\ncustom with 20% of the data results in performance to 85%. A closed-source\nbaseline (Gemini-2.0) has the best zero-shot accuracy yet fails on specific\nstyles. Notably, some specific image types, such as wide-angle photographs and\noil paintings, pose significant challenges to classification. These results\nindicate previously unexplored difficulties in classifying certain types of\nAI-generated images, revealing new and more specific questions in this domain\nthat are worth further investigation."}
{"id": "2505.10671", "pdf": "https://arxiv.org/pdf/2505.10671", "abs": "https://arxiv.org/abs/2505.10671", "authors": ["Yuki Kawana", "Shintaro Shiba", "Quan Kong", "Norimasa Kobori"], "title": "GA3CE: Unconstrained 3D Gaze Estimation with Gaze-Aware 3D Context Encoding", "categories": ["cs.CV"], "comment": "Accepted to CVPR2025. Project page:\n  https://woven-visionai.github.io/ga3ce-project/", "summary": "We propose a novel 3D gaze estimation approach that learns spatial\nrelationships between the subject and objects in the scene, and outputs 3D gaze\ndirection. Our method targets unconstrained settings, including cases where\nclose-up views of the subject's eyes are unavailable, such as when the subject\nis distant or facing away. Previous approaches typically rely on either 2D\nappearance alone or incorporate limited spatial cues using depth maps in the\nnon-learnable post-processing step. Estimating 3D gaze direction from 2D\nobservations in these scenarios is challenging; variations in subject pose,\nscene layout, and gaze direction, combined with differing camera poses, yield\ndiverse 2D appearances and 3D gaze directions even when targeting the same 3D\nscene. To address this issue, we propose GA3CE: Gaze-Aware 3D Context Encoding.\nOur method represents subject and scene using 3D poses and object positions,\ntreating them as 3D context to learn spatial relationships in 3D space.\nInspired by human vision, we align this context in an egocentric space,\nsignificantly reducing spatial complexity. Furthermore, we propose D$^3$\n(direction-distance-decomposed) positional encoding to better capture the\nspatial relationship between 3D context and gaze direction in direction and\ndistance space. Experiments demonstrate substantial improvements, reducing mean\nangle error by 13%-37% compared to leading baselines on benchmark datasets in\nsingle-frame settings."}
{"id": "2505.10679", "pdf": "https://arxiv.org/pdf/2505.10679", "abs": "https://arxiv.org/abs/2505.10679", "authors": ["Jianyang Xie", "Yitian Zhao", "Yanda Meng", "He Zhao", "Anh Nguyen", "Yalin Zheng"], "title": "Are Spatial-Temporal Graph Convolution Networks for Human Action Recognition Over-Parameterized?", "categories": ["cs.CV"], "comment": null, "summary": "Spatial-temporal graph convolutional networks (ST-GCNs) showcase impressive\nperformance in skeleton-based human action recognition (HAR). However, despite\nthe development of numerous models, their recognition performance does not\ndiffer significantly after aligning the input settings. With this observation,\nwe hypothesize that ST-GCNs are over-parameterized for HAR, a conjecture\nsubsequently confirmed through experiments employing the lottery ticket\nhypothesis. Additionally, a novel sparse ST-GCNs generator is proposed, which\ntrains a sparse architecture from a randomly initialized dense network while\nmaintaining comparable performance levels to the dense components. Moreover, we\ngenerate multi-level sparsity ST-GCNs by integrating sparse structures at\nvarious sparsity levels and demonstrate that the assembled model yields a\nsignificant enhancement in HAR performance. Thorough experiments on four\ndatasets, including NTU-RGB+D 60(120), Kinetics-400, and FineGYM, demonstrate\nthat the proposed sparse ST-GCNs can achieve comparable performance to their\ndense components. Even with 95% fewer parameters, the sparse ST-GCNs exhibit a\ndegradation of <1% in top-1 accuracy. Meanwhile, the multi-level sparsity\nST-GCNs, which require only 66% of the parameters of the dense ST-GCNs,\ndemonstrate an improvement of >1% in top-1 accuracy. The code is available at\nhttps://github.com/davelailai/Sparse-ST-GCN."}
{"id": "2505.10685", "pdf": "https://arxiv.org/pdf/2505.10685", "abs": "https://arxiv.org/abs/2505.10685", "authors": ["Lingjun Zhao", "Sizhe Wei", "James Hays", "Lu Gan"], "title": "GaussianFormer3D: Multi-Modal Gaussian-based Semantic Occupancy Prediction with 3D Deformable Attention", "categories": ["cs.CV"], "comment": null, "summary": "3D semantic occupancy prediction is critical for achieving safe and reliable\nautonomous driving. Compared to camera-only perception systems, multi-modal\npipelines, especially LiDAR-camera fusion methods, can produce more accurate\nand detailed predictions. Although most existing works utilize a dense\ngrid-based representation, in which the entire 3D space is uniformly divided\ninto discrete voxels, the emergence of 3D Gaussians provides a compact and\ncontinuous object-centric representation. In this work, we propose a\nmulti-modal Gaussian-based semantic occupancy prediction framework utilizing 3D\ndeformable attention, named as GaussianFormer3D. We introduce a\nvoxel-to-Gaussian initialization strategy to provide 3D Gaussians with geometry\npriors from LiDAR data, and design a LiDAR-guided 3D deformable attention\nmechanism for refining 3D Gaussians with LiDAR-camera fusion features in a\nlifted 3D space. We conducted extensive experiments on both on-road and\noff-road datasets, demonstrating that our GaussianFormer3D achieves high\nprediction accuracy that is comparable to state-of-the-art multi-modal\nfusion-based methods with reduced memory consumption and improved efficiency."}
{"id": "2505.10737", "pdf": "https://arxiv.org/pdf/2505.10737", "abs": "https://arxiv.org/abs/2505.10737", "authors": ["Mitchell Rogers", "Theo Thompson", "Isla Duporge", "Johannes Fischer", "Klemens Pütz", "Thomas Mattern", "Bing Xue", "Mengjie Zhang"], "title": "Automated Detection of Salvin's Albatrosses: Improving Deep Learning Tools for Aerial Wildlife Surveys", "categories": ["cs.CV"], "comment": "Accepted to the CV4Animals workshop at CVPR 2025", "summary": "Recent advancements in deep learning and aerial imaging have transformed\nwildlife monitoring, enabling researchers to survey wildlife populations at\nunprecedented scales. Unmanned Aerial Vehicles (UAVs) provide a cost-effective\nmeans of capturing high-resolution imagery, particularly for monitoring densely\npopulated seabird colonies. In this study, we assess the performance of a\ngeneral-purpose avian detection model, BirdDetector, in estimating the breeding\npopulation of Salvin's albatross (Thalassarche salvini) on the Bounty Islands,\nNew Zealand. Using drone-derived imagery, we evaluate the model's effectiveness\nin both zero-shot and fine-tuned settings, incorporating enhanced inference\ntechniques and stronger augmentation methods. Our findings indicate that while\napplying the model in a zero-shot setting offers a strong baseline, fine-tuning\nwith annotations from the target domain and stronger image augmentation leads\nto marked improvements in detection accuracy. These results highlight the\npotential of leveraging pre-trained deep-learning models for species-specific\nmonitoring in remote and challenging environments."}
{"id": "2505.10743", "pdf": "https://arxiv.org/pdf/2505.10743", "abs": "https://arxiv.org/abs/2505.10743", "authors": ["Amritanshu Tiwari", "Cherish Puniani", "Kaustubh Sharma", "Ojasva Nema"], "title": "IMAGE-ALCHEMY: Advancing subject fidelity in personalised text-to-image generation", "categories": ["cs.CV"], "comment": "8 pages", "summary": "Recent advances in text-to-image diffusion models, particularly Stable\nDiffusion, have enabled the generation of highly detailed and semantically rich\nimages. However, personalizing these models to represent novel subjects based\non a few reference images remains challenging. This often leads to catastrophic\nforgetting, overfitting, or large computational overhead.We propose a two-stage\npipeline that addresses these limitations by leveraging LoRA-based fine-tuning\non the attention weights within the U-Net of the Stable Diffusion XL (SDXL)\nmodel. First, we use the unmodified SDXL to generate a generic scene by\nreplacing the subject with its class label. Then, we selectively insert the\npersonalized subject through a segmentation-driven image-to-image (Img2Img)\npipeline that uses the trained LoRA weights.This framework isolates the subject\nencoding from the overall composition, thus preserving SDXL's broader\ngenerative capabilities while integrating the new subject in a high-fidelity\nmanner. Our method achieves a DINO similarity score of 0.789 on SDXL,\noutperforming existing personalized text-to-image approaches."}
{"id": "2505.10751", "pdf": "https://arxiv.org/pdf/2505.10751", "abs": "https://arxiv.org/abs/2505.10751", "authors": ["Francisco Raverta Capua", "Pablo De Cristoforis"], "title": "Mapping Semantic Segmentation to Point Clouds Using Structure from Motion for Forest Analysis", "categories": ["cs.CV"], "comment": "Work in progress, accepted in Novel Approaches for Precision\n  Agriculture and Forestry with Autonomous Robots, ICRA 2025 Workshop - May 23,\n  2025 - Atlanta, GA", "summary": "Although the use of remote sensing technologies for monitoring forested\nenvironments has gained increasing attention, publicly available point cloud\ndatasets remain scarce due to the high costs, sensor requirements, and\ntime-intensive nature of their acquisition. Moreover, as far as we are aware,\nthere are no public annotated datasets generated through Structure From Motion\n(SfM) algorithms applied to imagery, which may be due to the lack of SfM\nalgorithms that can map semantic segmentation information into an accurate\npoint cloud, especially in a challenging environment like forests.\n  In this work, we present a novel pipeline for generating semantically\nsegmented point clouds of forest environments. Using a custom-built forest\nsimulator, we generate realistic RGB images of diverse forest scenes along with\ntheir corresponding semantic segmentation masks. These labeled images are then\nprocessed using modified open-source SfM software capable of preserving\nsemantic information during 3D reconstruction. The resulting point clouds\nprovide both geometric and semantic detail, offering a valuable resource for\ntraining and evaluating deep learning models aimed at segmenting real forest\npoint clouds obtained via SfM."}
{"id": "2505.10764", "pdf": "https://arxiv.org/pdf/2505.10764", "abs": "https://arxiv.org/abs/2505.10764", "authors": ["Jiajun Cheng", "Xianwu Zhao", "Shan Lin"], "title": "Benchmarking performance, explainability, and evaluation strategies of vision-language models for surgery: Challenges and opportunities", "categories": ["cs.CV"], "comment": null, "summary": "Minimally invasive surgery (MIS) presents significant visual and technical\nchallenges, including surgical instrument classification and understanding\nsurgical action involving instruments, verbs, and anatomical targets. While\nmany machine learning-based methods have been developed for surgical\nunderstanding, they typically rely on procedure- and task-specific models\ntrained on small, manually annotated datasets. In contrast, the recent success\nof vision-language models (VLMs) trained on large volumes of raw image-text\npairs has demonstrated strong adaptability to diverse visual data and a range\nof downstream tasks. This opens meaningful research questions: how well do\nthese general-purpose VLMs perform in the surgical domain? In this work, we\nexplore those questions by benchmarking several VLMs across diverse surgical\ndatasets, including general laparoscopic procedures and endoscopic submucosal\ndissection, to assess their current capabilities and limitations. Our benchmark\nreveals key gaps in the models' ability to consistently link language to the\ncorrect regions in surgical scenes."}
{"id": "2505.10769", "pdf": "https://arxiv.org/pdf/2505.10769", "abs": "https://arxiv.org/abs/2505.10769", "authors": ["Manyu Li", "Ruian He", "Zixian Zhang", "Weimin Tan", "Bo Yan"], "title": "Unifying Segment Anything in Microscopy with Multimodal Large Language Model", "categories": ["cs.CV", "68T99"], "comment": "18 pages, 9 figures", "summary": "Accurate segmentation of regions of interest in biomedical images holds\nsubstantial value in image analysis. Although several foundation models for\nbiomedical segmentation have currently achieved excellent performance on\ncertain datasets, they typically demonstrate sub-optimal performance on unseen\ndomain data. We owe the deficiency to lack of vision-language knowledge before\nsegmentation. Multimodal Large Language Models (MLLMs) bring outstanding\nunderstanding and reasoning capabilities to multimodal tasks, which inspires us\nto leverage MLLMs to inject Vision-Language Knowledge (VLK), thereby enabling\nvision models to demonstrate superior generalization capabilities on\ncross-domain datasets. In this paper, we propose using MLLMs to guide SAM in\nlearning microscopy crose-domain data, unifying Segment Anything in Microscopy,\nnamed uLLSAM. Specifically, we propose the Vision-Language Semantic Alignment\n(VLSA) module, which injects VLK into Segment Anything Model (SAM). We find\nthat after SAM receives global VLK prompts, its performance improves\nsignificantly, but there are deficiencies in boundary contour perception.\nTherefore, we further propose Semantic Boundary Regularization (SBR) to prompt\nSAM. Our method achieves performance improvements of 7.71% in Dice and 12.10%\nin SA across 9 in-domain microscopy datasets, achieving state-of-the-art\nperformance. Our method also demonstrates improvements of 6.79% in Dice and\n10.08% in SA across 10 out-ofdomain datasets, exhibiting strong generalization\ncapabilities. Code is available at https://github.com/ieellee/uLLSAM."}
{"id": "2505.10781", "pdf": "https://arxiv.org/pdf/2505.10781", "abs": "https://arxiv.org/abs/2505.10781", "authors": ["David Minkwan Kim", "Soeun Lee", "Byeongkeun Kang"], "title": "Completely Weakly Supervised Class-Incremental Learning for Semantic Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": "8 pages", "summary": "This work addresses the task of completely weakly supervised\nclass-incremental learning for semantic segmentation to learn segmentation for\nboth base and additional novel classes using only image-level labels. While\nclass-incremental semantic segmentation (CISS) is crucial for handling diverse\nand newly emerging objects in the real world, traditional CISS methods require\nexpensive pixel-level annotations for training. To overcome this limitation,\npartially weakly-supervised approaches have recently been proposed. However, to\nthe best of our knowledge, this is the first work to introduce a completely\nweakly-supervised method for CISS. To achieve this, we propose to generate\nrobust pseudo-labels by combining pseudo-labels from a localizer and a sequence\nof foundation models based on their uncertainty. Moreover, to mitigate\ncatastrophic forgetting, we introduce an exemplar-guided data augmentation\nmethod that generates diverse images containing both previous and novel classes\nwith guidance. Finally, we conduct experiments in three common experimental\nsettings: 15-5 VOC, 10-10 VOC, and COCO-to-VOC, and in two scenarios: disjoint\nand overlap. The experimental results demonstrate that our completely weakly\nsupervised method outperforms even partially weakly supervised methods in the\n15-5 VOC and 10-10 VOC settings while achieving competitive accuracy in the\nCOCO-to-VOC setting."}
{"id": "2505.10784", "pdf": "https://arxiv.org/pdf/2505.10784", "abs": "https://arxiv.org/abs/2505.10784", "authors": ["Qiushi Guo", "Jason Rambach"], "title": "SynRailObs: A Synthetic Dataset for Obstacle Detection in Railway Scenarios", "categories": ["cs.CV"], "comment": null, "summary": "Detecting potential obstacles in railway environments is critical for\npreventing serious accidents. Identifying a broad range of obstacle categories\nunder complex conditions requires large-scale datasets with precisely\nannotated, high-quality images. However, existing publicly available datasets\nfail to meet these requirements, thereby hindering progress in railway safety\nresearch. To address this gap, we introduce SynRailObs, a high-fidelity\nsynthetic dataset designed to represent a diverse range of weather conditions\nand geographical features. Furthermore, diffusion models are employed to\ngenerate rare and difficult-to-capture obstacles that are typically challenging\nto obtain in real-world scenarios. To evaluate the effectiveness of SynRailObs,\nwe perform experiments in real-world railway environments, testing on both\nballasted and ballastless tracks across various weather conditions. The results\ndemonstrate that SynRailObs holds substantial potential for advancing obstacle\ndetection in railway safety applications. Models trained on this dataset show\nconsistent performance across different distances and environmental conditions.\nMoreover, the model trained on SynRailObs exhibits zero-shot capabilities,\nwhich are essential for applications in security-sensitive domains. The data is\navailable in https://www.kaggle.com/datasets/qiushi910/synrailobs."}
{"id": "2505.10787", "pdf": "https://arxiv.org/pdf/2505.10787", "abs": "https://arxiv.org/abs/2505.10787", "authors": ["Jianlin Guo", "Haihong Xiao", "Wenxiong Kang"], "title": "EA-3DGS: Efficient and Adaptive 3D Gaussians with Highly Enhanced Quality for outdoor scenes", "categories": ["cs.CV"], "comment": null, "summary": "Efficient scene representations are essential for many real-world\napplications, especially those involving spatial measurement. Although current\nNeRF-based methods have achieved impressive results in reconstructing\nbuilding-scale scenes, they still suffer from slow training and inference\nspeeds due to time-consuming stochastic sampling. Recently, 3D Gaussian\nSplatting (3DGS) has demonstrated excellent performance with its high-quality\nrendering and real-time speed, especially for objects and small-scale scenes.\nHowever, in outdoor scenes, its point-based explicit representation lacks an\neffective adjustment mechanism, and the millions of Gaussian points required\noften lead to memory constraints during training. To address these challenges,\nwe propose EA-3DGS, a high-quality real-time rendering method designed for\noutdoor scenes. First, we introduce a mesh structure to regulate the\ninitialization of Gaussian components by leveraging an adaptive tetrahedral\nmesh that partitions the grid and initializes Gaussian components on each face,\neffectively capturing geometric structures in low-texture regions. Second, we\npropose an efficient Gaussian pruning strategy that evaluates each 3D\nGaussian's contribution to the view and prunes accordingly. To retain\ngeometry-critical Gaussian points, we also present a structure-aware\ndensification strategy that densifies Gaussian points in low-curvature regions.\nAdditionally, we employ vector quantization for parameter quantization of\nGaussian components, significantly reducing disk space requirements with only a\nminimal impact on rendering quality. Extensive experiments on 13 scenes,\nincluding eight from four public datasets (MatrixCity-Aerial, Mill-19, Tanks \\&\nTemples, WHU) and five self-collected scenes acquired through UAV\nphotogrammetry measurement from SCUT-CA and plateau regions, further\ndemonstrate the superiority of our method."}
{"id": "2505.10810", "pdf": "https://arxiv.org/pdf/2505.10810", "abs": "https://arxiv.org/abs/2505.10810", "authors": ["Gabriel Maldonado", "Armin Danesh Pazho", "Ghazal Alinezhad Noghre", "Vinit Katariya", "Hamed Tabkhi"], "title": "MoCLIP: Motion-Aware Fine-Tuning and Distillation of CLIP for Human Motion Generation", "categories": ["cs.CV"], "comment": "11 pages, 5 figures, 2 tables. Presented at the CVPR 2025 Human\n  Motion Generation (HuMoGen) Workshop. Introduces MoCLIP, a CLIP-based\n  fine-tuning strategy for motion generation, with results on HumanML3D dataset\n  and ablation studies", "summary": "Human motion generation is essential for fields such as animation, robotics,\nand virtual reality, requiring models that effectively capture motion dynamics\nfrom text descriptions. Existing approaches often rely on Contrastive\nLanguage-Image Pretraining (CLIP)-based text encoders, but their training on\ntext-image pairs constrains their ability to understand temporal and kinematic\nstructures inherent in motion and motion generation. This work introduces\nMoCLIP, a fine-tuned CLIP model with an additional motion encoding head,\ntrained on motion sequences using contrastive learning and tethering loss. By\nexplicitly incorporating motion-aware representations, MoCLIP enhances motion\nfidelity while remaining compatible with existing CLIP-based pipelines and\nseamlessly integrating into various CLIP-based methods. Experiments demonstrate\nthat MoCLIP improves Top-1, Top-2, and Top-3 accuracy while maintaining\ncompetitive FID, leading to improved text-to-motion alignment results. These\nresults highlight MoCLIP's versatility and effectiveness, establishing it as a\nrobust framework for enhancing motion generation."}
{"id": "2505.10823", "pdf": "https://arxiv.org/pdf/2505.10823", "abs": "https://arxiv.org/abs/2505.10823", "authors": ["Xue Li", "Jameson Merkow", "Noel C. F. Codella", "Alberto Santamaria-Pang", "Naiteek Sangani", "Alexander Ersoy", "Christopher Burt", "John W. Garrett", "Richard J. Bruce", "Joshua D. Warner", "Tyler Bradshaw", "Ivan Tarapov", "Matthew P. Lungren", "Alan B. McMillan"], "title": "From Embeddings to Accuracy: Comparing Foundation Models for Radiographic Classification", "categories": ["cs.CV", "eess.IV"], "comment": "11 pages, 5 figures, 4 tables", "summary": "Foundation models, pretrained on extensive datasets, have significantly\nadvanced machine learning by providing robust and transferable embeddings\napplicable to various domains, including medical imaging diagnostics. This\nstudy evaluates the utility of embeddings derived from both general-purpose and\nmedical domain-specific foundation models for training lightweight adapter\nmodels in multi-class radiography classification, focusing specifically on tube\nplacement assessment. A dataset comprising 8842 radiographs classified into\nseven distinct categories was employed to extract embeddings using six\nfoundation models: DenseNet121, BiomedCLIP, Med-Flamingo, MedImageInsight,\nRad-DINO, and CXR-Foundation. Adapter models were subsequently trained using\nclassical machine learning algorithms. Among these combinations,\nMedImageInsight embeddings paired with an support vector machine adapter\nyielded the highest mean area under the curve (mAUC) at 93.8%, followed closely\nby Rad-DINO (91.1%) and CXR-Foundation (89.0%). In comparison, BiomedCLIP and\nDenseNet121 exhibited moderate performance with mAUC scores of 83.0% and 81.8%,\nrespectively, whereas Med-Flamingo delivered the lowest performance at 75.1%.\nNotably, most adapter models demonstrated computational efficiency, achieving\ntraining within one minute and inference within seconds on CPU, underscoring\ntheir practicality for clinical applications. Furthermore, fairness analyses on\nadapters trained on MedImageInsight-derived embeddings indicated minimal\ndisparities, with gender differences in performance within 2% and standard\ndeviations across age groups not exceeding 3%. These findings confirm that\nfoundation model embeddings-especially those from MedImageInsight-facilitate\naccurate, computationally efficient, and equitable diagnostic classification\nusing lightweight adapters for radiographic image analysis."}
{"id": "2505.10825", "pdf": "https://arxiv.org/pdf/2505.10825", "abs": "https://arxiv.org/abs/2505.10825", "authors": ["Jinke Li", "Yue Wu", "Xiaoyan Yang"], "title": "A High-Performance Thermal Infrared Object Detection Framework with Centralized Regulation", "categories": ["cs.CV", "cs.LG"], "comment": "This manuscript has been accepted for publication in the\n  International Journal for Housing Science and Its Applications (IJHSA), 2025", "summary": "Thermal Infrared (TIR) technology involves the use of sensors to detect and\nmeasure infrared radiation emitted by objects, and it is widely utilized across\na broad spectrum of applications. The advancements in object detection methods\nutilizing TIR images have sparked significant research interest. However, most\ntraditional methods lack the capability to effectively extract and fuse\nlocal-global information, which is crucial for TIR-domain feature attention. In\nthis study, we present a novel and efficient thermal infrared object detection\nframework, known as CRT-YOLO, that is based on centralized feature regulation,\nenabling the establishment of global-range interaction on TIR information. Our\nproposed model integrates efficient multi-scale attention (EMA) modules, which\nadeptly capture long-range dependencies while incurring minimal computational\noverhead. Additionally, it leverages the Centralized Feature Pyramid (CFP)\nnetwork, which offers global regulation of TIR features. Extensive experiments\nconducted on two benchmark datasets demonstrate that our CRT-YOLO model\nsignificantly outperforms conventional methods for TIR image object detection.\nFurthermore, the ablation study provides compelling evidence of the\neffectiveness of our proposed modules, reinforcing the potential impact of our\napproach on advancing the field of thermal infrared object detection."}
{"id": "2505.10827", "pdf": "https://arxiv.org/pdf/2505.10827", "abs": "https://arxiv.org/abs/2505.10827", "authors": ["Nail Ibrahimli", "Julian F. P. Kooij", "Liangliang Nan"], "title": "NeuSEditor: From Multi-View Images to Text-Guided Neural Surface Edits", "categories": ["cs.CV"], "comment": null, "summary": "Implicit surface representations are valued for their compactness and\ncontinuity, but they pose significant challenges for editing. Despite recent\nadvancements, existing methods often fail to preserve identity and maintain\ngeometric consistency during editing. To address these challenges, we present\nNeuSEditor, a novel method for text-guided editing of neural implicit surfaces\nderived from multi-view images. NeuSEditor introduces an identity-preserving\narchitecture that efficiently separates scenes into foreground and background,\nenabling precise modifications without altering the scene-specific elements.\nOur geometry-aware distillation loss significantly enhances rendering and\ngeometric quality. Our method simplifies the editing workflow by eliminating\nthe need for continuous dataset updates and source prompting. NeuSEditor\noutperforms recent state-of-the-art methods like PDS and InstructNeRF2NeRF,\ndelivering superior quantitative and qualitative results. For more visual\nresults, visit: neuseditor.github.io."}
{"id": "2505.10841", "pdf": "https://arxiv.org/pdf/2505.10841", "abs": "https://arxiv.org/abs/2505.10841", "authors": ["Jaeguk Kim", "Jaewoo Park", "Keuntek Lee", "Nam Ik Cho"], "title": "RefPose: Leveraging Reference Geometric Correspondences for Accurate 6D Pose Estimation of Unseen Objects", "categories": ["cs.CV"], "comment": "Accepted at CVPR 2025", "summary": "Estimating the 6D pose of unseen objects from monocular RGB images remains a\nchallenging problem, especially due to the lack of prior object-specific\nknowledge. To tackle this issue, we propose RefPose, an innovative approach to\nobject pose estimation that leverages a reference image and geometric\ncorrespondence as guidance. RefPose first predicts an initial pose by using\nobject templates to render the reference image and establish the geometric\ncorrespondence needed for the refinement stage. During the refinement stage,\nRefPose estimates the geometric correspondence of the query based on the\ngenerated references and iteratively refines the pose through a\nrender-and-compare approach. To enhance this estimation, we introduce a\ncorrelation volume-guided attention mechanism that effectively captures\ncorrelations between the query and reference images. Unlike traditional methods\nthat depend on pre-defined object models, RefPose dynamically adapts to new\nobject shapes by leveraging a reference image and geometric correspondence.\nThis results in robust performance across previously unseen objects. Extensive\nevaluation on the BOP benchmark datasets shows that RefPose achieves\nstate-of-the-art results while maintaining a competitive runtime."}
{"id": "2505.10869", "pdf": "https://arxiv.org/pdf/2505.10869", "abs": "https://arxiv.org/abs/2505.10869", "authors": ["Go Fukino", "Kanta Tachibana"], "title": "A Convolution-Based Gait Asymmetry Metric for Inter-Limb Synergistic Coordination", "categories": ["cs.CV", "cs.HC"], "comment": "7 pages, 13 figures, 3 tables", "summary": "This study focuses on the velocity patterns of various body parts during\nwalking and proposes a method for evaluating gait symmetry. Traditional motion\nanalysis studies have assessed gait symmetry based on differences in\nelectromyographic (EMG) signals or acceleration between the left and right\nsides. In contrast, this paper models intersegmental coordination using an LTI\nsystem and proposes a dissimilarity metric to evaluate symmetry. The method was\ntested on five subjects with both symmetric and asymmetric gait."}
{"id": "2505.10875", "pdf": "https://arxiv.org/pdf/2505.10875", "abs": "https://arxiv.org/abs/2505.10875", "authors": ["Alexey Magay", "Dhurba Tripathi", "Yu Hao", "Yi Fang"], "title": "A Light and Smart Wearable Platform with Multimodal Foundation Model for Enhanced Spatial Reasoning in People with Blindness and Low Vision", "categories": ["cs.CV"], "comment": "Project website and code: https://dktpt44.github.io/LV-GPT/", "summary": "People with blindness and low vision (pBLV) face significant challenges,\nstruggling to navigate environments and locate objects due to limited visual\ncues. Spatial reasoning is crucial for these individuals, as it enables them to\nunderstand and interpret the spatial relationships in their surroundings,\nenhancing their ability to navigate and interact more safely and independently.\nCurrent multi-modal large language (MLLM) models for low vision people lack the\nspatial reasoning capabilities needed to effectively assist in these tasks.\nMoreover, there is a notable absence of lightweight, easy-to-use systems that\nallow pBLV to effectively perceive and interact with their surrounding\nenvironment. In this paper, we propose a novel spatial enhanced multi-modal\nlarge language model based approach for visually impaired individuals. By\nfine-tuning the MLLM to incorporate spatial reasoning capabilities, our method\nsignificantly improves the understanding of environmental context, which is\ncritical for navigation and object recognition. The innovation extends to a\nhardware component, designed as an attachment for glasses, ensuring increased\naccessibility and ease of use. This integration leverages advanced VLMs to\ninterpret visual data and provide real-time, spatially aware feedback to the\nuser. Our approach aims to bridge the gap between advanced machine learning\nmodels and practical, user-friendly assistive devices, offering a robust\nsolution for visually impaired users to navigate their surroundings more\neffectively and independently. The paper includes an in-depth evaluation using\nthe VizWiz dataset, demonstrating substantial improvements in accuracy and user\nexperience. Additionally, we design a comprehensive dataset to evaluate our\nmethod's effectiveness in realworld situations, demonstrating substantial\nimprovements in accuracy and user experience."}
{"id": "2505.10888", "pdf": "https://arxiv.org/pdf/2505.10888", "abs": "https://arxiv.org/abs/2505.10888", "authors": ["Saad Manzur", "Bryan Vela", "Brandon Vela", "Aditya Agrawal", "Lan-Anh Dang-Vu", "David Li", "Wayne Hayes"], "title": "PoseBench3D: A Cross-Dataset Analysis Framework for 3D Human Pose Estimation", "categories": ["cs.CV"], "comment": "https://github.com/bryanjvela/PoseLab3D/tree/submission_branch", "summary": "Reliable three-dimensional human pose estimation is becoming increasingly\nimportant for real-world applications, yet much of prior work has focused\nsolely on the performance within a single dataset. In practice, however,\nsystems must adapt to diverse viewpoints, environments, and camera setups --\nconditions that differ significantly from those encountered during training,\nwhich is often the case in real-world scenarios. To address these challenges,\nwe present a standardized testing environment in which each method is evaluated\non a variety of datasets, ensuring consistent and fair cross-dataset\ncomparisons -- allowing for the analysis of methods on previously unseen data.\nTherefore, we propose PoseBench3D, a unified framework designed to\nsystematically re-evaluate prior and future models across four of the most\nwidely used datasets for human pose estimation -- with the framework able to\nsupport novel and future datasets as the field progresses. Through a unified\ninterface, our framework provides datasets in a pre-configured yet easily\nmodifiable format, ensuring compatibility with diverse model architectures. We\nre-evaluated the work of 18 methods, either trained or gathered from existing\nliterature, and reported results using both Mean Per Joint Position Error\n(MPJPE) and Procrustes Aligned Mean Per Joint Position Error (PA-MPJPE)\nmetrics, yielding more than 100 novel cross-dataset evaluation results.\nAdditionally, we analyze performance differences resulting from various\npre-processing techniques and dataset preparation parameters -- offering\nfurther insight into model generalization capabilities."}
{"id": "2505.10902", "pdf": "https://arxiv.org/pdf/2505.10902", "abs": "https://arxiv.org/abs/2505.10902", "authors": ["Shuo Wang", "Tong Ren", "Nan Cheng", "Rong Wang", "Li Zhang"], "title": "Patient-Specific Dynamic Digital-Physical Twin for Coronary Intervention Training: An Integrated Mixed Reality Approach", "categories": ["cs.CV", "cs.HC", "92C50", "I.3.8; I.6.8"], "comment": "34 pages, 24 figures", "summary": "Background and Objective: Precise preoperative planning and effective\nphysician training for coronary interventions are increasingly important.\nDespite advances in medical imaging technologies, transforming static or\nlimited dynamic imaging data into comprehensive dynamic cardiac models remains\nchallenging. Existing training systems lack accurate simulation of cardiac\nphysiological dynamics. This study develops a comprehensive dynamic cardiac\nmodel research framework based on 4D-CTA, integrating digital twin technology,\ncomputer vision, and physical model manufacturing to provide precise,\npersonalized tools for interventional cardiology. Methods: Using 4D-CTA data\nfrom a 60-year-old female with three-vessel coronary stenosis, we segmented\ncardiac chambers and coronary arteries, constructed dynamic models, and\nimplemented skeletal skinning weight computation to simulate vessel deformation\nacross 20 cardiac phases. Transparent vascular physical models were\nmanufactured using medical-grade silicone. We developed cardiac output analysis\nand virtual angiography systems, implemented guidewire 3D reconstruction using\nbinocular stereo vision, and evaluated the system through angiography\nvalidation and CABG training applications. Results: Morphological consistency\nbetween virtual and real angiography reached 80.9%. Dice similarity\ncoefficients for guidewire motion ranged from 0.741-0.812, with mean trajectory\nerrors below 1.1 mm. The transparent model demonstrated advantages in CABG\ntraining, allowing direct visualization while simulating beating heart\nchallenges. Conclusion: Our patient-specific digital-physical twin approach\neffectively reproduces both anatomical structures and dynamic characteristics\nof coronary vasculature, offering a dynamic environment with visual and tactile\nfeedback valuable for education and clinical planning."}
{"id": "2505.10917", "pdf": "https://arxiv.org/pdf/2505.10917", "abs": "https://arxiv.org/abs/2505.10917", "authors": ["Mingxiao Li", "Na Su", "Fang Qu", "Zhizhou Zhong", "Ziyang Chen", "Zhaopeng Tu", "Xiaolong Li"], "title": "VISTA: Enhancing Vision-Text Alignment in MLLMs via Cross-Modal Mutual Information Maximization", "categories": ["cs.CV"], "comment": null, "summary": "Current multimodal large language models (MLLMs) face a critical challenge in\nmodality alignment, often exhibiting a bias towards textual information at the\nexpense of other modalities like vision. This paper conducts a systematic\ninformation-theoretic analysis of the widely used cross-entropy loss in MLLMs,\nuncovering its implicit alignment objective. Our theoretical investigation\nreveals that this implicit objective has inherent limitations, leading to a\ndegradation of cross-modal alignment as text sequence length increases, thereby\nhindering effective multimodal information fusion. To overcome these drawbacks,\nwe propose Vision-Text Alignment (VISTA), a novel approach guided by our\ntheoretical insights. VISTA introduces an explicit alignment objective designed\nto maximize cross-modal mutual information, preventing the degradation of\nvisual alignment. Notably, VISTA enhances the visual understanding capabilities\nof existing MLLMs without requiring any additional trainable modules or extra\ntraining data, making it both efficient and practical. Our method significantly\noutperforms baseline models across more than a dozen benchmark datasets,\nincluding VQAv2, MMStar, and MME, paving the way for new directions in MLLM\nmodal alignment research."}
{"id": "2505.10921", "pdf": "https://arxiv.org/pdf/2505.10921", "abs": "https://arxiv.org/abs/2505.10921", "authors": ["Junyi Yuan", "Jian Zhang", "Fangyu Wu", "Dongming Lu", "Huanda Lu", "Qiufeng Wang"], "title": "Towards Cross-modal Retrieval in Chinese Cultural Heritage Documents: Dataset and Solution", "categories": ["cs.CV"], "comment": null, "summary": "China has a long and rich history, encompassing a vast cultural heritage that\nincludes diverse multimodal information, such as silk patterns, Dunhuang\nmurals, and their associated historical narratives. Cross-modal retrieval plays\na pivotal role in understanding and interpreting Chinese cultural heritage by\nbridging visual and textual modalities to enable accurate text-to-image and\nimage-to-text retrieval. However, despite the growing interest in multimodal\nresearch, there is a lack of specialized datasets dedicated to Chinese cultural\nheritage, limiting the development and evaluation of cross-modal learning\nmodels in this domain. To address this gap, we propose a multimodal dataset\nnamed CulTi, which contains 5,726 image-text pairs extracted from two series of\nprofessional documents, respectively related to ancient Chinese silk and\nDunhuang murals. Compared to existing general-domain multimodal datasets, CulTi\npresents a challenge for cross-modal retrieval: the difficulty of local\nalignment between intricate decorative motifs and specialized textual\ndescriptions. To address this challenge, we propose LACLIP, a training-free\nlocal alignment strategy built upon a fine-tuned Chinese-CLIP. LACLIP enhances\nthe alignment of global textual descriptions with local visual regions by\ncomputing weighted similarity scores during inference. Experimental results on\nCulTi demonstrate that LACLIP significantly outperforms existing models in\ncross-modal retrieval, particularly in handling fine-grained semantic\nassociations within Chinese cultural heritage."}
{"id": "2505.10931", "pdf": "https://arxiv.org/pdf/2505.10931", "abs": "https://arxiv.org/abs/2505.10931", "authors": ["Chao Wang", "Wei Lu", "Xiang Li", "Jian Yang", "Lei Luo"], "title": "M4-SAR: A Multi-Resolution, Multi-Polarization, Multi-Scene, Multi-Source Dataset and Benchmark for Optical-SAR Fusion Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "Single-source remote sensing object detection using optical or SAR images\nstruggles in complex environments. Optical images offer rich textural details\nbut are often affected by low-light, cloud-obscured, or low-resolution\nconditions, reducing the detection performance. SAR images are robust to\nweather, but suffer from speckle noise and limited semantic expressiveness.\nOptical and SAR images provide complementary advantages, and fusing them can\nsignificantly improve the detection accuracy. However, progress in this field\nis hindered by the lack of large-scale, standardized datasets. To address these\nchallenges, we propose the first comprehensive dataset for optical-SAR fusion\nobject detection, named Multi-resolution, Multi-polarization, Multi-scene,\nMulti-source SAR dataset (M4-SAR). It contains 112,184 precisely aligned image\npairs and nearly one million labeled instances with arbitrary orientations,\nspanning six key categories. To enable standardized evaluation, we develop a\nunified benchmarking toolkit that integrates six state-of-the-art multi-source\nfusion methods. Furthermore, we propose E2E-OSDet, a novel end-to-end\nmulti-source fusion detection framework that mitigates cross-domain\ndiscrepancies and establishes a robust baseline for future studies. Extensive\nexperiments on M4-SAR demonstrate that fusing optical and SAR data can improve\n$mAP$ by 5.7\\% over single-source inputs, with particularly significant gains\nin complex environments. The dataset and code are publicly available at\nhttps://github.com/wchao0601/M4-SAR."}
{"id": "2505.10996", "pdf": "https://arxiv.org/pdf/2505.10996", "abs": "https://arxiv.org/abs/2505.10996", "authors": ["Yunkang Cao", "Yuqi Cheng", "Xiaohao Xu", "Yiheng Zhang", "Yihan Sun", "Yuxiang Tan", "Yuxin Zhang", "Xiaonan Huang", "Weiming Shen"], "title": "Visual Anomaly Detection under Complex View-Illumination Interplay: A Large-Scale Benchmark", "categories": ["cs.CV"], "comment": "Homgepage: https://hustcyq.github.io/M2AD/. Yunkang Cao and Yuqi\n  Cheng contribute equally to this work", "summary": "The practical deployment of Visual Anomaly Detection (VAD) systems is\nhindered by their sensitivity to real-world imaging variations, particularly\nthe complex interplay between viewpoint and illumination which drastically\nalters defect visibility. Current benchmarks largely overlook this critical\nchallenge. We introduce Multi-View Multi-Illumination Anomaly Detection (M2AD),\na new large-scale benchmark comprising 119,880 high-resolution images designed\nexplicitly to probe VAD robustness under such interacting conditions. By\nsystematically capturing 999 specimens across 10 categories using 12\nsynchronized views and 10 illumination settings (120 configurations total),\nM2AD enables rigorous evaluation. We establish two evaluation protocols:\nM2AD-Synergy tests the ability to fuse information across diverse\nconfigurations, and M2AD-Invariant measures single-image robustness against\nrealistic view-illumination effects. Our extensive benchmarking shows that\nstate-of-the-art VAD methods struggle significantly on M2AD, demonstrating the\nprofound challenge posed by view-illumination interplay. This benchmark serves\nas an essential tool for developing and validating VAD methods capable of\novercoming real-world complexities. Our full dataset and test suite will be\nreleased at https://hustcyq.github.io/M2AD to facilitate the field."}
{"id": "2505.10999", "pdf": "https://arxiv.org/pdf/2505.10999", "abs": "https://arxiv.org/abs/2505.10999", "authors": ["Weilai Xiang", "Hongyu Yang", "Di Huang", "Yunhong Wang"], "title": "DDAE++: Enhancing Diffusion Models Towards Unified Generative and Discriminative Learning", "categories": ["cs.CV"], "comment": null, "summary": "While diffusion models have gained prominence in image synthesis, their\ngenerative pre-training has been shown to yield discriminative representations,\npaving the way towards unified visual generation and understanding. However,\ntwo key questions remain: 1) Can these representations be leveraged to improve\nthe training of diffusion models themselves, rather than solely benefiting\ndownstream tasks? 2) Can the feature quality be enhanced to rival or even\nsurpass modern self-supervised learners, without compromising generative\ncapability? This work addresses these questions by introducing\nself-conditioning, a straightforward yet effective mechanism that internally\nleverages the rich semantics inherent in denoising network to guide its own\ndecoding layers, forming a tighter bottleneck that condenses high-level\nsemantics to improve generation. Results are compelling: our method boosts both\ngeneration FID and recognition accuracy with 1% computational overhead and\ngeneralizes across diverse diffusion architectures. Crucially,\nself-conditioning facilitates an effective integration of discriminative\ntechniques, such as contrastive self-distillation, directly into diffusion\nmodels without sacrificing generation quality. Extensive experiments on\npixel-space and latent-space datasets show that in linear evaluations, our\nenhanced diffusion models, particularly UViT and DiT, serve as strong\nrepresentation learners, surpassing various self-supervised models."}
{"id": "2505.11003", "pdf": "https://arxiv.org/pdf/2505.11003", "abs": "https://arxiv.org/abs/2505.11003", "authors": ["Bo Du", "Xuekang Zhu", "Xiaochen Ma", "Chenfan Qu", "Kaiwen Feng", "Zhe Yang", "Chi-Man Pun", "Jian Liu", "Jizhe Zhou"], "title": "ForensicHub: A Unified Benchmark & Codebase for All-Domain Fake Image Detection and Localization", "categories": ["cs.CV"], "comment": "Technical report. Code available at:\n  https://github.com/scu-zjz/ForensicHub", "summary": "The field of Fake Image Detection and Localization (FIDL) is highly\nfragmented, encompassing four domains: deepfake detection (Deepfake), image\nmanipulation detection and localization (IMDL), artificial\nintelligence-generated image detection (AIGC), and document image manipulation\nlocalization (Doc). Although individual benchmarks exist in some domains, a\nunified benchmark for all domains in FIDL remains blank. The absence of a\nunified benchmark results in significant domain silos, where each domain\nindependently constructs its datasets, models, and evaluation protocols without\ninteroperability, preventing cross-domain comparisons and hindering the\ndevelopment of the entire FIDL field. To close the domain silo barrier, we\npropose ForensicHub, the first unified benchmark & codebase for all-domain fake\nimage detection and localization. Considering drastic variations on dataset,\nmodel, and evaluation configurations across all domains, as well as the\nscarcity of open-sourced baseline models and the lack of individual benchmarks\nin some domains, ForensicHub: i) proposes a modular and configuration-driven\narchitecture that decomposes forensic pipelines into interchangeable components\nacross datasets, transforms, models, and evaluators, allowing flexible\ncomposition across all domains; ii) fully implements 10 baseline models, 6\nbackbones, 2 new benchmarks for AIGC and Doc, and integrates 2 existing\nbenchmarks of DeepfakeBench and IMDLBenCo through an adapter-based design; iii)\nconducts indepth analysis based on the ForensicHub, offering 8 key actionable\ninsights into FIDL model architecture, dataset characteristics, and evaluation\nstandards. ForensicHub represents a significant leap forward in breaking the\ndomain silos in the FIDL field and inspiring future breakthroughs."}
{"id": "2505.11013", "pdf": "https://arxiv.org/pdf/2505.11013", "abs": "https://arxiv.org/abs/2505.11013", "authors": ["Zongye Zhang", "Bohan Kong", "Qingjie Liu", "Yunhong Wang"], "title": "Towards Robust and Controllable Text-to-Motion via Masked Autoregressive Diffusion", "categories": ["cs.CV", "cs.MM", "I.3.8"], "comment": "10 pages, 6 figures, 5 tables", "summary": "Generating 3D human motion from text descriptions remains challenging due to\nthe diverse and complex nature of human motion. While existing methods excel\nwithin the training distribution, they often struggle with out-of-distribution\nmotions, limiting their applicability in real-world scenarios. Existing\nVQVAE-based methods often fail to represent novel motions faithfully using\ndiscrete tokens, which hampers their ability to generalize beyond seen data.\nMeanwhile, diffusion-based methods operating on continuous representations\noften lack fine-grained control over individual frames. To address these\nchallenges, we propose a robust motion generation framework MoMADiff, which\ncombines masked modeling with diffusion processes to generate motion using\nframe-level continuous representations. Our model supports flexible\nuser-provided keyframe specification, enabling precise control over both\nspatial and temporal aspects of motion synthesis. MoMADiff demonstrates strong\ngeneralization capability on novel text-to-motion datasets with sparse\nkeyframes as motion prompts. Extensive experiments on two held-out datasets and\ntwo standard benchmarks show that our method consistently outperforms\nstate-of-the-art models in motion quality, instruction fidelity, and keyframe\nadherence."}
{"id": "2505.11015", "pdf": "https://arxiv.org/pdf/2505.11015", "abs": "https://arxiv.org/abs/2505.11015", "authors": ["An-Lan Wang", "Jingqun Tang", "Liao Lei", "Hao Feng", "Qi Liu", "Xiang Fei", "Jinghui Lu", "Han Wang", "Weiwei Liu", "Hao Liu", "Yuliang Liu", "Xiang Bai", "Can Huang"], "title": "WildDoc: How Far Are We from Achieving Comprehensive and Robust Document Understanding in the Wild?", "categories": ["cs.CV"], "comment": null, "summary": "The rapid advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced capabilities in Document Understanding. However,\nprevailing benchmarks like DocVQA and ChartQA predominantly comprise\n\\textit{scanned or digital} documents, inadequately reflecting the intricate\nchallenges posed by diverse real-world scenarios, such as variable illumination\nand physical distortions. This paper introduces WildDoc, the inaugural\nbenchmark designed specifically for assessing document understanding in natural\nenvironments. WildDoc incorporates a diverse set of manually captured document\nimages reflecting real-world conditions and leverages document sources from\nestablished benchmarks to facilitate comprehensive comparisons with digital or\nscanned documents. Further, to rigorously evaluate model robustness, each\ndocument is captured four times under different conditions. Evaluations of\nstate-of-the-art MLLMs on WildDoc expose substantial performance declines and\nunderscore the models' inadequate robustness compared to traditional\nbenchmarks, highlighting the unique challenges posed by real-world document\nunderstanding. Our project homepage is available at\nhttps://bytedance.github.io/WildDoc."}
{"id": "2505.11018", "pdf": "https://arxiv.org/pdf/2505.11018", "abs": "https://arxiv.org/abs/2505.11018", "authors": ["Pengchen Zhang", "Alan J. X. Guo", "Sipin Luo", "Zhe Han", "Lin Guo"], "title": "Rethinking the Mean Teacher Strategy from the Perspective of Self-paced Learning", "categories": ["cs.CV"], "comment": null, "summary": "Semi-supervised medical image segmentation has attracted significant\nattention due to its potential to reduce manual annotation costs. The mean\nteacher (MT) strategy, commonly understood as introducing smoothed, temporally\nlagged consistency regularization, has demonstrated strong performance across\nvarious tasks in this field. In this work, we reinterpret the MT strategy on\nsupervised data as a form of self-paced learning, regulated by the output\nagreement between the temporally lagged teacher model and the ground truth\nlabels. This idea is further extended to incorporate agreement between a\ntemporally lagged model and a cross-architectural model, which offers greater\nflexibility in regulating the learning pace and enables application to\nunlabeled data. Specifically, we propose dual teacher-student learning (DTSL),\na framework that introduces two groups of teacher-student models with different\narchitectures. The output agreement between the cross-group teacher and student\nmodels is used as pseudo-labels, generated via a Jensen-Shannon\ndivergence-based consensus label generator (CLG). Extensive experiments on\npopular datasets demonstrate that the proposed method consistently outperforms\nexisting state-of-the-art approaches. Ablation studies further validate the\neffectiveness of the proposed modules."}
{"id": "2505.11020", "pdf": "https://arxiv.org/pdf/2505.11020", "abs": "https://arxiv.org/abs/2505.11020", "authors": ["Yi-Lu Jiang", "Wen-Chang Chang", "Ching-Lin Wang", "Kung-Liang Hsu", "Chih-Yi Chiu"], "title": "Classifying Shelf Life Quality of Pineapples by Combining Audio and Visual Features", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS"], "comment": null, "summary": "Determining the shelf life quality of pineapples using non-destructive\nmethods is a crucial step to reduce waste and increase income. In this paper, a\nmultimodal and multiview classification model was constructed to classify\npineapples into four quality levels based on audio and visual characteristics.\nFor research purposes, we compiled and released the PQC500 dataset consisting\nof 500 pineapples with two modalities: one was tapping pineapples to record\nsounds by multiple microphones and the other was taking pictures by multiple\ncameras at different locations, providing multimodal and multi-view audiovisual\nfeatures. We modified the contrastive audiovisual masked autoencoder to train\nthe cross-modal-based classification model by abundant combinations of audio\nand visual pairs. In addition, we proposed to sample a compact size of training\ndata for efficient computation. The experiments were evaluated under various\ndata and model configurations, and the results demonstrated that the proposed\ncross-modal model trained using audio-major sampling can yield 84% accuracy,\noutperforming the unimodal models of only audio and only visual by 6% and 18%,\nrespectively."}
{"id": "2505.11034", "pdf": "https://arxiv.org/pdf/2505.11034", "abs": "https://arxiv.org/abs/2505.11034", "authors": ["Fabian Gröger", "Simone Lionetti", "Philippe Gottfrois", "Alvaro Gonzalez-Jimenez", "Ludovic Amruthalingam", "Elisabeth Victoria Goessinger", "Hanna Lindemann", "Marie Bargiela", "Marie Hofbauer", "Omar Badri", "Philipp Tschandl", "Arash Koochek", "Matthew Groh", "Alexander A. Navarini", "Marc Pouly"], "title": "CleanPatrick: A Benchmark for Image Data Cleaning", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Robust machine learning depends on clean data, yet current image data\ncleaning benchmarks rely on synthetic noise or narrow human studies, limiting\ncomparison and real-world relevance. We introduce CleanPatrick, the first\nlarge-scale benchmark for data cleaning in the image domain, built upon the\npublicly available Fitzpatrick17k dermatology dataset. We collect 496,377\nbinary annotations from 933 medical crowd workers, identify off-topic samples\n(4%), near-duplicates (21%), and label errors (22%), and employ an aggregation\nmodel inspired by item-response theory followed by expert review to derive\nhigh-quality ground truth. CleanPatrick formalizes issue detection as a ranking\ntask and adopts typical ranking metrics mirroring real audit workflows.\nBenchmarking classical anomaly detectors, perceptual hashing, SSIM, Confident\nLearning, NoiseRank, and SelfClean, we find that, on CleanPatrick,\nself-supervised representations excel at near-duplicate detection, classical\nmethods achieve competitive off-topic detection under constrained review\nbudgets, and label-error detection remains an open challenge for fine-grained\nmedical classification. By releasing both the dataset and the evaluation\nframework, CleanPatrick enables a systematic comparison of image-cleaning\nstrategies and paves the way for more reliable data-centric artificial\nintelligence."}
{"id": "2505.11046", "pdf": "https://arxiv.org/pdf/2505.11046", "abs": "https://arxiv.org/abs/2505.11046", "authors": ["Tim Alpherts", "Sennay Ghebreab", "Nanne van Noord"], "title": "Artifacts of Idiosyncracy in Global Street View Data", "categories": ["cs.CV"], "comment": "Published at FAccT '25", "summary": "Street view data is increasingly being used in computer vision applications\nin recent years. Machine learning datasets are collected for these applications\nusing simple sampling techniques. These datasets are assumed to be a systematic\nrepresentation of cities, especially when densely sampled. Prior works however,\nshow that there are clear gaps in coverage, with certain cities or regions\nbeing covered poorly or not at all. Here we demonstrate that a cities'\nidiosyncracies, such as city layout, may lead to biases in street view data for\n28 cities across the globe, even when they are densely covered. We\nquantitatively uncover biases in the distribution of coverage of street view\ndata and propose a method for evaluation of such distributions to get better\ninsight in idiosyncracies in a cities' coverage. In addition, we perform a case\nstudy of Amsterdam with semi-structured interviews, showing how idiosyncracies\nof the collection process impact representation of cities and regions and\nallowing us to address biases at their source."}
{"id": "2505.11060", "pdf": "https://arxiv.org/pdf/2505.11060", "abs": "https://arxiv.org/abs/2505.11060", "authors": ["David Méndez", "Gianpaolo Bontempo", "Elisa Ficarra", "Roberto Confalonieri", "Natalia Díaz-Rodríguez"], "title": "CUBIC: Concept Embeddings for Unsupervised Bias Identification using VLMs", "categories": ["cs.CV", "cs.AI", "68T10", "I.2.4; I.5.2"], "comment": "8 pages, 3 figures, 5 tables. Accepted at IJCNN 2025; to appear in\n  IEEE Xplore", "summary": "Deep vision models often rely on biases learned from spurious correlations in\ndatasets. To identify these biases, methods that interpret high-level,\nhuman-understandable concepts are more effective than those relying primarily\non low-level features like heatmaps. A major challenge for these concept-based\nmethods is the lack of image annotations indicating potentially bias-inducing\nconcepts, since creating such annotations requires detailed labeling for each\ndataset and concept, which is highly labor-intensive. We present CUBIC (Concept\nembeddings for Unsupervised Bias IdentifiCation), a novel method that\nautomatically discovers interpretable concepts that may bias classifier\nbehavior. Unlike existing approaches, CUBIC does not rely on predefined bias\ncandidates or examples of model failures tied to specific biases, as such\ninformation is not always available. Instead, it leverages image-text latent\nspace and linear classifier probes to examine how the latent representation of\na superclass label$\\unicode{x2014}$shared by all instances in the\ndataset$\\unicode{x2014}$is influenced by the presence of a given concept. By\nmeasuring these shifts against the normal vector to the classifier's decision\nboundary, CUBIC identifies concepts that significantly influence model\npredictions. Our experiments demonstrate that CUBIC effectively uncovers\npreviously unknown biases using Vision-Language Models (VLMs) without requiring\nthe samples in the dataset where the classifier underperforms or prior\nknowledge of potential biases."}
{"id": "2505.11062", "pdf": "https://arxiv.org/pdf/2505.11062", "abs": "https://arxiv.org/abs/2505.11062", "authors": ["Baisong Li", "Xingwang Wang", "Haixiao Xu"], "title": "HSRMamba: Efficient Wavelet Stripe State Space Model for Hyperspectral Image Super-Resolution", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Single hyperspectral image super-resolution (SHSR) aims to restore\nhigh-resolution images from low-resolution hyperspectral images. Recently, the\nVisual Mamba model has achieved an impressive balance between performance and\ncomputational efficiency. However, due to its 1D scanning paradigm, the model\nmay suffer from potential artifacts during image generation. To address this\nissue, we propose HSRMamba. While maintaining the computational efficiency of\nVisual Mamba, we introduce a strip-based scanning scheme to effectively reduce\nartifacts from global unidirectional scanning. Additionally, HSRMamba uses\nwavelet decomposition to alleviate modal conflicts between high-frequency\nspatial features and low-frequency spectral features, further improving\nsuper-resolution performance. Extensive experiments show that HSRMamba not only\nexcels in reducing computational load and model size but also outperforms\nexisting methods, achieving state-of-the-art results."}
{"id": "2505.11070", "pdf": "https://arxiv.org/pdf/2505.11070", "abs": "https://arxiv.org/abs/2505.11070", "authors": ["Renjie Chen", "Wenfeng Lin", "Yichen Zhang", "Jiangchuan Wei", "Boyuan Liu", "Chao Feng", "Jiao Ran", "Mingyu Guo"], "title": "Towards Self-Improvement of Diffusion Models via Group Preference Optimization", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Aligning text-to-image (T2I) diffusion models with Direct Preference\nOptimization (DPO) has shown notable improvements in generation quality.\nHowever, applying DPO to T2I faces two challenges: the sensitivity of DPO to\npreference pairs and the labor-intensive process of collecting and annotating\nhigh-quality data. In this work, we demonstrate that preference pairs with\nmarginal differences can degrade DPO performance. Since DPO relies exclusively\non relative ranking while disregarding the absolute difference of pairs, it may\nmisclassify losing samples as wins, or vice versa. We empirically show that\nextending the DPO from pairwise to groupwise and incorporating reward\nstandardization for reweighting leads to performance gains without explicit\ndata selection. Furthermore, we propose Group Preference Optimization (GPO), an\neffective self-improvement method that enhances performance by leveraging the\nmodel's own capabilities without requiring external data. Extensive experiments\ndemonstrate that GPO is effective across various diffusion models and tasks.\nSpecifically, combining with widely used computer vision models, such as YOLO\nand OCR, the GPO improves the accurate counting and text rendering capabilities\nof the Stable Diffusion 3.5 Medium by 20 percentage points. Notably, as a\nplug-and-play method, no extra overhead is introduced during inference."}
{"id": "2505.11075", "pdf": "https://arxiv.org/pdf/2505.11075", "abs": "https://arxiv.org/abs/2505.11075", "authors": ["Jianghang Lin", "Yilin Lu", "Yunhang Shen", "Chaoyang Zhu", "Shengchuan Zhang", "Liujuan Cao", "Rongrong Ji"], "title": "Pseudo-Label Quality Decoupling and Correction for Semi-Supervised Instance Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Semi-Supervised Instance Segmentation (SSIS) involves classifying and\ngrouping image pixels into distinct object instances using limited labeled\ndata. This learning paradigm usually faces a significant challenge of unstable\nperformance caused by noisy pseudo-labels of instance categories and pixel\nmasks. We find that the prevalent practice of filtering instance pseudo-labels\nassessing both class and mask quality with a single score threshold, frequently\nleads to compromises in the trade-off between the qualities of class and mask\nlabels. In this paper, we introduce a novel Pseudo-Label Quality Decoupling and\nCorrection (PL-DC) framework for SSIS to tackle the above challenges. Firstly,\nat the instance level, a decoupled dual-threshold filtering mechanism is\ndesigned to decouple class and mask quality estimations for instance-level\npseudo-labels, thereby independently controlling pixel classifying and grouping\nqualities. Secondly, at the category level, we introduce a dynamic instance\ncategory correction module to dynamically correct the pseudo-labels of instance\ncategories, effectively alleviating category confusion. Lastly, we introduce a\npixel-level mask uncertainty-aware mechanism at the pixel level to re-weight\nthe mask loss for different pixels, thereby reducing the impact of noise\nintroduced by pixel-level mask pseudo-labels. Extensive experiments on the COCO\nand Cityscapes datasets demonstrate that the proposed PL-DC achieves\nsignificant performance improvements, setting new state-of-the-art results for\nSSIS. Notably, our PL-DC shows substantial gains even with minimal labeled\ndata, achieving an improvement of +11.6 mAP with just 1% COCO labeled data and\n+15.5 mAP with 5% Cityscapes labeled data. The code will be public."}
{"id": "2505.11099", "pdf": "https://arxiv.org/pdf/2505.11099", "abs": "https://arxiv.org/abs/2505.11099", "authors": ["Bin Liu", "Chunyang Wang", "Xuelian Liu", "Guan Xi", "Ge Zhang", "Ziteng Yao", "Mengxue Dong"], "title": "Hybrid-Emba3D: Geometry-Aware and Cross-Path Feature Hybrid Enhanced State Space Model for Point Cloud Classification", "categories": ["cs.CV"], "comment": null, "summary": "The point cloud classification tasks face the dual challenge of efficiently\nextracting local geometric features while maintaining model complexity. The\nMamba architecture utilizes the linear complexity advantage of state space\nmodels (SSMs) to overcome the computational bottleneck of Transformers while\nbalancing global modeling capabilities. However, the inherent contradiction\nbetween its unidirectional dependency and the unordered nature of point clouds\nimpedes modeling spatial correlation in local neighborhoods, thus constraining\ngeometric feature extraction. This paper proposes Hybrid-Emba3D, a\nbidirectional Mamba model enhanced by geometry-feature coupling and cross-path\nfeature hybridization. The Local geometric pooling with geometry-feature\ncoupling mechanism significantly enhances local feature discriminative power\nvia coordinated propagation and dynamic aggregation of geometric information\nbetween local center points and their neighborhoods, without introducing\nadditional parameters. The designed Collaborative feature enhancer adopts\ndual-path hybridization, effectively handling local mutations and sparse key\nsignals, breaking through the limitations of traditional SSM long-range\nmodeling. Experimental results demonstrate that the proposed model achieves a\nnew SOTA classification accuracy of 95.99% on ModelNet40 with only 0.03M\nadditional."}
{"id": "2505.11109", "pdf": "https://arxiv.org/pdf/2505.11109", "abs": "https://arxiv.org/abs/2505.11109", "authors": ["Florinel-Alin Croitoru", "Vlad Hondru", "Marius Popescu", "Radu Tudor Ionescu", "Fahad Shahbaz Khan", "Mubarak Shah"], "title": "MAVOS-DD: Multilingual Audio-Video Open-Set Deepfake Detection Benchmark", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM"], "comment": "15 pages", "summary": "We present the first large-scale open-set benchmark for multilingual\naudio-video deepfake detection. Our dataset comprises over 250 hours of real\nand fake videos across eight languages, with 60% of data being generated. For\neach language, the fake videos are generated with seven distinct deepfake\ngeneration models, selected based on the quality of the generated content. We\norganize the training, validation and test splits such that only a subset of\nthe chosen generative models and languages are available during training, thus\ncreating several challenging open-set evaluation setups. We perform experiments\nwith various pre-trained and fine-tuned deepfake detectors proposed in recent\nliterature. Our results show that state-of-the-art detectors are not currently\nable to maintain their performance levels when tested in our open-set\nscenarios. We publicly release our data and code at:\nhttps://huggingface.co/datasets/unibuc-cs/MAVOS-DD."}
{"id": "2505.11110", "pdf": "https://arxiv.org/pdf/2505.11110", "abs": "https://arxiv.org/abs/2505.11110", "authors": ["Massimiliano Cassia", "Luca Guarnera", "Mirko Casu", "Ignazio Zangara", "Sebastiano Battiato"], "title": "Deepfake Forensic Analysis: Source Dataset Attribution and Legal Implications of Synthetic Media Manipulation", "categories": ["cs.CV"], "comment": null, "summary": "Synthetic media generated by Generative Adversarial Networks (GANs) pose\nsignificant challenges in verifying authenticity and tracing dataset origins,\nraising critical concerns in copyright enforcement, privacy protection, and\nlegal compliance. This paper introduces a novel forensic framework for\nidentifying the training dataset (e.g., CelebA or FFHQ) of GAN-generated images\nthrough interpretable feature analysis. By integrating spectral transforms\n(Fourier/DCT), color distribution metrics, and local feature descriptors\n(SIFT), our pipeline extracts discriminative statistical signatures embedded in\nsynthetic outputs. Supervised classifiers (Random Forest, SVM, XGBoost) achieve\n98-99% accuracy in binary classification (real vs. synthetic) and multi-class\ndataset attribution across diverse GAN architectures (StyleGAN, AttGAN, GDWCT,\nStarGAN, and StyleGAN2). Experimental results highlight the dominance of\nfrequency-domain features (DCT/FFT) in capturing dataset-specific artifacts,\nsuch as upsampling patterns and spectral irregularities, while color histograms\nreveal implicit regularization strategies in GAN training. We further examine\nlegal and ethical implications, showing how dataset attribution can address\ncopyright infringement, unauthorized use of personal data, and regulatory\ncompliance under frameworks like GDPR and California's AB 602. Our framework\nadvances accountability and governance in generative modeling, with\napplications in digital forensics, content moderation, and intellectual\nproperty litigation."}
{"id": "2505.11121", "pdf": "https://arxiv.org/pdf/2505.11121", "abs": "https://arxiv.org/abs/2505.11121", "authors": ["Mathis Jürgen Adler", "Leonard Hackel", "Gencer Sumbul", "Begüm Demir"], "title": "Redundancy-Aware Pretraining of Vision-Language Foundation Models in Remote Sensing", "categories": ["cs.CV"], "comment": "Accepted at IEEE International Geoscience and Remote Sensing\n  Symposium (IGARSS) 2025. Our code is available at\n  https://git.tu-berlin.de/rsim/redundacy-aware-rs-vlm", "summary": "The development of foundation models through pretraining of vision-language\nmodels (VLMs) has recently attracted great attention in remote sensing (RS).\nVLM pretraining aims to learn image and language alignments from a large number\nof image-text pairs. Each pretraining image is often associated with multiple\ncaptions containing redundant information due to repeated or semantically\nsimilar phrases, resulting in increased pretraining and inference time. To\novercome this, we introduce a weighted feature aggregation (WFA) strategy for\nVLM pretraining in RS. Our strategy aims to extract and exploit complementary\ninformation from multiple captions per image while reducing redundancies\nthrough feature aggregation with importance weighting. To calculate adaptive\nimportance weights for different captions of each image, we propose two\ntechniques: (i) non-parametric uniqueness and (ii) learning-based attention. In\nthe first technique, importance weights are calculated based on the bilingual\nevaluation understudy (BLEU) scores of the captions to emphasize unique\nsentences and reduce the influence of repetitive ones. In the second technique,\nimportance weights are learned through an attention mechanism instead of\nrelying on hand-crafted features. The effectiveness of the proposed WFA\nstrategy with the two techniques is analyzed in terms of downstream performance\non text-to-image retrieval in RS. Experimental results show that the proposed\nstrategy enables efficient and effective pretraining of VLMs in RS. Based on\nthe experimental analysis, we derive guidelines for selecting appropriate\ntechniques depending on downstream task requirements and resource constraints.\nThe code of this work is publicly available at\nhttps://git.tu-berlin.de/rsim/redundacy-aware-rs-vlm."}
{"id": "2505.11129", "pdf": "https://arxiv.org/pdf/2505.11129", "abs": "https://arxiv.org/abs/2505.11129", "authors": ["Makoto Yamada", "Kian Ming A. Chai", "Ayoub Rhim", "Satoki Ishikawa", "Mohammad Sabokrou", "Yao-Hung Hubert Tsai"], "title": "PhiNet v2: A Mask-Free Brain-Inspired Vision Foundation Model from Video", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "arXiv admin note: substantial text overlap with arXiv:2405.14650", "summary": "Recent advances in self-supervised learning (SSL) have revolutionized\ncomputer vision through innovative architectures and learning objectives, yet\nthey have not fully leveraged insights from biological visual processing\nsystems. Recently, a brain-inspired SSL model named PhiNet was proposed; it is\nbased on a ResNet backbone and operates on static image inputs with strong\naugmentation. In this paper, we introduce PhiNet v2, a novel Transformer-based\narchitecture that processes temporal visual input (that is, sequences of\nimages) without relying on strong augmentation. Our model leverages variational\ninference to learn robust visual representations from continuous input streams,\nsimilar to human visual processing. Through extensive experimentation, we\ndemonstrate that PhiNet v2 achieves competitive performance compared to\nstate-of-the-art vision foundation models, while maintaining the ability to\nlearn from sequential input without strong data augmentation. This work\nrepresents a significant step toward more biologically plausible computer\nvision systems that process visual information in a manner more closely aligned\nwith human cognitive processes."}
{"id": "2505.11131", "pdf": "https://arxiv.org/pdf/2505.11131", "abs": "https://arxiv.org/abs/2505.11131", "authors": ["Feiran Li", "Qianqian Xu", "Shilong Bao", "Zhiyong Yang", "Xiaochun Cao", "Qingming Huang"], "title": "One Image is Worth a Thousand Words: A Usability Preservable Text-Image Collaborative Erasing Framework", "categories": ["cs.CV", "cs.AI"], "comment": "This paper has been accepeted to ICML 2025. Not Final Version", "summary": "Concept erasing has recently emerged as an effective paradigm to prevent\ntext-to-image diffusion models from generating visually undesirable or even\nharmful content. However, current removal methods heavily rely on manually\ncrafted text prompts, making it challenging to achieve a high erasure\n(efficacy) while minimizing the impact on other benign concepts (usability). In\nthis paper, we attribute the limitations to the inherent gap between the text\nand image modalities, which makes it hard to transfer the intricately entangled\nconcept knowledge from text prompts to the image generation process. To address\nthis, we propose a novel solution by directly integrating visual supervision\ninto the erasure process, introducing the first text-image Collaborative\nConcept Erasing (Co-Erasing) framework. Specifically, Co-Erasing describes the\nconcept jointly by text prompts and the corresponding undesirable images\ninduced by the prompts, and then reduces the generating probability of the\ntarget concept through negative guidance. This approach effectively bypasses\nthe knowledge gap between text and image, significantly enhancing erasure\nefficacy. Additionally, we design a text-guided image concept refinement\nstrategy that directs the model to focus on visual features most relevant to\nthe specified text concept, minimizing disruption to other benign concepts.\nFinally, comprehensive experiments suggest that Co-Erasing outperforms\nstate-of-the-art erasure approaches significantly with a better trade-off\nbetween efficacy and usability. Codes are available at\nhttps://github.com/Ferry-Li/Co-Erasing."}
{"id": "2505.11141", "pdf": "https://arxiv.org/pdf/2505.11141", "abs": "https://arxiv.org/abs/2505.11141", "authors": ["Yansheng Qiu", "Li Xiao", "Zhaopan Xu", "Pengfei Zhou", "Zheng Wang", "Kaipeng Zhang"], "title": "Human-Aligned Bench: Fine-Grained Assessment of Reasoning Ability in MLLMs vs. Humans", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The goal of achieving Artificial General Intelligence (AGI) is to imitate\nhumans and surpass them. Models such as OpenAI's o1, o3, and DeepSeek's R1 have\ndemonstrated that large language models (LLMs) with human-like reasoning\ncapabilities exhibit exceptional performance and are being gradually integrated\ninto multimodal large language models (MLLMs). However, whether these models\npossess capabilities comparable to humans in handling reasoning tasks remains\nunclear at present. In this paper, we propose Human-Aligned Bench, a benchmark\nfor fine-grained alignment of multimodal reasoning with human performance.\nSpecifically, we collected 9,794 multimodal questions that solely rely on\ncontextual reasoning, including bilingual (Chinese and English) multimodal\nquestions and pure text-based questions, encompassing four question types:\nvisual reasoning, definition judgment, analogical reasoning, and logical\njudgment. More importantly, each question is accompanied by human success rates\nand options that humans are prone to choosing incorrectly. Extensive\nexperiments on the Human-Aligned Bench reveal notable differences between the\nperformance of current MLLMs in multimodal reasoning and human performance. The\nfindings on our benchmark provide insights into the development of the\nnext-generation models."}
{"id": "2505.11152", "pdf": "https://arxiv.org/pdf/2505.11152", "abs": "https://arxiv.org/abs/2505.11152", "authors": ["Daniel Sungho Jung", "Kyoung Mu Lee"], "title": "Learning Dense Hand Contact Estimation from Imbalanced Data", "categories": ["cs.CV"], "comment": "Project page: http://haco-release.github.io", "summary": "Hands are essential to human interaction, and understanding contact between\nhands and the world can promote comprehensive understanding of their function.\nRecently, there have been growing number of hand interaction datasets that\ncover interaction with object, other hand, scene, and body. Despite the\nsignificance of the task and increasing high-quality data, how to effectively\nlearn dense hand contact estimation remains largely underexplored. There are\ntwo major challenges for learning dense hand contact estimation. First, there\nexists class imbalance issue from hand contact datasets where majority of\nsamples are not in contact. Second, hand contact datasets contain spatial\nimbalance issue with most of hand contact exhibited in finger tips, resulting\nin challenges for generalization towards contacts in other hand regions. To\ntackle these issues, we present a framework that learns dense HAnd COntact\nestimation (HACO) from imbalanced data. To resolve the class imbalance issue,\nwe introduce balanced contact sampling, which builds and samples from multiple\nsampling groups that fairly represent diverse contact statistics for both\ncontact and non-contact samples. Moreover, to address the spatial imbalance\nissue, we propose vertex-level class-balanced (VCB) loss, which incorporates\nspatially varying contact distribution by separately reweighting loss\ncontribution of each vertex based on its contact frequency across dataset. As a\nresult, we effectively learn to predict dense hand contact estimation with\nlarge-scale hand contact data without suffering from class and spatial\nimbalance issue. The codes will be released."}
{"id": "2505.11168", "pdf": "https://arxiv.org/pdf/2505.11168", "abs": "https://arxiv.org/abs/2505.11168", "authors": ["Xinran Li", "Yu Liu", "Xiujuan Xu", "Xiaowei Zhao"], "title": "CheX-DS: Improving Chest X-ray Image Classification with Ensemble Learning Based on DenseNet and Swin Transformer", "categories": ["cs.CV", "cs.AI"], "comment": "BIBM", "summary": "The automatic diagnosis of chest diseases is a popular and challenging task.\nMost current methods are based on convolutional neural networks (CNNs), which\nfocus on local features while neglecting global features. Recently,\nself-attention mechanisms have been introduced into the field of computer\nvision, demonstrating superior performance. Therefore, this paper proposes an\neffective model, CheX-DS, for classifying long-tail multi-label data in the\nmedical field of chest X-rays. The model is based on the excellent CNN model\nDenseNet for medical imaging and the newly popular Swin Transformer model,\nutilizing ensemble deep learning techniques to combine the two models and\nleverage the advantages of both CNNs and Transformers. The loss function of\nCheX-DS combines weighted binary cross-entropy loss with asymmetric loss,\neffectively addressing the issue of data imbalance. The NIH ChestX-ray14\ndataset is selected to evaluate the model's effectiveness. The model\noutperforms previous studies with an excellent average AUC score of 83.76\\%,\ndemonstrating its superior performance."}
{"id": "2505.11178", "pdf": "https://arxiv.org/pdf/2505.11178", "abs": "https://arxiv.org/abs/2505.11178", "authors": ["Yixin Wan", "Kai-Wei Chang"], "title": "CompAlign: Improving Compositional Text-to-Image Generation with a Complex Benchmark and Fine-Grained Feedback", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "State-of-the-art T2I models are capable of generating high-resolution images\ngiven textual prompts. However, they still struggle with accurately depicting\ncompositional scenes that specify multiple objects, attributes, and spatial\nrelations. We present CompAlign, a challenging benchmark with an emphasis on\nassessing the depiction of 3D-spatial relationships, for evaluating and\nimproving models on compositional image generation. CompAlign consists of 900\ncomplex multi-subject image generation prompts that combine numerical and\n3D-spatial relationships with varied attribute bindings. Our benchmark is\nremarkably challenging, incorporating generation tasks with 3+ generation\nsubjects with complex 3D-spatial relationships. Additionally, we propose\nCompQuest, an interpretable and accurate evaluation framework that decomposes\ncomplex prompts into atomic sub-questions, then utilizes a MLLM to provide\nfine-grained binary feedback on the correctness of each aspect of generation\nelements in model-generated images. This enables precise quantification of\nalignment between generated images and compositional prompts. Furthermore, we\npropose an alignment framework that uses CompQuest's feedback as preference\nsignals to improve diffusion models' compositional image generation abilities.\nUsing adjustable per-image preferences, our method is easily scalable and\nflexible for different tasks. Evaluation of 9 T2I models reveals that: (1)\nmodels remarkable struggle more with compositional tasks with more complex\n3D-spatial configurations, and (2) a noticeable performance gap exists between\nopen-source accessible models and closed-source commercial models. Further\nempirical study on using CompAlign for model alignment yield promising results:\npost-alignment diffusion models achieve remarkable improvements in\ncompositional accuracy, especially on complex generation tasks, outperforming\nprevious approaches."}
{"id": "2505.11182", "pdf": "https://arxiv.org/pdf/2505.11182", "abs": "https://arxiv.org/abs/2505.11182", "authors": ["Yuzhuo Dai", "Jiaqi Jin", "Zhibin Dong", "Siwei Wang", "Xinwang Liu", "En Zhu", "Xihong Yang", "Xinbiao Gan", "Yu Feng"], "title": "Imputation-free and Alignment-free: Incomplete Multi-view Clustering Driven by Consensus Semantic Learning", "categories": ["cs.CV", "cs.AI"], "comment": "The paper has been accepted by the 42nd CVPR 2025. The main text has\n  9 pages, including 8 figures and 4 tables. The appendix has 8 pages, with 10\n  figures and 6 tables. The reference list has 3 pages", "summary": "In incomplete multi-view clustering (IMVC), missing data induce prototype\nshifts within views and semantic inconsistencies across views. A feasible\nsolution is to explore cross-view consistency in paired complete observations,\nfurther imputing and aligning the similarity relationships inherently shared\nacross views. Nevertheless, existing methods are constrained by two-tiered\nlimitations: (1) Neither instance- nor cluster-level consistency learning\nconstruct a semantic space shared across views to learn consensus semantics.\nThe former enforces cross-view instances alignment, and wrongly regards\nunpaired observations with semantic consistency as negative pairs; the latter\nfocuses on cross-view cluster counterparts while coarsely handling fine-grained\nintra-cluster relationships within views. (2) Excessive reliance on consistency\nresults in unreliable imputation and alignment without incorporating\nview-specific cluster information. Thus, we propose an IMVC framework,\nimputation- and alignment-free for consensus semantics learning (FreeCSL). To\nbridge semantic gaps across all observations, we learn consensus prototypes\nfrom available data to discover a shared space, where semantically similar\nobservations are pulled closer for consensus semantics learning. To capture\nsemantic relationships within specific views, we design a heuristic graph\nclustering based on modularity to recover cluster structure with intra-cluster\ncompactness and inter-cluster separation for cluster semantics enhancement.\nExtensive experiments demonstrate, compared to state-of-the-art competitors,\nFreeCSL achieves more confident and robust assignments on IMVC task."}
{"id": "2505.11192", "pdf": "https://arxiv.org/pdf/2505.11192", "abs": "https://arxiv.org/abs/2505.11192", "authors": ["Myunsoo Kim", "Seong-Woong Shim", "Byung-Jun Lee"], "title": "FALCON: False-Negative Aware Learning of Contrastive Negatives in Vision-Language Pretraining", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "False negatives pose a critical challenge in vision-language pretraining\n(VLP) due to the many-to-many correspondence between images and texts in\nlarge-scale datasets. These false negatives introduce conflicting supervision\nsignals that degrade the learned embedding space and diminish the effectiveness\nof hard negative sampling. In this paper, we propose FALCON (False-negative\nAware Learning of COntrastive Negatives), a learning-based mini-batch\nconstruction strategy that adaptively balances the trade-off between hard and\nfalse negatives during VLP. Rather than relying on fixed heuristics, FALCON\nemploys a negative mining scheduler that dynamically selects negative samples\nof appropriate hardness for each anchor instance during mini-batch\nconstruction, guided by a proxy for cross-modal alignment improvement.\nExperimental results demonstrate that FALCON significantly improves performance\nacross two widely adopted VLP frameworks (ALBEF, BLIP-2) and a broad range of\ndownstream tasks and evaluation settings, underscoring its effectiveness and\nrobustness in mitigating the impact of false negatives."}
{"id": "2505.11196", "pdf": "https://arxiv.org/pdf/2505.11196", "abs": "https://arxiv.org/abs/2505.11196", "authors": ["Yuang Ai", "Qihang Fan", "Xuefeng Hu", "Zhenheng Yang", "Ran He", "Huaibo Huang"], "title": "DiCo: Revitalizing ConvNets for Scalable and Efficient Diffusion Modeling", "categories": ["cs.CV"], "comment": "27 pages, 29 figures, 9 tables", "summary": "Diffusion Transformer (DiT), a promising diffusion model for visual\ngeneration, demonstrates impressive performance but incurs significant\ncomputational overhead. Intriguingly, analysis of pre-trained DiT models\nreveals that global self-attention is often redundant, predominantly capturing\nlocal patterns-highlighting the potential for more efficient alternatives. In\nthis paper, we revisit convolution as an alternative building block for\nconstructing efficient and expressive diffusion models. However, naively\nreplacing self-attention with convolution typically results in degraded\nperformance. Our investigations attribute this performance gap to the higher\nchannel redundancy in ConvNets compared to Transformers. To resolve this, we\nintroduce a compact channel attention mechanism that promotes the activation of\nmore diverse channels, thereby enhancing feature diversity. This leads to\nDiffusion ConvNet (DiCo), a family of diffusion models built entirely from\nstandard ConvNet modules, offering strong generative performance with\nsignificant efficiency gains. On class-conditional ImageNet benchmarks, DiCo\noutperforms previous diffusion models in both image quality and generation\nspeed. Notably, DiCo-XL achieves an FID of 2.05 at 256x256 resolution and 2.53\nat 512x512, with a 2.7x and 3.1x speedup over DiT-XL/2, respectively.\nFurthermore, our largest model, DiCo-H, scaled to 1B parameters, reaches an FID\nof 1.90 on ImageNet 256x256-without any additional supervision during training.\nCode: https://github.com/shallowdream204/DiCo."}
{"id": "2505.11216", "pdf": "https://arxiv.org/pdf/2505.11216", "abs": "https://arxiv.org/abs/2505.11216", "authors": ["Shibin Mei", "Hang Wang", "Bingbing Ni"], "title": "GeoMM: On Geodesic Perspective for Multi-modal Learning", "categories": ["cs.CV"], "comment": "15 pages, 3 figures, accepted by CVPR2025", "summary": "Geodesic distance serves as a reliable means of measuring distance in\nnonlinear spaces, and such nonlinear manifolds are prevalent in the current\nmultimodal learning. In these scenarios, some samples may exhibit high\nsimilarity, yet they convey different semantics, making traditional distance\nmetrics inadequate for distinguishing between positive and negative samples.\nThis paper introduces geodesic distance as a novel distance metric in\nmulti-modal learning for the first time, to mine correlations between samples,\naiming to address the limitations of common distance metric. Our approach\nincorporates a comprehensive series of strategies to adapt geodesic distance\nfor the current multimodal learning. Specifically, we construct a graph\nstructure to represent the adjacency relationships among samples by\nthresholding distances between them and then apply the shortest-path algorithm\nto obtain geodesic distance within this graph. To facilitate efficient\ncomputation, we further propose a hierarchical graph structure through\nclustering and combined with incremental update strategies for dynamic status\nupdates. Extensive experiments across various downstream tasks validate the\neffectiveness of our proposed method, demonstrating its capability to capture\ncomplex relationships between samples and improve the performance of multimodal\nlearning models."}
{"id": "2505.11232", "pdf": "https://arxiv.org/pdf/2505.11232", "abs": "https://arxiv.org/abs/2505.11232", "authors": ["Haiyu Li", "Charith Abhayaratne"], "title": "AW-GATCN: Adaptive Weighted Graph Attention Convolutional Network for Event Camera Data Joint Denoising and Object Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Event cameras, which capture brightness changes with high temporal\nresolution, inherently generate a significant amount of redundant and noisy\ndata beyond essential object structures. The primary challenge in event-based\nobject recognition lies in effectively removing this noise without losing\ncritical spatial-temporal information. To address this, we propose an Adaptive\nGraph-based Noisy Data Removal framework for Event-based Object Recognition.\nSpecifically, our approach integrates adaptive event segmentation based on\nnormalized density analysis, a multifactorial edge-weighting mechanism, and\nadaptive graph-based denoising strategies. These innovations significantly\nenhance the integration of spatiotemporal information, effectively filtering\nnoise while preserving critical structural features for robust recognition.\nExperimental evaluations on four challenging datasets demonstrate that our\nmethod achieves superior recognition accuracies of 83.77%, 76.79%, 99.30%, and\n96.89%, surpassing existing graph-based methods by up to 8.79%, and improving\nnoise reduction performance by up to 19.57%, with an additional accuracy gain\nof 6.26% compared to traditional Euclidean-based techniques."}
{"id": "2505.11245", "pdf": "https://arxiv.org/pdf/2505.11245", "abs": "https://arxiv.org/abs/2505.11245", "authors": ["Fu-Yun Wang", "Yunhao Shui", "Jingtan Piao", "Keqiang Sun", "Hongsheng Li"], "title": "Diffusion-NPO: Negative Preference Optimization for Better Preference Aligned Generation of Diffusion Models", "categories": ["cs.CV"], "comment": "Accepted to ICLR 2025", "summary": "Diffusion models have made substantial advances in image generation, yet\nmodels trained on large, unfiltered datasets often yield outputs misaligned\nwith human preferences. Numerous methods have been proposed to fine-tune\npre-trained diffusion models, achieving notable improvements in aligning\ngenerated outputs with human preferences. However, we argue that existing\npreference alignment methods neglect the critical role of handling\nunconditional/negative-conditional outputs, leading to a diminished capacity to\navoid generating undesirable outcomes. This oversight limits the efficacy of\nclassifier-free guidance~(CFG), which relies on the contrast between\nconditional generation and unconditional/negative-conditional generation to\noptimize output quality. In response, we propose a straightforward but\nversatile effective approach that involves training a model specifically\nattuned to negative preferences. This method does not require new training\nstrategies or datasets but rather involves minor modifications to existing\ntechniques. Our approach integrates seamlessly with models such as SD1.5, SDXL,\nvideo diffusion models and models that have undergone preference optimization,\nconsistently enhancing their alignment with human preferences."}
{"id": "2505.11246", "pdf": "https://arxiv.org/pdf/2505.11246", "abs": "https://arxiv.org/abs/2505.11246", "authors": ["Nirjhor Datta", "Afroza Akther", "M. Sohel Rahman"], "title": "Entropy-Driven Genetic Optimization for Deep-Feature-Guided Low-Light Image Enhancement", "categories": ["cs.CV"], "comment": null, "summary": "Image enhancement methods often prioritize pixel level information,\noverlooking the semantic features. We propose a novel, unsupervised,\nfuzzy-inspired image enhancement framework guided by NSGA-II algorithm that\noptimizes image brightness, contrast, and gamma parameters to achieve a balance\nbetween visual quality and semantic fidelity. Central to our proposed method is\nthe use of a pre trained deep neural network as a feature extractor. To find\nthe best enhancement settings, we use a GPU-accelerated NSGA-II algorithm that\nbalances multiple objectives, namely, increasing image entropy, improving\nperceptual similarity, and maintaining appropriate brightness. We further\nimprove the results by applying a local search phase to fine-tune the top\ncandidates from the genetic algorithm. Our approach operates entirely without\npaired training data making it broadly applicable across domains with limited\nor noisy labels. Quantitatively, our model achieves excellent performance with\naverage BRISQUE and NIQE scores of 19.82 and 3.652, respectively, in all\nunpaired datasets. Qualitatively, enhanced images by our model exhibit\nsignificantly improved visibility in shadowed regions, natural balance of\ncontrast and also preserve the richer fine detail without introducing noticable\nartifacts. This work opens new directions for unsupervised image enhancement\nwhere semantic consistency is critical."}
{"id": "2505.11257", "pdf": "https://arxiv.org/pdf/2505.11257", "abs": "https://arxiv.org/abs/2505.11257", "authors": ["Giulia Bertazzini", "Daniele Baracchi", "Dasara Shullani", "Isao Echizen", "Alessandro Piva"], "title": "DRAGON: A Large-Scale Dataset of Realistic Images Generated by Diffusion Models", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "The remarkable ease of use of diffusion models for image generation has led\nto a proliferation of synthetic content online. While these models are often\nemployed for legitimate purposes, they are also used to generate fake images\nthat support misinformation and hate speech. Consequently, it is crucial to\ndevelop robust tools capable of detecting whether an image has been generated\nby such models. Many current detection methods, however, require large volumes\nof sample images for training. Unfortunately, due to the rapid evolution of the\nfield, existing datasets often cover only a limited range of models and quickly\nbecome outdated. In this work, we introduce DRAGON, a comprehensive dataset\ncomprising images from 25 diffusion models, spanning both recent advancements\nand older, well-established architectures. The dataset contains a broad variety\nof images representing diverse subjects. To enhance image realism, we propose a\nsimple yet effective pipeline that leverages a large language model to expand\ninput prompts, thereby generating more diverse and higher-quality outputs, as\nevidenced by improvements in standard quality metrics. The dataset is provided\nin multiple sizes (ranging from extra-small to extra-large) to accomodate\ndifferent research scenarios. DRAGON is designed to support the forensic\ncommunity in developing and evaluating detection and attribution techniques for\nsynthetic content. Additionally, the dataset is accompanied by a dedicated test\nset, intended to serve as a benchmark for assessing the performance of newly\ndeveloped methods."}
{"id": "2505.11264", "pdf": "https://arxiv.org/pdf/2505.11264", "abs": "https://arxiv.org/abs/2505.11264", "authors": ["Mohamed Ali Chebbi", "Ewelina Rupnik", "Paul Lopes", "Marc Pierrot-Deseilligny"], "title": "Multi-view dense image matching with similarity learning and geometry priors", "categories": ["cs.CV"], "comment": null, "summary": "We introduce MV-DeepSimNets, a comprehensive suite of deep neural networks\ndesigned for multi-view similarity learning, leveraging epipolar geometry for\ntraining. Our approach incorporates an online geometry prior to characterize\npixel relationships, either along the epipolar line or through homography\nrectification. This enables the generation of geometry-aware features from\nnative images, which are then projected across candidate depth hypotheses using\nplane sweeping. Our method geometric preconditioning effectively adapts\nepipolar-based features for enhanced multi-view reconstruction, without\nrequiring the laborious multi-view training dataset creation. By aggregating\nlearned similarities, we construct and regularize the cost volume, leading to\nimproved multi-view surface reconstruction over traditional dense matching\napproaches. MV-DeepSimNets demonstrates superior performance against leading\nsimilarity learning networks and end-to-end regression models, especially in\nterms of generalization capabilities across both aerial and satellite imagery\nwith varied ground sampling distances. Our pipeline is integrated into MicMac\nsoftware and can be readily adopted in standard multi-resolution image matching\npipelines."}
{"id": "2505.11267", "pdf": "https://arxiv.org/pdf/2505.11267", "abs": "https://arxiv.org/abs/2505.11267", "authors": ["Wuzhou Quan", "Mingqiang Wei", "Jinhui Tang"], "title": "Equal is Not Always Fair: A New Perspective on Hyperspectral Representation Non-Uniformity", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Hyperspectral image (HSI) representation is fundamentally challenged by\npervasive non-uniformity, where spectral dependencies, spatial continuity, and\nfeature efficiency exhibit complex and often conflicting behaviors. Most\nexisting models rely on a unified processing paradigm that assumes homogeneity\nacross dimensions, leading to suboptimal performance and biased\nrepresentations. To address this, we propose FairHyp, a fairness-directed\nframework that explicitly disentangles and resolves the threefold\nnon-uniformity through cooperative yet specialized modules. We introduce a\nRunge-Kutta-inspired spatial variability adapter to restore spatial coherence\nunder resolution discrepancies, a multi-receptive field convolution module with\nsparse-aware refinement to enhance discriminative features while respecting\ninherent sparsity, and a spectral-context state space model that captures\nstable and long-range spectral dependencies via bidirectional Mamba scanning\nand statistical aggregation. Unlike one-size-fits-all solutions, FairHyp\nachieves dimension-specific adaptation while preserving global consistency and\nmutual reinforcement. This design is grounded in the view that non-uniformity\narises from the intrinsic structure of HSI representations, rather than any\nparticular task setting. To validate this, we apply FairHyp across four\nrepresentative tasks including classification, denoising, super-resolution, and\ninpaintin, demonstrating its effectiveness in modeling a shared structural\nflaw. Extensive experiments show that FairHyp consistently outperforms\nstate-of-the-art methods under varied imaging conditions. Our findings redefine\nfairness as a structural necessity in HSI modeling and offer a new paradigm for\nbalancing adaptability, efficiency, and fidelity in high-dimensional vision\ntasks."}
{"id": "2505.11282", "pdf": "https://arxiv.org/pdf/2505.11282", "abs": "https://arxiv.org/abs/2505.11282", "authors": ["Shrutarv Awasthi", "Anas Gouda", "Sven Franke", "Jérôme Rutinowski", "Frank Hoffmann", "Moritz Roidl"], "title": "MTevent: A Multi-Task Event Camera Dataset for 6D Pose Estimation and Moving Object Detection", "categories": ["cs.CV"], "comment": "accepted to CVPR 2025 Workshop on Event-based Vision", "summary": "Mobile robots are reaching unprecedented speeds, with platforms like Unitree\nB2, and Fraunhofer O3dyn achieving maximum speeds between 5 and 10 m/s.\nHowever, effectively utilizing such speeds remains a challenge due to the\nlimitations of RGB cameras, which suffer from motion blur and fail to provide\nreal-time responsiveness. Event cameras, with their asynchronous operation, and\nlow-latency sensing, offer a promising alternative for high-speed robotic\nperception. In this work, we introduce MTevent, a dataset designed for 6D pose\nestimation and moving object detection in highly dynamic environments with\nlarge detection distances. Our setup consists of a stereo-event camera and an\nRGB camera, capturing 75 scenes, each on average 16 seconds, and featuring 16\nunique objects under challenging conditions such as extreme viewing angles,\nvarying lighting, and occlusions. MTevent is the first dataset to combine\nhigh-speed motion, long-range perception, and real-world object interactions,\nmaking it a valuable resource for advancing event-based vision in robotics. To\nestablish a baseline, we evaluate the task of 6D pose estimation using NVIDIA's\nFoundationPose on RGB images, achieving an Average Recall of 0.22 with\nground-truth masks, highlighting the limitations of RGB-based approaches in\nsuch dynamic settings. With MTevent, we provide a novel resource to improve\nperception models and foster further research in high-speed robotic vision. The\ndataset is available for download\nhttps://huggingface.co/datasets/anas-gouda/MTevent"}
{"id": "2505.11293", "pdf": "https://arxiv.org/pdf/2505.11293", "abs": "https://arxiv.org/abs/2505.11293", "authors": ["Raghuveer Thirukovalluru", "Rui Meng", "Ye Liu", "Karthikeyan K", "Mingyi Su", "Ping Nie", "Semih Yavuz", "Yingbo Zhou", "Wenhu Chen", "Bhuwan Dhingra"], "title": "Breaking the Batch Barrier (B3) of Contrastive Learning via Smart Batch Mining", "categories": ["cs.CV"], "comment": "14 pages, 4 figures", "summary": "Contrastive learning (CL) is a prevalent technique for training embedding\nmodels, which pulls semantically similar examples (positives) closer in the\nrepresentation space while pushing dissimilar ones (negatives) further apart. A\nkey source of negatives are 'in-batch' examples, i.e., positives from other\nexamples in the batch. Effectiveness of such models is hence strongly\ninfluenced by the size and quality of training batches. In this work, we\npropose 'Breaking the Batch Barrier' (B3), a novel batch construction strategy\ndesigned to curate high-quality batches for CL. Our approach begins by using a\npretrained teacher embedding model to rank all examples in the dataset, from\nwhich a sparse similarity graph is constructed. A community detection algorithm\nis then applied to this graph to identify clusters of examples that serve as\nstrong negatives for one another. The clusters are then used to construct\nbatches that are rich in in-batch negatives. Empirical results on the MMEB\nmultimodal embedding benchmark (36 tasks) demonstrate that our method sets a\nnew state of the art, outperforming previous best methods by +1.3 and +2.9\npoints at the 7B and 2B model scales, respectively. Notably, models trained\nwith B3 surpass existing state-of-the-art results even with a batch size as\nsmall as 64, which is 4-16x smaller than that required by other methods."}
{"id": "2505.11314", "pdf": "https://arxiv.org/pdf/2505.11314", "abs": "https://arxiv.org/abs/2505.11314", "authors": ["Christoph Leiter", "Yuki M. Asano", "Margret Keuper", "Steffen Eger"], "title": "CROC: Evaluating and Training T2I Metrics with Pseudo- and Human-Labeled Contrastive Robustness Checks", "categories": ["cs.CV", "cs.CL"], "comment": "preprint", "summary": "The assessment of evaluation metrics (meta-evaluation) is crucial for\ndetermining the suitability of existing metrics in text-to-image (T2I)\ngeneration tasks. Human-based meta-evaluation is costly and time-intensive, and\nautomated alternatives are scarce. We address this gap and propose CROC: a\nscalable framework for automated Contrastive Robustness Checks that\nsystematically probes and quantifies metric robustness by synthesizing\ncontrastive test cases across a comprehensive taxonomy of image properties.\nWith CROC, we generate a pseudo-labeled dataset (CROC$^{syn}$) of over one\nmillion contrastive prompt-image pairs to enable a fine-grained comparison of\nevaluation metrics. We also use the dataset to train CROCScore, a new metric\nthat achieves state-of-the-art performance among open-source methods,\ndemonstrating an additional key application of our framework. To complement\nthis dataset, we introduce a human-supervised benchmark (CROC$^{hum}$)\ntargeting especially challenging categories. Our results highlight robustness\nissues in existing metrics: for example, many fail on prompts involving\nnegation, and all tested open-source metrics fail on at least 25% of cases\ninvolving correct identification of body parts."}
{"id": "2505.11326", "pdf": "https://arxiv.org/pdf/2505.11326", "abs": "https://arxiv.org/abs/2505.11326", "authors": ["Keunwoo Peter Yu", "Joyce Chai"], "title": "Temporally-Grounded Language Generation: A Benchmark for Real-Time Vision-Language Models", "categories": ["cs.CV", "cs.AI"], "comment": "18 pages", "summary": "Vision-language models (VLMs) have shown remarkable progress in offline tasks\nsuch as image captioning and video question answering. However, real-time\ninteractive environments impose new demands on VLMs, requiring them to generate\nutterances that are not only semantically accurate but also precisely timed. We\nidentify two core capabilities necessary for such settings --\n$\\textit{perceptual updating}$ and $\\textit{contingency awareness}$ -- and\npropose a new benchmark task, $\\textbf{Temporally-Grounded Language Generation\n(TGLG)}$, to evaluate them. TGLG requires models to generate utterances in\nresponse to streaming video such that both content and timing align with\ndynamic visual input. To support this benchmark, we curate evaluation datasets\nfrom sports broadcasting and egocentric human interaction domains, and\nintroduce a new metric, $\\textbf{TRACE}$, to evaluate TGLG by jointly measuring\nsemantic similarity and temporal alignment. Finally, we present\n$\\textbf{Vision-Language Model with Time-Synchronized Interleaving (VLM-TSI)}$,\na model that interleaves visual and linguistic tokens in a time-synchronized\nmanner, enabling real-time language generation without relying on turn-based\nassumptions. Experimental results show that VLM-TSI significantly outperforms a\nstrong baseline, yet overall performance remains modest -- highlighting the\ndifficulty of TGLG and motivating further research in real-time VLMs. Code and\ndata available $\\href{https://github.com/yukw777/tglg}{here}$."}
{"id": "2505.11334", "pdf": "https://arxiv.org/pdf/2505.11334", "abs": "https://arxiv.org/abs/2505.11334", "authors": ["Y. B. Wang", "S Wang", "J. N. Zhang", "J. F. Wu", "Q. D. He", "C. C. Fu", "C. J. Wang", "Y. Liu"], "title": "MARRS: Masked Autoregressive Unit-based Reaction Synthesis", "categories": ["cs.CV"], "comment": null, "summary": "This work aims at a challenging task: human action-reaction synthesis, i.e.,\ngenerating human reactions based on the action sequence of the other as\nconditions. Currently, autoregressive modeling approaches have achieved\nremarkable performance in motion generation tasks, e.g. text-to-motion.\nHowever, vector quantization (VQ) accompanying autoregressive generation has\ninherent disadvantages, including loss of quantization information, low\ncodebook utilization, etc. Moreover, unlike text-to-motion, which focuses\nsolely on the movement of body joints, human action-reaction synthesis also\nencompasses fine-grained hand movements. In this work, we propose MARRS, a\nnovel framework designed to generate coordinated and fine-grained reaction\nmotions in continuous representations. Initially, we present the\nUnit-distinguished Motion Variational AutoEncoder (UD-VAE), which segments the\nentire body into distinct body and hand units, encoding them independently.\nSubsequently, we propose Action-Conditioned Fusion (ACF), which involves\nrandomly masking a subset of reactive tokens and extracting specific\ninformation about the body and hands from the active tokens. Furthermore, we\nintroduce Adaptive Unit Modulation (AUM) to facilitate interaction between body\nand hand units by using the information from one unit to adaptively modulate\nthe other. Finally, for the diffusion model, we employ a compact MLP as a noise\npredictor for each distinct body unit and incorporate the diffusion loss to\nmodel the probability distribution of each token. Quantitative and qualitative\nresults demonstrate that our method achieves superior performance. The code\nwill be released upon acceptance."}
{"id": "2505.11344", "pdf": "https://arxiv.org/pdf/2505.11344", "abs": "https://arxiv.org/abs/2505.11344", "authors": ["Chenyu Huang", "Peng Ye", "Shenghe Zheng", "Xiaohui Wang", "Lei Bai", "Tao Chen", "Wanli Ouyang"], "title": "Dynamic Base model Shift for Delta Compression", "categories": ["cs.CV", "cs.LG"], "comment": "16 pages, 7 figures", "summary": "Transformer-based models with the pretrain-finetune paradigm bring about\nsignificant progress, along with the heavy storage and deployment costs of\nfinetuned models on multiple tasks. Delta compression attempts to lower the\ncosts by reducing the redundancy of delta parameters (i.e., the difference\nbetween the finetuned and pre-trained model weights) through pruning or\nquantization. However, existing methods by default employ the pretrained model\nas the base model and compress the delta parameters for every task, which may\ncauses significant performance degradation, especially when the compression\nrate is extremely high. To tackle this issue, we investigate the impact of\ndifferent base models on the performance of delta compression and find that the\npre-trained base model can hardly be optimal. To this end, we propose Dynamic\nBase Model Shift (DBMS), which dynamically adapts the base model to the target\ntask before performing delta compression. Specifically, we adjust two\nparameters, which respectively determine the magnitude of the base model shift\nand the overall scale of delta compression, to boost the compression\nperformance on each task. Through low-cost learning of these two parameters,\nour DBMS can maintain most of the finetuned model's performance even under an\nextremely high compression ratio setting, significantly surpassing existing\nmethods. Moreover, our DBMS is orthogonal and can be integrated with a variety\nof other methods, and it has been evaluated across different types of models\nincluding language, vision transformer, and multi-modal models."}
{"id": "2505.11383", "pdf": "https://arxiv.org/pdf/2505.11383", "abs": "https://arxiv.org/abs/2505.11383", "authors": ["Zihan Wang", "Seungjun Lee", "Gim Hee Lee"], "title": "Dynam3D: Dynamic Layered 3D Tokens Empower VLM for Vision-and-Language Navigation", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Vision-and-Language Navigation (VLN) is a core task where embodied agents\nleverage their spatial mobility to navigate in 3D environments toward\ndesignated destinations based on natural language instructions. Recently,\nvideo-language large models (Video-VLMs) with strong generalization\ncapabilities and rich commonsense knowledge have shown remarkable performance\nwhen applied to VLN tasks. However, these models still encounter the following\nchallenges when applied to real-world 3D navigation: 1) Insufficient\nunderstanding of 3D geometry and spatial semantics; 2) Limited capacity for\nlarge-scale exploration and long-term environmental memory; 3) Poor\nadaptability to dynamic and changing environments.To address these limitations,\nwe propose Dynam3D, a dynamic layered 3D representation model that leverages\nlanguage-aligned, generalizable, and hierarchical 3D representations as visual\ninput to train 3D-VLM in navigation action prediction. Given posed RGB-D\nimages, our Dynam3D projects 2D CLIP features into 3D space and constructs\nmulti-level 3D patch-instance-zone representations for 3D geometric and\nsemantic understanding with a dynamic and layer-wise update strategy. Our\nDynam3D is capable of online encoding and localization of 3D instances, and\ndynamically updates them in changing environments to provide large-scale\nexploration and long-term memory capabilities for navigation. By leveraging\nlarge-scale 3D-language pretraining and task-specific adaptation, our Dynam3D\nsets new state-of-the-art performance on VLN benchmarks including R2R-CE,\nREVERIE-CE and NavRAG-CE under monocular settings. Furthermore, experiments for\npre-exploration, lifelong memory, and real-world robot validate the\neffectiveness of practical deployment."}
{"id": "2505.11386", "pdf": "https://arxiv.org/pdf/2505.11386", "abs": "https://arxiv.org/abs/2505.11386", "authors": ["Zifan Wang", "Jingwei Li", "Yitang Li", "Yunze Liu"], "title": "MutualNeRF: Improve the Performance of NeRF under Limited Samples with Mutual Information Theory", "categories": ["cs.CV"], "comment": null, "summary": "This paper introduces MutualNeRF, a framework enhancing Neural Radiance Field\n(NeRF) performance under limited samples using Mutual Information Theory. While\nNeRF excels in 3D scene synthesis, challenges arise with limited data and\nexisting methods that aim to introduce prior knowledge lack theoretical support\nin a unified framework. We introduce a simple but theoretically robust concept,\nMutual Information, as a metric to uniformly measure the correlation between\nimages, considering both macro (semantic) and micro (pixel) levels.\n  For sparse view sampling, we strategically select additional viewpoints\ncontaining more non-overlapping scene information by minimizing mutual\ninformation without knowing ground truth images beforehand. Our framework\nemploys a greedy algorithm, offering a near-optimal solution.\n  For few-shot view synthesis, we maximize the mutual information between\ninferred images and ground truth, expecting inferred images to gain more\nrelevant information from known images. This is achieved by incorporating\nefficient, plug-and-play regularization terms.\n  Experiments under limited samples show consistent improvement over\nstate-of-the-art baselines in different settings, affirming the efficacy of our\nframework."}
{"id": "2505.11404", "pdf": "https://arxiv.org/pdf/2505.11404", "abs": "https://arxiv.org/abs/2505.11404", "authors": ["Wenchuan Zhang", "Penghao Zhang", "Jingru Guo", "Tao Cheng", "Jie Chen", "Shuwan Zhang", "Zhang Zhang", "Yuhao Yi", "Hong Bu"], "title": "Patho-R1: A Multimodal Reinforcement Learning-Based Pathology Expert Reasoner", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advances in vision language models (VLMs) have enabled broad progress\nin the general medical field. However, pathology still remains a more\nchallenging subdomain, with current pathology specific VLMs exhibiting\nlimitations in both diagnostic accuracy and reasoning plausibility. Such\nshortcomings are largely attributable to the nature of current pathology\ndatasets, which are primarily composed of image description pairs that lack the\ndepth and structured diagnostic paradigms employed by real world pathologists.\nIn this study, we leverage pathology textbooks and real world pathology experts\nto construct high-quality, reasoning-oriented datasets. Building on this, we\nintroduce Patho-R1, a multimodal RL-based pathology Reasoner, trained through a\nthree-stage pipeline: (1) continued pretraining on 3.5 million image-text pairs\nfor knowledge infusion; (2) supervised fine-tuning on 500k high-quality\nChain-of-Thought samples for reasoning incentivizing; (3) reinforcement\nlearning using Group Relative Policy Optimization and Decoupled Clip and\nDynamic sAmpling Policy Optimization strategies for multimodal reasoning\nquality refinement. To further assess the alignment quality of our dataset, we\npropose PathoCLIP, trained on the same figure-caption corpus used for continued\npretraining. Comprehensive experimental results demonstrate that both PathoCLIP\nand Patho-R1 achieve robust performance across a wide range of\npathology-related tasks, including zero-shot classification, cross-modal\nretrieval, Visual Question Answering, and Multiple Choice Question. Our project\nis available at the Patho-R1 repository:\nhttps://github.com/Wenchuan-Zhang/Patho-R1."}
{"id": "2505.11405", "pdf": "https://arxiv.org/pdf/2505.11405", "abs": "https://arxiv.org/abs/2505.11405", "authors": ["Bohao Xing", "Xin Liu", "Guoying Zhao", "Chengyu Liu", "Xiaolan Fu", "Heikki Kälviäinen"], "title": "EmotionHallucer: Evaluating Emotion Hallucinations in Multimodal Large Language Models", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Emotion understanding is a critical yet challenging task. Recent advances in\nMultimodal Large Language Models (MLLMs) have significantly enhanced their\ncapabilities in this area. However, MLLMs often suffer from hallucinations,\ngenerating irrelevant or nonsensical content. To the best of our knowledge,\ndespite the importance of this issue, there has been no dedicated effort to\nevaluate emotion-related hallucinations in MLLMs. In this work, we introduce\nEmotionHallucer, the first benchmark for detecting and analyzing emotion\nhallucinations in MLLMs. Unlike humans, whose emotion understanding stems from\nthe interplay of biology and social learning, MLLMs rely solely on data-driven\nlearning and lack innate emotional instincts. Fortunately, emotion psychology\nprovides a solid foundation of knowledge about human emotions. Building on\nthis, we assess emotion hallucinations from two dimensions: emotion psychology\nknowledge and real-world multimodal perception. To support robust evaluation,\nwe utilize an adversarial binary question-answer (QA) framework, which employs\ncarefully crafted basic and hallucinated pairs to assess the emotion\nhallucination tendencies of MLLMs. By evaluating 38 LLMs and MLLMs on\nEmotionHallucer, we reveal that: i) most current models exhibit substantial\nissues with emotion hallucinations; ii) closed-source models outperform\nopen-source ones in detecting emotion hallucinations, and reasoning capability\nprovides additional advantages; iii) existing models perform better in emotion\npsychology knowledge than in multimodal emotion perception. As a byproduct,\nthese findings inspire us to propose the PEP-MEK framework, which yields an\naverage improvement of 9.90% in emotion hallucination detection across selected\nmodels. Resources will be available at\nhttps://github.com/xxtars/EmotionHallucer."}
{"id": "2505.11424", "pdf": "https://arxiv.org/pdf/2505.11424", "abs": "https://arxiv.org/abs/2505.11424", "authors": ["Rana Poureskandar", "Shiva Razzagzadeh"], "title": "Improving Object Detection Performance through YOLOv8: A Comprehensive Training and Evaluation Study", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "This study evaluated the performance of a YOLOv8-based segmentation model for\ndetecting and segmenting wrinkles in facial images."}
{"id": "2505.11425", "pdf": "https://arxiv.org/pdf/2505.11425", "abs": "https://arxiv.org/abs/2505.11425", "authors": ["Michal Podstawski", "Malgorzata Kudelska", "Haohong Wang"], "title": "Face Consistency Benchmark for GenAI Video", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Video generation driven by artificial intelligence has advanced\nsignificantly, enabling the creation of dynamic and realistic content. However,\nmaintaining character consistency across video sequences remains a major\nchallenge, with current models struggling to ensure coherence in appearance and\nattributes. This paper introduces the Face Consistency Benchmark (FCB), a\nframework for evaluating and comparing the consistency of characters in\nAI-generated videos. By providing standardized metrics, the benchmark\nhighlights gaps in existing solutions and promotes the development of more\nreliable approaches. This work represents a crucial step toward improving\ncharacter consistency in AI video generation technologies."}
{"id": "2505.11439", "pdf": "https://arxiv.org/pdf/2505.11439", "abs": "https://arxiv.org/abs/2505.11439", "authors": ["Utsav Rai", "Haozheng Xu", "Stamatia Giannarou"], "title": "SurgPose: Generalisable Surgical Instrument Pose Estimation using Zero-Shot Learning and Stereo Vision", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": "To be published in 2025 International Conference on Robotics and\n  Automation (ICRA)", "summary": "Accurate pose estimation of surgical tools in Robot-assisted Minimally\nInvasive Surgery (RMIS) is essential for surgical navigation and robot control.\nWhile traditional marker-based methods offer accuracy, they face challenges\nwith occlusions, reflections, and tool-specific designs. Similarly, supervised\nlearning methods require extensive training on annotated datasets, limiting\ntheir adaptability to new tools. Despite their success in other domains,\nzero-shot pose estimation models remain unexplored in RMIS for pose estimation\nof surgical instruments, creating a gap in generalising to unseen surgical\ntools. This paper presents a novel 6 Degrees of Freedom (DoF) pose estimation\npipeline for surgical instruments, leveraging state-of-the-art zero-shot RGB-D\nmodels like the FoundationPose and SAM-6D. We advanced these models by\nincorporating vision-based depth estimation using the RAFT-Stereo method, for\nrobust depth estimation in reflective and textureless environments.\nAdditionally, we enhanced SAM-6D by replacing its instance segmentation module,\nSegment Anything Model (SAM), with a fine-tuned Mask R-CNN, significantly\nboosting segmentation accuracy in occluded and complex conditions. Extensive\nvalidation reveals that our enhanced SAM-6D surpasses FoundationPose in\nzero-shot pose estimation of unseen surgical instruments, setting a new\nbenchmark for zero-shot RGB-D pose estimation in RMIS. This work enhances the\ngeneralisability of pose estimation for unseen objects and pioneers the\napplication of RGB-D zero-shot methods in RMIS."}
{"id": "2505.11454", "pdf": "https://arxiv.org/pdf/2505.11454", "abs": "https://arxiv.org/abs/2505.11454", "authors": ["Shaina Raza", "Aravind Narayanan", "Vahid Reza Khazaie", "Ashmal Vayani", "Mukund S. Chettiar", "Amandeep Singh", "Mubarak Shah", "Deval Pandya"], "title": "HumaniBench: A Human-Centric Framework for Large Multimodal Models Evaluation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Large multimodal models (LMMs) now excel on many vision language benchmarks,\nhowever, they still struggle with human centered criteria such as fairness,\nethics, empathy, and inclusivity, key to aligning with human values. We\nintroduce HumaniBench, a holistic benchmark of 32K real-world image question\npairs, annotated via a scalable GPT4o assisted pipeline and exhaustively\nverified by domain experts. HumaniBench evaluates seven Human Centered AI\n(HCAI) principles: fairness, ethics, understanding, reasoning, language\ninclusivity, empathy, and robustness, across seven diverse tasks, including\nopen and closed ended visual question answering (VQA), multilingual QA, visual\ngrounding, empathetic captioning, and robustness tests. Benchmarking 15 state\nof the art LMMs (open and closed source) reveals that proprietary models\ngenerally lead, though robustness and visual grounding remain weak points. Some\nopen-source models also struggle to balance accuracy with adherence to\nhuman-aligned principles. HumaniBench is the first benchmark purpose built\naround HCAI principles. It provides a rigorous testbed for diagnosing alignment\ngaps and guiding LMMs toward behavior that is both accurate and socially\nresponsible. Dataset, annotation prompts, and evaluation code are available at:\nhttps://vectorinstitute.github.io/HumaniBench"}
{"id": "2505.11468", "pdf": "https://arxiv.org/pdf/2505.11468", "abs": "https://arxiv.org/abs/2505.11468", "authors": ["Dingbang Huang", "Wenbo Li", "Yifei Zhao", "Xinyu Pan", "Yanhong Zeng", "Bo Dai"], "title": "PSDiffusion: Harmonized Multi-Layer Image Generation via Layout and Appearance Alignment", "categories": ["cs.CV"], "comment": "Project Page: https://github.com/dingbang777/PSDiffusion/", "summary": "Diffusion models have made remarkable advancements in generating high-quality\nimages from textual descriptions. Recent works like LayerDiffuse have extended\nthe previous single-layer, unified image generation paradigm to transparent\nimage layer generation. However, existing multi-layer generation methods fail\nto handle the interactions among multiple layers such as rational global\nlayout, physics-plausible contacts and visual effects like shadows and\nreflections while maintaining high alpha quality. To solve this problem, we\npropose PSDiffusion, a unified diffusion framework for simultaneous multi-layer\ntext-to-image generation. Our model can automatically generate multi-layer\nimages with one RGB background and multiple RGBA foregrounds through a single\nfeed-forward process. Unlike existing methods that combine multiple tools for\npost-decomposition or generate layers sequentially and separately, our method\nintroduces a global-layer interactive mechanism that generates layered-images\nconcurrently and collaboratively, ensuring not only high quality and\ncompleteness for each layer, but also spatial and visual interactions among\nlayers for global coherence."}
{"id": "2505.11482", "pdf": "https://arxiv.org/pdf/2505.11482", "abs": "https://arxiv.org/abs/2505.11482", "authors": ["Shirin Shoushtari", "Edward P. Chandler", "Yuanhao Wang", "M. Salman Asif", "Ulugbek S. Kamilov"], "title": "Unsupervised Detection of Distribution Shift in Inverse Problems using Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion models are widely used as priors in imaging inverse problems.\nHowever, their performance often degrades under distribution shifts between the\ntraining and test-time images. Existing methods for identifying and quantifying\ndistribution shifts typically require access to clean test images, which are\nalmost never available while solving inverse problems (at test time). We\npropose a fully unsupervised metric for estimating distribution shifts using\nonly indirect (corrupted) measurements and score functions from diffusion\nmodels trained on different datasets. We theoretically show that this metric\nestimates the KL divergence between the training and test image distributions.\nEmpirically, we show that our score-based metric, using only corrupted\nmeasurements, closely approximates the KL divergence computed from clean\nimages. Motivated by this result, we show that aligning the out-of-distribution\nscore with the in-distribution score -- using only corrupted measurements --\nreduces the KL divergence and leads to improved reconstruction quality across\nmultiple inverse problems."}
{"id": "2505.11493", "pdf": "https://arxiv.org/pdf/2505.11493", "abs": "https://arxiv.org/abs/2505.11493", "authors": ["Yusu Qian", "Jiasen Lu", "Tsu-Jui Fu", "Xinze Wang", "Chen Chen", "Yinfei Yang", "Wenze Hu", "Zhe Gan"], "title": "GIE-Bench: Towards Grounded Evaluation for Text-Guided Image Editing", "categories": ["cs.CV"], "comment": null, "summary": "Editing images using natural language instructions has become a natural and\nexpressive way to modify visual content; yet, evaluating the performance of\nsuch models remains challenging. Existing evaluation approaches often rely on\nimage-text similarity metrics like CLIP, which lack precision. In this work, we\nintroduce a new benchmark designed to evaluate text-guided image editing models\nin a more grounded manner, along two critical dimensions: (i) functional\ncorrectness, assessed via automatically generated multiple-choice questions\nthat verify whether the intended change was successfully applied; and (ii)\nimage content preservation, which ensures that non-targeted regions of the\nimage remain visually consistent using an object-aware masking technique and\npreservation scoring. The benchmark includes over 1000 high-quality editing\nexamples across 20 diverse content categories, each annotated with detailed\nediting instructions, evaluation questions, and spatial object masks. We\nconduct a large-scale study comparing GPT-Image-1, the latest flagship in the\ntext-guided image editing space, against several state-of-the-art editing\nmodels, and validate our automatic metrics against human ratings. Results show\nthat GPT-Image-1 leads in instruction-following accuracy, but often\nover-modifies irrelevant image regions, highlighting a key trade-off in the\ncurrent model behavior. GIE-Bench provides a scalable, reproducible framework\nfor advancing more accurate evaluation of text-guided image editing."}
{"id": "2505.11497", "pdf": "https://arxiv.org/pdf/2505.11497", "abs": "https://arxiv.org/abs/2505.11497", "authors": ["Yushi Huang", "Ruihao Gong", "Jing Liu", "Yifu Ding", "Chengtao Lv", "Haotong Qin", "Jun Zhang"], "title": "QVGen: Pushing the Limit of Quantized Video Generative Models", "categories": ["cs.CV"], "comment": "Our code will be released upon acceptance", "summary": "Video diffusion models (DMs) have enabled high-quality video synthesis. Yet,\ntheir substantial computational and memory demands pose serious challenges to\nreal-world deployment, even on high-end GPUs. As a commonly adopted solution,\nquantization has proven notable success in reducing cost for image DMs, while\nits direct application to video DMs remains ineffective. In this paper, we\npresent QVGen, a novel quantization-aware training (QAT) framework tailored for\nhigh-performance and inference-efficient video DMs under extremely low-bit\nquantization (e.g., 4-bit or below). We begin with a theoretical analysis\ndemonstrating that reducing the gradient norm is essential to facilitate\nconvergence for QAT. To this end, we introduce auxiliary modules ($\\Phi$) to\nmitigate large quantization errors, leading to significantly enhanced\nconvergence. To eliminate the inference overhead of $\\Phi$, we propose a\nrank-decay strategy that progressively eliminates $\\Phi$. Specifically, we\nrepeatedly employ singular value decomposition (SVD) and a proposed rank-based\nregularization $\\mathbf{\\gamma}$ to identify and decay low-contributing\ncomponents. This strategy retains performance while zeroing out inference\noverhead. Extensive experiments across $4$ state-of-the-art (SOTA) video DMs,\nwith parameter sizes ranging from $1.3$B $\\sim14$B, show that QVGen is the\nfirst to reach full-precision comparable quality under 4-bit settings.\nMoreover, it significantly outperforms existing methods. For instance, our\n3-bit CogVideoX-2B achieves improvements of $+25.28$ in Dynamic Degree and\n$+8.43$ in Scene Consistency on VBench."}
{"id": "2505.10577", "pdf": "https://arxiv.org/pdf/2505.10577", "abs": "https://arxiv.org/abs/2505.10577", "authors": ["Yutong Guo"], "title": "GRNN:Recurrent Neural Network based on Ghost Features for Video Super-Resolution", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "Accepted by 2023 IEEE International Conference on Multimedia and Expo\n  (ICME 2023)", "summary": "Modern video super-resolution (VSR) systems based on convolutional neural\nnetworks (CNNs) require huge computational costs. The problem of feature\nredundancy is present in most models in many domains, but is rarely discussed\nin VSR. We experimentally observe that many features in VSR models are also\nsimilar to each other, so we propose to use \"Ghost features\" to reduce this\nredundancy. We also analyze the so-called \"gradient disappearance\" phenomenon\ngenerated by the conventional recurrent convolutional network (RNN) model, and\ncombine the Ghost module with RNN to complete the modeling on time series. The\ncurrent frame is used as input to the model together with the next frame, the\noutput of the previous frame and the hidden state. Extensive experiments on\nseveral benchmark models and datasets show that the PSNR and SSIM of our\nproposed modality are improved to some extent. Some texture details in the\nvideo are also better preserved."}
{"id": "2505.10578", "pdf": "https://arxiv.org/pdf/2505.10578", "abs": "https://arxiv.org/abs/2505.10578", "authors": ["Yunji Feng", "Chengpu Yu", "Fengrui Ran", "Zhi Yang", "Yinni Liu"], "title": "ExploreGS: a vision-based low overhead framework for 3D scene reconstruction", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "This paper proposes a low-overhead, vision-based 3D scene reconstruction\nframework for drones, named ExploreGS. By using RGB images, ExploreGS replaces\ntraditional lidar-based point cloud acquisition process with a vision model,\nachieving a high-quality reconstruction at a lower cost. The framework\nintegrates scene exploration and model reconstruction, and leverags a\nBag-of-Words(BoW) model to enable real-time processing capabilities, therefore,\nthe 3D Gaussian Splatting (3DGS) training can be executed on-board.\nComprehensive experiments in both simulation and real-world environments\ndemonstrate the efficiency and applicability of the ExploreGS framework on\nresource-constrained devices, while maintaining reconstruction quality\ncomparable to state-of-the-art methods."}
{"id": "2505.10672", "pdf": "https://arxiv.org/pdf/2505.10672", "abs": "https://arxiv.org/abs/2505.10672", "authors": ["Hania Ghouse", "Muzammil Behzad"], "title": "MOSAIC: A Multi-View 2.5D Organ Slice Selector with Cross-Attentional Reasoning for Anatomically-Aware CT Localization in Medical Organ Segmentation", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Efficient and accurate multi-organ segmentation from abdominal CT volumes is\na fundamental challenge in medical image analysis. Existing 3D segmentation\napproaches are computationally and memory intensive, often processing entire\nvolumes that contain many anatomically irrelevant slices. Meanwhile, 2D methods\nsuffer from class imbalance and lack cross-view contextual awareness. To\naddress these limitations, we propose a novel, anatomically-aware slice\nselector pipeline that reduces input volume prior to segmentation. Our unified\nframework introduces a vision-language model (VLM) for cross-view organ\npresence detection using fused tri-slice (2.5D) representations from axial,\nsagittal, and coronal planes. Our proposed model acts as an \"expert\" in\nanatomical localization, reasoning over multi-view representations to\nselectively retain slices with high structural relevance. This enables\nspatially consistent filtering across orientations while preserving contextual\ncues. More importantly, since standard segmentation metrics such as Dice or IoU\nfail to measure the spatial precision of such slice selection, we introduce a\nnovel metric, Slice Localization Concordance (SLC), which jointly captures\nanatomical coverage and spatial alignment with organ-centric reference slices.\nUnlike segmentation-specific metrics, SLC provides a model-agnostic evaluation\nof localization fidelity. Our model offers substantial improvement gains\nagainst several baselines across all organs, demonstrating both accurate and\nreliable organ-focused slice filtering. These results show that our method\nenables efficient and spatially consistent organ filtering, thereby\nsignificantly reducing downstream segmentation cost while maintaining high\nanatomical fidelity."}
{"id": "2505.10687", "pdf": "https://arxiv.org/pdf/2505.10687", "abs": "https://arxiv.org/abs/2505.10687", "authors": ["Sayed Mehedi Azim", "Brian Corbett", "Iman Dehzangi"], "title": "ROIsGAN: A Region Guided Generative Adversarial Framework for Murine Hippocampal Subregion Segmentation", "categories": ["eess.IV", "cs.CV", "cs.LG", "q-bio.NC"], "comment": null, "summary": "The hippocampus, a critical brain structure involved in memory processing and\nvarious neurodegenerative and psychiatric disorders, comprises three key\nsubregions: the dentate gyrus (DG), Cornu Ammonis 1 (CA1), and Cornu Ammonis 3\n(CA3). Accurate segmentation of these subregions from histological tissue\nimages is essential for advancing our understanding of disease mechanisms,\ndevelopmental dynamics, and therapeutic interventions. However, no existing\nmethods address the automated segmentation of hippocampal subregions from\ntissue images, particularly from immunohistochemistry (IHC) images. To bridge\nthis gap, we introduce a novel set of four comprehensive murine hippocampal IHC\ndatasets featuring distinct staining modalities: cFos, NeuN, and multiplexed\nstains combining cFos, NeuN, and either {\\Delta}FosB or GAD67, capturing\nstructural, neuronal activity, and plasticity associated information.\nAdditionally, we propose ROIsGAN, a region-guided U-Net-based generative\nadversarial network tailored for hippocampal subregion segmentation. By\nleveraging adversarial learning, ROIsGAN enhances boundary delineation and\nstructural detail refinement through a novel region-guided discriminator loss\ncombining Dice and binary cross-entropy loss. Evaluated across DG, CA1, and CA3\nsubregions, ROIsGAN consistently outperforms conventional segmentation models,\nachieving performance gains ranging from 1-10% in Dice score and up to 11% in\nIntersection over Union (IoU), particularly under challenging staining\nconditions. Our work establishes foundational datasets and methods for\nautomated hippocampal segmentation, enabling scalable, high-precision analysis\nof tissue images in neuroscience research. Our generated datasets, proposed\nmodel as a standalone tool, and its corresponding source code are publicly\navailable at: https://github.com/MehediAzim/ROIsGAN"}
{"id": "2505.10689", "pdf": "https://arxiv.org/pdf/2505.10689", "abs": "https://arxiv.org/abs/2505.10689", "authors": ["Gabriele Santini", "Francesco Paissan", "Elisabetta Farella"], "title": "A probabilistic framework for dynamic quantization", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "We propose a probabilistic framework for dynamic quantization of neural\nnetworks that allows for a computationally efficient input-adaptive rescaling\nof the quantization parameters. Our framework applies a probabilistic model to\nthe network's pre-activations through a lightweight surrogate, enabling the\nadaptive adjustment of the quantization parameters on a per-input basis without\nsignificant memory overhead. We validate our approach on a set of popular\ncomputer vision tasks and models, observing only a negligible loss in\nperformance. Our method strikes the best performance and computational overhead\ntradeoff compared to standard quantization strategies."}
{"id": "2505.10691", "pdf": "https://arxiv.org/pdf/2505.10691", "abs": "https://arxiv.org/abs/2505.10691", "authors": ["Wanying Dou", "Gorkem Durak", "Koushik Biswas", "Ziliang Hong", "Andrea Mia Bejar", "Elif Keles", "Kaan Akin", "Sukru Mehmet Erturk", "Alpay Medetalibeyoglu", "Marc Sala", "Alexander Misharin", "Hatice Savas", "Mary Salvatore", "Sachin Jambawalikar", "Drew Torigian", "Jayaram K. Udupa", "Ulas Bagci"], "title": "Predicting Risk of Pulmonary Fibrosis Formation in PASC Patients", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "While the acute phase of the COVID-19 pandemic has subsided, its long-term\neffects persist through Post-Acute Sequelae of COVID-19 (PASC), commonly known\nas Long COVID. There remains substantial uncertainty regarding both its\nduration and optimal management strategies. PASC manifests as a diverse array\nof persistent or newly emerging symptoms--ranging from fatigue, dyspnea, and\nneurologic impairments (e.g., brain fog), to cardiovascular, pulmonary, and\nmusculoskeletal abnormalities--that extend beyond the acute infection phase.\nThis heterogeneous presentation poses substantial challenges for clinical\nassessment, diagnosis, and treatment planning. In this paper, we focus on\nimaging findings that may suggest fibrotic damage in the lungs, a critical\nmanifestation characterized by scarring of lung tissue, which can potentially\naffect long-term respiratory function in patients with PASC. This study\nintroduces a novel multi-center chest CT analysis framework that combines deep\nlearning and radiomics for fibrosis prediction. Our approach leverages\nconvolutional neural networks (CNNs) and interpretable feature extraction,\nachieving 82.2% accuracy and 85.5% AUC in classification tasks. We demonstrate\nthe effectiveness of Grad-CAM visualization and radiomics-based feature\nanalysis in providing clinically relevant insights for PASC-related lung\nfibrosis prediction. Our findings highlight the potential of deep\nlearning-driven computational methods for early detection and risk assessment\nof PASC-related lung fibrosis--presented for the first time in the literature."}
{"id": "2505.10696", "pdf": "https://arxiv.org/pdf/2505.10696", "abs": "https://arxiv.org/abs/2505.10696", "authors": ["Manthan Patel", "Fan Yang", "Yuheng Qiu", "Cesar Cadena", "Sebastian Scherer", "Marco Hutter", "Wenshan Wang"], "title": "TartanGround: A Large-Scale Dataset for Ground Robot Perception and Navigation", "categories": ["cs.RO", "cs.CV"], "comment": "Under review for IEEE conference", "summary": "We present TartanGround, a large-scale, multi-modal dataset to advance the\nperception and autonomy of ground robots operating in diverse environments.\nThis dataset, collected in various photorealistic simulation environments\nincludes multiple RGB stereo cameras for 360-degree coverage, along with depth,\noptical flow, stereo disparity, LiDAR point clouds, ground truth poses,\nsemantic segmented images, and occupancy maps with semantic labels. Data is\ncollected using an integrated automatic pipeline, which generates trajectories\nmimicking the motion patterns of various ground robot platforms, including\nwheeled and legged robots. We collect 910 trajectories across 70 environments,\nresulting in 1.5 million samples. Evaluations on occupancy prediction and SLAM\ntasks reveal that state-of-the-art methods trained on existing datasets\nstruggle to generalize across diverse scenes. TartanGround can serve as a\ntestbed for training and evaluation of a broad range of learning-based tasks,\nincluding occupancy prediction, SLAM, neural scene representation,\nperception-based navigation, and more, enabling advancements in robotic\nperception and autonomy towards achieving robust models generalizable to more\ndiverse scenarios. The dataset and codebase for data collection will be made\npublicly available upon acceptance. Webpage: https://tartanair.org/tartanground"}
{"id": "2505.10729", "pdf": "https://arxiv.org/pdf/2505.10729", "abs": "https://arxiv.org/abs/2505.10729", "authors": ["NingFeng Que", "Xiaofei Wang", "Jingjing Chen", "Yixuan Jiang", "Chao Li"], "title": "Adaptive Spatial Transcriptomics Interpolation via Cross-modal Cross-slice Modeling", "categories": ["eess.IV", "cs.CV", "q-bio.QM"], "comment": "Early accepted by MICCAI 2025", "summary": "Spatial transcriptomics (ST) is a promising technique that characterizes the\nspatial gene profiling patterns within the tissue context. Comprehensive ST\nanalysis depends on consecutive slices for 3D spatial insights, whereas the\nmissing intermediate tissue sections and high costs limit the practical\nfeasibility of generating multi-slice ST. In this paper, we propose C2-STi, the\nfirst attempt for interpolating missing ST slices at arbitrary intermediate\npositions between adjacent ST slices. Despite intuitive, effective ST\ninterpolation presents significant challenges, including 1) limited continuity\nacross heterogeneous tissue sections, 2) complex intrinsic correlation across\ngenes, and 3) intricate cellular structures and biological semantics within\neach tissue section. To mitigate these challenges, in C2-STi, we design 1) a\ndistance-aware local structural modulation module to adaptively capture\ncross-slice deformations and enhance positional correlations between ST slices,\n2) a pyramid gene co-expression correlation module to capture multi-scale\nbiological associations among genes, and 3) a cross-modal alignment module that\nintegrates the ST-paired hematoxylin and eosin (H&E)-stained images to filter\nand align the essential cellular features across ST and H\\&E images. Extensive\nexperiments on the public dataset demonstrate our superiority over\nstate-of-the-art approaches on both single-slice and multi-slice ST\ninterpolation. Codes are available at\nhttps://github.com/XiaofeiWang2018/C2-STi."}
{"id": "2505.10824", "pdf": "https://arxiv.org/pdf/2505.10824", "abs": "https://arxiv.org/abs/2505.10824", "authors": ["Kaifa Yang", "Qi Yang", "Zhu Li", "Yiling Xu"], "title": "Textured mesh Quality Assessment using Geometry and Color Field Similarity", "categories": ["cs.GR", "cs.CV", "cs.MM"], "comment": "15 pages main content, 4 pages supplementary material. Submitted to\n  IEEE Transactions on Visualization and Computer Graphics (IEEE TVCG) for\n  review", "summary": "Textured mesh quality assessment (TMQA) is critical for various 3D mesh\napplications. However, existing TMQA methods often struggle to provide accurate\nand robust evaluations. Motivated by the effectiveness of fields in\nrepresenting both 3D geometry and color information, we propose a novel\npoint-based TMQA method called field mesh quality metric (FMQM). FMQM utilizes\nsigned distance fields and a newly proposed color field named nearest surface\npoint color field to realize effective mesh feature description. Four features\nrelated to visual perception are extracted from the geometry and color fields:\ngeometry similarity, geometry gradient similarity, space color distribution\nsimilarity, and space color gradient similarity. Experimental results on three\nbenchmark datasets demonstrate that FMQM outperforms state-of-the-art (SOTA)\nTMQA metrics. Furthermore, FMQM exhibits low computational complexity, making\nit a practical and efficient solution for real-world applications in 3D\ngraphics and visualization. Our code is publicly available at:\nhttps://github.com/yyyykf/FMQM."}
{"id": "2505.10836", "pdf": "https://arxiv.org/pdf/2505.10836", "abs": "https://arxiv.org/abs/2505.10836", "authors": ["Abhishek Dey", "Aabha Bothera", "Samhita Sarikonda", "Rishav Aryan", "Sanjay Kumar Podishetty", "Akshay Havalgi", "Gaurav Singh", "Saurabh Srivastava"], "title": "Multimodal Event Detection: Current Approaches and Defining the New Playground through LLMs and VLMs", "categories": ["cs.CL", "cs.CV"], "comment": "Accepted at NLDB 2025", "summary": "In this paper, we study the challenges of detecting events on social media,\nwhere traditional unimodal systems struggle due to the rapid and multimodal\nnature of data dissemination. We employ a range of models, including unimodal\nModernBERT and ConvNeXt-V2, multimodal fusion techniques, and advanced\ngenerative models like GPT-4o, and LLaVA. Additionally, we also study the\neffect of providing multimodal generative models (such as GPT-4o) with a single\nmodality to assess their efficacy. Our results indicate that while multimodal\napproaches notably outperform unimodal counterparts, generative approaches\ndespite having a large number of parameters, lag behind supervised methods in\nprecision. Furthermore, we also found that they lag behind instruction-tuned\nmodels because of their inability to generate event classes correctly. During\nour error analysis, we discovered that common social media issues such as leet\nspeak, text elongation, etc. are effectively handled by generative approaches\nbut are hard to tackle using supervised approaches."}
{"id": "2505.10855", "pdf": "https://arxiv.org/pdf/2505.10855", "abs": "https://arxiv.org/abs/2505.10855", "authors": ["Aneesh Rangnekar", "Nikhil Mankuzhy", "Jonas Willmann", "Chloe Choi", "Abraham Wu", "Maria Thor", "Andreas Rimner", "Harini Veeraraghavan"], "title": "Pretrained hybrid transformer for generalizable cardiac substructures segmentation from contrast and non-contrast CTs in lung and breast cancers", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "AI automated segmentations for radiation treatment planning (RTP) can\ndeteriorate when applied in clinical cases with different characteristics than\ntraining dataset. Hence, we refined a pretrained transformer into a hybrid\ntransformer convolutional network (HTN) to segment cardiac substructures lung\nand breast cancer patients acquired with varying imaging contrasts and patient\nscan positions. Cohort I, consisting of 56 contrast-enhanced (CECT) and 124\nnon-contrast CT (NCCT) scans from patients with non-small cell lung cancers\nacquired in supine position, was used to create oracle with all 180 training\ncases and balanced (CECT: 32, NCCT: 32 training) HTN models. Models were\nevaluated on a held-out validation set of 60 cohort I patients and 66 patients\nwith breast cancer from cohort II acquired in supine (n=45) and prone (n=21)\npositions. Accuracy was measured using DSC, HD95, and dose metrics. Publicly\navailable TotalSegmentator served as the benchmark. The oracle and balanced\nmodels were similarly accurate (DSC Cohort I: 0.80 \\pm 0.10 versus 0.81 \\pm\n0.10; Cohort II: 0.77 \\pm 0.13 versus 0.80 \\pm 0.12), outperforming\nTotalSegmentator. The balanced model, using half the training cases as oracle,\nproduced similar dose metrics as manual delineations for all cardiac\nsubstructures. This model was robust to CT contrast in 6 out of 8 substructures\nand patient scan position variations in 5 out of 8 substructures and showed low\ncorrelations of accuracy to patient size and age. A HTN demonstrated robustly\naccurate (geometric and dose metrics) cardiac substructures segmentation from\nCTs with varying imaging and patient characteristics, one key requirement for\nclinical use. Moreover, the model combining pretraining with balanced\ndistribution of NCCT and CECT scans was able to provide reliably accurate\nsegmentations under varied conditions with far fewer labeled datasets compared\nto an oracle model."}
{"id": "2505.10873", "pdf": "https://arxiv.org/pdf/2505.10873", "abs": "https://arxiv.org/abs/2505.10873", "authors": ["Filippo Leveni", "Luca Magri", "Cesare Alippi", "Giacomo Boracchi"], "title": "Hashing for Structure-based Anomaly Detection", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "comment": "Accepted at International Conference on Image Analysis and Processing\n  (ICIAP 2023)", "summary": "We focus on the problem of identifying samples in a set that do not conform\nto structured patterns represented by low-dimensional manifolds. An effective\nway to solve this problem is to embed data in a high dimensional space, called\nPreference Space, where anomalies can be identified as the most isolated\npoints. In this work, we employ Locality Sensitive Hashing to avoid explicit\ncomputation of distances in high dimensions and thus improve Anomaly Detection\nefficiency. Specifically, we present an isolation-based anomaly detection\ntechnique designed to work in the Preference Space which achieves\nstate-of-the-art performance at a lower computational cost. Code is publicly\navailable at\nhttps://github.com/ineveLoppiliF/Hashing-for-Structure-based-Anomaly-Detection."}
{"id": "2505.10874", "pdf": "https://arxiv.org/pdf/2505.10874", "abs": "https://arxiv.org/abs/2505.10874", "authors": ["Luca Magri", "Filippo Leveni", "Giacomo Boracchi"], "title": "MultiLink: Multi-class Structure Recovery via Agglomerative Clustering and Model Selection", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "Accepted at Computer Vision and Pattern Recognition (CVPR 2021)", "summary": "We address the problem of recovering multiple structures of different classes\nin a dataset contaminated by noise and outliers. In particular, we consider\ngeometric structures defined by a mixture of underlying parametric models (e.g.\nplanes and cylinders, homographies and fundamental matrices), and we tackle the\nrobust fitting problem by preference analysis and clustering. We present a new\nalgorithm, termed MultiLink, that simultaneously deals with multiple classes of\nmodels. MultiLink combines on-the-fly model fitting and model selection in a\nnovel linkage scheme that determines whether two clusters are to be merged. The\nresulting method features many practical advantages with respect to methods\nbased on preference analysis, being faster, less sensitive to the inlier\nthreshold, and able to compensate limitations deriving from hypotheses\nsampling. Experiments on several public datasets demonstrate that Multi-Link\nfavourably compares with state of the art alternatives, both in multi-class and\nsingle-class problems. Code is publicly made available for download."}
{"id": "2505.10876", "pdf": "https://arxiv.org/pdf/2505.10876", "abs": "https://arxiv.org/abs/2505.10876", "authors": ["Filippo Leveni", "Luca Magri", "Cesare Alippi", "Giacomo Boracchi"], "title": "Preference Isolation Forest for Structure-based Anomaly Detection", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "comment": "Submitted to Pattern Recognition", "summary": "We address the problem of detecting anomalies as samples that do not conform\nto structured patterns represented by low-dimensional manifolds. To this end,\nwe conceive a general anomaly detection framework called Preference Isolation\nForest (PIF), that combines the benefits of adaptive isolation-based methods\nwith the flexibility of preference embedding. The key intuition is to embed the\ndata into a high-dimensional preference space by fitting low-dimensional\nmanifolds, and to identify anomalies as isolated points. We propose three\nisolation approaches to identify anomalies: $i$) Voronoi-iForest, the most\ngeneral solution, $ii$) RuzHash-iForest, that avoids explicit computation of\ndistances via Local Sensitive Hashing, and $iii$) Sliding-PIF, that leverages a\nlocality prior to improve efficiency and effectiveness."}
{"id": "2505.10894", "pdf": "https://arxiv.org/pdf/2505.10894", "abs": "https://arxiv.org/abs/2505.10894", "authors": ["Yishuo Wang", "Feng Zhou", "Muping Zhou", "Qicheng Meng", "Zhijun Hu", "Yi Wang"], "title": "CTP: A hybrid CNN-Transformer-PINN model for ocean front forecasting", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "This paper proposes CTP, a novel deep learning framework that integrates\nconvolutional neural network(CNN), Transformer architectures, and\nphysics-informed neural network(PINN) for ocean front prediction. Ocean fronts,\nas dynamic interfaces between distinct water masses, play critical roles in\nmarine biogeochemical and physical processes. Existing methods such as LSTM,\nConvLSTM, and AttentionConv often struggle to maintain spatial continuity and\nphysical consistency over multi-step forecasts. CTP addresses these challenges\nby combining localized spatial encoding, long-range temporal attention, and\nphysical constraint enforcement. Experimental results across south China\nsea(SCS) and Kuroshio(KUR) regions from 1993 to 2020 demonstrate that CTP\nachieves state-of-the-art(SOTA) performance in both single-step and multi-step\npredictions, significantly outperforming baseline models in accuracy, $F_1$\nscore, and temporal stability."}
{"id": "2505.10923", "pdf": "https://arxiv.org/pdf/2505.10923", "abs": "https://arxiv.org/abs/2505.10923", "authors": ["Simeon Adebola", "Shuangyu Xie", "Chung Min Kim", "Justin Kerr", "Bart M. van Marrewijk", "Mieke van Vlaardingen", "Tim van Daalen", "Robert van Loo", "Jose Luis Susa Rincon", "Eugen Solowjow", "Rick van de Zedde", "Ken Goldberg"], "title": "GrowSplat: Constructing Temporal Digital Twins of Plants with Gaussian Splats", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Accurate temporal reconstructions of plant growth are essential for plant\nphenotyping and breeding, yet remain challenging due to complex geometries,\nocclusions, and non-rigid deformations of plants. We present a novel framework\nfor building temporal digital twins of plants by combining 3D Gaussian\nSplatting with a robust sample alignment pipeline. Our method begins by\nreconstructing Gaussian Splats from multi-view camera data, then leverages a\ntwo-stage registration approach: coarse alignment through feature-based\nmatching and Fast Global Registration, followed by fine alignment with\nIterative Closest Point. This pipeline yields a consistent 4D model of plant\ndevelopment in discrete time steps. We evaluate the approach on data from the\nNetherlands Plant Eco-phenotyping Center, demonstrating detailed temporal\nreconstructions of Sequoia and Quinoa species. Videos and Images can be seen at\nhttps://berkeleyautomation.github.io/GrowSplat/"}
{"id": "2505.10924", "pdf": "https://arxiv.org/pdf/2505.10924", "abs": "https://arxiv.org/abs/2505.10924", "authors": ["Ada Chen", "Yongjiang Wu", "Junyuan Zhang", "Shu Yang", "Jen-tse Huang", "Kun Wang", "Wenxuan Wang", "Shuai Wang"], "title": "A Survey on the Safety and Security Threats of Computer-Using Agents: JARVIS or Ultron?", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.CV", "cs.SE"], "comment": null, "summary": "Recently, AI-driven interactions with computing devices have advanced from\nbasic prototype tools to sophisticated, LLM-based systems that emulate\nhuman-like operations in graphical user interfaces. We are now witnessing the\nemergence of \\emph{Computer-Using Agents} (CUAs), capable of autonomously\nperforming tasks such as navigating desktop applications, web pages, and mobile\napps. However, as these agents grow in capability, they also introduce novel\nsafety and security risks. Vulnerabilities in LLM-driven reasoning, with the\nadded complexity of integrating multiple software components and multimodal\ninputs, further complicate the security landscape. In this paper, we present a\nsystematization of knowledge on the safety and security threats of CUAs. We\nconduct a comprehensive literature review and distill our findings along four\nresearch objectives: \\textit{\\textbf{(i)}} define the CUA that suits safety\nanalysis; \\textit{\\textbf{(ii)} } categorize current safety threats among CUAs;\n\\textit{\\textbf{(iii)}} propose a comprehensive taxonomy of existing defensive\nstrategies; \\textit{\\textbf{(iv)}} summarize prevailing benchmarks, datasets,\nand evaluation metrics used to assess the safety and performance of CUAs.\nBuilding on these insights, our work provides future researchers with a\nstructured foundation for exploring unexplored vulnerabilities and offers\npractitioners actionable guidance in designing and deploying secure\nComputer-Using Agents."}
{"id": "2505.10993", "pdf": "https://arxiv.org/pdf/2505.10993", "abs": "https://arxiv.org/abs/2505.10993", "authors": ["Yuan Zhang", "Xinfeng Zhang", "Xiaoming Qi Xinyu Wu", "Feng Chen", "Guanyu Yang", "Huazhu Fu"], "title": "Generative Models in Computational Pathology: A Comprehensive Survey on Methods, Applications, and Challenges", "categories": ["eess.IV", "cs.CV"], "comment": "18 pages,9 figures", "summary": "Generative modeling has emerged as a promising direction in computational\npathology, offering capabilities such as data-efficient learning, synthetic\ndata augmentation, and multimodal representation across diverse diagnostic\ntasks. This review provides a comprehensive synthesis of recent progress in the\nfield, organized into four key domains: image generation, text generation,\nmultimodal image-text generation, and other generative applications, including\nspatial simulation and molecular inference. By analyzing over 150\nrepresentative studies, we trace the evolution of generative architectures from\nearly generative adversarial networks to recent advances in diffusion models\nand foundation models with generative capabilities. We further examine the\ndatasets and evaluation protocols commonly used in this domain and highlight\nongoing limitations, including challenges in generating high-fidelity whole\nslide images, clinical interpretability, and concerns related to the ethical\nand legal implications of synthetic data. The review concludes with a\ndiscussion of open challenges and prospective research directions, with an\nemphasis on developing unified, multimodal, and clinically deployable\ngenerative systems. This work aims to provide a foundational reference for\nresearchers and practitioners developing and applying generative models in\ncomputational pathology."}
{"id": "2505.11032", "pdf": "https://arxiv.org/pdf/2505.11032", "abs": "https://arxiv.org/abs/2505.11032", "authors": ["Yuran Wang", "Ruihai Wu", "Yue Chen", "Jiarui Wang", "Jiaqi Liang", "Ziyu Zhu", "Haoran Geng", "Jitendra Malik", "Pieter Abbeel", "Hao Dong"], "title": "DexGarmentLab: Dexterous Garment Manipulation Environment with Generalizable Policy", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "Garment manipulation is a critical challenge due to the diversity in garment\ncategories, geometries, and deformations. Despite this, humans can effortlessly\nhandle garments, thanks to the dexterity of our hands. However, existing\nresearch in the field has struggled to replicate this level of dexterity,\nprimarily hindered by the lack of realistic simulations of dexterous garment\nmanipulation. Therefore, we propose DexGarmentLab, the first environment\nspecifically designed for dexterous (especially bimanual) garment manipulation,\nwhich features large-scale high-quality 3D assets for 15 task scenarios, and\nrefines simulation techniques tailored for garment modeling to reduce the\nsim-to-real gap. Previous data collection typically relies on teleoperation or\ntraining expert reinforcement learning (RL) policies, which are labor-intensive\nand inefficient. In this paper, we leverage garment structural correspondence\nto automatically generate a dataset with diverse trajectories using only a\nsingle expert demonstration, significantly reducing manual intervention.\nHowever, even extensive demonstrations cannot cover the infinite states of\ngarments, which necessitates the exploration of new algorithms. To improve\ngeneralization across diverse garment shapes and deformations, we propose a\nHierarchical gArment-manipuLation pOlicy (HALO). It first identifies\ntransferable affordance points to accurately locate the manipulation area, then\ngenerates generalizable trajectories to complete the task. Through extensive\nexperiments and detailed analysis of our method and baseline, we demonstrate\nthat HALO consistently outperforms existing methods, successfully generalizing\nto previously unseen instances even with significant variations in shape and\ndeformation where others fail. Our project page is available at:\nhttps://wayrise.github.io/DexGarmentLab/."}
{"id": "2505.11067", "pdf": "https://arxiv.org/pdf/2505.11067", "abs": "https://arxiv.org/abs/2505.11067", "authors": ["Omobayode Fagbohungbe", "Corey Lammie", "Malte J. Rasch", "Takashi Ando", "Tayfun Gokmen", "Vijay Narayanan"], "title": "Assessing the Performance of Analog Training for Transfer Learning", "categories": ["cs.LG", "cs.AI", "cs.AR", "cs.CV", "cs.DC", "cs.NE"], "comment": null, "summary": "Analog in-memory computing is a next-generation computing paradigm that\npromises fast, parallel, and energy-efficient deep learning training and\ntransfer learning (TL). However, achieving this promise has remained elusive\ndue to a lack of suitable training algorithms. Analog memory devices exhibit\nasymmetric and non-linear switching behavior in addition to device-to-device\nvariation, meaning that most, if not all, of the current off-the-shelf training\nalgorithms cannot achieve good training outcomes. Also, recently introduced\nalgorithms have enjoyed limited attention, as they require bi-directionally\nswitching devices of unrealistically high symmetry and precision and are highly\nsensitive. A new algorithm chopped TTv2 (c-TTv2), has been introduced, which\nleverages the chopped technique to address many of the challenges mentioned\nabove. In this paper, we assess the performance of the c-TTv2 algorithm for\nanalog TL using a Swin-ViT model on a subset of the CIFAR100 dataset. We also\ninvestigate the robustness of our algorithm to changes in some device\nspecifications, including weight transfer noise, symmetry point skew, and\nsymmetry point variability"}
{"id": "2505.11116", "pdf": "https://arxiv.org/pdf/2505.11116", "abs": "https://arxiv.org/abs/2505.11116", "authors": ["Liam Boyle", "Jonas Kühne", "Nicolas Baumann", "Niklas Bastuck", "Michele Magno"], "title": "Planar Velocity Estimation for Fast-Moving Mobile Robots Using Event-Based Optical Flow", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Accurate velocity estimation is critical in mobile robotics, particularly for\ndriver assistance systems and autonomous driving. Wheel odometry fused with\nInertial Measurement Unit (IMU) data is a widely used method for velocity\nestimation; however, it typically requires strong assumptions, such as non-slip\nsteering, or complex vehicle dynamics models that do not hold under varying\nenvironmental conditions like slippery surfaces. We introduce an approach to\nvelocity estimation that is decoupled from wheel-to-surface traction\nassumptions by leveraging planar kinematics in combination with optical flow\nfrom event cameras pointed perpendicularly at the ground. The asynchronous\nmicro-second latency and high dynamic range of event cameras make them highly\nrobust to motion blur, a common challenge in vision-based perception techniques\nfor autonomous driving. The proposed method is evaluated through in-field\nexperiments on a 1:10 scale autonomous racing platform and compared to precise\nmotion capture data, demonstrating not only performance on par with the\nstate-of-the-art Event-VIO method but also a 38.3 % improvement in lateral\nerror. Qualitative experiments at highway speeds of up to 32 m/s further\nconfirm the effectiveness of our approach, indicating significant potential for\nreal-world deployment."}
{"id": "2505.11128", "pdf": "https://arxiv.org/pdf/2505.11128", "abs": "https://arxiv.org/abs/2505.11128", "authors": ["Simone Azeglio", "Arianna Di Bernardo"], "title": "What's Inside Your Diffusion Model? A Score-Based Riemannian Metric to Explore the Data Manifold", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Recent advances in diffusion models have demonstrated their remarkable\nability to capture complex image distributions, but the geometric properties of\nthe learned data manifold remain poorly understood. We address this gap by\nintroducing a score-based Riemannian metric that leverages the Stein score\nfunction from diffusion models to characterize the intrinsic geometry of the\ndata manifold without requiring explicit parameterization. Our approach defines\na metric tensor in the ambient space that stretches distances perpendicular to\nthe manifold while preserving them along tangential directions, effectively\ncreating a geometry where geodesics naturally follow the manifold's contours.\nWe develop efficient algorithms for computing these geodesics and demonstrate\ntheir utility for both interpolation between data points and extrapolation\nbeyond the observed data distribution. Through experiments on synthetic data\nwith known geometry, Rotated MNIST, and complex natural images via Stable\nDiffusion, we show that our score-based geodesics capture meaningful\ntransformations that respect the underlying data distribution. Our method\nconsistently outperforms baseline approaches on perceptual metrics (LPIPS) and\ndistribution-level metrics (FID, KID), producing smoother, more realistic image\ntransitions. These results reveal the implicit geometric structure learned by\ndiffusion models and provide a principled way to navigate the manifold of\nnatural images through the lens of Riemannian geometry."}
{"id": "2505.11134", "pdf": "https://arxiv.org/pdf/2505.11134", "abs": "https://arxiv.org/abs/2505.11134", "authors": ["Desong Zhang", "Jia Hu", "Geyong Min"], "title": "Towards Robust Spiking Neural Networks:Mitigating Heterogeneous Training Vulnerability via Dominant Eigencomponent Projection", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Spiking Neural Networks (SNNs) process information via discrete spikes,\nenabling them to operate at remarkably low energy levels. However, our\nexperimental observations reveal a striking vulnerability when SNNs are trained\nusing the mainstream method--direct encoding combined with backpropagation\nthrough time (BPTT): even a single backward pass on data drawn from a slightly\ndifferent distribution can lead to catastrophic network collapse. Our\ntheoretical analysis attributes this vulnerability to the repeated inputs\ninherent in direct encoding and the gradient accumulation characteristic of\nBPTT, which together produce an exceptional large Hessian spectral radius. To\naddress this challenge, we develop a hyperparameter-free method called Dominant\nEigencomponent Projection (DEP). By orthogonally projecting gradients to\nprecisely remove their dominant components, DEP effectively reduces the Hessian\nspectral radius, thereby preventing SNNs from settling into sharp minima.\nExtensive experiments demonstrate that DEP not only mitigates the vulnerability\nof SNNs to heterogeneous data poisoning, but also significantly enhances\noverall robustness compared to key baselines, providing strong support for\nsafer and more reliable SNN deployment."}
{"id": "2505.11142", "pdf": "https://arxiv.org/pdf/2505.11142", "abs": "https://arxiv.org/abs/2505.11142", "authors": ["Guido Caccianiga", "Yarden Sharon", "Bernard Javot", "Senya Polikovsky", "Gökce Ergün", "Ivan Capobianco", "André L. Mihaljevic", "Anton Deguet", "Katherine J. Kuchenbecker"], "title": "Open-Source Multi-Viewpoint Surgical Telerobotics", "categories": ["cs.RO", "cs.CV"], "comment": "2 pages, 2 figures, ICRA-RAMI workshop long abstract", "summary": "As robots for minimally invasive surgery (MIS) gradually become more\naccessible and modular, we believe there is a great opportunity to rethink and\nexpand the visualization and control paradigms that have characterized surgical\nteleoperation since its inception. We conjecture that introducing one or more\nadditional adjustable viewpoints in the abdominal cavity would not only unlock\nnovel visualization and collaboration strategies for surgeons but also\nsubstantially boost the robustness of machine perception toward shared\nautonomy. Immediate advantages include controlling a second viewpoint and\nteleoperating surgical tools from a different perspective, which would allow\ncollaborating surgeons to adjust their views independently and still maneuver\ntheir robotic instruments intuitively. Furthermore, we believe that capturing\nsynchronized multi-view 3D measurements of the patient's anatomy would unlock\nadvanced scene representations. Accurate real-time intraoperative 3D perception\nwill allow algorithmic assistants to directly control one or more robotic\ninstruments and/or robotic cameras. Toward these goals, we are building a\nsynchronized multi-viewpoint, multi-sensor robotic surgery system by\nintegrating high-performance vision components and upgrading the da Vinci\nResearch Kit control logic. This short paper reports a functional summary of\nour setup and elaborates on its potential impacts in research and future\nclinical practice. By fully open-sourcing our system, we will enable the\nresearch community to reproduce our setup, improve it, and develop powerful\nalgorithms, effectively boosting clinical translation of cutting-edge research."}
{"id": "2505.11158", "pdf": "https://arxiv.org/pdf/2505.11158", "abs": "https://arxiv.org/abs/2505.11158", "authors": ["Xing Hu", "Xiangcheng Liu", "Qianqian Duan", "Danfeng Hong", "Dawei Zhang"], "title": "Diffusion Model in Hyperspectral Image Processing and Analysis: A Review", "categories": ["eess.IV", "cs.CV"], "comment": "33 pages,20 figures", "summary": "Hyperspectral image processing and analysis has important application value\nin remote sensing, agriculture and environmental monitoring, but its high\ndimensionality, data redundancy and noise interference etc. bring great\nchallenges to the analysis. Traditional models have limitations in dealing with\nthese complex data, and it is difficult to meet the increasing demand for\nanalysis. In recent years, Diffusion Model, as an emerging generative model,\nhas shown unique advantages in hyperspectral image processing. By simulating\nthe diffusion process of data in time, the Diffusion Model can effectively\nprocess high-dimensional data, generate high-quality samples, and perform well\nin denoising and data enhancement. In this paper, we review the recent research\nadvances in diffusion modeling for hyperspectral image processing and analysis,\nand discuss its applications in tasks such as high-dimensional data processing,\nnoise removal, classification, and anomaly detection. The performance of\ndiffusion-based models on image processing is compared and the challenges are\nsummarized. It is shown that the diffusion model can significantly improve the\naccuracy and efficiency of hyperspectral image analysis, providing a new\ndirection for future research."}
{"id": "2505.11165", "pdf": "https://arxiv.org/pdf/2505.11165", "abs": "https://arxiv.org/abs/2505.11165", "authors": ["Haiqing Hao", "Nikola Zubić", "Weihua He", "Zhipeng Sui", "Davide Scaramuzza", "Wenhui Wang"], "title": "Maximizing Asynchronicity in Event-based Neural Networks", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": "18 pages, 5 figures, 9 tables", "summary": "Event cameras deliver visual data with high temporal resolution, low latency,\nand minimal redundancy, yet their asynchronous, sparse sequential nature\nchallenges standard tensor-based machine learning (ML). While the recent\nasynchronous-to-synchronous (A2S) paradigm aims to bridge this gap by\nasynchronously encoding events into learned representations for ML pipelines,\nexisting A2S approaches often sacrifice representation expressivity and\ngeneralizability compared to dense, synchronous methods. This paper introduces\nEVA (EVent Asynchronous representation learning), a novel A2S framework to\ngenerate highly expressive and generalizable event-by-event representations.\nInspired by the analogy between events and language, EVA uniquely adapts\nadvances from language modeling in linear attention and self-supervised\nlearning for its construction. In demonstration, EVA outperforms prior A2S\nmethods on recognition tasks (DVS128-Gesture and N-Cars), and represents the\nfirst A2S framework to successfully master demanding detection tasks, achieving\na remarkable 47.7 mAP on the Gen1 dataset. These results underscore EVA's\ntransformative potential for advancing real-time event-based vision\napplications."}
{"id": "2505.11217", "pdf": "https://arxiv.org/pdf/2505.11217", "abs": "https://arxiv.org/abs/2505.11217", "authors": ["Yanhao Jia", "Ji Xie", "S Jivaganesh", "Hao Li", "Xu Wu", "Mengmi Zhang"], "title": "Seeing Sound, Hearing Sight: Uncovering Modality Bias and Conflict of AI models in Sound Localization", "categories": ["cs.SD", "cs.AI", "cs.CV", "cs.MM", "eess.AS"], "comment": "16 pages, 14 figures", "summary": "Imagine hearing a dog bark and turning toward the sound only to see a parked\ncar, while the real, silent dog sits elsewhere. Such sensory conflicts test\nperception, yet humans reliably resolve them by prioritizing sound over\nmisleading visuals. Despite advances in multimodal AI integrating vision and\naudio, little is known about how these systems handle cross-modal conflicts or\nwhether they favor one modality. In this study, we systematically examine\nmodality bias and conflict resolution in AI sound localization. We assess\nleading multimodal models and benchmark them against human performance in\npsychophysics experiments across six audiovisual conditions, including\ncongruent, conflicting, and absent cues. Humans consistently outperform AI,\ndemonstrating superior resilience to conflicting or missing visuals by relying\non auditory information. In contrast, AI models often default to visual input,\ndegrading performance to near chance levels. To address this, we finetune a\nstate-of-the-art model using a stereo audio-image dataset generated via 3D\nsimulations. Even with limited training data, the refined model surpasses\nexisting benchmarks. Notably, it also mirrors human-like horizontal\nlocalization bias favoring left-right precision-likely due to the stereo audio\nstructure reflecting human ear placement. These findings underscore how sensory\ninput quality and system architecture shape multimodal representation accuracy."}
{"id": "2505.11278", "pdf": "https://arxiv.org/pdf/2505.11278", "abs": "https://arxiv.org/abs/2505.11278", "authors": ["Fabian Falck", "Teodora Pandeva", "Kiarash Zahirnia", "Rachel Lawrence", "Richard Turner", "Edward Meeds", "Javier Zazo", "Sushrut Karmalkar"], "title": "A Fourier Space Perspective on Diffusion Models", "categories": ["stat.ML", "cs.CV", "cs.LG", "stat.ME"], "comment": null, "summary": "Diffusion models are state-of-the-art generative models on data modalities\nsuch as images, audio, proteins and materials. These modalities share the\nproperty of exponentially decaying variance and magnitude in the Fourier\ndomain. Under the standard Denoising Diffusion Probabilistic Models (DDPM)\nforward process of additive white noise, this property results in\nhigh-frequency components being corrupted faster and earlier in terms of their\nSignal-to-Noise Ratio (SNR) than low-frequency ones. The reverse process then\ngenerates low-frequency information before high-frequency details. In this\nwork, we study the inductive bias of the forward process of diffusion models in\nFourier space. We theoretically analyse and empirically demonstrate that the\nfaster noising of high-frequency components in DDPM results in violations of\nthe normality assumption in the reverse process. Our experiments show that this\nleads to degraded generation quality of high-frequency components. We then\nstudy an alternate forward process in Fourier space which corrupts all\nfrequencies at the same rate, removing the typical frequency hierarchy during\ngeneration, and demonstrate marked performance improvements on datasets where\nhigh frequencies are primary, while performing on par with DDPM on standard\nimaging benchmarks."}
{"id": "2505.11394", "pdf": "https://arxiv.org/pdf/2505.11394", "abs": "https://arxiv.org/abs/2505.11394", "authors": ["Alexander Oberstrass", "Esteban Vaca", "Eric Upschulte", "Meiqi Niu", "Nicola Palomero-Gallagher", "David Graessel", "Christian Schiffer", "Markus Axer", "Katrin Amunts", "Timo Dickscheid"], "title": "From Fibers to Cells: Fourier-Based Registration Enables Virtual Cresyl Violet Staining From 3D Polarized Light Imaging", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Comprehensive assessment of the various aspects of the brain's microstructure\nrequires the use of complementary imaging techniques. This includes measuring\nthe spatial distribution of cell bodies (cytoarchitecture) and nerve fibers\n(myeloarchitecture). The gold standard for cytoarchitectonic analysis is light\nmicroscopic imaging of cell-body stained tissue sections. To reveal the 3D\norientations of nerve fibers, 3D Polarized Light Imaging (3D-PLI) has been\nintroduced as a reliable technique providing a resolution in the micrometer\nrange while allowing processing of series of complete brain sections. 3D-PLI\nacquisition is label-free and allows subsequent staining of sections after\nmeasurement. By post-staining for cell bodies, a direct link between fiber- and\ncytoarchitecture can potentially be established within the same section.\nHowever, inevitable distortions introduced during the staining process make a\nnonlinear and cross-modal registration necessary in order to study the detailed\nrelationships between cells and fibers in the images. In addition, the\ncomplexity of processing histological sections for post-staining only allows\nfor a limited number of samples. In this work, we take advantage of deep\nlearning methods for image-to-image translation to generate a virtual staining\nof 3D-PLI that is spatially aligned at the cellular level. In a supervised\nsetting, we build on a unique dataset of brain sections, to which Cresyl violet\nstaining has been applied after 3D-PLI measurement. To ensure high\ncorrespondence between both modalities, we address the misalignment of training\ndata using Fourier-based registration methods. In this way, registration can be\nefficiently calculated during training for local image patches of target and\npredicted staining. We demonstrate that the proposed method enables prediction\nof a Cresyl violet staining from 3D-PLI, matching individual cell instances."}
{"id": "2505.11409", "pdf": "https://arxiv.org/pdf/2505.11409", "abs": "https://arxiv.org/abs/2505.11409", "authors": ["Yi Xu", "Chengzu Li", "Han Zhou", "Xingchen Wan", "Caiqi Zhang", "Anna Korhonen", "Ivan Vulić"], "title": "Visual Planning: Let's Think Only with Images", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": "10 pages, 6 figures, 1 table (26 pages, 12 figures, 8 tables\n  including references and appendices)", "summary": "Recent advancements in Large Language Models (LLMs) and their multimodal\nextensions (MLLMs) have substantially enhanced machine reasoning across diverse\ntasks. However, these models predominantly rely on pure text as the medium for\nboth expressing and structuring reasoning, even when visual information is\npresent. In this work, we argue that language may not always be the most\nnatural or effective modality for reasoning, particularly in tasks involving\nspatial and geometrical information. Motivated by this, we propose a new\nparadigm, Visual Planning, which enables planning through purely visual\nrepresentations, independent of text. In this paradigm, planning is executed\nvia sequences of images that encode step-by-step inference in the visual\ndomain, akin to how humans sketch or visualize future actions. We introduce a\nnovel reinforcement learning framework, Visual Planning via Reinforcement\nLearning (VPRL), empowered by GRPO for post-training large vision models,\nleading to substantial improvements in planning in a selection of\nrepresentative visual navigation tasks, FrozenLake, Maze, and MiniBehavior. Our\nvisual planning paradigm outperforms all other planning variants that conduct\nreasoning in the text-only space. Our results establish Visual Planning as a\nviable and promising alternative to language-based reasoning, opening new\navenues for tasks that benefit from intuitive, image-based inference."}
{"id": "2505.11467", "pdf": "https://arxiv.org/pdf/2505.11467", "abs": "https://arxiv.org/abs/2505.11467", "authors": ["Abhishek Kashyap", "Henrik Andreasson", "Todor Stoyanov"], "title": "Exploiting Radiance Fields for Grasp Generation on Novel Synthetic Views", "categories": ["cs.RO", "cs.CV"], "comment": "6 pages", "summary": "Vision based robot manipulation uses cameras to capture one or more images of\na scene containing the objects to be manipulated. Taking multiple images can\nhelp if any object is occluded from one viewpoint but more visible from another\nviewpoint. However, the camera has to be moved to a sequence of suitable\npositions for capturing multiple images, which requires time and may not always\nbe possible, due to reachability constraints. So while additional images can\nproduce more accurate grasp poses due to the extra information available, the\ntime-cost goes up with the number of additional views sampled. Scene\nrepresentations like Gaussian Splatting are capable of rendering accurate\nphotorealistic virtual images from user-specified novel viewpoints. In this\nwork, we show initial results which indicate that novel view synthesis can\nprovide additional context in generating grasp poses. Our experiments on the\nGraspnet-1billion dataset show that novel views contributed force-closure\ngrasps in addition to the force-closure grasps obtained from sparsely sampled\nreal views while also improving grasp coverage. In the future we hope this work\ncan be extended to improve grasp extraction from radiance fields constructed\nwith a single input image, using for example diffusion models or generalizable\nradiance fields."}
