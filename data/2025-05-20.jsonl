{"id": "2505.11521", "pdf": "https://arxiv.org/pdf/2505.11521", "abs": "https://arxiv.org/abs/2505.11521", "authors": ["Wang Fang", "Shirin Rahimi", "Olivia Bennett", "Sophie Carter", "Mitra Hassani", "Xu Lan", "Omid Javadi", "Lucas Mitchell"], "title": "Improving Open-Set Semantic Segmentation in 3D Point Clouds by Conditional Channel Capacity Maximization: Preliminary Results", "categories": ["cs.CV", "eess.SP"], "comment": null, "summary": "Point-cloud semantic segmentation underpins a wide range of critical\napplications. Although recent deep architectures and large-scale datasets have\ndriven impressive closed-set performance, these models struggle to recognize or\nproperly segment objects outside their training classes. This gap has sparked\ninterest in Open-Set Semantic Segmentation (O3S), where models must both\ncorrectly label known categories and detect novel, unseen classes. In this\npaper, we propose a plug and play framework for O3S. By modeling the\nsegmentation pipeline as a conditional Markov chain, we derive a novel\nregularizer term dubbed Conditional Channel Capacity Maximization (3CM), that\nmaximizes the mutual information between features and predictions conditioned\non each class. When incorporated into standard loss functions, 3CM encourages\nthe encoder to retain richer, label-dependent features, thereby enhancing the\nnetwork's ability to distinguish and segment previously unseen categories.\nExperimental results demonstrate effectiveness of proposed method on detecting\nunseen objects. We further outline future directions for dynamic open-world\nadaptation and efficient information-theoretic estimation."}
{"id": "2505.11581", "pdf": "https://arxiv.org/pdf/2505.11581", "abs": "https://arxiv.org/abs/2505.11581", "authors": ["Akarsh Kumar", "Jeff Clune", "Joel Lehman", "Kenneth O. Stanley"], "title": "Questioning Representational Optimism in Deep Learning: The Fractured Entangled Representation Hypothesis", "categories": ["cs.CV", "cs.LG", "cs.NE"], "comment": "43 pages, 25 figures", "summary": "Much of the excitement in modern AI is driven by the observation that scaling\nup existing systems leads to better performance. But does better performance\nnecessarily imply better internal representations? While the representational\noptimist assumes it must, this position paper challenges that view. We compare\nneural networks evolved through an open-ended search process to networks\ntrained via conventional stochastic gradient descent (SGD) on the simple task\nof generating a single image. This minimal setup offers a unique advantage:\neach hidden neuron's full functional behavior can be easily visualized as an\nimage, thus revealing how the network's output behavior is internally\nconstructed neuron by neuron. The result is striking: while both networks\nproduce the same output behavior, their internal representations differ\ndramatically. The SGD-trained networks exhibit a form of disorganization that\nwe term fractured entangled representation (FER). Interestingly, the evolved\nnetworks largely lack FER, even approaching a unified factored representation\n(UFR). In large models, FER may be degrading core model capacities like\ngeneralization, creativity, and (continual) learning. Therefore, understanding\nand mitigating FER could be critical to the future of representation learning."}
{"id": "2505.11620", "pdf": "https://arxiv.org/pdf/2505.11620", "abs": "https://arxiv.org/abs/2505.11620", "authors": ["Aaron Wilhelm", "Nils Napp"], "title": "Improved Bag-of-Words Image Retrieval with Geometric Constraints for Ground Texture Localization", "categories": ["cs.CV", "cs.RO"], "comment": "Accepted to ICRA 2025", "summary": "Ground texture localization using a downward-facing camera offers a low-cost,\nhigh-precision localization solution that is robust to dynamic environments and\nrequires no environmental modification. We present a significantly improved\nbag-of-words (BoW) image retrieval system for ground texture localization,\nachieving substantially higher accuracy for global localization and higher\nprecision and recall for loop closure detection in SLAM. Our approach leverages\nan approximate $k$-means (AKM) vocabulary with soft assignment, and exploits\nthe consistent orientation and constant scale constraints inherent to ground\ntexture localization. Identifying the different needs of global localization\nvs. loop closure detection for SLAM, we present both high-accuracy and\nhigh-speed versions of our algorithm. We test the effect of each of our\nproposed improvements through an ablation study and demonstrate our method's\neffectiveness for both global localization and loop closure detection. With\nnumerous ground texture localization systems already using BoW, our method can\nreadily replace other generic BoW systems in their pipeline and immediately\nimprove their results."}
{"id": "2505.11640", "pdf": "https://arxiv.org/pdf/2505.11640", "abs": "https://arxiv.org/abs/2505.11640", "authors": ["Pandula Thennakoon", "Avishka Ranasinghe", "Mario De Silva", "Buwaneka Epakanda", "Roshan Godaliyadda", "Parakrama Ekanayake", "Vijitha Herath"], "title": "BandRC: Band Shifted Raised Cosine Activated Implicit Neural Representations", "categories": ["cs.CV"], "comment": "Submitted as a conference paper to ICCV 2025", "summary": "In recent years, implicit neural representations(INRs) have gained popularity\nin the computer vision community. This is mainly due to the strong performance\nof INRs in many computer vision tasks. These networks can extract a continuous\nsignal representation given a discrete signal representation. In previous\nstudies, it has been repeatedly shown that INR performance has a strong\ncorrelation with the activation functions used in its multilayer perceptrons.\nAlthough numerous activation functions have been proposed that are competitive\nwith one another, they share some common set of challenges such as spectral\nbias(Lack of sensitivity to high-frequency content in signals), limited\nrobustness to signal noise and difficulties in simultaneous capturing both\nlocal and global features. and furthermore, the requirement for manual\nparameter tuning. To address these issues, we introduce a novel activation\nfunction, Band Shifted Raised Cosine Activated Implicit Neural Networks\n\\textbf{(BandRC)} tailored to enhance signal representation capacity further.\nWe also incorporate deep prior knowledge extracted from the signal to adjust\nthe activation functions through a task-specific model. Through a mathematical\nanalysis and a series of experiments which include image reconstruction (with a\n+8.93 dB PSNR improvement over the nearest counterpart), denoising (with a\n+0.46 dB increase in PSNR), super-resolution (with a +1.03 dB improvement over\nthe nearest State-Of-The-Art (SOTA) method for 6X super-resolution),\ninpainting, and 3D shape reconstruction we demonstrate the dominance of BandRC\nover existing state of the art activation functions."}
{"id": "2505.11676", "pdf": "https://arxiv.org/pdf/2505.11676", "abs": "https://arxiv.org/abs/2505.11676", "authors": ["Ziyu Zhao", "Xiaoguang Li", "Linjia Shi", "Nasrin Imanpour", "Song Wang"], "title": "DPSeg: Dual-Prompt Cost Volume Learning for Open-Vocabulary Semantic Segmentation", "categories": ["cs.CV"], "comment": "Accepted by CVPR2025", "summary": "Open-vocabulary semantic segmentation aims to segment images into distinct\nsemantic regions for both seen and unseen categories at the pixel level.\nCurrent methods utilize text embeddings from pre-trained vision-language models\nlike CLIP but struggle with the inherent domain gap between image and text\nembeddings, even after extensive alignment during training. Additionally,\nrelying solely on deep text-aligned features limits shallow-level feature\nguidance, which is crucial for detecting small objects and fine details,\nultimately reducing segmentation accuracy. To address these limitations, we\npropose a dual prompting framework, DPSeg, for this task. Our approach combines\ndual-prompt cost volume generation, a cost volume-guided decoder, and a\nsemantic-guided prompt refinement strategy that leverages our dual prompting\nscheme to mitigate alignment issues in visual prompt generation. By\nincorporating visual embeddings from a visual prompt encoder, our approach\nreduces the domain gap between text and image embeddings while providing\nmulti-level guidance through shallow features. Extensive experiments\ndemonstrate that our method significantly outperforms existing state-of-the-art\napproaches on multiple public datasets."}
{"id": "2505.11703", "pdf": "https://arxiv.org/pdf/2505.11703", "abs": "https://arxiv.org/abs/2505.11703", "authors": ["Jae Myung Kim", "Stephan Alaniz", "Cordelia Schmid", "Zeynep Akata"], "title": "LoFT: LoRA-fused Training Dataset Generation with Few-shot Guidance", "categories": ["cs.CV"], "comment": null, "summary": "Despite recent advances in text-to-image generation, using synthetically\ngenerated data seldom brings a significant boost in performance for supervised\nlearning. Oftentimes, synthetic datasets do not faithfully recreate the data\ndistribution of real data, i.e., they lack the fidelity or diversity needed for\neffective downstream model training. While previous work has employed few-shot\nguidance to address this issue, existing methods still fail to capture and\ngenerate features unique to specific real images. In this paper, we introduce a\nnovel dataset generation framework named LoFT, LoRA-Fused Training-data\nGeneration with Few-shot Guidance. Our method fine-tunes LoRA weights on\nindividual real images and fuses them at inference time, producing synthetic\nimages that combine the features of real images for improved diversity and\nfidelity of generated data. We evaluate the synthetic data produced by LoFT on\n10 datasets, using 8 to 64 real images per class as guidance and scaling up to\n1000 images per class. Our experiments show that training on LoFT-generated\ndata consistently outperforms other synthetic dataset methods, significantly\nincreasing accuracy as the dataset size increases. Additionally, our analysis\ndemonstrates that LoFT generates datasets with high fidelity and sufficient\ndiversity, which contribute to the performance improvement. The code is\navailable at https://github.com/ExplainableML/LoFT."}
{"id": "2505.11707", "pdf": "https://arxiv.org/pdf/2505.11707", "abs": "https://arxiv.org/abs/2505.11707", "authors": ["Haipeng Fang", "Sheng Tang", "Juan Cao", "Enshuo Zhang", "Fan Tang", "Tong-Yee Lee"], "title": "Attend to Not Attended: Structure-then-Detail Token Merging for Post-training DiT Acceleration", "categories": ["cs.CV"], "comment": "Comments: 14 pages, 14 figures. Accepted by the Proceedings of the\n  42nd IEEE/CVF Conference on Computer Vision and Pattern Recognition", "summary": "Diffusion transformers have shown exceptional performance in visual\ngeneration but incur high computational costs. Token reduction techniques that\ncompress models by sharing the denoising process among similar tokens have been\nintroduced. However, existing approaches neglect the denoising priors of the\ndiffusion models, leading to suboptimal acceleration and diminished image\nquality. This study proposes a novel concept: attend to prune feature\nredundancies in areas not attended by the diffusion process. We analyze the\nlocation and degree of feature redundancies based on the structure-then-detail\ndenoising priors. Subsequently, we introduce SDTM, a structure-then-detail\ntoken merging approach that dynamically compresses feature redundancies.\nSpecifically, we design dynamic visual token merging, compression ratio\nadjusting, and prompt reweighting for different stages. Served in a\npost-training way, the proposed method can be integrated seamlessly into any\nDiT architecture. Extensive experiments across various backbones, schedulers,\nand datasets showcase the superiority of our method, for example, it achieves\n1.55 times acceleration with negligible impact on image quality. Project page:\nhttps://github.com/ICTMCG/SDTM."}
{"id": "2505.11709", "pdf": "https://arxiv.org/pdf/2505.11709", "abs": "https://arxiv.org/abs/2505.11709", "authors": ["Ryan Hoque", "Peide Huang", "David J. Yoon", "Mouli Sivapurapu", "Jian Zhang"], "title": "EgoDex: Learning Dexterous Manipulation from Large-Scale Egocentric Video", "categories": ["cs.CV", "cs.LG", "cs.RO"], "comment": null, "summary": "Imitation learning for manipulation has a well-known data scarcity problem.\nUnlike natural language and 2D computer vision, there is no Internet-scale\ncorpus of data for dexterous manipulation. One appealing option is egocentric\nhuman video, a passively scalable data source. However, existing large-scale\ndatasets such as Ego4D do not have native hand pose annotations and do not\nfocus on object manipulation. To this end, we use Apple Vision Pro to collect\nEgoDex: the largest and most diverse dataset of dexterous human manipulation to\ndate. EgoDex has 829 hours of egocentric video with paired 3D hand and finger\ntracking data collected at the time of recording, where multiple calibrated\ncameras and on-device SLAM can be used to precisely track the pose of every\njoint of each hand. The dataset covers a wide range of diverse manipulation\nbehaviors with everyday household objects in 194 different tabletop tasks\nranging from tying shoelaces to folding laundry. Furthermore, we train and\nsystematically evaluate imitation learning policies for hand trajectory\nprediction on the dataset, introducing metrics and benchmarks for measuring\nprogress in this increasingly important area. By releasing this large-scale\ndataset, we hope to push the frontier of robotics, computer vision, and\nfoundation models."}
{"id": "2505.11720", "pdf": "https://arxiv.org/pdf/2505.11720", "abs": "https://arxiv.org/abs/2505.11720", "authors": ["Shijun Liang", "Ismail R. Alkhouri", "Siddhant Gautam", "Qing Qu", "Saiprasad Ravishankar"], "title": "UGoDIT: Unsupervised Group Deep Image Prior Via Transferable Weights", "categories": ["cs.CV", "cs.LG", "eess.IV"], "comment": null, "summary": "Recent advances in data-centric deep generative models have led to\nsignificant progress in solving inverse imaging problems. However, these models\n(e.g., diffusion models (DMs)) typically require large amounts of fully sampled\n(clean) training data, which is often impractical in medical and scientific\nsettings such as dynamic imaging.\n  On the other hand, training-data-free approaches like the Deep Image Prior\n(DIP) do not require clean ground-truth images but suffer from noise\noverfitting and can be computationally expensive as the network parameters need\nto be optimized for each measurement set independently. Moreover, DIP-based\nmethods often overlook the potential of learning a prior using a small number\nof sub-sampled measurements (or degraded images) available during training. In\nthis paper, we propose UGoDIT, an Unsupervised Group DIP via Transferable\nweights, designed for the low-data regime where only a very small number, M, of\nsub-sampled measurement vectors are available during training. Our method\nlearns a set of transferable weights by optimizing a shared encoder and M\ndisentangled decoders. At test time, we reconstruct the unseen degraded image\nusing a DIP network, where part of the parameters are fixed to the learned\nweights, while the remaining are optimized to enforce measurement consistency.\nWe evaluate UGoDIT on both medical (multi-coil MRI) and natural (super\nresolution and non-linear deblurring) image recovery tasks under various\nsettings. Compared to recent standalone DIP methods, UGoDIT provides\naccelerated convergence and notable improvement in reconstruction quality.\nFurthermore, our method achieves performance competitive with SOTA DM-based and\nsupervised approaches, despite not requiring large amounts of clean training\ndata."}
{"id": "2505.11724", "pdf": "https://arxiv.org/pdf/2505.11724", "abs": "https://arxiv.org/abs/2505.11724", "authors": ["Kai Zhu", "Vignesh Edithal", "Le Zhang", "Ilia Blank", "Imran Junejo"], "title": "Semantically-Aware Game Image Quality Assessment", "categories": ["cs.CV", "eess.IV"], "comment": "16 pages, 12 figures", "summary": "Assessing the visual quality of video game graphics presents unique\nchallenges due to the absence of reference images and the distinct types of\ndistortions, such as aliasing, texture blur, and geometry level of detail (LOD)\nissues, which differ from those in natural images or user-generated content.\nExisting no-reference image and video quality assessment (NR-IQA/VQA) methods\nfail to generalize to gaming environments as they are primarily designed for\ndistortions like compression artifacts. This study introduces a\nsemantically-aware NR-IQA model tailored to gaming. The model employs a\nknowledge-distilled Game distortion feature extractor (GDFE) to detect and\nquantify game-specific distortions, while integrating semantic gating via CLIP\nembeddings to dynamically weight feature importance based on scene content.\nTraining on gameplay data recorded across graphical quality presets enables the\nmodel to produce quality scores that align with human perception. Our results\ndemonstrate that the GDFE, trained through knowledge distillation from binary\nclassifiers, generalizes effectively to intermediate distortion levels unseen\nduring training. Semantic gating further improves contextual relevance and\nreduces prediction variance. In the absence of in-domain NR-IQA baselines, our\nmodel outperforms out-of-domain methods and exhibits robust, monotonic quality\ntrends across unseen games in the same genre. This work establishes a\nfoundation for automated graphical quality assessment in gaming, advancing\nNR-IQA methods in this domain."}
{"id": "2505.11753", "pdf": "https://arxiv.org/pdf/2505.11753", "abs": "https://arxiv.org/abs/2505.11753", "authors": ["Valentina Bazyleva", "Nicolo Bonettini", "Gaurav Bharaj"], "title": "X-Edit: Detecting and Localizing Edits in Images Altered by Text-Guided Diffusion Models", "categories": ["cs.CV"], "comment": "CVPR (XAI4CV) 2025", "summary": "Text-guided diffusion models have significantly advanced image editing,\nenabling highly realistic and local modifications based on textual prompts.\nWhile these developments expand creative possibilities, their malicious use\nposes substantial challenges for detection of such subtle deepfake edits. To\nthis end, we introduce Explain Edit (X-Edit), a novel method for localizing\ndiffusion-based edits in images. To localize the edits for an image, we invert\nthe image using a pretrained diffusion model, then use these inverted features\nas input to a segmentation network that explicitly predicts the edited masked\nregions via channel and spatial attention. Further, we finetune the model using\na combined segmentation and relevance loss. The segmentation loss ensures\naccurate mask prediction by balancing pixel-wise errors and perceptual\nsimilarity, while the relevance loss guides the model to focus on low-frequency\nregions and mitigate high-frequency artifacts, enhancing the localization of\nsubtle edits. To the best of our knowledge, we are the first to address and\nmodel the problem of localizing diffusion-based modified regions in images. We\nadditionally contribute a new dataset of paired original and edited images\naddressing the current lack of resources for this task. Experimental results\ndemonstrate that X-Edit accurately localizes edits in images altered by\ntext-guided diffusion models, outperforming baselines in PSNR and SSIM metrics.\nThis highlights X-Edit's potential as a robust forensic tool for detecting and\npinpointing manipulations introduced by advanced image editing techniques."}
{"id": "2505.11758", "pdf": "https://arxiv.org/pdf/2505.11758", "abs": "https://arxiv.org/abs/2505.11758", "authors": ["Sriram Mandalika"], "title": "Generalizable Vision-Language Few-Shot Adaptation with Predictive Prompts and Negative Learning", "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.RO"], "comment": null, "summary": "Few-shot adaptation remains a core challenge for vision-language models\n(VLMs), especially under limited supervision and noisy support samples. We\npropose PromptFuseNL, a unified framework that enhances few-shot generalization\nby combining predictive prompt tuning with dual-branch positive and negative\nlearning. The method refines class prototypes through task-conditioned\nresiduals, multi-stage cross-modal coordination, and semantic hard negative\nmining. To address label noise, we introduce an unsupervised instance\nreweighting strategy that downweights unreliable support examples without\nrequiring additional labels or structural changes. PromptFuseNL fuses visual\nand textual cues through lightweight modules for efficient and discriminative\nprediction. Evaluated across 15 benchmarks, it consistently surpasses existing\nprompt- and adapter-based methods in all shot settings while remaining highly\nefficient, achieving up to 300x faster training and 1000x lower FLOPs compared\nto full prompt tuning, achieving a new state-of-the-art for robust and scalable\nfew-shot vision-language adaptation."}
{"id": "2505.11769", "pdf": "https://arxiv.org/pdf/2505.11769", "abs": "https://arxiv.org/abs/2505.11769", "authors": ["Wonjune Kim", "Lae-kyoung Lee", "Su-Yong An"], "title": "Technical Report for ICRA 2025 GOOSE 2D Semantic Segmentation Challenge: Boosting Off-Road Segmentation via Photometric Distortion and Exponential Moving Average", "categories": ["cs.CV"], "comment": "Winners of the GOOSE 2D Semantic Segmentation Challenge at the IEEE\n  ICRA Workshop on Field Robotics 2025", "summary": "We report on the application of a high-capacity semantic segmentation\npipeline to the GOOSE 2D Semantic Segmentation Challenge for unstructured\noff-road environments. Using a FlashInternImage-B backbone together with a\nUPerNet decoder, we adapt established techniques, rather than designing new\nones, to the distinctive conditions of off-road scenes. Our training recipe\ncouples strong photometric distortion augmentation (to emulate the wide\nlighting variations of outdoor terrain) with an Exponential Moving Average\n(EMA) of weights for better generalization. Using only the GOOSE training\ndataset, we achieve 88.8\\% mIoU on the validation set."}
{"id": "2505.11777", "pdf": "https://arxiv.org/pdf/2505.11777", "abs": "https://arxiv.org/abs/2505.11777", "authors": ["Fu-Yun Wang", "Keqiang Sun", "Yao Teng", "Xihui Liu", "Jiaming Song", "Hongsheng Li"], "title": "Self-NPO: Negative Preference Optimization of Diffusion Models by Simply Learning from Itself without Explicit Preference Annotations", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion models have demonstrated remarkable success in various visual\ngeneration tasks, including image, video, and 3D content generation. Preference\noptimization (PO) is a prominent and growing area of research that aims to\nalign these models with human preferences. While existing PO methods primarily\nconcentrate on producing favorable outputs, they often overlook the\nsignificance of classifier-free guidance (CFG) in mitigating undesirable\nresults. Diffusion-NPO addresses this gap by introducing negative preference\noptimization (NPO), training models to generate outputs opposite to human\npreferences and thereby steering them away from unfavorable outcomes. However,\nprior NPO approaches, including Diffusion-NPO, rely on costly and fragile\nprocedures for obtaining explicit preference annotations (e.g., manual pairwise\nlabeling or reward model training), limiting their practicality in domains\nwhere such data are scarce or difficult to acquire. In this work, we introduce\nSelf-NPO, a Negative Preference Optimization approach that learns exclusively\nfrom the model itself, thereby eliminating the need for manual data labeling or\nreward model training. Moreover, our method is highly efficient and does not\nrequire exhaustive data sampling. We demonstrate that Self-NPO integrates\nseamlessly into widely used diffusion models, including SD1.5, SDXL, and\nCogVideoX, as well as models already optimized for human preferences,\nconsistently enhancing both their generation quality and alignment with human\npreferences."}
{"id": "2505.11793", "pdf": "https://arxiv.org/pdf/2505.11793", "abs": "https://arxiv.org/abs/2505.11793", "authors": ["Jianing Wang", "Siying Guo", "Zheng Hua", "Runhu Huang", "Jinyu Hu", "Maoguo Gong"], "title": "CL-CaGAN: Capsule differential adversarial continuous learning for cross-domain hyperspectral anomaly detection", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": null, "summary": "Anomaly detection (AD) has attracted remarkable attention in hyperspectral\nimage (HSI) processing fields, and most existing deep learning (DL)-based\nalgorithms indicate dramatic potential for detecting anomaly samples through\nspecific training process under current scenario. However, the limited prior\ninformation and the catastrophic forgetting problem indicate crucial challenges\nfor existing DL structure in open scenarios cross-domain detection. In order to\nimprove the detection performance, a novel continual learning-based capsule\ndifferential generative adversarial network (CL-CaGAN) is proposed to elevate\nthe cross-scenario learning performance for facilitating the real application\nof DL-based structure in hyperspectral AD (HAD) task. First, a modified capsule\nstructure with adversarial learning network is constructed to estimate the\nbackground distribution for surmounting the deficiency of prior information. To\nmitigate the catastrophic forgetting phenomenon, clustering-based sample replay\nstrategy and a designed extra self-distillation regularization are integrated\nfor merging the history and future knowledge in continual AD task, while the\ndiscriminative learning ability from previous detection scenario to current\nscenario is retained by the elaborately designed structure with continual\nlearning (CL) strategy. In addition, the differentiable enhancement is enforced\nto augment the generation performance of the training data. This further\nstabilizes the training process with better convergence and efficiently\nconsolidates the reconstruction ability of background samples. To verify the\neffectiveness of our proposed CL-CaGAN, we conduct experiments on several real\nHSIs, and the results indicate that the proposed CL-CaGAN demonstrates higher\ndetection performance and continuous learning capacity for mitigating the\ncatastrophic forgetting under cross-domain scenarios."}
{"id": "2505.11796", "pdf": "https://arxiv.org/pdf/2505.11796", "abs": "https://arxiv.org/abs/2505.11796", "authors": ["Jianing Wang", "Zheng Hua", "Wan Zhang", "Shengjia Hao", "Yuqiong Yao", "Maoguo Gong"], "title": "CL-BioGAN: Biologically-Inspired Cross-Domain Continual Learning for Hyperspectral Anomaly Detection", "categories": ["cs.CV"], "comment": null, "summary": "Memory stability and learning flexibility in continual learning (CL) is a\ncore challenge for cross-scene Hyperspectral Anomaly Detection (HAD) task.\nBiological neural networks can actively forget history knowledge that conflicts\nwith the learning of new experiences by regulating learning-triggered synaptic\nexpansion and synaptic convergence. Inspired by this phenomenon, we propose a\nnovel Biologically-Inspired Continual Learning Generative Adversarial Network\n(CL-BioGAN) for augmenting continuous distribution fitting ability for\ncross-domain HAD task, where Continual Learning Bio-inspired Loss (CL-Bio Loss)\nand self-attention Generative Adversarial Network (BioGAN) are incorporated to\nrealize forgetting history knowledge as well as involving replay strategy in\nthe proposed BioGAN. Specifically, a novel Bio-Inspired Loss composed with an\nActive Forgetting Loss (AF Loss) and a CL loss is designed to realize\nparameters releasing and enhancing between new task and history tasks from a\nBayesian perspective. Meanwhile, BioGAN loss with L2-Norm enhances\nself-attention (SA) to further balance the stability and flexibility for better\nfitting background distribution for open scenario HAD (OHAD) tasks. Experiment\nresults underscore that the proposed CL-BioGAN can achieve more robust and\nsatisfying accuracy for cross-domain HAD with fewer parameters and computation\ncost. This dual contribution not only elevates CL performance but also offers\nnew insights into neural adaptation mechanisms in OHAD task."}
{"id": "2505.11800", "pdf": "https://arxiv.org/pdf/2505.11800", "abs": "https://arxiv.org/abs/2505.11800", "authors": ["Jian Zhu", "He Wang", "Yang Xu", "Zebin Wu", "Zhihui Wei"], "title": "Self-Learning Hyperspectral and Multispectral Image Fusion via Adaptive Residual Guided Subspace Diffusion Model", "categories": ["cs.CV", "eess.IV"], "comment": "cvpr", "summary": "Hyperspectral and multispectral image (HSI-MSI) fusion involves combining a\nlow-resolution hyperspectral image (LR-HSI) with a high-resolution\nmultispectral image (HR-MSI) to generate a high-resolution hyperspectral image\n(HR-HSI). Most deep learning-based methods for HSI-MSI fusion rely on large\namounts of hyperspectral data for supervised training, which is often scarce in\npractical applications. In this paper, we propose a self-learning Adaptive\nResidual Guided Subspace Diffusion Model (ARGS-Diff), which only utilizes the\nobserved images without any extra training data. Specifically, as the LR-HSI\ncontains spectral information and the HR-MSI contains spatial information, we\ndesign two lightweight spectral and spatial diffusion models to separately\nlearn the spectral and spatial distributions from them. Then, we use these two\nmodels to reconstruct HR-HSI from two low-dimensional components, i.e, the\nspectral basis and the reduced coefficient, during the reverse diffusion\nprocess. Furthermore, we introduce an Adaptive Residual Guided Module (ARGM),\nwhich refines the two components through a residual guided function at each\nsampling step, thereby stabilizing the sampling process. Extensive experimental\nresults demonstrate that ARGS-Diff outperforms existing state-of-the-art\nmethods in terms of both performance and computational efficiency in the field\nof HSI-MSI fusion. Code is available at https://github.com/Zhu1116/ARGS-Diff."}
{"id": "2505.11804", "pdf": "https://arxiv.org/pdf/2505.11804", "abs": "https://arxiv.org/abs/2505.11804", "authors": ["Xi Wang", "Eric Nalisnick"], "title": "Are vision language models robust to uncertain inputs?", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Robustness against uncertain and ambiguous inputs is a critical challenge for\ndeep learning models. While recent advancements in large scale vision language\nmodels (VLMs, e.g. GPT4o) might suggest that increasing model and training\ndataset size would mitigate this issue, our empirical evaluation shows a more\ncomplicated picture. Testing models using two classic uncertainty\nquantification tasks, anomaly detection and classification under inherently\nambiguous conditions, we find that newer and larger VLMs indeed exhibit\nimproved robustness compared to earlier models, but still suffer from a\ntendency to strictly follow instructions, often causing them to hallucinate\nconfident responses even when faced with unclear or anomalous inputs.\nRemarkably, for natural images such as ImageNet, this limitation can be\novercome without pipeline modifications: simply prompting models to abstain\nfrom uncertain predictions enables significant reliability gains, achieving\nnear-perfect robustness in several settings. However, for domain-specific tasks\nsuch as galaxy morphology classification, a lack of specialized knowledge\nprevents reliable uncertainty estimation. Finally, we propose a novel mechanism\nbased on caption diversity to reveal a model's internal uncertainty, enabling\npractitioners to predict when models will successfully abstain without relying\non labeled data."}
{"id": "2505.11809", "pdf": "https://arxiv.org/pdf/2505.11809", "abs": "https://arxiv.org/abs/2505.11809", "authors": ["Zicheng Fan", "Kunihiko Fujiwara", "Pengyuan Liu", "Fan Zhang", "Filip Biljecki"], "title": "Image-based Visibility Analysis Replacing Line-of-Sight Simulation: An Urban Landmark Perspective", "categories": ["cs.CV"], "comment": null, "summary": "Visibility analysis is one of the fundamental analytics methods in urban\nplanning and landscape research, traditionally conducted through computational\nsimulations based on the Line-of-Sight (LoS) principle. However, when assessing\nthe visibility of named urban objects such as landmarks, geometric intersection\nalone fails to capture the contextual and perceptual dimensions of visibility\nas experienced in the real world. The study challenges the traditional\nLoS-based approaches by introducing a new, image-based visibility analysis\nmethod. Specifically, a Vision Language Model (VLM) is applied to detect the\ntarget object within a direction-zoomed Street View Image (SVI). Successful\ndetection represents the object's visibility at the corresponding SVI location.\nFurther, a heterogeneous visibility graph is constructed to address the complex\ninteraction between observers and target objects. In the first case study, the\nmethod proves its reliability in detecting the visibility of six tall landmark\nconstructions in global cities, with an overall accuracy of 87%. Furthermore,\nit reveals broader contextual differences when the landmarks are perceived and\nexperienced. In the second case, the proposed visibility graph uncovers the\nform and strength of connections for multiple landmarks along the River Thames\nin London, as well as the places where these connections occur. Notably,\nbridges on the River Thames account for approximately 30% of total connections.\nOur method complements and enhances traditional LoS-based visibility analysis,\nand showcases the possibility of revealing the prevalent connection of any\nvisual objects in the urban environment. It opens up new research perspectives\nfor urban planning, heritage conservation, and computational social science."}
{"id": "2505.11813", "pdf": "https://arxiv.org/pdf/2505.11813", "abs": "https://arxiv.org/abs/2505.11813", "authors": ["Yixuan Dong", "Fang-Yi Su", "Jung-Hsien Chiang"], "title": "SGD-Mix: Enhancing Domain-Specific Image Classification with Label-Preserving Data Augmentation", "categories": ["cs.CV", "cs.AI"], "comment": "11 pages, 6 figures, 6 tables", "summary": "Data augmentation for domain-specific image classification tasks often\nstruggles to simultaneously address diversity, faithfulness, and label clarity\nof generated data, leading to suboptimal performance in downstream tasks. While\nexisting generative diffusion model-based methods aim to enhance augmentation,\nthey fail to cohesively tackle these three critical aspects and often overlook\nintrinsic challenges of diffusion models, such as sensitivity to model\ncharacteristics and stochasticity under strong transformations. In this paper,\nwe propose a novel framework that explicitly integrates diversity,\nfaithfulness, and label clarity into the augmentation process. Our approach\nemploys saliency-guided mixing and a fine-tuned diffusion model to preserve\nforeground semantics, enrich background diversity, and ensure label\nconsistency, while mitigating diffusion model limitations. Extensive\nexperiments across fine-grained, long-tail, few-shot, and background robustness\ntasks demonstrate our method's superior performance over state-of-the-art\napproaches."}
{"id": "2505.11815", "pdf": "https://arxiv.org/pdf/2505.11815", "abs": "https://arxiv.org/abs/2505.11815", "authors": ["Jiajun Qin", "Yuan Pu", "Zhuolun He", "Seunggeun Kim", "David Z. Pan", "Bei Yu"], "title": "UniMoCo: Unified Modality Completion for Robust Multi-Modal Embeddings", "categories": ["cs.CV"], "comment": null, "summary": "Current research has explored vision-language models for multi-modal\nembedding tasks, such as information retrieval, visual grounding, and\nclassification. However, real-world scenarios often involve diverse modality\ncombinations between queries and targets, such as text and image to text, text\nand image to text and image, and text to text and image. These diverse\ncombinations pose significant challenges for existing models, as they struggle\nto align all modality combinations within a unified embedding space during\ntraining, which degrades performance at inference. To address this limitation,\nwe propose UniMoCo, a novel vision-language model architecture designed for\nmulti-modal embedding tasks. UniMoCo introduces a modality-completion module\nthat generates visual features from textual inputs, ensuring modality\ncompleteness for both queries and targets. Additionally, we develop a\nspecialized training strategy to align embeddings from both original and\nmodality-completed inputs, ensuring consistency within the embedding space.\nThis enables the model to robustly handle a wide range of modality combinations\nacross embedding tasks. Experiments show that UniMoCo outperforms previous\nmethods while demonstrating consistent robustness across diverse settings. More\nimportantly, we identify and quantify the inherent bias in conventional\napproaches caused by imbalance of modality combinations in training data, which\ncan be mitigated through our modality-completion paradigm. The code is\navailable at https://github.com/HobbitQia/UniMoCo."}
{"id": "2505.11816", "pdf": "https://arxiv.org/pdf/2505.11816", "abs": "https://arxiv.org/abs/2505.11816", "authors": ["Quan Cheng", "Yuanyu Wan", "Lingyu Wu", "Chenping Hou", "Lijun Zhang"], "title": "Continuous Subspace Optimization for Continual Learning", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Continual learning aims to learn multiple tasks sequentially while preserving\nprior knowledge, but faces the challenge of catastrophic forgetting when\nacquiring new knowledge. Recently, approaches leveraging pre-trained models\nhave gained increasing popularity to mitigate this issue, due to the strong\ngeneralization ability of foundation models. To adjust pre-trained models for\nnew tasks, existing methods usually employ low-rank adaptation, which restricts\nparameter updates to a fixed low-rank subspace. However, constraining the\noptimization space inherently compromises the model's learning capacity,\nresulting in inferior performance. To address the limitation, we propose\nContinuous Subspace Optimization for Continual Learning (CoSO) to fine-tune the\nmodel in a series of subspaces rather than a single one. These sequential\nsubspaces are dynamically determined through the singular value decomposition\nof gradients. CoSO updates the model by projecting gradients into these\nsubspaces, ensuring memory-efficient optimization. To mitigate forgetting, the\noptimization subspaces of each task are set to be orthogonal to the historical\ntask subspace. During task learning, CoSO maintains a task-specific component\nthat captures the critical update directions associated with the current task.\nUpon completing a task, this component is used to update the historical task\nsubspace, laying the groundwork for subsequent learning. Extensive experiments\non multiple datasets demonstrate that CoSO significantly outperforms\nstate-of-the-art methods, especially in challenging scenarios with long task\nsequences."}
{"id": "2505.11822", "pdf": "https://arxiv.org/pdf/2505.11822", "abs": "https://arxiv.org/abs/2505.11822", "authors": ["Ke Li", "Di Wang", "Xiaowei Wang", "Zhihong Wu", "Yiming Zhang", "Yifeng Wang", "Quan Wang"], "title": "Robust Cross-View Geo-Localization via Content-Viewpoint Disentanglement", "categories": ["cs.CV"], "comment": null, "summary": "Cross-view geo-localization (CVGL) aims to match images of the same\ngeographic location captured from different perspectives, such as drones and\nsatellites. Despite recent advances, CVGL remains highly challenging due to\nsignificant appearance changes and spatial distortions caused by viewpoint\nvariations. Existing methods typically assume that cross-view images can be\ndirectly aligned within a shared feature space by maximizing feature similarity\nthrough contrastive learning. Nonetheless, this assumption overlooks the\ninherent conflicts induced by viewpoint discrepancies, resulting in extracted\nfeatures containing inconsistent information that hinders precise localization.\nIn this study, we take a manifold learning perspective and model the feature\nspace of cross-view images as a composite manifold jointly governed by content\nand viewpoint information. Building upon this insight, we propose\n$\\textbf{CVD}$, a new CVGL framework that explicitly disentangles\n$\\textit{content}$ and $\\textit{viewpoint}$ factors. To promote effective\ndisentanglement, we introduce two constraints: $\\textit{(i)}$ An intra-view\nindependence constraint, which encourages statistical independence between the\ntwo factors by minimizing their mutual information. $\\textit{(ii)}$ An\ninter-view reconstruction constraint that reconstructs each view by\ncross-combining $\\textit{content}$ and $\\textit{viewpoint}$ from paired images,\nensuring factor-specific semantics are preserved. As a plug-and-play module,\nCVD can be seamlessly integrated into existing geo-localization pipelines.\nExtensive experiments on four benchmarks, i.e., University-1652, SUES-200,\nCVUSA, and CVACT, demonstrate that CVD consistently improves both localization\naccuracy and generalization across multiple baselines."}
{"id": "2505.11825", "pdf": "https://arxiv.org/pdf/2505.11825", "abs": "https://arxiv.org/abs/2505.11825", "authors": ["Xudong Ma"], "title": "Bootstrapping Diffusion: Diffusion Model Training Leveraging Partial and Corrupted Data", "categories": ["cs.CV", "cs.AI"], "comment": "21 pages, 1 figure", "summary": "Training diffusion models requires large datasets. However, acquiring large\nvolumes of high-quality data can be challenging, for example, collecting large\nnumbers of high-resolution images and long videos. On the other hand, there are\nmany complementary data that are usually considered corrupted or partial, such\nas low-resolution images and short videos. Other examples of corrupted data\ninclude videos that contain subtitles, watermarks, and logos. In this study, we\ninvestigate the theoretical problem of whether the above partial data can be\nutilized to train conventional diffusion models. Motivated by our theoretical\nanalysis in this study, we propose a straightforward approach of training\ndiffusion models utilizing partial data views, where we consider each form of\ncomplementary data as a view of conventional data. Our proposed approach first\ntrains one separate diffusion model for each individual view, and then trains a\nmodel for predicting the residual score function. We prove generalization error\nbounds, which show that the proposed diffusion model training approach can\nachieve lower generalization errors if proper regularizations are adopted in\nthe residual score function training. In particular, we prove that the\ndifficulty in training the residual score function scales proportionally with\nthe signal correlations not captured by partial data views. Consequently, the\nproposed approach achieves near first-order optimal data efficiency."}
{"id": "2505.11830", "pdf": "https://arxiv.org/pdf/2505.11830", "abs": "https://arxiv.org/abs/2505.11830", "authors": ["Hongbo Jin", "Ruyang Liu", "Wenhao Zhang", "Guibo Luo", "Ge Li"], "title": "CoT-Vid: Dynamic Chain-of-Thought Routing with Self Verification for Training-Free Video Reasoning", "categories": ["cs.CV", "cs.AI"], "comment": "19 pages, 7 figures", "summary": "System2 reasoning is developing rapidly these days with the emergence of\nDeep- Thinking Models and chain-of-thought technology, which has become a\ncentralized discussion point in the AI community. However, there is a relative\ngap in the research on complex video reasoning at present. In this work, we\npropose CoT-Vid, a novel training-free paradigm for the video domain with a\nmultistage complex reasoning design. Distinguishing from existing video LLMs,\nwhich rely heavily on perceptual abilities, it achieved surprising performance\ngain with explicit reasoning mechanism. The paradigm consists of three main\ncomponents: dynamic inference path routing, problem decoupling strategy, and\nvideo self-consistency verification. In addition, we propose a new standard for\ncategorization of video questions. CoT- Vid showed outstanding results on a\nwide range of benchmarks, and outperforms its base model by 9.3% on Egochema\nand 5.6% on VideoEspresso, rivalling or even surpassing larger and proprietary\nmodels, such as GPT-4V, GPT-4o and Gemini-1.5-flash. Our codebase will be\npublicly available soon."}
{"id": "2505.11838", "pdf": "https://arxiv.org/pdf/2505.11838", "abs": "https://arxiv.org/abs/2505.11838", "authors": ["Yiqing Shen", "Chenjia Li", "Chenxiao Fan", "Mathias Unberath"], "title": "RVTBench: A Benchmark for Visual Reasoning Tasks", "categories": ["cs.CV"], "comment": null, "summary": "Visual reasoning, the capability to interpret visual input in response to\nimplicit text query through multi-step reasoning, remains a challenge for deep\nlearning models due to the lack of relevant benchmarks. Previous work in visual\nreasoning has primarily focused on reasoning segmentation, where models aim to\nsegment objects based on implicit text queries. This paper introduces reasoning\nvisual tasks (RVTs), a unified formulation that extends beyond traditional\nvideo reasoning segmentation to a diverse family of visual language reasoning\nproblems, which can therefore accommodate multiple output formats including\nbounding boxes, natural language descriptions, and question-answer pairs.\nCorrespondingly, we identify the limitations in current benchmark construction\nmethods that rely solely on large language models (LLMs), which inadequately\ncapture complex spatial-temporal relationships and multi-step reasoning chains\nin video due to their reliance on token representation, resulting in benchmarks\nwith artificially limited reasoning complexity. To address this limitation, we\npropose a novel automated RVT benchmark construction pipeline that leverages\ndigital twin (DT) representations as structured intermediaries between\nperception and the generation of implicit text queries. Based on this method,\nwe construct RVTBench, a RVT benchmark containing 3,896 queries of over 1.2\nmillion tokens across four types of RVT (segmentation, grounding, VQA and\nsummary), three reasoning categories (semantic, spatial, and temporal), and\nfour increasing difficulty levels, derived from 200 video sequences. Finally,\nwe propose RVTagent, an agent framework for RVT that allows for zero-shot\ngeneralization across various types of RVT without task-specific fine-tuning."}
{"id": "2505.11842", "pdf": "https://arxiv.org/pdf/2505.11842", "abs": "https://arxiv.org/abs/2505.11842", "authors": ["Xuannan Liu", "Zekun Li", "Zheqi He", "Peipei Li", "Shuhan Xia", "Xing Cui", "Huaibo Huang", "Xi Yang", "Ran He"], "title": "Video-SafetyBench: A Benchmark for Safety Evaluation of Video LVLMs", "categories": ["cs.CV", "cs.CL"], "comment": "Project page:\n  https://liuxuannan.github.io/Video-SafetyBench.github.io/", "summary": "The increasing deployment of Large Vision-Language Models (LVLMs) raises\nsafety concerns under potential malicious inputs. However, existing multimodal\nsafety evaluations primarily focus on model vulnerabilities exposed by static\nimage inputs, ignoring the temporal dynamics of video that may induce distinct\nsafety risks. To bridge this gap, we introduce Video-SafetyBench, the first\ncomprehensive benchmark designed to evaluate the safety of LVLMs under\nvideo-text attacks. It comprises 2,264 video-text pairs spanning 48\nfine-grained unsafe categories, each pairing a synthesized video with either a\nharmful query, which contains explicit malice, or a benign query, which appears\nharmless but triggers harmful behavior when interpreted alongside the video. To\ngenerate semantically accurate videos for safety evaluation, we design a\ncontrollable pipeline that decomposes video semantics into subject images (what\nis shown) and motion text (how it moves), which jointly guide the synthesis of\nquery-relevant videos. To effectively evaluate uncertain or borderline harmful\noutputs, we propose RJScore, a novel LLM-based metric that incorporates the\nconfidence of judge models and human-aligned decision threshold calibration.\nExtensive experiments show that benign-query video composition achieves average\nattack success rates of 67.2%, revealing consistent vulnerabilities to\nvideo-induced attacks. We believe Video-SafetyBench will catalyze future\nresearch into video-based safety evaluation and defense strategies."}
{"id": "2505.11845", "pdf": "https://arxiv.org/pdf/2505.11845", "abs": "https://arxiv.org/abs/2505.11845", "authors": ["Tasrifur Riahi", "Md. Azizul Hakim Bappy", "Md. Mehedi Islam"], "title": "ElderFallGuard: Real-Time IoT and Computer Vision-Based Fall Detection System for Elderly Safety", "categories": ["cs.CV"], "comment": "9 page, 1 table, 5 figure", "summary": "For the elderly population, falls pose a serious and increasing risk of\nserious injury and loss of independence. In order to overcome this difficulty,\nwe present ElderFallGuard: A Computer Vision Based IoT Solution for Elderly\nFall Detection and Notification, a cutting-edge, non-invasive system intended\nfor quick caregiver alerts and real-time fall detection. Our approach leverages\nthe power of computer vision, utilizing MediaPipe for accurate human pose\nestimation from standard video streams. We developed a custom dataset\ncomprising 7200 samples across 12 distinct human poses to train and evaluate\nvarious machine learning classifiers, with Random Forest ultimately selected\nfor its superior performance. ElderFallGuard employs a specific detection\nlogic, identifying a fall when a designated prone pose (\"Pose6\") is held for\nover 3 seconds coupled with a significant drop in motion detected for more than\n2 seconds. Upon confirmation, the system instantly dispatches an alert,\nincluding a snapshot of the event, to a designated Telegram group via a custom\nbot, incorporating cooldown logic to prevent notification overload. Rigorous\ntesting on our dataset demonstrated exceptional results, achieving 100%\naccuracy, precision, recall, and F1-score. ElderFallGuard offers a promising,\nvision-based IoT solution to enhance elderly safety and provide peace of mind\nfor caregivers through intelligent, timely alerts."}
{"id": "2505.11852", "pdf": "https://arxiv.org/pdf/2505.11852", "abs": "https://arxiv.org/abs/2505.11852", "authors": ["Jingkun Yue", "Siqi Zhang", "Zinan Jia", "Huihuan Xu", "Zongbo Han", "Xiaohong Liu", "Guangyu Wang"], "title": "MedSG-Bench: A Benchmark for Medical Image Sequences Grounding", "categories": ["cs.CV"], "comment": null, "summary": "Visual grounding is essential for precise perception and reasoning in\nmultimodal large language models (MLLMs), especially in medical imaging\ndomains. While existing medical visual grounding benchmarks primarily focus on\nsingle-image scenarios, real-world clinical applications often involve\nsequential images, where accurate lesion localization across different\nmodalities and temporal tracking of disease progression (e.g., pre- vs.\npost-treatment comparison) require fine-grained cross-image semantic alignment\nand context-aware reasoning. To remedy the underrepresentation of image\nsequences in existing medical visual grounding benchmarks, we propose\nMedSG-Bench, the first benchmark tailored for Medical Image Sequences\nGrounding. It comprises eight VQA-style tasks, formulated into two paradigms of\nthe grounding tasks, including 1) Image Difference Grounding, which focuses on\ndetecting change regions across images, and 2) Image Consistency Grounding,\nwhich emphasizes detection of consistent or shared semantics across sequential\nimages. MedSG-Bench covers 76 public datasets, 10 medical imaging modalities,\nand a wide spectrum of anatomical structures and diseases, totaling 9,630\nquestion-answer pairs. We benchmark both general-purpose MLLMs (e.g.,\nQwen2.5-VL) and medical-domain specialized MLLMs (e.g., HuatuoGPT-vision),\nobserving that even the advanced models exhibit substantial limitations in\nmedical sequential grounding tasks. To advance this field, we construct\nMedSG-188K, a large-scale instruction-tuning dataset tailored for sequential\nvisual grounding, and further develop MedSeq-Grounder, an MLLM designed to\nfacilitate future research on fine-grained understanding across medical\nsequential images. The benchmark, dataset, and model are available at\nhttps://huggingface.co/MedSG-Bench"}
{"id": "2505.11868", "pdf": "https://arxiv.org/pdf/2505.11868", "abs": "https://arxiv.org/abs/2505.11868", "authors": ["Hongyi Zhou", "Xiaogang Wang", "Yulan Guo", "Kai Xu"], "title": "MonoMobility: Zero-Shot 3D Mobility Analysis from Monocular Videos", "categories": ["cs.CV"], "comment": null, "summary": "Accurately analyzing the motion parts and their motion attributes in dynamic\nenvironments is crucial for advancing key areas such as embodied intelligence.\nAddressing the limitations of existing methods that rely on dense multi-view\nimages or detailed part-level annotations, we propose an innovative framework\nthat can analyze 3D mobility from monocular videos in a zero-shot manner. This\nframework can precisely parse motion parts and motion attributes only using a\nmonocular video, completely eliminating the need for annotated training data.\nSpecifically, our method first constructs the scene geometry and roughly\nanalyzes the motion parts and their initial motion attributes combining depth\nestimation, optical flow analysis and point cloud registration method, then\nemploys 2D Gaussian splatting for scene representation. Building on this, we\nintroduce an end-to-end dynamic scene optimization algorithm specifically\ndesigned for articulated objects, refining the initial analysis results to\nensure the system can handle 'rotation', 'translation', and even complex\nmovements ('rotation+translation'), demonstrating high flexibility and\nversatility. To validate the robustness and wide applicability of our method,\nwe created a comprehensive dataset comprising both simulated and real-world\nscenarios. Experimental results show that our framework can effectively analyze\narticulated object motions in an annotation-free manner, showcasing its\nsignificant potential in future embodied intelligence applications."}
{"id": "2505.11872", "pdf": "https://arxiv.org/pdf/2505.11872", "abs": "https://arxiv.org/abs/2505.11872", "authors": ["Quoc-Huy Trinh", "Minh-Van Nguyen", "Jung Peng", "Ulas Bagci", "Debesh Jha"], "title": "PRS-Med: Position Reasoning Segmentation with Vision-Language Model in Medical Imaging", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in prompt-based medical image segmentation have enabled\nclinicians to identify tumors using simple input like bounding boxes or text\nprompts. However, existing methods face challenges when doctors need to\ninteract through natural language or when position reasoning is required -\nunderstanding spatial relationships between anatomical structures and\npathologies. We present PRS-Med, a framework that integrates vision-language\nmodels with segmentation capabilities to generate both accurate segmentation\nmasks and corresponding spatial reasoning outputs. Additionally, we introduce\nthe MMRS dataset (Multimodal Medical in Positional Reasoning Segmentation),\nwhich provides diverse, spatially-grounded question-answer pairs to address the\nlack of position reasoning data in medical imaging. PRS-Med demonstrates\nsuperior performance across six imaging modalities (CT, MRI, X-ray, ultrasound,\nendoscopy, RGB), significantly outperforming state-of-the-art methods in both\nsegmentation accuracy and position reasoning. Our approach enables intuitive\ndoctor-system interaction through natural language, facilitating more efficient\ndiagnoses. Our dataset pipeline, model, and codebase will be released to foster\nfurther research in spatially-aware multimodal reasoning for medical\napplications."}
{"id": "2505.11881", "pdf": "https://arxiv.org/pdf/2505.11881", "abs": "https://arxiv.org/abs/2505.11881", "authors": ["Giyeong Oh", "Woohyun Cho", "Siyeol Kim", "Suhwan Choi", "Younjae Yu"], "title": "Revisiting Residual Connections: Orthogonal Updates for Stable and Efficient Deep Networks", "categories": ["cs.CV", "cs.AI"], "comment": "27 pages, WIP", "summary": "Residual connections are pivotal for deep neural networks, enabling greater\ndepth by mitigating vanishing gradients. However, in standard residual updates,\nthe module's output is directly added to the input stream. This can lead to\nupdates that predominantly reinforce or modulate the existing stream direction,\npotentially underutilizing the module's capacity for learning entirely novel\nfeatures. In this work, we introduce Orthogonal Residual Update: we decompose\nthe module's output relative to the input stream and add only the component\northogonal to this stream. This design aims to guide modules to contribute\nprimarily new representational directions, fostering richer feature learning\nwhile promoting more efficient training. We demonstrate that our orthogonal\nupdate strategy improves generalization accuracy and training stability across\ndiverse architectures (ResNetV2, Vision Transformers) and datasets (CIFARs,\nTinyImageNet, ImageNet-1k), achieving, for instance, a +4.3\\%p top-1 accuracy\ngain for ViT-B on ImageNet-1k."}
{"id": "2505.11882", "pdf": "https://arxiv.org/pdf/2505.11882", "abs": "https://arxiv.org/abs/2505.11882", "authors": ["Shiming Chen", "Dingjie Fu", "Salman Khan", "Fahad Shahbaz Khan"], "title": "GenZSL: Generative Zero-Shot Learning Via Inductive Variational Autoencoder", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted to ICML'25", "summary": "Remarkable progress in zero-shot learning (ZSL) has been achieved using\ngenerative models. However, existing generative ZSL methods merely generate\n(imagine) the visual features from scratch guided by the strong class semantic\nvectors annotated by experts, resulting in suboptimal generative performance\nand limited scene generalization. To address these and advance ZSL, we propose\nan inductive variational autoencoder for generative zero-shot learning, dubbed\nGenZSL. Mimicking human-level concept learning, GenZSL operates by inducting\nnew class samples from similar seen classes using weak class semantic vectors\nderived from target class names (i.e., CLIP text embedding). To ensure the\ngeneration of informative samples for training an effective ZSL classifier, our\nGenZSL incorporates two key strategies. Firstly, it employs class diversity\npromotion to enhance the diversity of class semantic vectors. Secondly, it\nutilizes target class-guided information boosting criteria to optimize the\nmodel. Extensive experiments conducted on three popular benchmark datasets\nshowcase the superiority and potential of our GenZSL with significant efficacy\nand efficiency over f-VAEGAN, e.g., 24.7% performance gains and more than\n$60\\times$ faster training speed on AWA2. Codes are available at\nhttps://github.com/shiming-chen/GenZSL."}
{"id": "2505.11884", "pdf": "https://arxiv.org/pdf/2505.11884", "abs": "https://arxiv.org/abs/2505.11884", "authors": ["Zhongwen Li", "Zongwei Li", "Xiaoqi Li"], "title": "Facial Recognition Leveraging Generative Adversarial Networks", "categories": ["cs.CV", "cs.CR"], "comment": null, "summary": "Face recognition performance based on deep learning heavily relies on\nlarge-scale training data, which is often difficult to acquire in practical\napplications. To address this challenge, this paper proposes a GAN-based data\naugmentation method with three key contributions: (1) a residual-embedded\ngenerator to alleviate gradient vanishing/exploding problems, (2) an Inception\nResNet-V1 based FaceNet discriminator for improved adversarial training, and\n(3) an end-to-end framework that jointly optimizes data generation and\nrecognition performance. Experimental results demonstrate that our approach\nachieves stable training dynamics and significantly improves face recognition\naccuracy by 12.7% on the LFW benchmark compared to baseline methods, while\nmaintaining good generalization capability with limited training samples."}
{"id": "2505.11895", "pdf": "https://arxiv.org/pdf/2505.11895", "abs": "https://arxiv.org/abs/2505.11895", "authors": ["Chih-Ting Liao", "Bin Ren", "Guofeng Mei", "Xu Zheng"], "title": "Adversarial Robustness for Unified Multi-Modal Encoders via Efficient Calibration", "categories": ["cs.CV"], "comment": null, "summary": "Recent unified multi-modal encoders align a wide range of modalities into a\nshared representation space, enabling diverse cross-modal tasks. Despite their\nimpressive capabilities, the robustness of these models under adversarial\nperturbations remains underexplored, which is a critical concern for\nsafety-sensitive applications. In this work, we present the first comprehensive\nstudy of adversarial vulnerability in unified multi-modal encoders. We find\nthat even mild adversarial perturbations lead to substantial performance drops\nacross all modalities. Non-visual inputs, such as audio and point clouds, are\nespecially fragile, while visual inputs like images and videos also degrade\nsignificantly. To address this, we propose an efficient adversarial calibration\nframework that improves robustness across modalities without modifying\npretrained encoders or semantic centers, ensuring compatibility with existing\nfoundation models. Our method introduces modality-specific projection heads\ntrained solely on adversarial examples, while keeping the backbone and\nembeddings frozen. We explore three training objectives: fixed-center\ncross-entropy, clean-to-adversarial L2 alignment, and clean-adversarial\nInfoNCE, and we introduce a regularization strategy to ensure\nmodality-consistent alignment under attack. Experiments on six modalities and\nthree Bind-style models show that our method improves adversarial robustness by\nup to 47.3 percent at epsilon = 4/255, while preserving or even improving clean\nzero-shot and retrieval performance with less than 1 percent trainable\nparameters."}
{"id": "2505.11897", "pdf": "https://arxiv.org/pdf/2505.11897", "abs": "https://arxiv.org/abs/2505.11897", "authors": ["Seonghak Kim"], "title": "FiGKD: Fine-Grained Knowledge Distillation via High-Frequency Detail Transfer", "categories": ["cs.CV"], "comment": "14 pages, 6 figures. This work has been submitted to the Elsevier for\n  possible publication", "summary": "Knowledge distillation (KD) is a widely adopted technique for transferring\nknowledge from a high-capacity teacher model to a smaller student model by\naligning their output distributions. However, existing methods often\nunderperform in fine-grained visual recognition tasks, where distinguishing\nsubtle differences between visually similar classes is essential. This\nperformance gap stems from the fact that conventional approaches treat the\nteacher's output logits as a single, undifferentiated signal-assuming all\ncontained information is equally beneficial to the student. Consequently,\nstudent models may become overloaded with redundant signals and fail to capture\nthe teacher's nuanced decision boundaries. To address this issue, we propose\nFine-Grained Knowledge Distillation (FiGKD), a novel frequency-aware framework\nthat decomposes a model's logits into low-frequency (content) and\nhigh-frequency (detail) components using the discrete wavelet transform (DWT).\nFiGKD selectively transfers only the high-frequency components, which encode\nthe teacher's semantic decision patterns, while discarding redundant\nlow-frequency content already conveyed through ground-truth supervision. Our\napproach is simple, architecture-agnostic, and requires no access to\nintermediate feature maps. Extensive experiments on CIFAR-100, TinyImageNet,\nand multiple fine-grained recognition benchmarks show that FiGKD consistently\noutperforms state-of-the-art logit-based and feature-based distillation methods\nacross a variety of teacher-student configurations. These findings confirm that\nfrequency-aware logit decomposition enables more efficient and effective\nknowledge transfer, particularly in resource-constrained settings."}
{"id": "2505.11905", "pdf": "https://arxiv.org/pdf/2505.11905", "abs": "https://arxiv.org/abs/2505.11905", "authors": ["Takuya Ikeda", "Sergey Zakharov", "Muhammad Zubair Irshad", "Istvan Balazs Opra", "Shun Iwase", "Dian Chen", "Mark Tjersland", "Robert Lee", "Alexandre Dilly", "Rares Ambrus", "Koichi Nishiwaki"], "title": "GTR: Gaussian Splatting Tracking and Reconstruction of Unknown Objects Based on Appearance and Geometric Complexity", "categories": ["cs.CV", "cs.RO"], "comment": "main contains 10 pages, 9 figures. And supplementary material\n  contains 10 pages, 27 figures", "summary": "We present a novel method for 6-DoF object tracking and high-quality 3D\nreconstruction from monocular RGBD video. Existing methods, while achieving\nimpressive results, often struggle with complex objects, particularly those\nexhibiting symmetry, intricate geometry or complex appearance. To bridge these\ngaps, we introduce an adaptive method that combines 3D Gaussian Splatting,\nhybrid geometry/appearance tracking, and key frame selection to achieve robust\ntracking and accurate reconstructions across a diverse range of objects.\nAdditionally, we present a benchmark covering these challenging object classes,\nproviding high-quality annotations for evaluating both tracking and\nreconstruction performance. Our approach demonstrates strong capabilities in\nrecovering high-fidelity object meshes, setting a new standard for\nsingle-sensor 3D reconstruction in open-world environments."}
{"id": "2505.11907", "pdf": "https://arxiv.org/pdf/2505.11907", "abs": "https://arxiv.org/abs/2505.11907", "authors": ["Zihao Dongfang", "Xu Zheng", "Ziqiao Weng", "Yuanhuiyi Lyu", "Danda Pani Paudel", "Luc Van Gool", "Kailun Yang", "Xuming Hu"], "title": "Are Multimodal Large Language Models Ready for Omnidirectional Spatial Reasoning?", "categories": ["cs.CV"], "comment": null, "summary": "The 180x360 omnidirectional field of view captured by 360-degree cameras\nenables their use in a wide range of applications such as embodied AI and\nvirtual reality. Although recent advances in multimodal large language models\n(MLLMs) have shown promise in visual-spatial reasoning, most studies focus on\nstandard pinhole-view images, leaving omnidirectional perception largely\nunexplored. In this paper, we ask: Are MLLMs ready for omnidirectional spatial\nreasoning? To investigate this, we introduce OSR-Bench, the first benchmark\nspecifically designed for this setting. OSR-Bench includes over 153,000 diverse\nquestion-answer pairs grounded in high-fidelity panoramic indoor scene maps. It\ncovers key reasoning types including object counting, relative distance, and\ndirection. We also propose a negative sampling strategy that inserts\nnon-existent objects into prompts to evaluate hallucination and grounding\nrobustness. For fine-grained analysis, we design a two-stage evaluation\nframework assessing both cognitive map generation and QA accuracy using\nrotation-invariant matching and a combination of rule-based and LLM-based\nmetrics. We evaluate eight state-of-the-art MLLMs, including GPT-4o, Gemini 1.5\nPro, and leading open-source models under zero-shot settings. Results show that\ncurrent models struggle with spatial reasoning in panoramic contexts,\nhighlighting the need for more perceptually grounded MLLMs. OSR-Bench and code\nwill be released at: https://huggingface.co/datasets/UUUserna/OSR-Bench"}
{"id": "2505.11921", "pdf": "https://arxiv.org/pdf/2505.11921", "abs": "https://arxiv.org/abs/2505.11921", "authors": ["Haitao Li", "Ziyu Li", "Yiheng Mao", "Zhengyao Ding", "Zhengxing Huang"], "title": "DC-Seg: Disentangled Contrastive Learning for Brain Tumor Segmentation with Missing Modalities", "categories": ["cs.CV"], "comment": null, "summary": "Accurate segmentation of brain images typically requires the integration of\ncomplementary information from multiple image modalities. However, clinical\ndata for all modalities may not be available for every patient, creating a\nsignificant challenge. To address this, previous studies encode multiple\nmodalities into a shared latent space. While somewhat effective, it remains\nsuboptimal, as each modality contains distinct and valuable information. In\nthis study, we propose DC-Seg (Disentangled Contrastive Learning for\nSegmentation), a new method that explicitly disentangles images into\nmodality-invariant anatomical representation and modality-specific\nrepresentation, by using anatomical contrastive learning and modality\ncontrastive learning respectively. This solution improves the separation of\nanatomical and modality-specific features by considering the modality gaps,\nleading to more robust representations. Furthermore, we introduce a\nsegmentation-based regularizer that enhances the model's robustness to missing\nmodalities. Extensive experiments on the BraTS 2020 and a private white matter\nhyperintensity(WMH) segmentation dataset demonstrate that DC-Seg outperforms\nstate-of-the-art methods in handling incomplete multimodal brain tumor\nsegmentation tasks with varying missing modalities, while also demonstrate\nstrong generalizability in WMH segmentation. The code is available at\nhttps://github.com/CuCl-2/DC-Seg."}
{"id": "2505.11926", "pdf": "https://arxiv.org/pdf/2505.11926", "abs": "https://arxiv.org/abs/2505.11926", "authors": ["Yixu Wang", "Jiaxin Song", "Yifeng Gao", "Xin Wang", "Yang Yao", "Yan Teng", "Xingjun Ma", "Yingchun Wang", "Yu-Gang Jiang"], "title": "SafeVid: Toward Safety Aligned Video Large Multimodal Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "As Video Large Multimodal Models (VLMMs) rapidly advance, their inherent\ncomplexity introduces significant safety challenges, particularly the issue of\nmismatched generalization where static safety alignments fail to transfer to\ndynamic video contexts. We introduce SafeVid, a framework designed to instill\nvideo-specific safety principles in VLMMs. SafeVid uniquely transfers robust\ntextual safety alignment capabilities to the video domain by employing detailed\ntextual video descriptions as an interpretive bridge, facilitating LLM-based\nrule-driven safety reasoning. This is achieved through a closed-loop system\ncomprising: 1) generation of SafeVid-350K, a novel 350,000-pair video-specific\nsafety preference dataset; 2) targeted alignment of VLMMs using Direct\nPreference Optimization (DPO); and 3) comprehensive evaluation via our new\nSafeVidBench benchmark. Alignment with SafeVid-350K significantly enhances VLMM\nsafety, with models like LLaVA-NeXT-Video demonstrating substantial\nimprovements (e.g., up to 42.39%) on SafeVidBench. SafeVid provides critical\nresources and a structured approach, demonstrating that leveraging textual\ndescriptions as a conduit for safety reasoning markedly improves the safety\nalignment of VLMMs. We have made SafeVid-350K dataset\n(https://huggingface.co/datasets/yxwang/SafeVid-350K) publicly available."}
{"id": "2505.11934", "pdf": "https://arxiv.org/pdf/2505.11934", "abs": "https://arxiv.org/abs/2505.11934", "authors": ["Yian Zhao", "Wanshi Xu", "Ruochong Zheng", "Pengchong Qiao", "Chang Liu", "Jie Chen"], "title": "iSegMan: Interactive Segment-and-Manipulate 3D Gaussians", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "The efficient rendering and explicit nature of 3DGS promote the advancement\nof 3D scene manipulation. However, existing methods typically encounter\nchallenges in controlling the manipulation region and are unable to furnish the\nuser with interactive feedback, which inevitably leads to unexpected results.\nIntuitively, incorporating interactive 3D segmentation tools can compensate for\nthis deficiency. Nevertheless, existing segmentation frameworks impose a\npre-processing step of scene-specific parameter training, which limits the\nefficiency and flexibility of scene manipulation. To deliver a 3D region\ncontrol module that is well-suited for scene manipulation with reliable\nefficiency, we propose interactive Segment-and-Manipulate 3D Gaussians\n(iSegMan), an interactive segmentation and manipulation framework that only\nrequires simple 2D user interactions in any view. To propagate user\ninteractions to other views, we propose Epipolar-guided Interaction Propagation\n(EIP), which innovatively exploits epipolar constraint for efficient and robust\ninteraction matching. To avoid scene-specific training to maintain efficiency,\nwe further propose the novel Visibility-based Gaussian Voting (VGV), which\nobtains 2D segmentations from SAM and models the region extraction as a voting\ngame between 2D Pixels and 3D Gaussians based on Gaussian visibility. Taking\nadvantage of the efficient and precise region control of EIP and VGV, we put\nforth a Manipulation Toolbox to implement various functions on selected\nregions, enhancing the controllability, flexibility and practicality of scene\nmanipulation. Extensive results on 3D scene manipulation and segmentation tasks\nfully demonstrate the significant advantages of iSegMan. Project page is\navailable at https://zhao-yian.github.io/iSegMan."}
{"id": "2505.11945", "pdf": "https://arxiv.org/pdf/2505.11945", "abs": "https://arxiv.org/abs/2505.11945", "authors": ["Bonan li", "Zicheng Zhang", "Songhua Liu", "Weihao Yu", "Xinchao Wang"], "title": "Top-Down Compression: Revisit Efficient Vision Token Projection for Visual Instruction Tuning", "categories": ["cs.CV"], "comment": "Under Review", "summary": "Visual instruction tuning aims to enable large language models to comprehend\nthe visual world, with a pivotal challenge lying in establishing an effective\nvision-to-language projection. However, existing methods often grapple with the\nintractable trade-off between accuracy and efficiency. In this paper, we\npresent LLaVA-Meteor, a novel approach designed to break this deadlock,\nequipped with a novel Top-Down Compression paradigm that strategically\ncompresses visual tokens without compromising core information. Specifically,\nwe construct a trainable Flash Global Fusion module based on efficient\nselective state space operators, which aligns the feature space while enabling\neach token to perceive holistic visual context and instruction preference at\nlow cost. Furthermore, a local-to-single scanning manner is employed to\neffectively capture local dependencies, thereby enhancing the model's\ncapability in vision modeling. To alleviate computational overhead, we explore\na Visual-Native Selection mechanism that independently assesses token\nsignificance by both the visual and native experts, followed by aggregation to\nretain the most critical subset. Extensive experiments show that our approach\nreduces visual tokens by 75--95% while achieving comparable or superior\nperformance across 12 benchmarks, significantly improving efficiency."}
{"id": "2505.11976", "pdf": "https://arxiv.org/pdf/2505.11976", "abs": "https://arxiv.org/abs/2505.11976", "authors": ["Soumya Swarup Prusty", "Astha Agarwal", "Srinivasan Iyenger"], "title": "Advanced Integration of Discrete Line Segments in Digitized P&ID for Continuous Instrument Connectivity", "categories": ["cs.CV"], "comment": "6 pages, 13 figures", "summary": "Piping and Instrumentation Diagrams (P&IDs) constitute the foundational\nblueprint of a plant, depicting the interconnections among process equipment,\ninstrumentation for process control, and the flow of fluids and control\nsignals. In their existing setup, the manual mapping of information from P&ID\nsheets holds a significant challenge. This is a time-consuming process, taking\naround 3-6 months, and is susceptible to errors. It also depends on the\nexpertise of the domain experts and often requires multiple rounds of review.\nThe digitization of P&IDs entails merging detected line segments, which is\nessential for linking various detected instruments, thereby creating a\ncomprehensive digitized P&ID. This paper focuses on explaining how line\nsegments which are detected using a computer vision model are merged and\neventually building the connection between equipment and merged lines. Hence\npresenting a digitized form of information stating the interconnection between\nprocess equipment, instrumentation, flow of fluids and control signals.\nEventually, which can be stored in a knowledge graph and that information along\nwith the help of advanced algorithms can be leveraged for tasks like finding\noptimal routes, detecting system cycles, computing transitive closures, and\nmore."}
{"id": "2505.11980", "pdf": "https://arxiv.org/pdf/2505.11980", "abs": "https://arxiv.org/abs/2505.11980", "authors": ["Yi Chen", "Mu-Young Son", "Chuanbo Hua", "Joo-Young Kim"], "title": "AoP-SAM: Automation of Prompts for Efficient Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted at AAAI 2025", "summary": "The Segment Anything Model (SAM) is a powerful foundation model for image\nsegmentation, showing robust zero-shot generalization through prompt\nengineering. However, relying on manual prompts is impractical for real-world\napplications, particularly in scenarios where rapid prompt provision and\nresource efficiency are crucial. In this paper, we propose the Automation of\nPrompts for SAM (AoP-SAM), a novel approach that learns to generate essential\nprompts in optimal locations automatically. AoP-SAM enhances SAM's efficiency\nand usability by eliminating manual input, making it better suited for\nreal-world tasks. Our approach employs a lightweight yet efficient Prompt\nPredictor model that detects key entities across images and identifies the\noptimal regions for placing prompt candidates. This method leverages SAM's\nimage embeddings, preserving its zero-shot generalization capabilities without\nrequiring fine-tuning. Additionally, we introduce a test-time instance-level\nAdaptive Sampling and Filtering mechanism that generates prompts in a\ncoarse-to-fine manner. This notably enhances both prompt and mask generation\nefficiency by reducing computational overhead and minimizing redundant mask\nrefinements. Evaluations of three datasets demonstrate that AoP-SAM\nsubstantially improves both prompt generation efficiency and mask generation\naccuracy, making SAM more effective for automated segmentation tasks."}
{"id": "2505.11983", "pdf": "https://arxiv.org/pdf/2505.11983", "abs": "https://arxiv.org/abs/2505.11983", "authors": ["Ting Xiao", "Lei Shi", "Yang Zhang", "HaoFeng Yang", "Zhe Wang", "Chenjia Bai"], "title": "Online Iterative Self-Alignment for Radiology Report Generation", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by ACL 2025 Main", "summary": "Radiology Report Generation (RRG) is an important research topic for\nrelieving radiologist' heavy workload. Existing RRG models mainly rely on\nsupervised fine-tuning (SFT) based on different model architectures using data\npairs of radiological images and corresponding radiologist-annotated reports.\nRecent research has shifted focus to post-training improvements, aligning RRG\nmodel outputs with human preferences using reinforcement learning (RL).\nHowever, the limited data coverage of high-quality annotated data poses risks\nof overfitting and generalization. This paper proposes a novel Online Iterative\nSelf-Alignment (OISA) method for RRG that consists of four stages:\nself-generation of diverse data, self-evaluation for multi-objective preference\ndata,self-alignment for multi-objective optimization and self-iteration for\nfurther improvement. Our approach allows for generating varied reports tailored\nto specific clinical objectives, enhancing the overall performance of the RRG\nmodel iteratively. Unlike existing methods, our frame-work significantly\nincreases data quality and optimizes performance through iterative\nmulti-objective optimization. Experimental results demonstrate that our method\nsurpasses previous approaches, achieving state-of-the-art performance across\nmultiple evaluation metrics."}
{"id": "2505.11992", "pdf": "https://arxiv.org/pdf/2505.11992", "abs": "https://arxiv.org/abs/2505.11992", "authors": ["Songchun Zhang", "Huiyao Xu", "Sitong Guo", "Zhongwei Xie", "Pengwei Liu", "Hujun Bao", "Weiwei Xu", "Changqing Zou"], "title": "SpatialCrafter: Unleashing the Imagination of Video Diffusion Models for Scene Reconstruction from Limited Observations", "categories": ["cs.CV"], "comment": "18 pages, 16 figures", "summary": "Novel view synthesis (NVS) boosts immersive experiences in computer vision\nand graphics. Existing techniques, though progressed, rely on dense multi-view\nobservations, restricting their application. This work takes on the challenge\nof reconstructing photorealistic 3D scenes from sparse or single-view inputs.\nWe introduce SpatialCrafter, a framework that leverages the rich knowledge in\nvideo diffusion models to generate plausible additional observations, thereby\nalleviating reconstruction ambiguity. Through a trainable camera encoder and an\nepipolar attention mechanism for explicit geometric constraints, we achieve\nprecise camera control and 3D consistency, further reinforced by a unified\nscale estimation strategy to handle scale discrepancies across datasets.\nFurthermore, by integrating monocular depth priors with semantic features in\nthe video latent space, our framework directly regresses 3D Gaussian primitives\nand efficiently processes long-sequence features using a hybrid network\nstructure. Extensive experiments show our method enhances sparse view\nreconstruction and restores the realistic appearance of 3D scenes."}
{"id": "2505.11997", "pdf": "https://arxiv.org/pdf/2505.11997", "abs": "https://arxiv.org/abs/2505.11997", "authors": ["Mingcheng Qu", "Guang Yang", "Donglin", "Tonghua Su", "Yue Gao", "Yang Song", "Lei Fan"], "title": "Multimodal Cancer Survival Analysis via Hypergraph Learning with Cross-Modality Rebalance", "categories": ["cs.CV"], "comment": "Code: https://github.com/MCPathology/MRePath", "summary": "Multimodal pathology-genomic analysis has become increasingly prominent in\ncancer survival prediction. However, existing studies mainly utilize\nmulti-instance learning to aggregate patch-level features, neglecting the\ninformation loss of contextual and hierarchical details within pathology\nimages. Furthermore, the disparity in data granularity and dimensionality\nbetween pathology and genomics leads to a significant modality imbalance. The\nhigh spatial resolution inherent in pathology data renders it a dominant role\nwhile overshadowing genomics in multimodal integration. In this paper, we\npropose a multimodal survival prediction framework that incorporates hypergraph\nlearning to effectively capture both contextual and hierarchical details from\npathology images. Moreover, it employs a modality rebalance mechanism and an\ninteractive alignment fusion strategy to dynamically reweight the contributions\nof the two modalities, thereby mitigating the pathology-genomics imbalance.\nQuantitative and qualitative experiments are conducted on five TCGA datasets,\ndemonstrating that our model outperforms advanced methods by over 3.4\\% in\nC-Index performance."}
{"id": "2505.12000", "pdf": "https://arxiv.org/pdf/2505.12000", "abs": "https://arxiv.org/abs/2505.12000", "authors": ["Tan-Hanh Pham", "Phu-Vinh Nguyen", "Dang The Hung", "Bui Trong Duong", "Vu Nguyen Thanh", "Chris Ngo", "Tri Quang Truong", "Truong-Son Hy"], "title": "IQBench: How \"Smart'' Are Vision-Language Models? A Study with Human IQ Tests", "categories": ["cs.CV"], "comment": "IQ Test for Multimodal Models", "summary": "Although large Vision-Language Models (VLMs) have demonstrated remarkable\nperformance in a wide range of multimodal tasks, their true reasoning\ncapabilities on human IQ tests remain underexplored. To advance research on the\nfluid intelligence of VLMs, we introduce **IQBench**, a new benchmark designed\nto evaluate VLMs on standardized visual IQ tests. We focus on evaluating the\nreasoning capabilities of VLMs, which we argue are more important than the\naccuracy of the final prediction. **Our benchmark is visually centric,\nminimizing the dependence on unnecessary textual content**, thus encouraging\nmodels to derive answers primarily from image-based information rather than\nlearned textual knowledge. To this end, we manually collected and annotated 500\nvisual IQ questions to **prevent unintentional data leakage during training**.\nUnlike prior work that focuses primarily on the accuracy of the final answer,\nwe evaluate the reasoning ability of the models by assessing their explanations\nand the patterns used to solve each problem, along with the accuracy of the\nfinal prediction and human evaluation. Our experiments show that there are\nsubstantial performance disparities between tasks, with models such as\n`o4-mini`, `gemini-2.5-flash`, and `claude-3.7-sonnet` achieving the highest\naverage accuracies of 0.615, 0.578, and 0.548, respectively. However, all\nmodels struggle with 3D spatial and anagram reasoning tasks, highlighting\nsignificant limitations in current VLMs' general reasoning abilities. In terms\nof reasoning scores, `o4-mini`, `gemini-2.5-flash`, and `claude-3.7-sonnet`\nachieved top averages of 0.696, 0.586, and 0.516, respectively. These results\nhighlight inconsistencies between the reasoning processes of the models and\ntheir final answers, emphasizing the importance of evaluating the accuracy of\nthe reasoning in addition to the final predictions."}
{"id": "2505.12005", "pdf": "https://arxiv.org/pdf/2505.12005", "abs": "https://arxiv.org/abs/2505.12005", "authors": ["Dong Liu", "Yifan Yang", "Zixiong Huang", "Yuxin Gao", "Mingkui Tan"], "title": "CHRIS: Clothed Human Reconstruction with Side View Consistency", "categories": ["cs.CV", "cs.AI"], "comment": "ICME 2025", "summary": "Creating a realistic clothed human from a single-view RGB image is crucial\nfor applications like mixed reality and filmmaking. Despite some progress in\nrecent years, mainstream methods often fail to fully utilize side-view\ninformation, as the input single-view image contains front-view information\nonly. This leads to globally unrealistic topology and local surface\ninconsistency in side views. To address these, we introduce Clothed Human\nReconstruction with Side View Consistency, namely CHRIS, which consists of 1) A\nSide-View Normal Discriminator that enhances global visual reasonability by\ndistinguishing the generated side-view normals from the ground truth ones; 2) A\nMulti-to-One Gradient Computation (M2O) that ensures local surface consistency.\nM2O calculates the gradient of a sampling point by integrating the gradients of\nthe nearby points, effectively acting as a smooth operation. Experimental\nresults demonstrate that CHRIS achieves state-of-the-art performance on public\nbenchmarks and outperforms the prior work."}
{"id": "2505.12007", "pdf": "https://arxiv.org/pdf/2505.12007", "abs": "https://arxiv.org/abs/2505.12007", "authors": ["Runduo Han", "Xiuping Liu", "Shangxuan Yi", "Yi Zhang", "Hongchen Tan"], "title": "Multi-modal Collaborative Optimization and Expansion Network for Event-assisted Single-eye Expression Recognition", "categories": ["cs.CV"], "comment": null, "summary": "In this paper, we proposed a Multi-modal Collaborative Optimization and\nExpansion Network (MCO-E Net), to use event modalities to resist challenges\nsuch as low light, high exposure, and high dynamic range in single-eye\nexpression recognition tasks. The MCO-E Net introduces two innovative designs:\nMulti-modal Collaborative Optimization Mamba (MCO-Mamba) and Heterogeneous\nCollaborative and Expansion Mixture-of-Experts (HCE-MoE). MCO-Mamba, building\nupon Mamba, leverages dual-modal information to jointly optimize the model,\nfacilitating collaborative interaction and fusion of modal semantics. This\napproach encourages the model to balance the learning of both modalities and\nharness their respective strengths. HCE-MoE, on the other hand, employs a\ndynamic routing mechanism to distribute structurally varied experts (deep,\nattention, and focal), fostering collaborative learning of complementary\nsemantics. This heterogeneous architecture systematically integrates diverse\nfeature extraction paradigms to comprehensively capture expression semantics.\nExtensive experiments demonstrate that our proposed network achieves\ncompetitive performance in the task of single-eye expression recognition,\nespecially under poor lighting conditions."}
{"id": "2505.12009", "pdf": "https://arxiv.org/pdf/2505.12009", "abs": "https://arxiv.org/abs/2505.12009", "authors": ["Zhiying Li", "Guanggang Geng", "Yeying Jin", "Zhizhi Guo", "Bruce Gu", "Jidong Huo", "Zhaoxin Fan", "Wenjun Wu"], "title": "Black-box Adversaries from Latent Space: Unnoticeable Attacks on Human Pose and Shape Estimation", "categories": ["cs.CV"], "comment": "17 pages, 6 figures", "summary": "Expressive human pose and shape (EHPS) estimation is vital for digital human\ngeneration, particularly in live-streaming applications. However, most existing\nEHPS models focus primarily on minimizing estimation errors, with limited\nattention on potential security vulnerabilities. Current adversarial attacks on\nEHPS models often require white-box access (e.g., model details or gradients)\nor generate visually conspicuous perturbations, limiting their practicality and\nability to expose real-world security threats. To address these limitations, we\npropose a novel Unnoticeable Black-Box Attack (UBA) against EHPS models. UBA\nleverages the latent-space representations of natural images to generate an\noptimal adversarial noise pattern and iteratively refine its attack potency\nalong an optimized direction in digital space. Crucially, this process relies\nsolely on querying the model's output, requiring no internal knowledge of the\nEHPS architecture, while guiding the noise optimization toward greater stealth\nand effectiveness. Extensive experiments and visual analyses demonstrate the\nsuperiority of UBA. Notably, UBA increases the pose estimation errors of EHPS\nmodels by 17.27%-58.21% on average, revealing critical vulnerabilities. These\nfindings underscore the urgent need to address and mitigate security risks\nassociated with digital human generation systems."}
{"id": "2505.12021", "pdf": "https://arxiv.org/pdf/2505.12021", "abs": "https://arxiv.org/abs/2505.12021", "authors": ["Kazuhiko Kawamoto", "Atsuhiro Endo", "Hiroshi Kera"], "title": "Cross-Model Transfer of Task Vectors via Few-Shot Orthogonal Alignment", "categories": ["cs.CV"], "comment": "8 pages", "summary": "Task arithmetic enables efficient model editing by representing task-specific\nchanges as vectors in parameter space. Task arithmetic typically assumes that\nthe source and target models are initialized from the same pre-trained\nparameters. This assumption limits its applicability in cross-model transfer\nsettings, where models are independently pre-trained on different datasets. To\naddress this challenge, we propose a method based on few-shot orthogonal\nalignment, which aligns task vectors to the parameter space of a differently\npre-trained target model. These transformations preserve key properties of task\nvectors, such as norm and rank, and are learned using only a small number of\nlabeled examples. We evaluate the method using two Vision Transformers\npre-trained on YFCC100M and LAION400M, and test on eight classification\ndatasets. Experimental results show that our method improves transfer accuracy\nover direct task vector application and achieves performance comparable to\nfew-shot fine-tuning, while maintaining the modularity and reusability of task\nvectors. Our code is available at\nhttps://github.com/kawakera-lab/CrossModelTransfer."}
{"id": "2505.12045", "pdf": "https://arxiv.org/pdf/2505.12045", "abs": "https://arxiv.org/abs/2505.12045", "authors": ["Shuai Yuan", "Guowen Xu", "Hongwei Li", "Rui Zhang", "Xinyuan Qian", "Wenbo Jiang", "Hangcheng Cao", "Qingchuan Zhao"], "title": "FIGhost: Fluorescent Ink-based Stealthy and Flexible Backdoor Attacks on Physical Traffic Sign Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Traffic sign recognition (TSR) systems are crucial for autonomous driving but\nare vulnerable to backdoor attacks. Existing physical backdoor attacks either\nlack stealth, provide inflexible attack control, or ignore emerging\nVision-Large-Language-Models (VLMs). In this paper, we introduce FIGhost, the\nfirst physical-world backdoor attack leveraging fluorescent ink as triggers.\nFluorescent triggers are invisible under normal conditions and activated\nstealthily by ultraviolet light, providing superior stealthiness, flexibility,\nand untraceability. Inspired by real-world graffiti, we derive realistic\ntrigger shapes and enhance their robustness via an interpolation-based\nfluorescence simulation algorithm. Furthermore, we develop an automated\nbackdoor sample generation method to support three attack objectives. Extensive\nevaluations in the physical world demonstrate FIGhost's effectiveness against\nstate-of-the-art detectors and VLMs, maintaining robustness under environmental\nvariations and effectively evading existing defenses."}
{"id": "2505.12048", "pdf": "https://arxiv.org/pdf/2505.12048", "abs": "https://arxiv.org/abs/2505.12048", "authors": ["Rui Qin", "Qijie Wang", "Ming Sun", "Haowei Zhu", "Chao Zhou", "Bin Wang"], "title": "Accelerating Diffusion-based Super-Resolution with Dynamic Time-Spatial Sampling", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion models have gained attention for their success in modeling complex\ndistributions, achieving impressive perceptual quality in SR tasks. However,\nexisting diffusion-based SR methods often suffer from high computational costs,\nrequiring numerous iterative steps for training and inference. Existing\nacceleration techniques, such as distillation and solver optimization, are\ngenerally task-agnostic and do not fully leverage the specific characteristics\nof low-level tasks like super-resolution (SR). In this study, we analyze the\nfrequency- and spatial-domain properties of diffusion-based SR methods,\nrevealing key insights into the temporal and spatial dependencies of\nhigh-frequency signal recovery. Specifically, high-frequency details benefit\nfrom concentrated optimization during early and late diffusion iterations,\nwhile spatially textured regions demand adaptive denoising strategies. Building\non these observations, we propose the Time-Spatial-aware Sampling strategy\n(TSS) for the acceleration of Diffusion SR without any extra training cost. TSS\ncombines Time Dynamic Sampling (TDS), which allocates more iterations to\nrefining textures, and Spatial Dynamic Sampling (SDS), which dynamically\nadjusts strategies based on image content. Extensive evaluations across\nmultiple benchmarks demonstrate that TSS achieves state-of-the-art (SOTA)\nperformance with significantly fewer iterations, improving MUSIQ scores by 0.2\n- 3.0 and outperforming the current acceleration methods with only half the\nnumber of steps."}
{"id": "2505.12053", "pdf": "https://arxiv.org/pdf/2505.12053", "abs": "https://arxiv.org/abs/2505.12053", "authors": ["Tianxiong Zhong", "Xingye Tian", "Boyuan Jiang", "Xuebo Wang", "Xin Tao", "Pengfei Wan", "Zhiwei Zhang"], "title": "VFRTok: Variable Frame Rates Video Tokenizer with Duration-Proportional Information Assumption", "categories": ["cs.CV", "cs.AI"], "comment": "11 pages, 10 figures", "summary": "Modern video generation frameworks based on Latent Diffusion Models suffer\nfrom inefficiencies in tokenization due to the Frame-Proportional Information\nAssumption. Existing tokenizers provide fixed temporal compression rates,\ncausing the computational cost of the diffusion model to scale linearly with\nthe frame rate. The paper proposes the Duration-Proportional Information\nAssumption: the upper bound on the information capacity of a video is\nproportional to the duration rather than the number of frames. Based on this\ninsight, the paper introduces VFRTok, a Transformer-based video tokenizer, that\nenables variable frame rate encoding and decoding through asymmetric frame rate\ntraining between the encoder and decoder. Furthermore, the paper proposes\nPartial Rotary Position Embeddings (RoPE) to decouple position and content\nmodeling, which groups correlated patches into unified tokens. The Partial RoPE\neffectively improves content-awareness, enhancing the video generation\ncapability. Benefiting from the compact and continuous spatio-temporal\nrepresentation, VFRTok achieves competitive reconstruction quality and\nstate-of-the-art generation fidelity while using only 1/8 tokens compared to\nexisting tokenizers."}
{"id": "2505.12066", "pdf": "https://arxiv.org/pdf/2505.12066", "abs": "https://arxiv.org/abs/2505.12066", "authors": ["Yijie Zheng", "Jinxuan Yang", "Yu Chen", "Yaxuan Wang", "Yihang Lu", "Guoqing Li"], "title": "Beluga Whale Detection from Satellite Imagery with Point Labels", "categories": ["cs.CV"], "comment": "Accepted for oral presentation at IGARSS 2025. Session at\n  https://www.2025.ieeeigarss.org/view_paper.php?PaperNum=2430&SessionID=1426", "summary": "Very high-resolution (VHR) satellite imagery has emerged as a powerful tool\nfor monitoring marine animals on a large scale. However, existing deep\nlearning-based whale detection methods usually require manually created,\nhigh-quality bounding box annotations, which are labor-intensive to produce.\nMoreover, existing studies often exclude ``uncertain whales'', individuals that\nhave ambiguous appearances in satellite imagery, limiting the applicability of\nthese models in real-world scenarios. To address these limitations, this study\nintroduces an automated pipeline for detecting beluga whales and harp seals in\nVHR satellite imagery. The pipeline leverages point annotations and the Segment\nAnything Model (SAM) to generate precise bounding box annotations, which are\nused to train YOLOv8 for multiclass detection of certain whales, uncertain\nwhales, and harp seals. Experimental results demonstrated that SAM-generated\nannotations significantly improved detection performance, achieving higher\n$\\text{F}_\\text{1}$-scores compared to traditional buffer-based annotations.\nYOLOv8 trained on SAM-labeled boxes achieved an overall\n$\\text{F}_\\text{1}$-score of 72.2% for whales overall and 70.3% for harp seals,\nwith superior performance in dense scenes. The proposed approach not only\nreduces the manual effort required for annotation but also enhances the\ndetection of uncertain whales, offering a more comprehensive solution for\nmarine animal monitoring. This method holds great potential for extending to\nother species, habitats, and remote sensing platforms, as well as for\nestimating whale biometrics, thereby advancing ecological monitoring and\nconservation efforts. The codes for our label and detection pipeline are\npublicly available at http://github.com/voyagerxvoyagerx/beluga-seeker ."}
{"id": "2505.12069", "pdf": "https://arxiv.org/pdf/2505.12069", "abs": "https://arxiv.org/abs/2505.12069", "authors": ["Shenzhou Liu", "Di Wang", "Haonan Guo", "Chengxi Han", "Wenzhi Zeng"], "title": "MT-CYP-Net: Multi-Task Network for Pixel-Level Crop Yield Prediction Under Very Few Samples", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Accurate and fine-grained crop yield prediction plays a crucial role in\nadvancing global agriculture. However, the accuracy of pixel-level yield\nestimation based on satellite remote sensing data has been constrained by the\nscarcity of ground truth data. To address this challenge, we propose a novel\napproach called the Multi-Task Crop Yield Prediction Network (MT-CYP-Net). This\nframework introduces an effective multi-task feature-sharing strategy, where\nfeatures extracted from a shared backbone network are simultaneously utilized\nby both crop yield prediction decoders and crop classification decoders with\nthe ability to fuse information between them. This design allows MT-CYP-Net to\nbe trained with extremely sparse crop yield point labels and crop type labels,\nwhile still generating detailed pixel-level crop yield maps. Concretely, we\ncollected 1,859 yield point labels along with corresponding crop type labels\nand satellite images from eight farms in Heilongjiang Province, China, in 2023,\ncovering soybean, maize, and rice crops, and constructed a sparse crop yield\nlabel dataset. MT-CYP-Net is compared with three classical machine learning and\ndeep learning benchmark methods in this dataset. Experimental results not only\nindicate the superiority of MT-CYP-Net compared to previous methods on multiple\ntypes of crops but also demonstrate the potential of deep networks on precise\npixel-level crop yield prediction, especially with limited data labels."}
{"id": "2505.12074", "pdf": "https://arxiv.org/pdf/2505.12074", "abs": "https://arxiv.org/abs/2505.12074", "authors": ["Chen Shu", "Boyu Fu", "Yiman Li", "Ting Yin", "Wenchuan Zhang", "Jie Chen", "Yuhao Yi", "Hong Bu"], "title": "Denoising Mutual Knowledge Distillation in Bi-Directional Multiple Instance Learning", "categories": ["cs.CV"], "comment": "12 pages, 2 figures", "summary": "Multiple Instance Learning is the predominant method for Whole Slide Image\nclassification in digital pathology, enabling the use of slide-level labels to\nsupervise model training. Although MIL eliminates the tedious fine-grained\nannotation process for supervised learning, whether it can learn accurate bag-\nand instance-level classifiers remains a question. To address the issue,\ninstance-level classifiers and instance masks were incorporated to ground the\nprediction on supporting patches. These methods, while practically improving\nthe performance of MIL methods, may potentially introduce noisy labels. We\npropose to bridge the gap between commonly used MIL and fully supervised\nlearning by augmenting both the bag- and instance-level learning processes with\npseudo-label correction capabilities elicited from weak to strong\ngeneralization techniques. The proposed algorithm improves the performance of\ndual-level MIL algorithms on both bag- and instance-level predictions.\nExperiments on public pathology datasets showcase the advantage of the proposed\nmethods."}
{"id": "2505.12081", "pdf": "https://arxiv.org/pdf/2505.12081", "abs": "https://arxiv.org/abs/2505.12081", "authors": ["Yuqi Liu", "Tianyuan Qu", "Zhisheng Zhong", "Bohao Peng", "Shu Liu", "Bei Yu", "Jiaya Jia"], "title": "VisionReasoner: Unified Visual Perception and Reasoning via Reinforcement Learning", "categories": ["cs.CV"], "comment": null, "summary": "Large vision-language models exhibit inherent capabilities to handle diverse\nvisual perception tasks. In this paper, we introduce VisionReasoner, a unified\nframework capable of reasoning and solving multiple visual perception tasks\nwithin a shared model. Specifically, by designing novel multi-object cognitive\nlearning strategies and systematic task reformulation, VisionReasoner enhances\nits reasoning capabilities to analyze visual inputs, and addresses diverse\nperception tasks in a unified framework. The model generates a structured\nreasoning process before delivering the desired outputs responding to user\nqueries. To rigorously assess unified visual perception capabilities, we\nevaluate VisionReasoner on ten diverse tasks spanning three critical domains:\ndetection, segmentation, and counting. Experimental results show that\nVisionReasoner achieves superior performance as a unified model, outperforming\nQwen2.5VL by relative margins of 29.1% on COCO (detection), 22.1% on ReasonSeg\n(segmentation), and 15.3% on CountBench (counting)."}
{"id": "2505.12098", "pdf": "https://arxiv.org/pdf/2505.12098", "abs": "https://arxiv.org/abs/2505.12098", "authors": ["Jiarui Wang", "Huiyu Duan", "Ziheng Jia", "Yu Zhao", "Woo Yi Yang", "Zicheng Zhang", "Zijian Chen", "Juntong Wang", "Yuke Xing", "Guangtao Zhai", "Xiongkuo Min"], "title": "LOVE: Benchmarking and Evaluating Text-to-Video Generation and Video-to-Text Interpretation", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in large multimodal models (LMMs) have driven substantial\nprogress in both text-to-video (T2V) generation and video-to-text (V2T)\ninterpretation tasks. However, current AI-generated videos (AIGVs) still\nexhibit limitations in terms of perceptual quality and text-video alignment.\nTherefore, a reliable and scalable automatic model for AIGV evaluation is\ndesirable, which heavily relies on the scale and quality of human annotations.\nTo this end, we present AIGVE-60K, a comprehensive dataset and benchmark for\nAI-Generated Video Evaluation, which features (i) comprehensive tasks,\nencompassing 3,050 extensive prompts across 20 fine-grained task dimensions,\n(ii) the largest human annotations, including 120K mean-opinion scores (MOSs)\nand 60K question-answering (QA) pairs annotated on 58,500 videos generated from\n30 T2V models, and (iii) bidirectional benchmarking and evaluating for both T2V\ngeneration and V2T interpretation capabilities. Based on AIGVE-60K, we propose\nLOVE, a LMM-based metric for AIGV Evaluation from multiple dimensions including\nperceptual preference, text-video correspondence, and task-specific accuracy in\nterms of both instance level and model level. Comprehensive experiments\ndemonstrate that LOVE not only achieves state-of-the-art performance on the\nAIGVE-60K dataset, but also generalizes effectively to a wide range of other\nAIGV evaluation benchmarks. These findings highlight the significance of the\nAIGVE-60K dataset. Database and codes are anonymously available at\nhttps://github.com/IntMeGroup/LOVE."}
{"id": "2505.12099", "pdf": "https://arxiv.org/pdf/2505.12099", "abs": "https://arxiv.org/abs/2505.12099", "authors": ["Aybora Koksal", "A. Aydin Alatan"], "title": "TinyRS-R1: Compact Multimodal Language Model for Remote Sensing", "categories": ["cs.CV"], "comment": "Submitted to BMVC 2025. Code, models, and the captions for datasets\n  will be released", "summary": "Remote-sensing applications often run on edge hardware that cannot host\ntoday's 7B-parameter multimodal language models. This paper introduces TinyRS,\nthe first 2B-parameter multimodal small language model (MSLM) optimized for\nremote sensing tasks, and TinyRS-R1, its reasoning-augmented variant. Built\nupon Qwen2-VL-2B, TinyRS is trained through a four-stage pipeline: pre-training\non million satellite images, instruction tuning on visual instruction examples,\nfine-tuning with Chain-of-Thought (CoT) annotations from the proposed reasoning\ndataset, and alignment via Group Relative Policy Optimization (GRPO). TinyRS-R1\nachieves or surpasses the performance of recent 7B-parameter remote sensing\nmodels across classification, VQA, visual grounding, and open-ended question\nanswering-while requiring just one-third of the memory and latency. Our\nanalysis shows that CoT reasoning substantially benefits spatial grounding and\nscene understanding, while the non-reasoning TinyRS excels in concise,\nlatency-sensitive VQA tasks. TinyRS-R1 represents the first domain-specialized\nMSLM with GRPO-aligned CoT reasoning for general-purpose remote sensing."}
{"id": "2505.12108", "pdf": "https://arxiv.org/pdf/2505.12108", "abs": "https://arxiv.org/abs/2505.12108", "authors": ["Jiancheng Pan", "Shiye Lei", "Yuqian Fu", "Jiahao Li", "Yanxing Liu", "Yuze Sun", "Xiao He", "Long Peng", "Xiaomeng Huang", "Bo Zhao"], "title": "EarthSynth: Generating Informative Earth Observation with Diffusion Models", "categories": ["cs.CV", "cs.AI"], "comment": "23 pages", "summary": "Remote sensing image (RSI) interpretation typically faces challenges due to\nthe scarcity of labeled data, which limits the performance of RSI\ninterpretation tasks. To tackle this challenge, we propose EarthSynth, a\ndiffusion-based generative foundation model that enables synthesizing\nmulti-category, cross-satellite labeled Earth observation for downstream RSI\ninterpretation tasks. To the best of our knowledge, EarthSynth is the first to\nexplore multi-task generation for remote sensing. EarthSynth, trained on the\nEarthSynth-180K dataset, employs the Counterfactual Composition training\nstrategy to improve training data diversity and enhance category control.\nFurthermore, a rule-based method of R-Filter is proposed to filter more\ninformative synthetic data for downstream tasks. We evaluate our EarthSynth on\nscene classification, object detection, and semantic segmentation in open-world\nscenarios, offering a practical solution for advancing RSI interpretation."}
{"id": "2505.12130", "pdf": "https://arxiv.org/pdf/2505.12130", "abs": "https://arxiv.org/abs/2505.12130", "authors": ["Niaz Ahmad", "Jawad Khan", "Kang G. Shin", "Youngmoon Lee", "Guanghui Wang"], "title": "Keypoints as Dynamic Centroids for Unified Human Pose and Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The dynamic movement of the human body presents a fundamental challenge for\nhuman pose estimation and body segmentation. State-of-the-art approaches\nprimarily rely on combining keypoint heatmaps with segmentation masks but often\nstruggle in scenarios involving overlapping joints or rapidly changing poses\nduring instance-level segmentation. To address these limitations, we propose\nKeypoints as Dynamic Centroid (KDC), a new centroid-based representation for\nunified human pose estimation and instance-level segmentation. KDC adopts a\nbottom-up paradigm to generate keypoint heatmaps for both easily\ndistinguishable and complex keypoints and improves keypoint detection and\nconfidence scores by introducing KeyCentroids using a keypoint disk. It\nleverages high-confidence keypoints as dynamic centroids in the embedding space\nto generate MaskCentroids, allowing for swift clustering of pixels to specific\nhuman instances during rapid body movements in live environments. Our\nexperimental evaluations on the CrowdPose, OCHuman, and COCO benchmarks\ndemonstrate KDC's effectiveness and generalizability in challenging scenarios\nin terms of both accuracy and runtime performance. The implementation is\navailable at: https://sites.google.com/view/niazahmad/projects/kdc."}
{"id": "2505.12154", "pdf": "https://arxiv.org/pdf/2505.12154", "abs": "https://arxiv.org/abs/2505.12154", "authors": ["Chao Huang", "Ruohan Gao", "J. M. F. Tsang", "Jan Kurcius", "Cagdas Bilen", "Chenliang Xu", "Anurag Kumar", "Sanjeel Parekh"], "title": "Learning to Highlight Audio by Watching Movies", "categories": ["cs.CV", "cs.SD", "eess.AS"], "comment": "CVPR 2025. Project page: https://wikichao.github.io/VisAH/", "summary": "Recent years have seen a significant increase in video content creation and\nconsumption. Crafting engaging content requires the careful curation of both\nvisual and audio elements. While visual cue curation, through techniques like\noptimal viewpoint selection or post-editing, has been central to media\nproduction, its natural counterpart, audio, has not undergone equivalent\nadvancements. This often results in a disconnect between visual and acoustic\nsaliency. To bridge this gap, we introduce a novel task: visually-guided\nacoustic highlighting, which aims to transform audio to deliver appropriate\nhighlighting effects guided by the accompanying video, ultimately creating a\nmore harmonious audio-visual experience. We propose a flexible,\ntransformer-based multimodal framework to solve this task. To train our model,\nwe also introduce a new dataset -- the muddy mix dataset, leveraging the\nmeticulous audio and video crafting found in movies, which provides a form of\nfree supervision. We develop a pseudo-data generation process to simulate\npoorly mixed audio, mimicking real-world scenarios through a three-step process\n-- separation, adjustment, and remixing. Our approach consistently outperforms\nseveral baselines in both quantitative and subjective evaluation. We also\nsystematically study the impact of different types of contextual guidance and\ndifficulty levels of the dataset. Our project page is here:\nhttps://wikichao.github.io/VisAH/."}
{"id": "2505.12155", "pdf": "https://arxiv.org/pdf/2505.12155", "abs": "https://arxiv.org/abs/2505.12155", "authors": ["Ranit Karmakar", "Simon F. Nrrelykke"], "title": "SoftPQ: Robust Instance Segmentation Evaluation via Soft Matching and Tunable Thresholds", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Segmentation evaluation metrics traditionally rely on binary decision logic:\npredictions are either correct or incorrect, based on rigid IoU thresholds.\nDetection--based metrics such as F1 and mAP determine correctness at the object\nlevel using fixed overlap cutoffs, while overlap--based metrics like\nIntersection over Union (IoU) and Dice operate at the pixel level, often\noverlooking instance--level structure. Panoptic Quality (PQ) attempts to unify\ndetection and segmentation assessment, but it remains dependent on\nhard-threshold matching--treating predictions below the threshold as entirely\nincorrect. This binary framing obscures important distinctions between\nqualitatively different errors and fails to reward gradual model improvements.\nWe propose SoftPQ, a flexible and interpretable instance segmentation metric\nthat redefines evaluation as a graded continuum rather than a binary\nclassification. SoftPQ introduces tunable upper and lower IoU thresholds to\ndefine a partial matching region and applies a sublinear penalty function to\nambiguous or fragmented predictions. These extensions allow SoftPQ to exhibit\nsmoother score behavior, greater robustness to structural segmentation errors,\nand more informative feedback for model development and evaluation. Through\ncontrolled perturbation experiments, we show that SoftPQ captures meaningful\ndifferences in segmentation quality that existing metrics overlook, making it a\npractical and principled alternative for both benchmarking and iterative model\nrefinement."}
{"id": "2505.12191", "pdf": "https://arxiv.org/pdf/2505.12191", "abs": "https://arxiv.org/abs/2505.12191", "authors": ["Wenquan Lu", "Jiaqi Zhang", "Hugues Van Assel", "Randall Balestriero"], "title": "Ditch the Denoiser: Emergence of Noise Robustness in Self-Supervised Learning from Data Curriculum", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Self-Supervised Learning (SSL) has become a powerful solution to extract rich\nrepresentations from unlabeled data. Yet, SSL research is mostly focused on\nclean, curated and high-quality datasets. As a result, applying SSL on noisy\ndata remains a challenge, despite being crucial to applications such as\nastrophysics, medical imaging, geophysics or finance. In this work, we present\na fully self-supervised framework that enables noise-robust representation\nlearning without requiring a denoiser at inference or downstream fine-tuning.\nOur method first trains an SSL denoiser on noisy data, then uses it to\nconstruct a denoised-to-noisy data curriculum (i.e., training first on\ndenoised, then noisy samples) for pretraining a SSL backbone (e.g., DINOv2),\ncombined with a teacher-guided regularization that anchors noisy embeddings to\ntheir denoised counterparts. This process encourages the model to internalize\nnoise robustness. Notably, the denoiser can be discarded after pretraining,\nsimplifying deployment. On ImageNet-1k with ViT-B under extreme Gaussian noise\n($\\sigma=255$, SNR = 0.72 dB), our method improves linear probing accuracy by\n4.8% over DINOv2, demonstrating that denoiser-free robustness can emerge from\nnoise-aware pretraining. The code is available at\nhttps://github.com/wenquanlu/noisy_dinov2."}
{"id": "2505.12199", "pdf": "https://arxiv.org/pdf/2505.12199", "abs": "https://arxiv.org/abs/2505.12199", "authors": ["Kui Jiang", "Jing Cao", "Zhaocheng Yu", "Junjun Jiang", "Jingchun Zhou"], "title": "Always Clear Depth: Robust Monocular Depth Estimation under Adverse Weather", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Monocular depth estimation is critical for applications such as autonomous\ndriving and scene reconstruction. While existing methods perform well under\nnormal scenarios, their performance declines in adverse weather, due to\nchallenging domain shifts and difficulties in extracting scene information. To\naddress this issue, we present a robust monocular depth estimation method\ncalled \\textbf{ACDepth} from the perspective of high-quality training data\ngeneration and domain adaptation. Specifically, we introduce a one-step\ndiffusion model for generating samples that simulate adverse weather\nconditions, constructing a multi-tuple degradation dataset during training. To\nensure the quality of the generated degradation samples, we employ LoRA\nadapters to fine-tune the generation weights of diffusion model. Additionally,\nwe integrate circular consistency loss and adversarial training to guarantee\nthe fidelity and naturalness of the scene contents. Furthermore, we elaborate\non a multi-granularity knowledge distillation strategy (MKD) that encourages\nthe student network to absorb knowledge from both the teacher model and\npretrained Depth Anything V2. This strategy guides the student model in\nlearning degradation-agnostic scene information from various degradation\ninputs. In particular, we introduce an ordinal guidance distillation mechanism\n(OGD) that encourages the network to focus on uncertain regions through\ndifferential ranking, leading to a more precise depth estimation. Experimental\nresults demonstrate that our ACDepth surpasses md4all-DD by 2.50\\% for night\nscene and 2.61\\% for rainy scene on the nuScenes dataset in terms of the absRel\nmetric."}
{"id": "2505.12200", "pdf": "https://arxiv.org/pdf/2505.12200", "abs": "https://arxiv.org/abs/2505.12200", "authors": ["Bohan Jia", "Wenxuan Huang", "Yuntian Tang", "Junbo Qiao", "Jincheng Liao", "Shaosheng Cao", "Fei Zhao", "Zhaopeng Feng", "Zhouhong Gu", "Zhenfei Yin", "Lei Bai", "Wanli Ouyang", "Lin Chen", "Fei Zhao", "Zihan Wang", "Yuan Xie", "Shaohui Lin"], "title": "CompBench: Benchmarking Complex Instruction-guided Image Editing", "categories": ["cs.CV"], "comment": null, "summary": "While real-world applications increasingly demand intricate scene\nmanipulation, existing instruction-guided image editing benchmarks often\noversimplify task complexity and lack comprehensive, fine-grained instructions.\nTo bridge this gap, we introduce, a large-scale benchmark specifically designed\nfor complex instruction-guided image editing. CompBench features challenging\nediting scenarios that incorporate fine-grained instruction following, spatial\nand contextual reasoning, thereby enabling comprehensive evaluation of image\nediting models' precise manipulation capabilities. To construct CompBench, We\npropose an MLLM-human collaborative framework with tailored task pipelines.\nFurthermore, we propose an instruction decoupling strategy that disentangles\nediting intents into four key dimensions: location, appearance, dynamics, and\nobjects, ensuring closer alignment between instructions and complex editing\nrequirements. Extensive evaluations reveal that CompBench exposes fundamental\nlimitations of current image editing models and provides critical insights for\nthe development of next-generation instruction-guided image editing systems."}
{"id": "2505.12206", "pdf": "https://arxiv.org/pdf/2505.12206", "abs": "https://arxiv.org/abs/2505.12206", "authors": ["Mathanesh Vellingiri Ramasamy", "Dimas Rizky Kurniasalim"], "title": "Road Segmentation for ADAS/AD Applications", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Accurate road segmentation is essential for autonomous driving and ADAS,\nenabling effective navigation in complex environments. This study examines how\nmodel architecture and dataset choice affect segmentation by training a\nmodified VGG-16 on the Comma10k dataset and a modified U-Net on the KITTI Road\ndataset. Both models achieved high accuracy, with cross-dataset testing showing\nVGG-16 outperforming U-Net despite U-Net being trained for more epochs. We\nanalyze model performance using metrics such as F1-score, mean intersection\nover union, and precision, discussing how architecture and dataset impact\nresults."}
{"id": "2505.12207", "pdf": "https://arxiv.org/pdf/2505.12207", "abs": "https://arxiv.org/abs/2505.12207", "authors": ["Qingmei Li", "Yang Zhang", "Zurong Mai", "Yuhang Chen", "Shuohong Lou", "Henglian Huang", "Jiarui Zhang", "Zhiwei Zhang", "Yibin Wen", "Weijia Li", "Haohuan Fu", "Jianxi Huang", "Juepeng Zheng"], "title": "Can Large Multimodal Models Understand Agricultural Scenes? Benchmarking with AgroMind", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Large Multimodal Models (LMMs) has demonstrated capabilities across various\ndomains, but comprehensive benchmarks for agricultural remote sensing (RS)\nremain scarce. Existing benchmarks designed for agricultural RS scenarios\nexhibit notable limitations, primarily in terms of insufficient scene diversity\nin the dataset and oversimplified task design. To bridge this gap, we introduce\nAgroMind, a comprehensive agricultural remote sensing benchmark covering four\ntask dimensions: spatial perception, object understanding, scene understanding,\nand scene reasoning, with a total of 13 task types, ranging from crop\nidentification and health monitoring to environmental analysis. We curate a\nhigh-quality evaluation set by integrating eight public datasets and one\nprivate farmland plot dataset, containing 25,026 QA pairs and 15,556 images.\nThe pipeline begins with multi-source data preprocessing, including collection,\nformat standardization, and annotation refinement. We then generate a diverse\nset of agriculturally relevant questions through the systematic definition of\ntasks. Finally, we employ LMMs for inference, generating responses, and\nperforming detailed examinations. We evaluated 18 open-source LMMs and 3\nclosed-source models on AgroMind. Experiments reveal significant performance\ngaps, particularly in spatial reasoning and fine-grained recognition, it is\nnotable that human performance lags behind several leading LMMs. By\nestablishing a standardized evaluation framework for agricultural RS, AgroMind\nreveals the limitations of LMMs in domain knowledge and highlights critical\nchallenges for future work. Data and code can be accessed at\nhttps://rssysu.github.io/AgroMind/."}
{"id": "2505.12217", "pdf": "https://arxiv.org/pdf/2505.12217", "abs": "https://arxiv.org/abs/2505.12217", "authors": ["Aryan Das", "Tanishq Rachamalla", "Pravendra Singh", "Koushik Biswas", "Vinay Kumar Verma", "Swalpa Kumar Roy"], "title": "Hyperspectral Image Land Cover Captioning Dataset for Vision Language Models", "categories": ["cs.CV"], "comment": null, "summary": "We introduce HyperCap, the first large-scale hyperspectral captioning dataset\ndesigned to enhance model performance and effectiveness in remote sensing\napplications. Unlike traditional hyperspectral imaging (HSI) datasets that\nfocus solely on classification tasks, HyperCap integrates spectral data with\npixel-wise textual annotations, enabling deeper semantic understanding of\nhyperspectral imagery. This dataset enhances model performance in tasks like\nclassification and feature extraction, providing a valuable resource for\nadvanced remote sensing applications. HyperCap is constructed from four\nbenchmark datasets and annotated through a hybrid approach combining automated\nand manual methods to ensure accuracy and consistency. Empirical evaluations\nusing state-of-the-art encoders and diverse fusion techniques demonstrate\nsignificant improvements in classification performance. These results\nunderscore the potential of vision-language learning in HSI and position\nHyperCap as a foundational dataset for future research in the field."}
{"id": "2505.12228", "pdf": "https://arxiv.org/pdf/2505.12228", "abs": "https://arxiv.org/abs/2505.12228", "authors": ["Karthik Gopinath", "Annabel Sorby-Adams", "Jonathan W. Ramirez", "Dina Zemlyanker", "Jennifer Guo", "David Hunt", "Christine L. Mac Donald", "C. Dirk Keene", "Timothy Coalson", "Matthew F. Glasser", "David Van Essen", "Matthew S. Rosen", "Oula Puonti", "W. Taylor Kimberly", "Juan Eugenio Iglesias"], "title": "From Low Field to High Value: Robust Cortical Mapping from Low-Field MRI", "categories": ["cs.CV", "cs.LG"], "comment": "32 pages", "summary": "Three-dimensional reconstruction of cortical surfaces from MRI for\nmorphometric analysis is fundamental for understanding brain structure. While\nhigh-field MRI (HF-MRI) is standard in research and clinical settings, its\nlimited availability hinders widespread use. Low-field MRI (LF-MRI),\nparticularly portable systems, offers a cost-effective and accessible\nalternative. However, existing cortical surface analysis tools are optimized\nfor high-resolution HF-MRI and struggle with the lower signal-to-noise ratio\nand resolution of LF-MRI. In this work, we present a machine learning method\nfor 3D reconstruction and analysis of portable LF-MRI across a range of\ncontrasts and resolutions. Our method works \"out of the box\" without\nretraining. It uses a 3D U-Net trained on synthetic LF-MRI to predict signed\ndistance functions of cortical surfaces, followed by geometric processing to\nensure topological accuracy. We evaluate our method using paired HF/LF-MRI\nscans of the same subjects, showing that LF-MRI surface reconstruction accuracy\ndepends on acquisition parameters, including contrast type (T1 vs T2),\norientation (axial vs isotropic), and resolution. A 3mm isotropic T2-weighted\nscan acquired in under 4 minutes, yields strong agreement with HF-derived\nsurfaces: surface area correlates at r=0.96, cortical parcellations reach\nDice=0.98, and gray matter volume achieves r=0.93. Cortical thickness remains\nmore challenging with correlations up to r=0.70, reflecting the difficulty of\nsub-mm precision with 3mm voxels. We further validate our method on challenging\npostmortem LF-MRI, demonstrating its robustness. Our method represents a step\ntoward enabling cortical surface analysis on portable LF-MRI. Code is available\nat https://surfer.nmr.mgh.harvard.edu/fswiki/ReconAny"}
{"id": "2505.12235", "pdf": "https://arxiv.org/pdf/2505.12235", "abs": "https://arxiv.org/abs/2505.12235", "authors": ["Jia Li", "Nan Gao", "Huaibo Huang", "Ran He"], "title": "NOFT: Test-Time Noise Finetune via Information Bottleneck for Highly Correlated Asset Creation", "categories": ["cs.CV"], "comment": null, "summary": "The diffusion model has provided a strong tool for implementing text-to-image\n(T2I) and image-to-image (I2I) generation. Recently, topology and texture\ncontrol are popular explorations, e.g., ControlNet, IP-Adapter, Ctrl-X, and\nDSG. These methods explicitly consider high-fidelity controllable editing based\non external signals or diffusion feature manipulations. As for diversity, they\ndirectly choose different noise latents. However, the diffused noise is capable\nof implicitly representing the topological and textural manifold of the\ncorresponding image. Moreover, it's an effective workbench to conduct the\ntrade-off between content preservation and controllable variations. Previous\nT2I and I2I diffusion works do not explore the information within the\ncompressed contextual latent. In this paper, we first propose a plug-and-play\nnoise finetune NOFT module employed by Stable Diffusion to generate highly\ncorrelated and diverse images. We fine-tune seed noise or inverse noise through\nan optimal-transported (OT) information bottleneck (IB) with around only 14K\ntrainable parameters and 10 minutes of training. Our test-time NOFT is good at\nproducing high-fidelity image variations considering topology and texture\nalignments. Comprehensive experiments demonstrate that NOFT is a powerful\ngeneral reimagine approach to efficiently fine-tune the 2D/3D AIGC assets with\ntext or image guidance."}
{"id": "2505.12237", "pdf": "https://arxiv.org/pdf/2505.12237", "abs": "https://arxiv.org/abs/2505.12237", "authors": ["Yuzhi Li", "Haojun Xu", "Fang Tian"], "title": "From Shots to Stories: LLM-Assisted Video Editing with Unified Language Representations", "categories": ["cs.CV"], "comment": null, "summary": "Large Language Models (LLMs) and Vision-Language Models (VLMs) have\ndemonstrated remarkable reasoning and generalization capabilities in video\nunderstanding; however, their application in video editing remains largely\nunderexplored. This paper presents the first systematic study of LLMs in the\ncontext of video editing. To bridge the gap between visual information and\nlanguage-based reasoning, we introduce L-Storyboard, an intermediate\nrepresentation that transforms discrete video shots into structured language\ndescriptions suitable for LLM processing. We categorize video editing tasks\ninto Convergent Tasks and Divergent Tasks, focusing on three core tasks: Shot\nAttributes Classification, Next Shot Selection, and Shot Sequence Ordering. To\naddress the inherent instability of divergent task outputs, we propose the\nStoryFlow strategy, which converts the divergent multi-path reasoning process\ninto a convergent selection mechanism, effectively enhancing task accuracy and\nlogical coherence. Experimental results demonstrate that L-Storyboard\nfacilitates a more robust mapping between visual information and language\ndescriptions, significantly improving the interpretability and privacy\nprotection of video editing tasks. Furthermore, StoryFlow enhances the logical\nconsistency and output stability in Shot Sequence Ordering, underscoring the\nsubstantial potential of LLMs in intelligent video editing."}
{"id": "2505.12246", "pdf": "https://arxiv.org/pdf/2505.12246", "abs": "https://arxiv.org/abs/2505.12246", "authors": ["Muleilan Pei", "Jiayao Shan", "Peiliang Li", "Jieqi Shi", "Jing Huo", "Yang Gao", "Shaojie Shen"], "title": "SEPT: Standard-Definition Map Enhanced Scene Perception and Topology Reasoning for Autonomous Driving", "categories": ["cs.CV"], "comment": "Accepted by IEEE Robotics and Automation Letters", "summary": "Online scene perception and topology reasoning are critical for autonomous\nvehicles to understand their driving environments, particularly for mapless\ndriving systems that endeavor to reduce reliance on costly High-Definition (HD)\nmaps. However, recent advances in online scene understanding still face\nlimitations, especially in long-range or occluded scenarios, due to the\ninherent constraints of onboard sensors. To address this challenge, we propose\na Standard-Definition (SD) Map Enhanced scene Perception and Topology reasoning\n(SEPT) framework, which explores how to effectively incorporate the SD map as\nprior knowledge into existing perception and reasoning pipelines. Specifically,\nwe introduce a novel hybrid feature fusion strategy that combines SD maps with\nBird's-Eye-View (BEV) features, considering both rasterized and vectorized\nrepresentations, while mitigating potential misalignment between SD maps and\nBEV feature spaces. Additionally, we leverage the SD map characteristics to\ndesign an auxiliary intersection-aware keypoint detection task, which further\nenhances the overall scene understanding performance. Experimental results on\nthe large-scale OpenLane-V2 dataset demonstrate that by effectively integrating\nSD map priors, our framework significantly improves both scene perception and\ntopology reasoning, outperforming existing methods by a substantial margin."}
{"id": "2505.12251", "pdf": "https://arxiv.org/pdf/2505.12251", "abs": "https://arxiv.org/abs/2505.12251", "authors": ["Haozhe Xiang", "Han Zhang", "Yu Cheng", "Xiongwen Quan", "Wanwan Huang"], "title": "SMFusion: Semantic-Preserving Fusion of Multimodal Medical Images for Enhanced Clinical Diagnosis", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal medical image fusion plays a crucial role in medical diagnosis by\nintegrating complementary information from different modalities to enhance\nimage readability and clinical applicability. However, existing methods mainly\nfollow computer vision standards for feature extraction and fusion strategy\nformulation, overlooking the rich semantic information inherent in medical\nimages. To address this limitation, we propose a novel semantic-guided medical\nimage fusion approach that, for the first time, incorporates medical prior\nknowledge into the fusion process. Specifically, we construct a publicly\navailable multimodal medical image-text dataset, upon which text descriptions\ngenerated by BiomedGPT are encoded and semantically aligned with image features\nin a high-dimensional space via a semantic interaction alignment module. During\nthis process, a cross attention based linear transformation automatically maps\nthe relationship between textual and visual features to facilitate\ncomprehensive learning. The aligned features are then embedded into a\ntext-injection module for further feature-level fusion. Unlike traditional\nmethods, we further generate diagnostic reports from the fused images to assess\nthe preservation of medical information. Additionally, we design a medical\nsemantic loss function to enhance the retention of textual cues from the source\nimages. Experimental results on test datasets demonstrate that the proposed\nmethod achieves superior performance in both qualitative and quantitative\nevaluations while preserving more critical medical information."}
{"id": "2505.12253", "pdf": "https://arxiv.org/pdf/2505.12253", "abs": "https://arxiv.org/abs/2505.12253", "authors": ["Hanyu Zhou", "Gim Hee Lee"], "title": "LLaVA-4D: Embedding SpatioTemporal Prompt into LMMs for 4D Scene Understanding", "categories": ["cs.CV"], "comment": null, "summary": "Despite achieving significant progress in 2D image understanding, large\nmultimodal models (LMMs) struggle in the physical world due to the lack of\nspatial representation. Typically, existing 3D LMMs mainly embed 3D positions\nas fixed spatial prompts within visual features to represent the scene.\nHowever, these methods are limited to understanding the static background and\nfail to capture temporally varying dynamic objects. In this paper, we propose\nLLaVA-4D, a general LMM framework with a novel spatiotemporal prompt for visual\nrepresentation in 4D scene understanding. The spatiotemporal prompt is\ngenerated by encoding 3D position and 1D time into a dynamic-aware 4D\ncoordinate embedding. Moreover, we demonstrate that spatial and temporal\ncomponents disentangled from visual features are more effective in\ndistinguishing the background from objects. This motivates embedding the 4D\nspatiotemporal prompt into these features to enhance the dynamic scene\nrepresentation. By aligning visual spatiotemporal embeddings with language\nembeddings, LMMs gain the ability to understand both spatial and temporal\ncharacteristics of static background and dynamic objects in the physical world.\nAdditionally, we construct a 4D vision-language dataset with spatiotemporal\ncoordinate annotations for instruction fine-tuning LMMs. Extensive experiments\nhave been conducted to demonstrate the effectiveness of our method across\ndifferent tasks in 4D scene understanding."}
{"id": "2505.12254", "pdf": "https://arxiv.org/pdf/2505.12254", "abs": "https://arxiv.org/abs/2505.12254", "authors": ["Yiwei Ou", "Xiaobin Ren", "Ronggui Sun", "Guansong Gao", "Ziyi Jiang", "Kaiqi Zhao", "Manfredo Manfredini"], "title": "MMS-VPR: Multimodal Street-Level Visual Place Recognition Dataset and Benchmark", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Existing visual place recognition (VPR) datasets predominantly rely on\nvehicle-mounted imagery, lack multimodal diversity and underrepresent dense,\nmixed-use street-level spaces, especially in non-Western urban contexts. To\naddress these gaps, we introduce MMS-VPR, a large-scale multimodal dataset for\nstreet-level place recognition in complex, pedestrian-only environments. The\ndataset comprises 78,575 annotated images and 2,512 video clips captured across\n207 locations in a ~70,800 $\\mathrm{m}^2$ open-air commercial district in\nChengdu, China. Each image is labeled with precise GPS coordinates, timestamp,\nand textual metadata, and covers varied lighting conditions, viewpoints, and\ntimeframes. MMS-VPR follows a systematic and replicable data collection\nprotocol with minimal device requirements, lowering the barrier for scalable\ndataset creation. Importantly, the dataset forms an inherent spatial graph with\n125 edges, 81 nodes, and 1 subgraph, enabling structure-aware place\nrecognition. We further define two application-specific subsets --\nDataset_Edges and Dataset_Points -- to support fine-grained and graph-based\nevaluation tasks. Extensive benchmarks using conventional VPR models, graph\nneural networks, and multimodal baselines show substantial improvements when\nleveraging multimodal and structural cues. MMS-VPR facilitates future research\nat the intersection of computer vision, geospatial understanding, and\nmultimodal reasoning. The dataset is publicly available at\nhttps://huggingface.co/datasets/Yiwei-Ou/MMS-VPR."}
{"id": "2505.12266", "pdf": "https://arxiv.org/pdf/2505.12266", "abs": "https://arxiv.org/abs/2505.12266", "authors": ["ZhanFeng Feng", "Long Peng", "Xin Di", "Yong Guo", "Wenbo Li", "Yulun Zhang", "Renjing Pei", "Yang Wang", "Yang Cao", "Zheng-Jun Zha"], "title": "PMQ-VE: Progressive Multi-Frame Quantization for Video Enhancement", "categories": ["cs.CV"], "comment": null, "summary": "Multi-frame video enhancement tasks aim to improve the spatial and temporal\nresolution and quality of video sequences by leveraging temporal information\nfrom multiple frames, which are widely used in streaming video processing,\nsurveillance, and generation. Although numerous Transformer-based enhancement\nmethods have achieved impressive performance, their computational and memory\ndemands hinder deployment on edge devices. Quantization offers a practical\nsolution by reducing the bit-width of weights and activations to improve\nefficiency. However, directly applying existing quantization methods to video\nenhancement tasks often leads to significant performance degradation and loss\nof fine details. This stems from two limitations: (a) inability to allocate\nvarying representational capacity across frames, which results in suboptimal\ndynamic range adaptation; (b) over-reliance on full-precision teachers, which\nlimits the learning of low-bit student models. To tackle these challenges, we\npropose a novel quantization method for video enhancement: Progressive\nMulti-Frame Quantization for Video Enhancement (PMQ-VE). This framework\nfeatures a coarse-to-fine two-stage process: Backtracking-based Multi-Frame\nQuantization (BMFQ) and Progressive Multi-Teacher Distillation (PMTD). BMFQ\nutilizes a percentile-based initialization and iterative search with pruning\nand backtracking for robust clipping bounds. PMTD employs a progressive\ndistillation strategy with both full-precision and multiple high-bit (INT)\nteachers to enhance low-bit models' capacity and quality. Extensive experiments\ndemonstrate that our method outperforms existing approaches, achieving\nstate-of-the-art performance across multiple tasks and benchmarks.The code will\nbe made publicly available at: https://github.com/xiaoBIGfeng/PMQ-VE."}
{"id": "2505.12274", "pdf": "https://arxiv.org/pdf/2505.12274", "abs": "https://arxiv.org/abs/2505.12274", "authors": ["Yixiao Chen", "Zhiyuan Ma", "Guoli Jia", "Che Jiang", "Jianjun Li", "Bowen Zhou"], "title": "Context-Aware Autoregressive Models for Multi-Conditional Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "Autoregressive transformers have recently shown impressive image generation\nquality and efficiency on par with state-of-the-art diffusion models. Unlike\ndiffusion architectures, autoregressive models can naturally incorporate\narbitrary modalities into a single, unified token sequence--offering a concise\nsolution for multi-conditional image generation tasks. In this work, we propose\n$\\textbf{ContextAR}$, a flexible and effective framework for multi-conditional\nimage generation. ContextAR embeds diverse conditions (e.g., canny edges, depth\nmaps, poses) directly into the token sequence, preserving modality-specific\nsemantics. To maintain spatial alignment while enhancing discrimination among\ndifferent condition types, we introduce hybrid positional encodings that fuse\nRotary Position Embedding with Learnable Positional Embedding. We design\nConditional Context-aware Attention to reduces computational complexity while\npreserving effective intra-condition perception. Without any fine-tuning,\nContextAR supports arbitrary combinations of conditions during inference time.\nExperimental results demonstrate the powerful controllability and versatility\nof our approach, and show that the competitive perpormance than diffusion-based\nmulti-conditional control approaches the existing autoregressive baseline\nacross diverse multi-condition driven scenarios. Project page:\n$\\href{https://context-ar.github.io/}{https://context-ar.github.io/.}$"}
{"id": "2505.12280", "pdf": "https://arxiv.org/pdf/2505.12280", "abs": "https://arxiv.org/abs/2505.12280", "authors": ["Sijie Zhao", "Feng Liu", "Xueliang Zhang", "Hao Chen", "Pengfeng Xiao", "Lei Bai"], "title": "Temporal-Spectral-Spatial Unified Remote Sensing Dense Prediction", "categories": ["cs.CV"], "comment": "12 pages, 4 figures, Code\n  link:https://github.com/walking-shadow/Official_TSSUN", "summary": "The proliferation of diverse remote sensing data has spurred advancements in\ndense prediction tasks, yet significant challenges remain in handling data\nheterogeneity. Remote sensing imagery exhibits substantial variability across\ntemporal, spectral, and spatial (TSS) dimensions, complicating unified data\nprocessing. Current deep learning models for dense prediction tasks, such as\nsemantic segmentation and change detection, are typically tailored to specific\ninput-output configurations. Consequently, variations in data dimensionality or\ntask requirements often lead to significant performance degradation or model\nincompatibility, necessitating costly retraining or fine-tuning efforts for\ndifferent application scenarios. This paper introduces the\nTemporal-Spectral-Spatial Unified Network (TSSUN), a novel architecture\ndesigned for unified representation and modeling of remote sensing data across\ndiverse TSS characteristics and task types. TSSUN employs a\nTemporal-Spectral-Spatial Unified Strategy that leverages meta-information to\ndecouple and standardize input representations from varied temporal, spectral,\nand spatial configurations, and similarly unifies output structures for\ndifferent dense prediction tasks and class numbers. Furthermore, a Local-Global\nWindow Attention mechanism is proposed to efficiently capture both local\ncontextual details and global dependencies, enhancing the model's adaptability\nand feature extraction capabilities. Extensive experiments on multiple datasets\ndemonstrate that a single TSSUN model effectively adapts to heterogeneous\ninputs and unifies various dense prediction tasks. The proposed approach\nconsistently achieves or surpasses state-of-the-art performance, highlighting\nits robustness and generalizability for complex remote sensing applications\nwithout requiring task-specific modifications."}
{"id": "2505.12307", "pdf": "https://arxiv.org/pdf/2505.12307", "abs": "https://arxiv.org/abs/2505.12307", "authors": ["Maoyuan Ye", "Jing Zhang", "Juhua Liu", "Bo Du", "Dacheng Tao"], "title": "LogicOCR: Do Your Large Multimodal Models Excel at Logical Reasoning on Text-Rich Images?", "categories": ["cs.CV", "cs.CL"], "comment": "GitHub: \\url{https://github.com/MiliLab/LogicOCR}", "summary": "Recent advances in Large Multimodal Models (LMMs) have significantly improved\ntheir reasoning and Optical Character Recognition (OCR) capabilities. However,\ntheir performance on complex logical reasoning tasks involving text-rich images\nremains underexplored. To bridge this gap, we introduce LogicOCR, a benchmark\ncomprising 1,100 multiple-choice questions designed to evaluate LMMs' logical\nreasoning abilities on text-rich images, while minimizing reliance on\ndomain-specific knowledge (e.g., mathematics). We construct LogicOCR by\ncurating a text corpus from the Chinese National Civil Servant Examination and\ndevelop a scalable, automated pipeline to convert it into multimodal samples.\nFirst, we design prompt templates to steer GPT-Image-1 to generate images with\ndiverse backgrounds, interleaved text-illustration layouts, and varied fonts,\nensuring contextual relevance and visual realism. Then, the generated images\nare manually verified, with low-quality examples discarded. We evaluate a range\nof representative open-source and proprietary LMMs under both Chain-of-Thought\n(CoT) and direct-answer settings. Our multi-dimensional analysis reveals key\ninsights, such as the impact of test-time scaling, input modality differences,\nand sensitivity to visual-text orientation. Notably, LMMs still lag in\nmultimodal reasoning compared to text-only inputs, indicating that they have\nnot fully bridged visual reading with reasoning. We hope LogicOCR will serve as\na valuable resource for advancing multimodal reasoning research. The dataset is\navailable at https://github.com/MiliLab/LogicOCR."}
{"id": "2505.12310", "pdf": "https://arxiv.org/pdf/2505.12310", "abs": "https://arxiv.org/abs/2505.12310", "authors": ["Shouyi Lu", "Huanyu Zhou", "Guirong Zhuo"], "title": "DNOI-4DRO: Deep 4D Radar Odometry with Differentiable Neural-Optimization Iterations", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "16 pages,10 figures", "summary": "A novel learning-optimization-combined 4D radar odometry model, named\nDNOI-4DRO, is proposed in this paper. The proposed model seamlessly integrates\ntraditional geometric optimization with end-to-end neural network training,\nleveraging an innovative differentiable neural-optimization iteration operator.\nIn this framework, point-wise motion flow is first estimated using a neural\nnetwork, followed by the construction of a cost function based on the\nrelationship between point motion and pose in 3D space. The radar pose is then\nrefined using Gauss-Newton updates. Additionally, we design a dual-stream 4D\nradar backbone that integrates multi-scale geometric features and\nclustering-based class-aware features to enhance the representation of sparse\n4D radar point clouds. Extensive experiments on the VoD and Snail-Radar\ndatasets demonstrate the superior performance of our model, which outperforms\nrecent classical and learning-based approaches. Notably, our method even\nachieves results comparable to A-LOAM with mapping optimization using LiDAR\npoint clouds as input. Our models and code will be publicly released."}
{"id": "2505.12312", "pdf": "https://arxiv.org/pdf/2505.12312", "abs": "https://arxiv.org/abs/2505.12312", "authors": ["Qi Feng", "Hidetoshi Shimodaira"], "title": "Visuospatial Cognitive Assistant", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.RO"], "comment": "31 pages, 10 figures, 6 tables. The implementation and fine-tuned\n  model (ViCA-7B) are publicly available at https://huggingface.co/nkkbr/ViCA.\n  The ViCA-322K dataset can be found at\n  https://huggingface.co/datasets/nkkbr/ViCA-322K, and the ViCA-Thinking-2.68K\n  dataset is at https://huggingface.co/datasets/nkkbr/ViCA-thinking-2.68k", "summary": "Video-based spatial cognition is vital for robotics and embodied AI but\nchallenges current Vision-Language Models (VLMs). This paper makes two key\ncontributions. First, we introduce ViCA (Visuospatial Cognitive\nAssistant)-322K, a diverse dataset of 322,003 QA pairs from real-world indoor\nvideos (ARKitScenes, ScanNet, ScanNet++), offering supervision for 3D\nmetadata-grounded queries and video-based complex reasoning. Second, we develop\nViCA-7B, fine-tuned on ViCA-322K, which achieves new state-of-the-art on all\neight VSI-Bench tasks, outperforming existing models, including larger ones\n(e.g., +26.1 on Absolute Distance). For interpretability, we present\nViCA-Thinking-2.68K, a dataset with explicit reasoning chains, and fine-tune\nViCA-7B to create ViCA-7B-Thinking, a model that articulates its spatial\nreasoning. Our work highlights the importance of targeted data and suggests\npaths for improved temporal-spatial modeling. We release all resources to\nfoster research in robust visuospatial intelligence."}
{"id": "2505.12317", "pdf": "https://arxiv.org/pdf/2505.12317", "abs": "https://arxiv.org/abs/2505.12317", "authors": ["Ruoqi Wang", "Haitao Wang", "Shaojie Guo", "Qiong Luo"], "title": "Improving Out-of-Domain Robustness with Targeted Augmentation in Frequency and Pixel Spaces", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Out-of-domain (OOD) robustness under domain adaptation settings, where\nlabeled source data and unlabeled target data come from different\ndistributions, is a key challenge in real-world applications. A common approach\nto improving OOD robustness is through data augmentations. However, in\nreal-world scenarios, models trained with generic augmentations can only\nimprove marginally when generalized under distribution shifts toward unlabeled\ntarget domains. While dataset-specific targeted augmentations can address this\nissue, they typically require expert knowledge and extensive prior data\nanalysis to identify the nature of the datasets and domain shift. To address\nthese challenges, we propose Frequency-Pixel Connect, a domain-adaptation\nframework that enhances OOD robustness by introducing a targeted augmentation\nin both the frequency space and pixel space. Specifically, we mix the amplitude\nspectrum and pixel content of a source image and a target image to generate\naugmented samples that introduce domain diversity while preserving the semantic\nstructure of the source image. Unlike previous targeted augmentation methods\nthat are both dataset-specific and limited to the pixel space, Frequency-Pixel\nConnect is dataset-agnostic, enabling broader and more flexible applicability\nbeyond natural image datasets. We further analyze the effectiveness of\nFrequency-Pixel Connect by evaluating the performance of our method connecting\nsame-class cross-domain samples while separating different-class examples. We\ndemonstrate that Frequency-Pixel Connect significantly improves cross-domain\nconnectivity and outperforms previous generic methods on four diverse\nreal-world benchmarks across vision, medical, audio, and astronomical domains,\nand it also outperforms other dataset-specific targeted augmentation methods."}
{"id": "2505.12335", "pdf": "https://arxiv.org/pdf/2505.12335", "abs": "https://arxiv.org/abs/2505.12335", "authors": ["Ziqiang Li", "Jiazhen Yan", "Ziwen He", "Kai Zeng", "Weiwei Jiang", "Lizhi Xiong", "Zhangjie Fu"], "title": "Is Artificial Intelligence Generated Image Detection a Solved Problem?", "categories": ["cs.CV", "cs.CR"], "comment": "Under Review", "summary": "The rapid advancement of generative models, such as GANs and Diffusion\nmodels, has enabled the creation of highly realistic synthetic images, raising\nserious concerns about misinformation, deepfakes, and copyright infringement.\nAlthough numerous Artificial Intelligence Generated Image (AIGI) detectors have\nbeen proposed, often reporting high accuracy, their effectiveness in real-world\nscenarios remains questionable. To bridge this gap, we introduce AIGIBench, a\ncomprehensive benchmark designed to rigorously evaluate the robustness and\ngeneralization capabilities of state-of-the-art AIGI detectors. AIGIBench\nsimulates real-world challenges through four core tasks: multi-source\ngeneralization, robustness to image degradation, sensitivity to data\naugmentation, and impact of test-time pre-processing. It includes 23 diverse\nfake image subsets that span both advanced and widely adopted image generation\ntechniques, along with real-world samples collected from social media and AI\nart platforms. Extensive experiments on 11 advanced detectors demonstrate that,\ndespite their high reported accuracy in controlled settings, these detectors\nsuffer significant performance drops on real-world data, limited benefits from\ncommon augmentations, and nuanced effects of pre-processing, highlighting the\nneed for more robust detection strategies. By providing a unified and realistic\nevaluation framework, AIGIBench offers valuable insights to guide future\nresearch toward dependable and generalizable AIGI detection."}
{"id": "2505.12339", "pdf": "https://arxiv.org/pdf/2505.12339", "abs": "https://arxiv.org/abs/2505.12339", "authors": ["Midou Guo", "Qilin Yin", "Wei Lu", "Xiangyang Luo"], "title": "Towards Open-world Generalized Deepfake Detection: General Feature Extraction via Unsupervised Domain Adaptation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "With the development of generative artificial intelligence, new forgery\nmethods are rapidly emerging. Social platforms are flooded with vast amounts of\nunlabeled synthetic data and authentic data, making it increasingly challenging\nto distinguish real from fake. Due to the lack of labels, existing supervised\ndetection methods struggle to effectively address the detection of unknown\ndeepfake methods. Moreover, in open world scenarios, the amount of unlabeled\ndata greatly exceeds that of labeled data. Therefore, we define a new deepfake\ndetection generalization task which focuses on how to achieve efficient\ndetection of large amounts of unlabeled data based on limited labeled data to\nsimulate a open world scenario. To solve the above mentioned task, we propose a\nnovel Open-World Deepfake Detection Generalization Enhancement Training\nStrategy (OWG-DS) to improve the generalization ability of existing methods.\nOur approach aims to transfer deepfake detection knowledge from a small amount\nof labeled source domain data to large-scale unlabeled target domain data.\nSpecifically, we introduce the Domain Distance Optimization (DDO) module to\nalign different domain features by optimizing both inter-domain and\nintra-domain distances. Additionally, the Similarity-based Class Boundary\nSeparation (SCBS) module is used to enhance the aggregation of similar samples\nto ensure clearer class boundaries, while an adversarial training mechanism is\nadopted to learn the domain-invariant features. Extensive experiments show that\nthe proposed deepfake detection generalization enhancement training strategy\nexcels in cross-method and cross-dataset scenarios, improving the model's\ngeneralization."}
{"id": "2505.12340", "pdf": "https://arxiv.org/pdf/2505.12340", "abs": "https://arxiv.org/abs/2505.12340", "authors": ["Jirong Zha", "Yuxuan Fan", "Kai Li", "Han Li", "Chen Gao", "Xinlei Chen", "Yong Li"], "title": "DIMM: Decoupled Multi-hierarchy Kalman Filter for 3D Object Tracking", "categories": ["cs.CV"], "comment": "10 pages", "summary": "State estimation is challenging for 3D object tracking with high\nmaneuverability, as the target's state transition function changes rapidly,\nirregularly, and is unknown to the estimator. Existing work based on\ninteracting multiple model (IMM) achieves more accurate estimation than\nsingle-filter approaches through model combination, aligning appropriate models\nfor different motion modes of the target object over time. However, two\nlimitations of conventional IMM remain unsolved. First, the solution space of\nthe model combination is constrained as the target's diverse kinematic\nproperties in different directions are ignored. Second, the model combination\nweights calculated by the observation likelihood are not accurate enough due to\nthe measurement uncertainty. In this paper, we propose a novel framework, DIMM,\nto effectively combine estimates from different motion models in each\ndirection, thus increasing the 3D object tracking accuracy. First, DIMM extends\nthe model combination solution space of conventional IMM from a hyperplane to a\nhypercube by designing a 3D-decoupled multi-hierarchy filter bank, which\ndescribes the target's motion with various-order linear models. Second, DIMM\ngenerates more reliable combination weight matrices through a differentiable\nadaptive fusion network for importance allocation rather than solely relying on\nthe observation likelihood; it contains an attention-based twin delayed deep\ndeterministic policy gradient (TD3) method with a hierarchical reward.\nExperiments demonstrate that DIMM significantly improves the tracking accuracy\nof existing state estimation methods by 31.61%~99.23%."}
{"id": "2505.12363", "pdf": "https://arxiv.org/pdf/2505.12363", "abs": "https://arxiv.org/abs/2505.12363", "authors": ["Qi Feng", "Hidetoshi Shimodaira"], "title": "Towards Visuospatial Cognition via Hierarchical Fusion of Visual Experts", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.RO"], "comment": "26 pages, 19 figures, 4 tables. Code, models, and dataset are\n  available at our project page: https://github.com/nkkbr/ViCA", "summary": "While Multimodal Large Language Models (MLLMs) excel at general\nvision-language tasks, visuospatial cognition - reasoning about spatial\nlayouts, relations, and dynamics - remains a significant challenge. Existing\nmodels often lack the necessary architectural components and specialized\ntraining data for fine-grained spatial understanding. We introduce ViCA2\n(Visuospatial Cognitive Assistant 2), a novel MLLM designed to enhance spatial\nreasoning. ViCA2 features a dual vision encoder architecture integrating SigLIP\nfor semantics and Hiera for spatial structure, coupled with a token ratio\ncontrol mechanism for efficiency. We also developed ViCA-322K, a new\nlarge-scale dataset with over 322,000 spatially grounded question-answer pairs\nfor targeted instruction tuning. On the challenging VSI-Bench benchmark, our\nViCA2-7B model achieves a state-of-the-art average score of 56.8, significantly\nsurpassing larger open-source models (e.g., LLaVA-NeXT-Video-72B, 40.9) and\nleading proprietary models (Gemini-1.5 Pro, 45.4). This demonstrates the\neffectiveness of our approach in achieving strong visuospatial intelligence\nwith a compact model. We release ViCA2, its codebase, and the ViCA-322K dataset\nto facilitate further research."}
{"id": "2505.12391", "pdf": "https://arxiv.org/pdf/2505.12391", "abs": "https://arxiv.org/abs/2505.12391", "authors": ["Zhengyang Lu", "Qian Xia", "Weifan Wang", "Feng Wang"], "title": "CLIP-aware Domain-Adaptive Super-Resolution", "categories": ["cs.CV"], "comment": null, "summary": "This work introduces CLIP-aware Domain-Adaptive Super-Resolution (CDASR), a\nnovel framework that addresses the critical challenge of domain generalization\nin single image super-resolution. By leveraging the semantic capabilities of\nCLIP (Contrastive Language-Image Pre-training), CDASR achieves unprecedented\nperformance across diverse domains and extreme scaling factors. The proposed\nmethod integrates CLIP-guided feature alignment mechanism with a meta-learning\ninspired few-shot adaptation strategy, enabling efficient knowledge transfer\nand rapid adaptation to target domains. A custom domain-adaptive module\nprocesses CLIP features alongside super-resolution features through a\nmulti-stage transformation process, including CLIP feature processing, spatial\nfeature generation, and feature fusion. This intricate process ensures\neffective incorporation of semantic information into the super-resolution\npipeline. Additionally, CDASR employs a multi-component loss function that\ncombines pixel-wise reconstruction, perceptual similarity, and semantic\nconsistency. Extensive experiments on benchmark datasets demonstrate CDASR's\nsuperiority, particularly in challenging scenarios. On the Urban100 dataset at\n$\\times$8 scaling, CDASR achieves a significant PSNR gain of 0.15dB over\nexisting methods, with even larger improvements of up to 0.30dB observed at\n$\\times$16 scaling."}
{"id": "2505.12408", "pdf": "https://arxiv.org/pdf/2505.12408", "abs": "https://arxiv.org/abs/2505.12408", "authors": ["Minxu Liu", "Donghai Guan", "Chuhang Zheng", "Chunwei Tian", "Jie Wen", "Qi Zhu"], "title": "ViEEG: Hierarchical Neural Coding with Cross-Modal Progressive Enhancement for EEG-Based Visual Decoding", "categories": ["cs.CV", "cs.AI", "cs.HC"], "comment": "24 pages, 18 figures", "summary": "Understanding and decoding brain activity into visual representations is a\nfundamental challenge at the intersection of neuroscience and artificial\nintelligence. While EEG-based visual decoding has shown promise due to its\nnon-invasive, low-cost nature and millisecond-level temporal resolution,\nexisting methods are limited by their reliance on flat neural representations\nthat overlook the brain's inherent visual hierarchy. In this paper, we\nintroduce ViEEG, a biologically inspired hierarchical EEG decoding framework\nthat aligns with the Hubel-Wiesel theory of visual processing. ViEEG decomposes\neach visual stimulus into three biologically aligned components-contour,\nforeground object, and contextual scene-serving as anchors for a three-stream\nEEG encoder. These EEG features are progressively integrated via\ncross-attention routing, simulating cortical information flow from V1 to IT to\nthe association cortex. We further adopt hierarchical contrastive learning to\nalign EEG representations with CLIP embeddings, enabling zero-shot object\nrecognition. Extensive experiments on the THINGS-EEG dataset demonstrate that\nViEEG achieves state-of-the-art performance, with 40.9% Top-1 accuracy in\nsubject-dependent and 22.9% Top-1 accuracy in cross-subject settings,\nsurpassing existing methods by over 45%. Our framework not only advances the\nperformance frontier but also sets a new paradigm for biologically grounded\nbrain decoding in AI."}
{"id": "2505.12425", "pdf": "https://arxiv.org/pdf/2505.12425", "abs": "https://arxiv.org/abs/2505.12425", "authors": ["Edgar Riba", "Jian Shi", "Aditya Kumar", "Andrew Shen", "Gary Bradski"], "title": "Kornia-rs: A Low-Level 3D Computer Vision Library In Rust", "categories": ["cs.CV"], "comment": null, "summary": "We present \\textit{kornia-rs}, a high-performance 3D computer vision library\nwritten entirely in native Rust, designed for safety-critical and real-time\napplications. Unlike C++-based libraries like OpenCV or wrapper-based solutions\nlike OpenCV-Rust, \\textit{kornia-rs} is built from the ground up to leverage\nRust's ownership model and type system for memory and thread safety.\n\\textit{kornia-rs} adopts a statically-typed tensor system and a modular set of\ncrates, providing efficient image I/O, image processing and 3D operations. To\naid cross-platform compatibility, \\textit{kornia-rs} offers Python bindings,\nenabling seamless and efficient integration with Rust code. Empirical results\nshow that \\textit{kornia-rs} achieves a 3~ 5 times speedup in image\ntransformation tasks over native Rust alternatives, while offering comparable\nperformance to C++ wrapper-based libraries. In addition to 2D vision\ncapabilities, \\textit{kornia-rs} addresses a significant gap in the Rust\necosystem by providing a set of 3D computer vision operators. This paper\npresents the architecture and performance characteristics of\n\\textit{kornia-rs}, demonstrating its effectiveness in real-world computer\nvision applications."}
{"id": "2505.12427", "pdf": "https://arxiv.org/pdf/2505.12427", "abs": "https://arxiv.org/abs/2505.12427", "authors": ["Siwei Xia", "Li Sun", "Tiantian Sun", "Qingli Li"], "title": "DragLoRA: Online Optimization of LoRA Adapters for Drag-based Image Editing in Diffusion Model", "categories": ["cs.CV"], "comment": "Accepted by ICML2025", "summary": "Drag-based editing within pretrained diffusion model provides a precise and\nflexible way to manipulate foreground objects. Traditional methods optimize the\ninput feature obtained from DDIM inversion directly, adjusting them iteratively\nto guide handle points towards target locations. However, these approaches\noften suffer from limited accuracy due to the low representation ability of the\nfeature in motion supervision, as well as inefficiencies caused by the large\nsearch space required for point tracking. To address these limitations, we\npresent DragLoRA, a novel framework that integrates LoRA (Low-Rank Adaptation)\nadapters into the drag-based editing pipeline. To enhance the training of LoRA\nadapters, we introduce an additional denoising score distillation loss which\nregularizes the online model by aligning its output with that of the original\nmodel. Additionally, we improve the consistency of motion supervision by\nadapting the input features using the updated LoRA, giving a more stable and\naccurate input feature for subsequent operations. Building on this, we design\nan adaptive optimization scheme that dynamically toggles between two modes,\nprioritizing efficiency without compromising precision. Extensive experiments\ndemonstrate that DragLoRA significantly enhances the control precision and\ncomputational efficiency for drag-based image editing. The Codes of DragLoRA\nare available at: https://github.com/Sylvie-X/DragLoRA."}
{"id": "2505.12431", "pdf": "https://arxiv.org/pdf/2505.12431", "abs": "https://arxiv.org/abs/2505.12431", "authors": ["Yating Liu", "Yujie Zhang", "Qi Yang", "Yiling Xu", "Zhu Li", "Ye-Kui Wang"], "title": "DPCD: A Quality Assessment Database for Dynamic Point Clouds", "categories": ["cs.CV", "cs.DB"], "comment": null, "summary": "Recently, the advancements in Virtual/Augmented Reality (VR/AR) have driven\nthe demand for Dynamic Point Clouds (DPC). Unlike static point clouds, DPCs are\ncapable of capturing temporal changes within objects or scenes, offering a more\naccurate simulation of the real world. While significant progress has been made\nin the quality assessment research of static point cloud, little study has been\ndone on Dynamic Point Cloud Quality Assessment (DPCQA), which hinders the\ndevelopment of quality-oriented applications, such as interframe compression\nand transmission in practical scenarios. In this paper, we introduce a\nlarge-scale DPCQA database, named DPCD, which includes 15 reference DPCs and\n525 distorted DPCs from seven types of lossy compression and noise distortion.\nBy rendering these samples to Processed Video Sequences (PVS), a comprehensive\nsubjective experiment is conducted to obtain Mean Opinion Scores (MOS) from 21\nviewers for analysis. The characteristic of contents, impact of various\ndistortions, and accuracy of MOSs are presented to validate the heterogeneity\nand reliability of the proposed database. Furthermore, we evaluate the\nperformance of several objective metrics on DPCD. The experiment results show\nthat DPCQA is more challenge than that of static point cloud. The DPCD, which\nserves as a catalyst for new research endeavors on DPCQA, is publicly available\nat https://huggingface.co/datasets/Olivialyt/DPCD."}
{"id": "2505.12433", "pdf": "https://arxiv.org/pdf/2505.12433", "abs": "https://arxiv.org/abs/2505.12433", "authors": ["Haodong Yang", "Lei Wang", "Md Zakir Hossain"], "title": "SRLoRA: Subspace Recomposition in Low-Rank Adaptation via Importance-Based Fusion and Reinitialization", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Research report", "summary": "Low-Rank Adaptation (LoRA) is a widely adopted parameter-efficient\nfine-tuning (PEFT) method that injects two trainable low-rank matrices (A and\nB) into frozen pretrained models. While efficient, LoRA constrains updates to a\nfixed low-rank subspace (Delta W = BA), which can limit representational\ncapacity and hinder downstream performance. We introduce Subspace Recomposition\nin Low-Rank Adaptation (SRLoRA) via importance-based fusion and\nreinitialization, a novel approach that enhances LoRA's expressiveness without\ncompromising its lightweight structure. SRLoRA assigns importance scores to\neach LoRA pair (a column of B and the corresponding row of A), and dynamically\nrecomposes the subspace during training. Less important pairs are fused into\nthe frozen backbone, freeing capacity to reinitialize new pairs along unused\nprincipal directions derived from the pretrained weight's singular value\ndecomposition. This mechanism enables continual subspace refreshment and richer\nadaptation over time, without increasing the number of trainable parameters. We\nevaluate SRLoRA on both language and vision tasks, including the GLUE benchmark\nand various image classification datasets. SRLoRA consistently achieves faster\nconvergence and improved accuracy over standard LoRA, demonstrating its\ngenerality, efficiency, and potential for broader PEFT applications."}
{"id": "2505.12434", "pdf": "https://arxiv.org/pdf/2505.12434", "abs": "https://arxiv.org/abs/2505.12434", "authors": ["Qi Wang", "Yanrui Yu", "Ye Yuan", "Rui Mao", "Tianfei Zhou"], "title": "VideoRFT: Incentivizing Video Reasoning Capability in MLLMs via Reinforced Fine-Tuning", "categories": ["cs.CV"], "comment": "Code: https://github.com/QiWang98/VideoRFT", "summary": "Reinforcement fine-tuning (RFT) has shown great promise in achieving\nhumanlevel reasoning capabilities of Large Language Models (LLMs), and has\nrecently been extended to MLLMs. Nevertheless, reasoning about videos, which is\na fundamental aspect of human intelligence, remains a persistent challenge due\nto the complex logic, temporal and causal structures inherent in video data. To\nfill this gap, we propose VIDEORFT, a novel approach that extends the RFT\nparadigm to cultivate human-like video reasoning capabilities in MLLMs.\nVIDEORFT follows the standard two-stage scheme in RFT: supervised fine-tuning\n(SFT) with chain-of-thought (CoT) annotations, followed by reinforcement\nlearning (RL) to improve generalization. A central challenge to achieve this in\nthe video domain lies in the scarcity of large-scale, high-quality video CoT\ndatasets. We address this by building a fully automatic CoT curation pipeline.\nFirst, we devise a cognitioninspired prompting strategy to elicit a reasoning\nLLM to generate preliminary CoTs based solely on rich, structured, and literal\nrepresentations of video content. Subsequently, these CoTs are revised by a\nvisual-language model conditioned on the actual video, ensuring visual\nconsistency and reducing visual hallucinations. This pipeline results in two\nnew datasets - VideoRFT-CoT-102K for SFT and VideoRFT-RL-310K for RL. To\nfurther strength the RL phase, we introduce a novel semantic-consistency reward\nthat explicitly promotes the alignment between textual reasoning with visual\nevidence. This reward encourages the model to produce coherent, context-aware\nreasoning outputs grounded in visual input. Extensive experiments show that\nVIDEORFT achieves state-of-the-art performance on six video reasoning\nbenchmarks."}
{"id": "2505.12448", "pdf": "https://arxiv.org/pdf/2505.12448", "abs": "https://arxiv.org/abs/2505.12448", "authors": ["Yang Liu", "Ming Ma", "Xiaomin Yu", "Pengxiang Ding", "Han Zhao", "Mingyang Sun", "Siteng Huang", "Donglin Wang"], "title": "SSR: Enhancing Depth Perception in Vision-Language Models via Rationale-Guided Spatial Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "Despite impressive advancements in Visual-Language Models (VLMs) for\nmulti-modal tasks, their reliance on RGB inputs limits precise spatial\nunderstanding. Existing methods for integrating spatial cues, such as point\nclouds or depth, either require specialized sensors or fail to effectively\nexploit depth information for higher-order reasoning. To this end, we propose a\nnovel Spatial Sense and Reasoning method, dubbed SSR, a novel framework that\ntransforms raw depth data into structured, interpretable textual rationales.\nThese textual rationales serve as meaningful intermediate representations to\nsignificantly enhance spatial reasoning capabilities. Additionally, we leverage\nknowledge distillation to compress the generated rationales into compact latent\nembeddings, which facilitate resource-efficient and plug-and-play integration\ninto existing VLMs without retraining. To enable comprehensive evaluation, we\nintroduce a new dataset named SSR-CoT, a million-scale visual-language\nreasoning dataset enriched with intermediate spatial reasoning annotations, and\npresent SSRBench, a comprehensive multi-task benchmark. Extensive experiments\non multiple benchmarks demonstrate SSR substantially improves depth utilization\nand enhances spatial reasoning, thereby advancing VLMs toward more human-like\nmulti-modal understanding. Our project page is at\nhttps://yliu-cs.github.io/SSR."}
{"id": "2505.12482", "pdf": "https://arxiv.org/pdf/2505.12482", "abs": "https://arxiv.org/abs/2505.12482", "authors": ["Wenchen Chen", "Yanmei Zhang", "Zhongwei Xiao", "Jianping Chu", "Xingbo Wang"], "title": "Spectral-Spatial Self-Supervised Learning for Few-Shot Hyperspectral Image Classification", "categories": ["cs.CV"], "comment": "https://github.com/Wenchen-Chen/S4L-FSC", "summary": "Few-shot classification of hyperspectral images (HSI) faces the challenge of\nscarce labeled samples. Self-Supervised learning (SSL) and Few-Shot Learning\n(FSL) offer promising avenues to address this issue. However, existing methods\noften struggle to adapt to the spatial geometric diversity of HSIs and lack\nsufficient spectral prior knowledge. To tackle these challenges, we propose a\nmethod, Spectral-Spatial Self-Supervised Learning for Few-Shot Hyperspectral\nImage Classification (S4L-FSC), aimed at improving the performance of few-shot\nHSI classification. Specifically, we first leverage heterogeneous datasets to\npretrain a spatial feature extractor using a designed Rotation-Mirror\nSelf-Supervised Learning (RM-SSL) method, combined with FSL. This approach\nenables the model to learn the spatial geometric diversity of HSIs using\nrotation and mirroring labels as supervisory signals, while acquiring\ntransferable spatial meta-knowledge through few-shot learning. Subsequently,\nhomogeneous datasets are utilized to pretrain a spectral feature extractor via\na combination of FSL and Masked Reconstruction Self-Supervised Learning\n(MR-SSL). The model learns to reconstruct original spectral information from\nrandomly masked spectral vectors, inferring spectral dependencies. In parallel,\nFSL guides the model to extract pixel-level discriminative features, thereby\nembedding rich spectral priors into the model. This spectral-spatial\npretraining method, along with the integration of knowledge from heterogeneous\nand homogeneous sources, significantly enhances model performance. Extensive\nexperiments on four HSI datasets demonstrate the effectiveness and superiority\nof the proposed S4L-FSC approach for few-shot HSI classification."}
{"id": "2505.12486", "pdf": "https://arxiv.org/pdf/2505.12486", "abs": "https://arxiv.org/abs/2505.12486", "authors": ["Sangmin Jung", "Utkarsh Nath", "Yezhou Yang", "Giulia Pedrielli", "Joydeep Biswas", "Amy Zhang", "Hassan Ghasemzadeh", "Pavan Turaga"], "title": "Guiding Diffusion with Deep Geometric Moments: Balancing Fidelity and Variation", "categories": ["cs.CV"], "comment": "Accepted in CVPR Workshop GMCV 2025", "summary": "Text-to-image generation models have achieved remarkable capabilities in\nsynthesizing images, but often struggle to provide fine-grained control over\nthe output. Existing guidance approaches, such as segmentation maps and depth\nmaps, introduce spatial rigidity that restricts the inherent diversity of\ndiffusion models. In this work, we introduce Deep Geometric Moments (DGM) as a\nnovel form of guidance that encapsulates the subject's visual features and\nnuances through a learned geometric prior. DGMs focus specifically on the\nsubject itself compared to DINO or CLIP features, which suffer from\noveremphasis on global image features or semantics. Unlike ResNets, which are\nsensitive to pixel-wise perturbations, DGMs rely on robust geometric moments.\nOur experiments demonstrate that DGM effectively balance control and diversity\nin diffusion-based image generation, allowing a flexible control mechanism for\nsteering the diffusion process."}
{"id": "2505.12489", "pdf": "https://arxiv.org/pdf/2505.12489", "abs": "https://arxiv.org/abs/2505.12489", "authors": ["Shaobin Zhuang", "Zhipeng Huang", "Ying Zhang", "Fangyikang Wang", "Canmiao Fu", "Binxin Yang", "Chong Sun", "Chen Li", "Yali Wang"], "title": "Video-GPT via Next Clip Diffusion", "categories": ["cs.CV", "cs.AI"], "comment": "22 pages, 12 figures, 18 tables", "summary": "GPT has shown its remarkable success in natural language processing. However,\nthe language sequence is not sufficient to describe spatial-temporal details in\nthe visual world. Alternatively, the video sequence is good at capturing such\ndetails. Motivated by this fact, we propose a concise Video-GPT in this paper\nby treating video as new language for visual world modeling. By analogy to next\ntoken prediction in GPT, we introduce a novel next clip diffusion paradigm for\npretraining Video-GPT. Different from the previous works, this distinct\nparadigm allows Video-GPT to tackle both short-term generation and long-term\nprediction, by autoregressively denoising the noisy clip according to the clean\nclips in the history. Extensive experiments show our Video-GPT achieves the\nstate-of-the-art performance on video prediction, which is the key factor\ntowards world modeling (Physics-IQ Benchmark: Video-GPT 34.97 vs. Kling 23.64\nvs. Wan 20.89). Moreover, it can be well adapted on 6 mainstream video tasks in\nboth video generation and understanding, showing its great generalization\ncapacity in downstream. The project page is at https://Video-GPT.github.io."}
{"id": "2505.12499", "pdf": "https://arxiv.org/pdf/2505.12499", "abs": "https://arxiv.org/abs/2505.12499", "authors": ["Jian Xiao", "Zijie Song", "Jialong Hu", "Hao Cheng", "Zhenzhen Hu", "Jia Li", "Richang Hong"], "title": "Rebalancing Contrastive Alignment with Learnable Semantic Gaps in Text-Video Retrieval", "categories": ["cs.CV", "cs.IR", "cs.MM"], "comment": null, "summary": "Recent advances in text-video retrieval have been largely driven by\ncontrastive learning frameworks. However, existing methods overlook a key\nsource of optimization tension: the separation between text and video\ndistributions in the representation space (referred to as the modality gap),\nand the prevalence of false negatives in batch sampling. These factors lead to\nconflicting gradients under the InfoNCE loss, impeding stable alignment. To\nmitigate this, we propose GARE, a Gap-Aware Retrieval framework that introduces\na learnable, pair-specific increment Delta_ij between text t_i and video v_j to\noffload the tension from the global anchor representation. We first derive the\nideal form of Delta_ij via a coupled multivariate first-order Taylor\napproximation of the InfoNCE loss under a trust-region constraint, revealing it\nas a mechanism for resolving gradient conflicts by guiding updates along a\nlocally optimal descent direction. Due to the high cost of directly computing\nDelta_ij, we introduce a lightweight neural module conditioned on the semantic\ngap between each video-text pair, enabling structure-aware correction guided by\ngradient supervision. To further stabilize learning and promote\ninterpretability, we regularize Delta using three components: a trust-region\nconstraint to prevent oscillation, a directional diversity term to promote\nsemantic coverage, and an information bottleneck to limit redundancy.\nExperiments across four retrieval benchmarks show that GARE consistently\nimproves alignment accuracy and robustness to noisy supervision, confirming the\neffectiveness of gap-aware tension mitigation."}
{"id": "2505.12513", "pdf": "https://arxiv.org/pdf/2505.12513", "abs": "https://arxiv.org/abs/2505.12513", "authors": ["Yang Mu", "Zhitong Xiong", "Yi Wang", "Muhammad Shahzad", "Franz Essl", "Mark van Kleunen", "Xiao Xiang Zhu"], "title": "GlobalGeoTree: A Multi-Granular Vision-Language Dataset for Global Tree Species Classification", "categories": ["cs.CV"], "comment": null, "summary": "Global tree species mapping using remote sensing data is vital for\nbiodiversity monitoring, forest management, and ecological research. However,\nprogress in this field has been constrained by the scarcity of large-scale,\nlabeled datasets. To address this, we introduce GlobalGeoTree, a comprehensive\nglobal dataset for tree species classification. GlobalGeoTree comprises 6.3\nmillion geolocated tree occurrences, spanning 275 families, 2,734 genera, and\n21,001 species across the hierarchical taxonomic levels. Each sample is paired\nwith Sentinel-2 image time series and 27 auxiliary environmental variables,\nencompassing bioclimatic, geographic, and soil data. The dataset is partitioned\ninto GlobalGeoTree-6M for model pretraining and curated evaluation subsets,\nprimarily GlobalGeoTree-10kEval for zero-shot and few-shot benchmarking. To\ndemonstrate the utility of the dataset, we introduce a baseline model,\nGeoTreeCLIP, which leverages paired remote sensing data and taxonomic text\nlabels within a vision-language framework pretrained on GlobalGeoTree-6M.\nExperimental results show that GeoTreeCLIP achieves substantial improvements in\nzero- and few-shot classification on GlobalGeoTree-10kEval over existing\nadvanced models. By making the dataset, models, and code publicly available, we\naim to establish a benchmark to advance tree species classification and foster\ninnovation in biodiversity research and ecological applications."}
{"id": "2505.12532", "pdf": "https://arxiv.org/pdf/2505.12532", "abs": "https://arxiv.org/abs/2505.12532", "authors": ["Ahmet Bilican", "M. Akn Ylmaz", "A. Murat Tekalp", "R. Gkberk Cinbi"], "title": "Exploring Sparsity for Parameter Efficient Fine Tuning Using Wavelets", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV", "eess.SP"], "comment": null, "summary": "Efficiently adapting large foundation models is critical, especially with\ntight compute and memory budgets. Parameter-Efficient Fine-Tuning (PEFT)\nmethods such as LoRA offer limited granularity and effectiveness in\nfew-parameter regimes. We propose Wavelet Fine-Tuning (WaveFT), a novel PEFT\nmethod that learns highly sparse updates in the wavelet domain of residual\nmatrices. WaveFT allows precise control of trainable parameters, offering\nfine-grained capacity adjustment and excelling with remarkably low parameter\ncount, potentially far fewer than LoRA's minimum -- ideal for extreme\nparameter-efficient scenarios. In order to demonstrate the effect of the\nwavelet transform, we compare WaveFT with a special case, called SHiRA, that\nentails applying sparse updates directly in the weight domain. Evaluated on\npersonalized text-to-image generation using Stable Diffusion XL as baseline,\nWaveFT significantly outperforms LoRA and other PEFT methods, especially at low\nparameter counts; achieving superior subject fidelity, prompt alignment, and\nimage diversity."}
{"id": "2505.12547", "pdf": "https://arxiv.org/pdf/2505.12547", "abs": "https://arxiv.org/abs/2505.12547", "authors": ["Florent Chiaroni", "Ali Ayub", "Ola Ahmad"], "title": "ProMi: An Efficient Prototype-Mixture Baseline for Few-Shot Segmentation with Bounding-Box Annotations", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": null, "summary": "In robotics applications, few-shot segmentation is crucial because it allows\nrobots to perform complex tasks with minimal training data, facilitating their\nadaptation to diverse, real-world environments. However, pixel-level\nannotations of even small amount of images is highly time-consuming and costly.\nIn this paper, we present a novel few-shot binary segmentation method based on\nbounding-box annotations instead of pixel-level labels. We introduce, ProMi, an\nefficient prototype-mixture-based method that treats the background class as a\nmixture of distributions. Our approach is simple, training-free, and effective,\naccommodating coarse annotations with ease. Compared to existing baselines,\nProMi achieves the best results across different datasets with significant\ngains, demonstrating its effectiveness. Furthermore, we present qualitative\nexperiments tailored to real-world mobile robot tasks, demonstrating the\napplicability of our approach in such scenarios. Our code:\nhttps://github.com/ThalesGroup/promi."}
{"id": "2505.12549", "pdf": "https://arxiv.org/pdf/2505.12549", "abs": "https://arxiv.org/abs/2505.12549", "authors": ["Dominic Maggio", "Hyungtae Lim", "Luca Carlone"], "title": "VGGT-SLAM: Dense RGB SLAM Optimized on the SL(4) Manifold", "categories": ["cs.CV"], "comment": null, "summary": "We present VGGT-SLAM, a dense RGB SLAM system constructed by incrementally\nand globally aligning submaps created from the feed-forward scene\nreconstruction approach VGGT using only uncalibrated monocular cameras. While\nrelated works align submaps using similarity transforms (i.e., translation,\nrotation, and scale), we show that such approaches are inadequate in the case\nof uncalibrated cameras. In particular, we revisit the idea of reconstruction\nambiguity, where given a set of uncalibrated cameras with no assumption on the\ncamera motion or scene structure, the scene can only be reconstructed up to a\n15-degrees-of-freedom projective transformation of the true geometry. This\ninspires us to recover a consistent scene reconstruction across submaps by\noptimizing over the SL(4) manifold, thus estimating 15-degrees-of-freedom\nhomography transforms between sequential submaps while accounting for potential\nloop closure constraints. As verified by extensive experiments, we demonstrate\nthat VGGT-SLAM achieves improved map quality using long video sequences that\nare infeasible for VGGT due to its high GPU requirements."}
{"id": "2505.12580", "pdf": "https://arxiv.org/pdf/2505.12580", "abs": "https://arxiv.org/abs/2505.12580", "authors": ["Priyank Pathak", "Yogesh S Rawat"], "title": "Coarse Attribute Prediction with Task Agnostic Distillation for Real World Clothes Changing ReID", "categories": ["cs.CV"], "comment": null, "summary": "This work focuses on Clothes Changing Re-IDentification (CC-ReID) for the\nreal world. Existing works perform well with high-quality (HQ) images, but\nstruggle with low-quality (LQ) where we can have artifacts like pixelation,\nout-of-focus blur, and motion blur. These artifacts introduce noise to not only\nexternal biometric attributes (e.g. pose, body shape, etc.) but also corrupt\nthe model's internal feature representation. Models usually cluster LQ image\nfeatures together, making it difficult to distinguish between them, leading to\nincorrect matches. We propose a novel framework Robustness against Low-Quality\n(RLQ) to improve CC-ReID model on real-world data. RLQ relies on Coarse\nAttributes Prediction (CAP) and Task Agnostic Distillation (TAD) operating in\nalternate steps in a novel training mechanism. CAP enriches the model with\nexternal fine-grained attributes via coarse predictions, thereby reducing the\neffect of noisy inputs. On the other hand, TAD enhances the model's internal\nfeature representation by bridging the gap between HQ and LQ features, via an\nexternal dataset through task-agnostic self-supervision and distillation. RLQ\noutperforms the existing approaches by 1.6%-2.9% Top-1 on real-world datasets\nlike LaST, and DeepChange, while showing consistent improvement of 5.3%-6%\nTop-1 on PRCC with competitive performance on LTCC. *The code will be made\npublic soon.*"}
{"id": "2505.12588", "pdf": "https://arxiv.org/pdf/2505.12588", "abs": "https://arxiv.org/abs/2505.12588", "authors": ["Samya Bagchi", "Peter Anastasiou", "Matthew Tetlow", "Tat-Jun Chin", "Yasir Latif"], "title": "Event-based Star Tracking under Spacecraft Jitter: the e-STURT Dataset", "categories": ["cs.CV", "eess.SP"], "comment": null, "summary": "Jitter degrades a spacecraft's fine-pointing ability required for optical\ncommunication, earth observation, and space domain awareness. Development of\njitter estimation and compensation algorithms requires high-fidelity sensor\nobservations representative of on-board jitter. In this work, we present the\nEvent-based Star Tracking Under Jitter (e-STURT) dataset -- the first event\ncamera based dataset of star observations under controlled jitter conditions.\nSpecialized hardware employed for the dataset emulates an event-camera\nundergoing on-board jitter. While the event camera provides asynchronous, high\ntemporal resolution star observations, systematic and repeatable jitter is\nintroduced using a micrometer accurate piezoelectric actuator. Various jitter\nsources are simulated using distinct frequency bands and utilizing both axes of\nmotion. Ground-truth jitter is captured in hardware from the piezoelectric\nactuator. The resulting dataset consists of 200 sequences and is made publicly\navailable. This work highlights the dataset generation process, technical\nchallenges and the resulting limitations. To serve as a baseline, we propose a\nhigh-frequency jitter estimation algorithm that operates directly on the event\nstream. The e-STURT dataset will enable the development of jitter aware\nalgorithms for mission critical event-based space sensing applications."}
{"id": "2505.12589", "pdf": "https://arxiv.org/pdf/2505.12589", "abs": "https://arxiv.org/abs/2505.12589", "authors": ["Bo Liu", "Pengfei Qiao", "Minhan Ma", "Xuange Zhang", "Yinan Tang", "Peng Xu", "Kun Liu", "Tongtong Yuan"], "title": "SurveillanceVQA-589K: A Benchmark for Comprehensive Surveillance Video-Language Understanding with Large Models", "categories": ["cs.CV"], "comment": "The dataset and code are publicly available at:\n  https://huggingface.co/datasets/fei213/SurveillanceVQA-589K", "summary": "Understanding surveillance video content remains a critical yet underexplored\nchallenge in vision-language research, particularly due to its real-world\ncomplexity, irregular event dynamics, and safety-critical implications. In this\nwork, we introduce SurveillanceVQA-589K, the largest open-ended video question\nanswering benchmark tailored to the surveillance domain. The dataset comprises\n589,380 QA pairs spanning 12 cognitively diverse question types, including\ntemporal reasoning, causal inference, spatial understanding, and anomaly\ninterpretation, across both normal and abnormal video scenarios. To construct\nthe benchmark at scale, we design a hybrid annotation pipeline that combines\ntemporally aligned human-written captions with Large Vision-Language\nModel-assisted QA generation using prompt-based techniques. We also propose a\nmulti-dimensional evaluation protocol to assess contextual, temporal, and\ncausal comprehension. We evaluate eight LVLMs under this framework, revealing\nsignificant performance gaps, especially in causal and anomaly-related tasks,\nunderscoring the limitations of current models in real-world surveillance\ncontexts. Our benchmark provides a practical and comprehensive resource for\nadvancing video-language understanding in safety-critical applications such as\nintelligent monitoring, incident analysis, and autonomous decision-making."}
{"id": "2505.12593", "pdf": "https://arxiv.org/pdf/2505.12593", "abs": "https://arxiv.org/abs/2505.12593", "authors": ["Mia Thomas", "Trevor Ablett", "Jonathan Kelly"], "title": "Learning Cross-Spectral Point Features with Task-Oriented Training", "categories": ["cs.CV"], "comment": "Proceedings of the {IEEE} International Conference on Robotics and\n  Automation {(ICRA'25)} Thermal Infrared in Robotics (TIRO) Workshop, Atlanta,\n  Georgia, USA, May 19, 2025", "summary": "Unmanned aerial vehicles (UAVs) enable operations in remote and hazardous\nenvironments, yet the visible-spectrum, camera-based navigation systems often\nrelied upon by UAVs struggle in low-visibility conditions. Thermal cameras,\nwhich capture long-wave infrared radiation, are able to function effectively in\ndarkness and smoke, where visible-light cameras fail. This work explores\nlearned cross-spectral (thermal-visible) point features as a means to integrate\nthermal imagery into established camera-based navigation systems. Existing\nmethods typically train a feature network's detection and description outputs\ndirectly, which often focuses training on image regions where thermal and\nvisible-spectrum images exhibit similar appearance. Aiming to more fully\nutilize the available data, we propose a method to train the feature network on\nthe tasks of matching and registration. We run our feature network on\nthermal-visible image pairs, then feed the network response into a\ndifferentiable registration pipeline. Losses are applied to the matching and\nregistration estimates of this pipeline. Our selected model, trained on the\ntask of matching, achieves a registration error (corner error) below 10 pixels\nfor more than 75% of estimates on the MultiPoint dataset. We further\ndemonstrate that our model can also be used with a classical pipeline for\nmatching and registration."}
{"id": "2505.12605", "pdf": "https://arxiv.org/pdf/2505.12605", "abs": "https://arxiv.org/abs/2505.12605", "authors": ["Thong Nguyen", "Zhiyuan Hu", "Xu Lin", "Cong-Duy Nguyen", "See-Kiong Ng", "Luu Anh Tuan"], "title": "Temporal-Oriented Recipe for Transferring Large Vision-Language Model to Video Understanding", "categories": ["cs.CV"], "comment": "In Progress", "summary": "Recent years have witnessed outstanding advances of large vision-language\nmodels (LVLMs). In order to tackle video understanding, most of them depend\nupon their implicit temporal understanding capacity. As such, they have not\ndeciphered important components that contribute to temporal understanding\nability, which might limit the potential of these LVLMs for video\nunderstanding. In this work, we conduct a thorough empirical study to demystify\ncrucial components that influence the temporal understanding of LVLMs. Our\nempirical study reveals that significant impacts are centered around the\nintermediate interface between the visual encoder and the large language model.\nBuilding on these insights, we propose a temporal-oriented recipe that\nencompasses temporal-oriented training schemes and an upscaled interface. Our\nfinal model developed using our recipe significantly enhances previous LVLMs on\nstandard video understanding tasks."}
{"id": "2505.12606", "pdf": "https://arxiv.org/pdf/2505.12606", "abs": "https://arxiv.org/abs/2505.12606", "authors": ["Shiyu Xuan", "Zechao Li", "Jinhui Tang"], "title": "Diff-MM: Exploring Pre-trained Text-to-Image Generation Model for Unified Multi-modal Object Tracking", "categories": ["cs.CV"], "comment": null, "summary": "Multi-modal object tracking integrates auxiliary modalities such as depth,\nthermal infrared, event flow, and language to provide additional information\nbeyond RGB images, showing great potential in improving tracking stabilization\nin complex scenarios. Existing methods typically start from an RGB-based\ntracker and learn to understand auxiliary modalities only from training data.\nConstrained by the limited multi-modal training data, the performance of these\nmethods is unsatisfactory. To alleviate this limitation, this work proposes a\nunified multi-modal tracker Diff-MM by exploiting the multi-modal understanding\ncapability of the pre-trained text-to-image generation model. Diff-MM leverages\nthe UNet of pre-trained Stable Diffusion as a tracking feature extractor\nthrough the proposed parallel feature extraction pipeline, which enables\npairwise image inputs for object tracking. We further introduce a multi-modal\nsub-module tuning method that learns to gain complementary information between\ndifferent modalities. By harnessing the extensive prior knowledge in the\ngeneration model, we achieve a unified tracker with uniform parameters for\nRGB-N/D/T/E tracking. Experimental results demonstrate the promising\nperformance of our method compared with recently proposed trackers, e.g., its\nAUC outperforms OneTracker by 8.3% on TNL2K."}
{"id": "2505.12620", "pdf": "https://arxiv.org/pdf/2505.12620", "abs": "https://arxiv.org/abs/2505.12620", "authors": ["Haiquan Wen", "Yiwei He", "Zhenglin Huang", "Tianxiao Li", "Zihan YU", "Xingru Huang", "Lu Qi", "Baoyuan Wu", "Xiangtai Li", "Guangliang Cheng"], "title": "BusterX: MLLM-Powered AI-Generated Video Forgery Detection and Explanation", "categories": ["cs.CV"], "comment": null, "summary": "Advances in AI generative models facilitate super-realistic video synthesis,\namplifying misinformation risks via social media and eroding trust in digital\ncontent. Several research works have explored new deepfake detection methods on\nAI-generated images to alleviate these risks. However, with the fast\ndevelopment of video generation models, such as Sora and WanX, there is\ncurrently a lack of large-scale, high-quality AI-generated video datasets for\nforgery detection. In addition, existing detection approaches predominantly\ntreat the task as binary classification, lacking explainability in model\ndecision-making and failing to provide actionable insights or guidance for the\npublic. To address these challenges, we propose \\textbf{GenBuster-200K}, a\nlarge-scale AI-generated video dataset featuring 200K high-resolution video\nclips, diverse latest generative techniques, and real-world scenes. We further\nintroduce \\textbf{BusterX}, a novel AI-generated video detection and\nexplanation framework leveraging multimodal large language model (MLLM) and\nreinforcement learning for authenticity determination and explainable\nrationale. To our knowledge, GenBuster-200K is the {\\it \\textbf{first}}\nlarge-scale, high-quality AI-generated video dataset that incorporates the\nlatest generative techniques for real-world scenarios. BusterX is the {\\it\n\\textbf{first}} framework to integrate MLLM with reinforcement learning for\nexplainable AI-generated video detection. Extensive comparisons with\nstate-of-the-art methods and ablation studies validate the effectiveness and\ngeneralizability of BusterX. The code, models, and datasets will be released."}
{"id": "2505.12630", "pdf": "https://arxiv.org/pdf/2505.12630", "abs": "https://arxiv.org/abs/2505.12630", "authors": ["Xiangpeng Tian", "Xiangyu Liao", "Xiao Liu", "Meng Li", "Chao Ren"], "title": "Degradation-Aware Feature Perturbation for All-in-One Image Restoration", "categories": ["cs.CV", "cs.AI", "I.4.5"], "comment": "Accepted to CVPR 2025. 8 pages, 7 figures", "summary": "All-in-one image restoration aims to recover clear images from various\ndegradation types and levels with a unified model. Nonetheless, the significant\nvariations among degradation types present challenges for training a universal\nmodel, often resulting in task interference, where the gradient update\ndirections of different tasks may diverge due to shared parameters. To address\nthis issue, motivated by the routing strategy, we propose DFPIR, a novel\nall-in-one image restorer that introduces Degradation-aware Feature\nPerturbations(DFP) to adjust the feature space to align with the unified\nparameter space. In this paper, the feature perturbations primarily include\nchannel-wise perturbations and attention-wise perturbations. Specifically,\nchannel-wise perturbations are implemented by shuffling the channels in\nhigh-dimensional space guided by degradation types, while attention-wise\nperturbations are achieved through selective masking in the attention space. To\nachieve these goals, we propose a Degradation-Guided Perturbation Block (DGPB)\nto implement these two functions, positioned between the encoding and decoding\nstages of the encoder-decoder architecture. Extensive experimental results\ndemonstrate that DFPIR achieves state-of-the-art performance on several\nall-in-one image restoration tasks including image denoising, image dehazing,\nimage deraining, motion deblurring, and low-light image enhancement. Our codes\nare available at https://github.com/TxpHome/DFPIR."}
{"id": "2505.12631", "pdf": "https://arxiv.org/pdf/2505.12631", "abs": "https://arxiv.org/abs/2505.12631", "authors": ["Li Lin"], "title": "Multi-Resolution Haar Network: Enhancing human motion prediction via Haar transform", "categories": ["cs.CV"], "comment": null, "summary": "The 3D human pose is vital for modern computer vision and computer graphics,\nand its prediction has drawn attention in recent years. 3D human pose\nprediction aims at forecasting a human's future motion from the previous\nsequence. Ignoring that the arbitrariness of human motion sequences has a firm\norigin in transition in both temporal and spatial axes limits the performance\nof state-of-the-art methods, leading them to struggle with making precise\npredictions on complex cases, e.g., arbitrarily posing or greeting. To\nalleviate this problem, a network called HaarMoDic is proposed in this paper,\nwhich utilizes the 2D Haar transform to project joints to higher resolution\ncoordinates where the network can access spatial and temporal information\nsimultaneously. An ablation study proves that the significant contributing\nmodule within the HaarModic Network is the Multi-Resolution Haar (MR-Haar)\nblock. Instead of mining in one of two axes or extracting separately, the\nMR-Haar block projects whole motion sequences to a mixed-up coordinate in\nhigher resolution with 2D Haar Transform, allowing the network to give scope to\ninformation from both axes in different resolutions. With the MR-Haar block,\nthe HaarMoDic network can make predictions referring to a broader range of\ninformation. Experimental results demonstrate that HaarMoDic surpasses\nstate-of-the-art methods in every testing interval on the Human3.6M dataset in\nthe Mean Per Joint Position Error (MPJPE) metric."}
{"id": "2505.12632", "pdf": "https://arxiv.org/pdf/2505.12632", "abs": "https://arxiv.org/abs/2505.12632", "authors": ["Yunseok Jang", "Yeda Song", "Sungryull Sohn", "Lajanugen Logeswaran", "Tiange Luo", "Dong-Ki Kim", "Kyunghoon Bae", "Honglak Lee"], "title": "Scalable Video-to-Dataset Generation for Cross-Platform Mobile Agents", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "CVPR 2025", "summary": "Recent advancements in Large Language Models (LLMs) and Vision-Language\nModels (VLMs) have sparked significant interest in developing GUI visual\nagents. We introduce MONDAY (Mobile OS Navigation Task Dataset for Agents from\nYouTube), a large-scale dataset of 313K annotated frames from 20K instructional\nvideos capturing diverse real-world mobile OS navigation across multiple\nplatforms. Models that include MONDAY in their pre-training phases demonstrate\nrobust cross-platform generalization capabilities, consistently outperforming\nmodels trained on existing single OS datasets while achieving an average\nperformance gain of 18.11%p on an unseen mobile OS platform. To enable\ncontinuous dataset expansion as mobile platforms evolve, we present an\nautomated framework that leverages publicly available video content to create\ncomprehensive task datasets without manual annotation. Our framework comprises\nrobust OCR-based scene detection (95.04% F1score), near-perfect UI element\ndetection (99.87% hit ratio), and novel multi-step action identification to\nextract reliable action sequences across diverse interface configurations. We\ncontribute both the MONDAY dataset and our automated collection framework to\nfacilitate future research in mobile OS navigation."}
{"id": "2505.12635", "pdf": "https://arxiv.org/pdf/2505.12635", "abs": "https://arxiv.org/abs/2505.12635", "authors": ["Mingqi Shao", "Feng Xiong", "Zhaoxu Sun", "Mu Xu"], "title": "MVPainter: Accurate and Detailed 3D Texture Generation via Multi-View Diffusion with Geometric Control", "categories": ["cs.CV"], "comment": "Project page: https://amap-cvlab.github.io/MV-Painter", "summary": "Recently, significant advances have been made in 3D object generation.\nBuilding upon the generated geometry, current pipelines typically employ image\ndiffusion models to generate multi-view RGB images, followed by UV texture\nreconstruction through texture baking. While 3D geometry generation has\nimproved significantly, supported by multiple open-source frameworks, 3D\ntexture generation remains underexplored. In this work, we systematically\ninvestigate 3D texture generation through the lens of three core dimensions:\nreference-texture alignment, geometry-texture consistency, and local texture\nquality. To tackle these issues, we propose MVPainter, which employs data\nfiltering and augmentation strategies to enhance texture fidelity and detail,\nand introduces ControlNet-based geometric conditioning to improve\ntexture-geometry alignment. Furthermore, we extract physically-based rendering\n(PBR) attributes from the generated views to produce PBR meshes suitable for\nreal-world rendering applications. MVPainter achieves state-of-the-art results\nacross all three dimensions, as demonstrated by human-aligned evaluations. To\nfacilitate further research and reproducibility, we also release our full\npipeline as an open-source system, including data construction, model\narchitecture, and evaluation tools."}
{"id": "2505.12641", "pdf": "https://arxiv.org/pdf/2505.12641", "abs": "https://arxiv.org/abs/2505.12641", "authors": ["Yue Huang", "Zi'ang Li", "Tianle Hu", "Jie Wen", "Guanbin Li", "Jinglin Zhang", "Guoxu Zhou", "Xiaozhao Fang"], "title": "Single Image Reflection Removal via inter-layer Complementarity", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Although dual-stream architectures have achieved remarkable success in single\nimage reflection removal, they fail to fully exploit inter-layer\ncomplementarity in their physical modeling and network design, which limits the\nquality of image separation. To address this fundamental limitation, we propose\ntwo targeted improvements to enhance dual-stream architectures: First, we\nintroduce a novel inter-layer complementarity model where low-frequency\ncomponents extracted from the residual layer interact with the transmission\nlayer through dual-stream architecture to enhance inter-layer complementarity.\nMeanwhile, high-frequency components from the residual layer provide inverse\nmodulation to both streams, improving the detail quality of the transmission\nlayer. Second, we propose an efficient inter-layer complementarity attention\nmechanism which first cross-reorganizes dual streams at the channel level to\nobtain reorganized streams with inter-layer complementary structures, then\nperforms attention computation on the reorganized streams to achieve better\ninter-layer separation, and finally restores the original stream structure for\noutput. Experimental results demonstrate that our method achieves\nstate-of-the-art separation quality on multiple public datasets while\nsignificantly reducing both computational cost and model complexity."}
{"id": "2505.12644", "pdf": "https://arxiv.org/pdf/2505.12644", "abs": "https://arxiv.org/abs/2505.12644", "authors": ["Bo Yang", "Hengwei Zhang", "Jindong Wang", "Yuchen Ren", "Chenhao Lin", "Chao Shen", "Zhengyu Zhao"], "title": "Use as Many Surrogates as You Want: Selective Ensemble Attack to Unleash Transferability without Sacrificing Resource Efficiency", "categories": ["cs.CV"], "comment": null, "summary": "In surrogate ensemble attacks, using more surrogate models yields higher\ntransferability but lower resource efficiency. This practical trade-off between\ntransferability and efficiency has largely limited existing attacks despite\nmany pre-trained models are easily accessible online. In this paper, we argue\nthat such a trade-off is caused by an unnecessary common assumption, i.e., all\nmodels should be identical across iterations. By lifting this assumption, we\ncan use as many surrogates as we want to unleash transferability without\nsacrificing efficiency. Concretely, we propose Selective Ensemble Attack (SEA),\nwhich dynamically selects diverse models (from easily accessible pre-trained\nmodels) across iterations based on our new interpretation of decoupling\nwithin-iteration and cross-iteration model diversity.In this way, the number of\nwithin-iteration models is fixed for maintaining efficiency, while only\ncross-iteration model diversity is increased for higher transferability.\nExperiments on ImageNet demonstrate the superiority of SEA in various\nscenarios. For example, when dynamically selecting 4 from 20 accessible models,\nSEA yields 8.5% higher transferability than existing attacks under the same\nefficiency. The superiority of SEA also generalizes to real-world systems, such\nas commercial vision APIs and large vision-language models. Overall, SEA opens\nup the possibility of adaptively balancing transferability and efficiency\naccording to specific resource requirements."}
{"id": "2505.12650", "pdf": "https://arxiv.org/pdf/2505.12650", "abs": "https://arxiv.org/abs/2505.12650", "authors": ["Yaotian Yang", "Yiwen Tang", "Yizhe Chen", "Xiao Chen", "Jiangjie Qiu", "Hao Xiong", "Haoyu Yin", "Zhiyao Luo", "Yifei Zhang", "Sijia Tao", "Wentao Li", "Qinghua Zhang", "Yuqiang Li", "Wanli Ouyang", "Bin Zhao", "Xiaonan Wang", "Fei Wei"], "title": "AutoMat: Enabling Automated Crystal Structure Reconstruction from Microscopy via Agentic Tool Use", "categories": ["cs.CV", "cs.AI"], "comment": "The code and dataset are publicly available at\n  https://github.com/yyt-2378/AutoMat and\n  https://huggingface.co/datasets/yaotianvector/STEM2Mat", "summary": "Machine learning-based interatomic potentials and force fields depend\ncritically on accurate atomic structures, yet such data are scarce due to the\nlimited availability of experimentally resolved crystals. Although\natomic-resolution electron microscopy offers a potential source of structural\ndata, converting these images into simulation-ready formats remains\nlabor-intensive and error-prone, creating a bottleneck for model training and\nvalidation. We introduce AutoMat, an end-to-end, agent-assisted pipeline that\nautomatically transforms scanning transmission electron microscopy (STEM)\nimages into atomic crystal structures and predicts their physical properties.\nAutoMat combines pattern-adaptive denoising, physics-guided template retrieval,\nsymmetry-aware atomic reconstruction, fast relaxation and property prediction\nvia MatterSim, and coordinated orchestration across all stages. We propose the\nfirst dedicated STEM2Mat-Bench for this task and evaluate performance using\nlattice RMSD, formation energy MAE, and structure-matching success rate. By\norchestrating external tool calls, AutoMat enables a text-only LLM to\noutperform vision-language models in this domain, achieving closed-loop\nreasoning throughout the pipeline. In large-scale experiments over 450\nstructure samples, AutoMat substantially outperforms existing multimodal large\nlanguage models and tools. These results validate both AutoMat and\nSTEM2Mat-Bench, marking a key step toward bridging microscopy and atomistic\nsimulation in materials science.The code and dataset are publicly available at\nhttps://github.com/yyt-2378/AutoMat and\nhttps://huggingface.co/datasets/yaotianvector/STEM2Mat."}
{"id": "2505.12656", "pdf": "https://arxiv.org/pdf/2505.12656", "abs": "https://arxiv.org/abs/2505.12656", "authors": ["Yongchang Gao", "Meiling Jin", "Zhaofei Yu", "Tiejun Huang", "Guozhang Chen"], "title": "SPKLIP: Aligning Spike Video Streams with Natural Language", "categories": ["cs.CV"], "comment": null, "summary": "Spike cameras offer unique sensing capabilities but their sparse,\nasynchronous output challenges semantic understanding, especially for Spike\nVideo-Language Alignment (Spike-VLA) where models like CLIP underperform due to\nmodality mismatch. We introduce SPKLIP, the first architecture specifically for\nSpike-VLA. SPKLIP employs a hierarchical spike feature extractor that\nadaptively models multi-scale temporal dynamics in event streams, and uses\nspike-text contrastive learning to directly align spike video with language,\nenabling effective few-shot learning. A full-spiking visual encoder variant,\nintegrating SNN components into our pipeline, demonstrates enhanced energy\nefficiency. Experiments show state-of-the-art performance on benchmark spike\ndatasets and strong few-shot generalization on a newly contributed real-world\ndataset. SPKLIP's energy efficiency highlights its potential for neuromorphic\ndeployment, advancing event-based multimodal research. The source code and\ndataset are available at [link removed for anonymity]."}
{"id": "2505.12660", "pdf": "https://arxiv.org/pdf/2505.12660", "abs": "https://arxiv.org/abs/2505.12660", "authors": ["Ziqi Wen", "Jonathan Skaza", "Shravan Murlidaran", "William Y. Wang", "Miguel P. Eckstein"], "title": "Predicting Reaction Time to Comprehend Scenes with Foveated Scene Understanding Maps", "categories": ["cs.CV"], "comment": null, "summary": "Although models exist that predict human response times (RTs) in tasks such\nas target search and visual discrimination, the development of image-computable\npredictors for scene understanding time remains an open challenge. Recent\nadvances in vision-language models (VLMs), which can generate scene\ndescriptions for arbitrary images, combined with the availability of\nquantitative metrics for comparing linguistic descriptions, offer a new\nopportunity to model human scene understanding. We hypothesize that the primary\nbottleneck in human scene understanding and the driving source of variability\nin response times across scenes is the interaction between the foveated nature\nof the human visual system and the spatial distribution of task-relevant visual\ninformation within an image. Based on this assumption, we propose a novel\nimage-computable model that integrates foveated vision with VLMs to produce a\nspatially resolved map of scene understanding as a function of fixation\nlocation (Foveated Scene Understanding Map, or F-SUM), along with an aggregate\nF-SUM score. This metric correlates with average (N=17) human RTs (r=0.47) and\nnumber of saccades (r=0.51) required to comprehend a scene (across 277 scenes).\nThe F-SUM score also correlates with average (N=16) human description accuracy\n(r=-0.56) in time-limited presentations. These correlations significantly\nexceed those of standard image-based metrics such as clutter, visual\ncomplexity, and scene ambiguity based on language entropy. Together, our work\nintroduces a new image-computable metric for predicting human response times in\nscene understanding and demonstrates the importance of foveated visual\nprocessing in shaping comprehension difficulty."}
{"id": "2505.12667", "pdf": "https://arxiv.org/pdf/2505.12667", "abs": "https://arxiv.org/abs/2505.12667", "authors": ["Zihan Su", "Xuerui Qiu", "Hongbin Xu", "Tangyu Jiang", "Junhao Zhuang", "Chun Yuan", "Ming Li", "Shengfeng He", "Fei Richard Yu"], "title": "Safe-Sora: Safe Text-to-Video Generation via Graphical Watermarking", "categories": ["cs.CV"], "comment": null, "summary": "The explosive growth of generative video models has amplified the demand for\nreliable copyright preservation of AI-generated content. Despite its popularity\nin image synthesis, invisible generative watermarking remains largely\nunderexplored in video generation. To address this gap, we propose Safe-Sora,\nthe first framework to embed graphical watermarks directly into the video\ngeneration process. Motivated by the observation that watermarking performance\nis closely tied to the visual similarity between the watermark and cover\ncontent, we introduce a hierarchical coarse-to-fine adaptive matching\nmechanism. Specifically, the watermark image is divided into patches, each\nassigned to the most visually similar video frame, and further localized to the\noptimal spatial region for seamless embedding. To enable spatiotemporal fusion\nof watermark patches across video frames, we develop a 3D wavelet\ntransform-enhanced Mamba architecture with a novel spatiotemporal local\nscanning strategy, effectively modeling long-range dependencies during\nwatermark embedding and retrieval. To the best of our knowledge, this is the\nfirst attempt to apply state space models to watermarking, opening new avenues\nfor efficient and robust watermark protection. Extensive experiments\ndemonstrate that Safe-Sora achieves state-of-the-art performance in terms of\nvideo quality, watermark fidelity, and robustness, which is largely attributed\nto our proposals. We will release our code upon publication."}
{"id": "2505.12670", "pdf": "https://arxiv.org/pdf/2505.12670", "abs": "https://arxiv.org/abs/2505.12670", "authors": ["Lihong Chen", "Hossein Hassani", "Soodeh Nikan"], "title": "TS-VLM: Text-Guided SoftSort Pooling for Vision-Language Models in Multi-View Driving Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "Vision-Language Models (VLMs) have shown remarkable potential in advancing\nautonomous driving by leveraging multi-modal fusion in order to enhance scene\nperception, reasoning, and decision-making. Despite their potential, existing\nmodels suffer from computational overhead and inefficient integration of\nmulti-view sensor data that make them impractical for real-time deployment in\nsafety-critical autonomous driving applications. To address these shortcomings,\nthis paper is devoted to designing a lightweight VLM called TS-VLM, which\nincorporates a novel Text-Guided SoftSort Pooling (TGSSP) module. By resorting\nto semantics of the input queries, TGSSP ranks and fuses visual features from\nmultiple views, enabling dynamic and query-aware multi-view aggregation without\nreliance on costly attention mechanisms. This design ensures the query-adaptive\nprioritization of semantically related views, which leads to improved\ncontextual accuracy in multi-view reasoning for autonomous driving. Extensive\nevaluations on the DriveLM benchmark demonstrate that, on the one hand, TS-VLM\noutperforms state-of-the-art models with a BLEU-4 score of 56.82, METEOR of\n41.91, ROUGE-L of 74.64, and CIDEr of 3.39. On the other hand, TS-VLM reduces\ncomputational cost by up to 90%, where the smallest version contains only 20.1\nmillion parameters, making it more practical for real-time deployment in\nautonomous vehicles."}
{"id": "2505.12674", "pdf": "https://arxiv.org/pdf/2505.12674", "abs": "https://arxiv.org/abs/2505.12674", "authors": ["Mingyuan Zhou", "Yi Gu", "Zhendong Wang"], "title": "Few-Step Diffusion via Score identity Distillation", "categories": ["cs.CV", "cs.LG", "stat.ML"], "comment": null, "summary": "Diffusion distillation has emerged as a promising strategy for accelerating\ntext-to-image (T2I) diffusion models by distilling a pretrained score network\ninto a one- or few-step generator. While existing methods have made notable\nprogress, they often rely on real or teacher-synthesized images to perform well\nwhen distilling high-resolution T2I diffusion models such as Stable Diffusion\nXL (SDXL), and their use of classifier-free guidance (CFG) introduces a\npersistent trade-off between text-image alignment and generation diversity. We\naddress these challenges by optimizing Score identity Distillation (SiD) -- a\ndata-free, one-step distillation framework -- for few-step generation. Backed\nby theoretical analysis that justifies matching a uniform mixture of outputs\nfrom all generation steps to the data distribution, our few-step distillation\nalgorithm avoids step-specific networks and integrates seamlessly into existing\npipelines, achieving state-of-the-art performance on SDXL at 1024x1024\nresolution. To mitigate the alignment-diversity trade-off when real text-image\npairs are available, we introduce a Diffusion GAN-based adversarial loss\napplied to the uniform mixture and propose two new guidance strategies:\nZero-CFG, which disables CFG in the teacher and removes text conditioning in\nthe fake score network, and Anti-CFG, which applies negative CFG in the fake\nscore network. This flexible setup improves diversity without sacrificing\nalignment. Comprehensive experiments on SD1.5 and SDXL demonstrate\nstate-of-the-art performance in both one-step and few-step generation settings,\nalong with robustness to the absence of real images. Our efficient PyTorch\nimplementation, along with the resulting one- and few-step distilled\ngenerators, will be released publicly as a separate branch at\nhttps://github.com/mingyuanzhou/SiD-LSG."}
{"id": "2505.12677", "pdf": "https://arxiv.org/pdf/2505.12677", "abs": "https://arxiv.org/abs/2505.12677", "authors": ["Shristi Das Biswas", "Arani Roy", "Kaushik Roy"], "title": "CURE: Concept Unlearning via Orthogonal Representation Editing in Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "As Text-to-Image models continue to evolve, so does the risk of generating\nunsafe, copyrighted, or privacy-violating content. Existing safety\ninterventions - ranging from training data curation and model fine-tuning to\ninference-time filtering and guidance - often suffer from incomplete concept\nremoval, susceptibility to jail-breaking, computational inefficiency, or\ncollateral damage to unrelated capabilities. In this paper, we introduce CURE,\na training-free concept unlearning framework that operates directly in the\nweight space of pre-trained diffusion models, enabling fast, interpretable, and\nhighly specific suppression of undesired concepts. At the core of our method is\nthe Spectral Eraser, a closed-form, orthogonal projection module that\nidentifies discriminative subspaces using Singular Value Decomposition over\ntoken embeddings associated with the concepts to forget and retain.\nIntuitively, the Spectral Eraser identifies and isolates features unique to the\nundesired concept while preserving safe attributes. This operator is then\napplied in a single step update to yield an edited model in which the target\nconcept is effectively unlearned - without retraining, supervision, or\niterative optimization. To balance the trade-off between filtering toxicity and\npreserving unrelated concepts, we further introduce an Expansion Mechanism for\nspectral regularization which selectively modulates singular vectors based on\ntheir relative significance to control the strength of forgetting. All the\nprocesses above are in closed-form, guaranteeing extremely efficient erasure in\nonly $2$ seconds. Benchmarking against prior approaches, CURE achieves a more\nefficient and thorough removal for targeted artistic styles, objects,\nidentities, or explicit content, with minor damage to original generation\nability and demonstrates enhanced robustness against red-teaming."}
{"id": "2505.12685", "pdf": "https://arxiv.org/pdf/2505.12685", "abs": "https://arxiv.org/abs/2505.12685", "authors": ["Fei Xie", "Jiahao Nie", "Yujin Tang", "Wenkang Zhang", "Hongshen Zhao"], "title": "Mamba-Adaptor: State Space Model Adaptor for Visual Recognition", "categories": ["cs.CV"], "comment": "CVPR paper", "summary": "Recent State Space Models (SSM), especially Mamba, have demonstrated\nimpressive performance in visual modeling and possess superior model\nefficiency. However, the application of Mamba to visual tasks suffers inferior\nperformance due to three main constraints existing in the sequential model: 1)\nCasual computing is incapable of accessing global context; 2) Long-range\nforgetting when computing the current hidden states; 3) Weak spatial structural\nmodeling due to the transformed sequential input. To address these issues, we\ninvestigate a simple yet powerful vision task Adaptor for Mamba models, which\nconsists of two functional modules: Adaptor-T and Adaptor-S. When solving the\nhidden states for SSM, we apply a lightweight prediction module Adaptor-T to\nselect a set of learnable locations as memory augmentations to ease long-range\nforgetting issues. Moreover, we leverage Adapator-S, composed of multi-scale\ndilated convolutional kernels, to enhance the spatial modeling and introduce\nthe image inductive bias into the feature output. Both modules can enlarge the\ncontext modeling in casual computing, as the output is enhanced by the\ninaccessible features. We explore three usages of Mamba-Adaptor: A general\nvisual backbone for various vision tasks; A booster module to raise the\nperformance of pretrained backbones; A highly efficient fine-tuning module that\nadapts the base model for transfer learning tasks. Extensive experiments verify\nthe effectiveness of Mamba-Adaptor in three settings. Notably, our\nMamba-Adaptor achieves state-of the-art performance on the ImageNet and COCO\nbenchmarks."}
{"id": "2505.12693", "pdf": "https://arxiv.org/pdf/2505.12693", "abs": "https://arxiv.org/abs/2505.12693", "authors": ["Luyao Lei", "Shuo Xu", "Yifan Bai", "Xing Wei"], "title": "TACOcc:Target-Adaptive Cross-Modal Fusion with Volume Rendering for 3D Semantic Occupancy", "categories": ["cs.CV"], "comment": null, "summary": "The performance of multi-modal 3D occupancy prediction is limited by\nineffective fusion, mainly due to geometry-semantics mismatch from fixed fusion\nstrategies and surface detail loss caused by sparse, noisy annotations. The\nmismatch stems from the heterogeneous scale and distribution of point cloud and\nimage features, leading to biased matching under fixed neighborhood fusion. To\naddress this, we propose a target-scale adaptive, bidirectional symmetric\nretrieval mechanism. It expands the neighborhood for large targets to enhance\ncontext awareness and shrinks it for small ones to improve efficiency and\nsuppress noise, enabling accurate cross-modal feature alignment. This mechanism\nexplicitly establishes spatial correspondences and improves fusion accuracy.\nFor surface detail loss, sparse labels provide limited supervision, resulting\nin poor predictions for small objects. We introduce an improved volume\nrendering pipeline based on 3D Gaussian Splatting, which takes fused features\nas input to render images, applies photometric consistency supervision, and\njointly optimizes 2D-3D consistency. This enhances surface detail\nreconstruction while suppressing noise propagation. In summary, we propose\nTACOcc, an adaptive multi-modal fusion framework for 3D semantic occupancy\nprediction, enhanced by volume rendering supervision. Experiments on the\nnuScenes and SemanticKITTI benchmarks validate its effectiveness."}
{"id": "2505.12702", "pdf": "https://arxiv.org/pdf/2505.12702", "abs": "https://arxiv.org/abs/2505.12702", "authors": ["Tianming Liang", "Haichao Jiang", "Yuting Yang", "Chaolei Tan", "Shuai Li", "Wei-Shi Zheng", "Jian-Fang Hu"], "title": "Long-RVOS: A Comprehensive Benchmark for Long-term Referring Video Object Segmentation", "categories": ["cs.CV"], "comment": "Project Page: \\url{https://isee-laboratory.github.io/Long-RVOS}", "summary": "Referring video object segmentation (RVOS) aims to identify, track and\nsegment the objects in a video based on language descriptions, which has\nreceived great attention in recent years. However, existing datasets remain\nfocus on short video clips within several seconds, with salient objects visible\nin most frames. To advance the task towards more practical scenarios, we\nintroduce \\textbf{Long-RVOS}, a large-scale benchmark for long-term referring\nvideo object segmentation. Long-RVOS contains 2,000+ videos of an average\nduration exceeding 60 seconds, covering a variety of objects that undergo\nocclusion, disappearance-reappearance and shot changing. The objects are\nmanually annotated with three different types of descriptions to individually\nevaluate the understanding of static attributes, motion patterns and\nspatiotemporal relationships. Moreover, unlike previous benchmarks that rely\nsolely on the per-frame spatial evaluation, we introduce two new metrics to\nassess the temporal and spatiotemporal consistency. We benchmark 6\nstate-of-the-art methods on Long-RVOS. The results show that current approaches\nstruggle severely with the long-video challenges. To address this, we further\npropose ReferMo, a promising baseline method that integrates motion information\nto expand the temporal receptive field, and employs a local-to-global\narchitecture to capture both short-term dynamics and long-term dependencies.\nDespite simplicity, ReferMo achieves significant improvements over current\nmethods in long-term scenarios. We hope that Long-RVOS and our baseline can\ndrive future RVOS research towards tackling more realistic and long-form\nvideos."}
{"id": "2505.12703", "pdf": "https://arxiv.org/pdf/2505.12703", "abs": "https://arxiv.org/abs/2505.12703", "authors": ["Jiabin Chen", "Haiping Wang", "Jinpeng Li", "Yuan Liu", "Zhen Dong", "Bisheng Yang"], "title": "SpatialLLM: From Multi-modality Data to Urban Spatial Intelligence", "categories": ["cs.CV"], "comment": null, "summary": "We propose SpatialLLM, a novel approach advancing spatial intelligence tasks\nin complex urban scenes. Unlike previous methods requiring geographic analysis\ntools or domain expertise, SpatialLLM is a unified language model directly\naddressing various spatial intelligence tasks without any training,\nfine-tuning, or expert intervention. The core of SpatialLLM lies in\nconstructing detailed and structured scene descriptions from raw spatial data\nto prompt pre-trained LLMs for scene-based analysis. Extensive experiments show\nthat, with our designs, pretrained LLMs can accurately perceive spatial\ndistribution information and enable zero-shot execution of advanced spatial\nintelligence tasks, including urban planning, ecological analysis, traffic\nmanagement, etc. We argue that multi-field knowledge, context length, and\nreasoning ability are key factors influencing LLM performances in urban\nanalysis. We hope that SpatialLLM will provide a novel viable perspective for\nurban intelligent analysis and management. The code and dataset are available\nat https://github.com/WHU-USI3DV/SpatialLLM."}
{"id": "2505.12711", "pdf": "https://arxiv.org/pdf/2505.12711", "abs": "https://arxiv.org/abs/2505.12711", "authors": ["Qichen Sun", "Zhengrui Guo", "Rui Peng", "Hao Chen", "Jinzhuo Wang"], "title": "Any-to-Any Learning in Computational Pathology via Triplet Multimodal Pretraining", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advances in computational pathology and artificial intelligence have\nsignificantly enhanced the utilization of gigapixel whole-slide images and and\nadditional modalities (e.g., genomics) for pathological diagnosis. Although\ndeep learning has demonstrated strong potential in pathology, several key\nchallenges persist: (1) fusing heterogeneous data types requires sophisticated\nstrategies beyond simple concatenation due to high computational costs; (2)\ncommon scenarios of missing modalities necessitate flexible strategies that\nallow the model to learn robustly in the absence of certain modalities; (3) the\ndownstream tasks in CPath are diverse, ranging from unimodal to multimodal,\ncnecessitating a unified model capable of handling all modalities. To address\nthese challenges, we propose ALTER, an any-to-any tri-modal pretraining\nframework that integrates WSIs, genomics, and pathology reports. The term \"any\"\nemphasizes ALTER's modality-adaptive design, enabling flexible pretraining with\nany subset of modalities, and its capacity to learn robust, cross-modal\nrepresentations beyond WSI-centric approaches. We evaluate ALTER across\nextensive clinical tasks including survival prediction, cancer subtyping, gene\nmutation prediction, and report generation, achieving superior or comparable\nperformance to state-of-the-art baselines."}
{"id": "2505.12714", "pdf": "https://arxiv.org/pdf/2505.12714", "abs": "https://arxiv.org/abs/2505.12714", "authors": ["Yinzhe Wang", "Yiwen Xiao", "Hu Wang", "Yiping Xu", "Yan Tian"], "title": "IA-MVS: Instance-Focused Adaptive Depth Sampling for Multi-View Stereo", "categories": ["cs.CV"], "comment": null, "summary": "Multi-view stereo (MVS) models based on progressive depth hypothesis\nnarrowing have made remarkable advancements. However, existing methods haven't\nfully utilized the potential that the depth coverage of individual instances is\nsmaller than that of the entire scene, which restricts further improvements in\ndepth estimation precision. Moreover, inevitable deviations in the initial\nstage accumulate as the process advances. In this paper, we propose\nInstance-Adaptive MVS (IA-MVS). It enhances the precision of depth estimation\nby narrowing the depth hypothesis range and conducting refinement on each\ninstance. Additionally, a filtering mechanism based on intra-instance depth\ncontinuity priors is incorporated to boost robustness. Furthermore, recognizing\nthat existing confidence estimation can degrade IA-MVS performance on point\nclouds. We have developed a detailed mathematical model for confidence\nestimation based on conditional probability. The proposed method can be widely\napplied in models based on MVSNet without imposing extra training burdens. Our\nmethod achieves state-of-the-art performance on the DTU benchmark. The source\ncode is available at https://github.com/KevinWang73106/IA-MVS."}
{"id": "2505.12715", "pdf": "https://arxiv.org/pdf/2505.12715", "abs": "https://arxiv.org/abs/2505.12715", "authors": ["Aditya Taparia", "Noel Ngu", "Mario Leiva", "Joshua Shay Kricheli", "John Corcoran", "Nathaniel D. Bastian", "Gerardo Simari", "Paulo Shakarian", "Ransalu Senanayake"], "title": "VLC Fusion: Vision-Language Conditioned Sensor Fusion for Robust Object Detection", "categories": ["cs.CV"], "comment": "12 pages, 19 figures", "summary": "Although fusing multiple sensor modalities can enhance object detection\nperformance, existing fusion approaches often overlook subtle variations in\nenvironmental conditions and sensor inputs. As a result, they struggle to\nadaptively weight each modality under such variations. To address this\nchallenge, we introduce Vision-Language Conditioned Fusion (VLC Fusion), a\nnovel fusion framework that leverages a Vision-Language Model (VLM) to\ncondition the fusion process on nuanced environmental cues. By capturing\nhigh-level environmental context such as as darkness, rain, and camera\nblurring, the VLM guides the model to dynamically adjust modality weights based\non the current scene. We evaluate VLC Fusion on real-world autonomous driving\nand military target detection datasets that include image, LIDAR, and mid-wave\ninfrared modalities. Our experiments show that VLC Fusion consistently\noutperforms conventional fusion baselines, achieving improved detection\naccuracy in both seen and unseen scenarios."}
{"id": "2505.12728", "pdf": "https://arxiv.org/pdf/2505.12728", "abs": "https://arxiv.org/abs/2505.12728", "authors": ["Zihua Wang", "Ruibo Li", "Haozhe Du", "Joey Tianyi Zhou", "Yu Zhang", "Xu Yang"], "title": "FLASH: Latent-Aware Semi-Autoregressive Speculative Decoding for Multimodal Tasks", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Large language and multimodal models (LLMs and LMMs) exhibit strong inference\ncapabilities but are often limited by slow decoding speeds. This challenge is\nespecially acute in LMMs, where visual inputs typically comprise more tokens\nwith lower information density than text -- an issue exacerbated by recent\ntrends toward finer-grained visual tokenizations to boost performance.\nSpeculative decoding has been effective in accelerating LLM inference by using\na smaller draft model to generate candidate tokens, which are then selectively\nverified by the target model, improving speed without sacrificing output\nquality. While this strategy has been extended to LMMs, existing methods\nlargely overlook the unique properties of visual inputs and depend solely on\ntext-based draft models. In this work, we propose \\textbf{FLASH} (Fast\nLatent-Aware Semi-Autoregressive Heuristics), a speculative decoding framework\ndesigned specifically for LMMs, which leverages two key properties of\nmultimodal data to design the draft model. First, to address redundancy in\nvisual tokens, we propose a lightweight latent-aware token compression\nmechanism. Second, recognizing that visual objects often co-occur within a\nscene, we employ a semi-autoregressive decoding strategy to generate multiple\ntokens per forward pass. These innovations accelerate draft decoding while\nmaintaining high acceptance rates, resulting in faster overall inference.\nExperiments show that FLASH significantly outperforms prior speculative\ndecoding approaches in both unimodal and multimodal settings, achieving up to\n\\textbf{2.68$\\times$} speed-up on video captioning and \\textbf{2.55$\\times$} on\nvisual instruction tuning tasks compared to the original LMM."}
{"id": "2505.12742", "pdf": "https://arxiv.org/pdf/2505.12742", "abs": "https://arxiv.org/abs/2505.12742", "authors": ["Jinhua Zhang", "Wei Long", "Minghao Han", "Weiyi You", "Shuhang Gu"], "title": "MVAR: Visual Autoregressive Modeling with Scale and Spatial Markovian Conditioning", "categories": ["cs.CV"], "comment": null, "summary": "Essential to visual generation is efficient modeling of visual data priors.\nConventional next-token prediction methods define the process as learning the\nconditional probability distribution of successive tokens. Recently, next-scale\nprediction methods redefine the process to learn the distribution over\nmulti-scale representations, significantly reducing generation latency.\nHowever, these methods condition each scale on all previous scales and require\neach token to consider all preceding tokens, exhibiting scale and spatial\nredundancy. To better model the distribution by mitigating redundancy, we\npropose Markovian Visual AutoRegressive modeling (MVAR), a novel autoregressive\nframework that introduces scale and spatial Markov assumptions to reduce the\ncomplexity of conditional probability modeling. Specifically, we introduce a\nscale-Markov trajectory that only takes as input the features of adjacent\npreceding scale for next-scale prediction, enabling the adoption of a parallel\ntraining strategy that significantly reduces GPU memory consumption.\nFurthermore, we propose spatial-Markov attention, which restricts the attention\nof each token to a localized neighborhood of size k at corresponding positions\non adjacent scales, rather than attending to every token across these scales,\nfor the pursuit of reduced modeling complexity. Building on these improvements,\nwe reduce the computational complexity of attention calculation from O(N^2) to\nO(Nk), enabling training with just eight NVIDIA RTX 4090 GPUs and eliminating\nthe need for KV cache during inference. Extensive experiments on ImageNet\ndemonstrate that MVAR achieves comparable or superior performance with both\nsmall model trained from scratch and large fine-tuned models, while reducing\nthe average GPU memory footprint by 3.0x."}
{"id": "2505.12753", "pdf": "https://arxiv.org/pdf/2505.12753", "abs": "https://arxiv.org/abs/2505.12753", "authors": ["Martha Teiko Teye", "Ori Maoz", "Matthias Rottmann"], "title": "LiDAR MOT-DETR: A LiDAR-based Two-Stage Transformer for 3D Multiple Object Tracking", "categories": ["cs.CV"], "comment": null, "summary": "Multi-object tracking from LiDAR point clouds presents unique challenges due\nto the sparse and irregular nature of the data, compounded by the need for\ntemporal coherence across frames. Traditional tracking systems often rely on\nhand-crafted features and motion models, which can struggle to maintain\nconsistent object identities in crowded or fast-moving scenes. We present a\nlidar-based two-staged DETR inspired transformer; a smoother and tracker. The\nsmoother stage refines lidar object detections, from any off-the-shelf\ndetector, across a moving temporal window. The tracker stage uses a DETR-based\nattention block to maintain tracks across time by associating tracked objects\nwith the refined detections using the point cloud as context. The model is\ntrained on the datasets nuScenes and KITTI in both online and offline (forward\npeeking) modes demonstrating strong performance across metrics such as\nID-switch and multiple object tracking accuracy (MOTA). The numerical results\nindicate that the online mode outperforms the lidar-only baseline and SOTA\nmodels on the nuScenes dataset, with an aMOTA of 0.722 and an aMOTP of 0.475,\nwhile the offline mode provides an additional 3 pp aMOTP"}
{"id": "2505.12758", "pdf": "https://arxiv.org/pdf/2505.12758", "abs": "https://arxiv.org/abs/2505.12758", "authors": ["Matias Quintana", "Youlong Gu", "Xiucheng Liang", "Yujun Hou", "Koichi Ito", "Yihan Zhu", "Mahmoud Abdelrahman", "Filip Biljecki"], "title": "It's not you, it's me -- Global urban visual perception varies across demographics and personalities", "categories": ["cs.CV", "cs.LG"], "comment": "Under review", "summary": "Understanding people's preferences and needs is crucial for urban planning\ndecisions, yet current approaches often combine them from multi-cultural and\nmulti-city populations, obscuring important demographic differences and risking\namplifying biases. We conducted a large-scale urban visual perception survey of\nstreetscapes worldwide using street view imagery, examining how demographics --\nincluding gender, age, income, education, race and ethnicity, and, for the\nfirst time, personality traits -- shape perceptions among 1,000 participants,\nwith balanced demographics, from five countries and 45 nationalities. This\ndataset, introduced as Street Perception Evaluation Considering Socioeconomics\n(SPECS), exhibits statistically significant differences in perception scores in\nsix traditionally used indicators (safe, lively, wealthy, beautiful, boring,\nand depressing) and four new ones we propose (live nearby, walk, cycle, green)\namong demographics and personalities. We revealed that location-based\nsentiments are carried over in people's preferences when comparing urban\nstreetscapes with other cities. Further, we compared the perception scores\nbased on where participants and streetscapes are from. We found that an\noff-the-shelf machine learning model trained on an existing global perception\ndataset tends to overestimate positive indicators and underestimate negative\nones compared to human responses, suggesting that targeted intervention should\nconsider locals' perception. Our study aspires to rectify the myopic treatment\nof street perception, which rarely considers demographics or personality\ntraits."}
{"id": "2505.12766", "pdf": "https://arxiv.org/pdf/2505.12766", "abs": "https://arxiv.org/abs/2505.12766", "authors": ["Haibin He", "Maoyuan Ye", "Jing Zhang", "Xiantao Cai", "Juhua Liu", "Bo Du", "Dacheng Tao"], "title": "Reasoning-OCR: Can Large Multimodal Models Solve Complex Logical Reasoning Problems from OCR Cues?", "categories": ["cs.CV"], "comment": null, "summary": "Large Multimodal Models (LMMs) have become increasingly versatile,\naccompanied by impressive Optical Character Recognition (OCR) related\ncapabilities. Existing OCR-related benchmarks emphasize evaluating LMMs'\nabilities of relatively simple visual question answering, visual-text parsing,\netc. However, the extent to which LMMs can deal with complex logical reasoning\nproblems based on OCR cues is relatively unexplored. To this end, we introduce\nthe Reasoning-OCR benchmark, which challenges LMMs to solve complex reasoning\nproblems based on the cues that can be extracted from rich visual-text.\nReasoning-OCR covers six visual scenarios and encompasses 150 meticulously\ndesigned questions categorized into six reasoning challenges. Additionally,\nReasoning-OCR minimizes the impact of field-specialized knowledge. Our\nevaluation offers some insights for proprietary and open-source LMMs in\ndifferent reasoning challenges, underscoring the urgent to improve the\nreasoning performance. We hope Reasoning-OCR can inspire and facilitate future\nresearch on enhancing complex reasoning ability based on OCR cues.\nReasoning-OCR is publicly available at\nhttps://github.com/Hxyz-123/ReasoningOCR."}
{"id": "2505.12772", "pdf": "https://arxiv.org/pdf/2505.12772", "abs": "https://arxiv.org/abs/2505.12772", "authors": ["Junyi Hu", "Tian Bai", "Fengyi Wu", "Zhengming Peng", "Yi Zhang"], "title": "Pyramid Sparse Transformer: Enhancing Multi-Scale Feature Fusion with Dynamic Token Selection", "categories": ["cs.CV"], "comment": "13 pages, 5 figures", "summary": "Feature fusion is critical for high-performance vision models but often\nincurs prohibitive complexity. However, prevailing attention-based fusion\nmethods often involve significant computational complexity and implementation\nchallenges, limiting their efficiency in resource-constrained environments. To\naddress these issues, we introduce the Pyramid Sparse Transformer (PST), a\nlightweight, plug-and-play module that integrates coarse-to-fine token\nselection and shared attention parameters to reduce computation while\npreserving spatial detail. PST can be trained using only coarse attention and\nseamlessly activated at inference for further accuracy gains without\nretraining. When added to state-of-the-art real-time detection models, such as\nYOLOv11-N/S/M, PST yields mAP improvements of 0.9%, 0.5%, and 0.4% on MS COCO\nwith minimal latency impact. Likewise, embedding PST into ResNet-18/50/101 as\nbackbones, boosts ImageNet top-1 accuracy by 6.5%, 1.7%, and 1.0%,\nrespectively. These results demonstrate PST's effectiveness as a simple,\nhardware-friendly enhancement for both detection and classification tasks."}
{"id": "2505.12789", "pdf": "https://arxiv.org/pdf/2505.12789", "abs": "https://arxiv.org/abs/2505.12789", "authors": ["Hemanth Saratchandran", "Simon Lucey"], "title": "Enhancing Transformers Through Conditioned Embedded Tokens", "categories": ["cs.CV"], "comment": null, "summary": "Transformers have transformed modern machine learning, driving breakthroughs\nin computer vision, natural language processing, and robotics. At the core of\ntheir success lies the attention mechanism, which enables the modeling of\nglobal dependencies among input tokens. However, we reveal that the attention\nblock in transformers suffers from inherent ill-conditioning, which hampers\ngradient-based optimization and leads to inefficient training. To address this,\nwe develop a theoretical framework that establishes a direct relationship\nbetween the conditioning of the attention block and that of the embedded\ntokenized data. Building on this insight, we introduce conditioned embedded\ntokens, a method that systematically modifies the embedded tokens to improve\nthe conditioning of the attention mechanism. Our analysis demonstrates that\nthis approach significantly mitigates ill-conditioning, leading to more stable\nand efficient training. We validate our methodology across various transformer\narchitectures, achieving consistent improvements in image classification,\nobject detection, instance segmentation, and natural language processing,\nhighlighting its broad applicability and effectiveness."}
{"id": "2505.12803", "pdf": "https://arxiv.org/pdf/2505.12803", "abs": "https://arxiv.org/abs/2505.12803", "authors": ["Jiawen Xu", "Odej Kao", "Margret Keuper"], "title": "Informed Mixing -- Improving Open Set Recognition via Attribution-based Augmentation", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Open set recognition (OSR) is devised to address the problem of detecting\nnovel classes during model inference. Even in recent vision models, this\nremains an open issue which is receiving increasing attention. Thereby, a\ncrucial challenge is to learn features that are relevant for unseen categories\nfrom given data, for which these features might not be discriminative. To\nfacilitate this process and \"optimize to learn\" more diverse features, we\npropose GradMix, a data augmentation method that dynamically leverages\ngradient-based attribution maps of the model during training to mask out\nalready learned concepts. Thus GradMix encourages the model to learn a more\ncomplete set of representative features from the same data source. Extensive\nexperiments on open set recognition, close set classification, and\nout-of-distribution detection reveal that our method can often outperform the\nstate-of-the-art. GradMix can further increase model robustness to corruptions\nas well as downstream classification performance for self-supervised learning,\nindicating its benefit for model generalization."}
{"id": "2505.12820", "pdf": "https://arxiv.org/pdf/2505.12820", "abs": "https://arxiv.org/abs/2505.12820", "authors": ["Hulin Li"], "title": "Rethinking Features-Fused-Pyramid-Neck for Object Detection", "categories": ["cs.CV"], "comment": "ECCV 2024", "summary": "Multi-head detectors typically employ a features-fused-pyramid-neck for\nmulti-scale detection and are widely adopted in the industry. However, this\napproach faces feature misalignment when representations from different\nhierarchical levels of the feature pyramid are forcibly fused point-to-point.\nTo address this issue, we designed an independent hierarchy pyramid (IHP)\narchitecture to evaluate the effectiveness of the features-unfused-pyramid-neck\nfor multi-head detectors. Subsequently, we introduced soft nearest neighbor\ninterpolation (SNI) with a weight downscaling factor to mitigate the impact of\nfeature fusion at different hierarchies while preserving key textures.\nFurthermore, we present a features adaptive selection method for down sampling\nin extended spatial windows (ESD) to retain spatial features and enhance\nlightweight convolutional techniques (GSConvE). These advancements culminate in\nour secondary features alignment solution (SA) for real-time detection,\nachieving state-of-the-art results on Pascal VOC and MS COCO. Code will be\nreleased at https://github.com/AlanLi1997/rethinking-fpn. This paper has been\naccepted by ECCV2024 and published on Springer Nature."}
{"id": "2505.12826", "pdf": "https://arxiv.org/pdf/2505.12826", "abs": "https://arxiv.org/abs/2505.12826", "authors": ["Jianfeng Cai", "Wengang Zhou", "Zongmeng Zhang", "Jiale Hong", "Nianji Zhan", "Houqiang Li"], "title": "Mitigating Hallucination in VideoLLMs via Temporal-Aware Activation Engineering", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal large language models (MLLMs) have achieved remarkable progress in\nvideo understanding.However, hallucination, where the model generates plausible\nyet incorrect outputs, persists as a significant and under-addressed challenge\nin the video domain. Among existing solutions, activation engineering has\nproven successful in mitigating hallucinations in LLMs and ImageLLMs, yet its\napplicability to VideoLLMs remains largely unexplored. In this work, we are the\nfirst to systematically investigate the effectiveness and underlying mechanisms\nof activation engineering for mitigating hallucinations in VideoLLMs. We\ninitially conduct an investigation of the key factors affecting the performance\nof activation engineering and find that a model's sensitivity to hallucination\ndepends on $\\textbf{temporal variation}$ rather than task type. Moreover,\nselecting appropriate internal modules and dataset for activation engineering\nis critical for reducing hallucination. Guided by these findings, we propose a\ntemporal-aware activation engineering framework for VideoLLMs, which adaptively\nidentifies and manipulates hallucination-sensitive modules based on the\ntemporal variation characteristic, substantially mitigating hallucinations\nwithout additional LLM fine-tuning. Experiments across multiple models and\nbenchmarks demonstrate that our method markedly reduces hallucination in\nVideoLLMs, thereby validating the robustness of our findings."}
{"id": "2505.12834", "pdf": "https://arxiv.org/pdf/2505.12834", "abs": "https://arxiv.org/abs/2505.12834", "authors": ["Avinash Kumar", "Kyeolhee Kang", "Ammar ul Hassan", "Jaeyoung Choi"], "title": "A Study on the Refining Handwritten Font by Mixing Font Styles", "categories": ["cs.CV"], "comment": "4 pages, 3 figures, MITA 2023 (The 19th International Conference on\n  Multimedia Information Technology and Applications July. 11 ~ July 14, 2023,\n  Technical University of Ostrava, Ostrava, Czech)", "summary": "Handwritten fonts have a distinct expressive character, but they are often\ndifficult to read due to unclear or inconsistent handwriting. FontFusionGAN\n(FFGAN) is a novel method for improving handwritten fonts by combining them\nwith printed fonts. Our method implements generative adversarial network (GAN)\nto generate font that mix the desirable features of handwritten and printed\nfonts. By training the GAN on a dataset of handwritten and printed fonts, it\ncan generate legible and visually appealing font images. We apply our method to\na dataset of handwritten fonts and demonstrate that it significantly enhances\nthe readability of the original fonts while preserving their unique aesthetic.\nOur method has the potential to improve the readability of handwritten fonts,\nwhich would be helpful for a variety of applications including document\ncreation, letter writing, and assisting individuals with reading and writing\ndifficulties. In addition to addressing the difficulties of font creation for\nlanguages with complex character sets, our method is applicable to other\ntext-image-related tasks, such as font attribute control and multilingual font\nstyle transfer."}
{"id": "2505.12849", "pdf": "https://arxiv.org/pdf/2505.12849", "abs": "https://arxiv.org/abs/2505.12849", "authors": ["Ben Liu", "Zhen Qin"], "title": "Accelerate TarFlow Sampling with GS-Jacobi Iteration", "categories": ["cs.CV"], "comment": "17 pages, 7 figures, 5 tables", "summary": "Image generation models have achieved widespread applications. As an\ninstance, the TarFlow model combines the transformer architecture with\nNormalizing Flow models, achieving state-of-the-art results on multiple\nbenchmarks. However, due to the causal form of attention requiring sequential\ncomputation, TarFlow's sampling process is extremely slow. In this paper, we\ndemonstrate that through a series of optimization strategies, TarFlow sampling\ncan be greatly accelerated by using the Gauss-Seidel-Jacobi (abbreviated as\nGS-Jacobi) iteration method. Specifically, we find that blocks in the TarFlow\nmodel have varying importance: a small number of blocks play a major role in\nimage generation tasks, while other blocks contribute relatively little; some\nblocks are sensitive to initial values and prone to numerical overflow, while\nothers are relatively robust. Based on these two characteristics, we propose\nthe Convergence Ranking Metric (CRM) and the Initial Guessing Metric (IGM): CRM\nis used to identify whether a TarFlow block is \"simple\" (converges in few\niterations) or \"tough\" (requires more iterations); IGM is used to evaluate\nwhether the initial value of the iteration is good. Experiments on four TarFlow\nmodels demonstrate that GS-Jacobi sampling can significantly enhance sampling\nefficiency while maintaining the quality of generated images (measured by FID),\nachieving speed-ups of 4.53x in Img128cond, 5.32x in AFHQ, 2.96x in\nImg64uncond, and 2.51x in Img64cond without degrading FID scores or sample\nquality. Code and checkpoints are accessible on\nhttps://github.com/encoreus/GS-Jacobi_for_TarFlow"}
{"id": "2505.12854", "pdf": "https://arxiv.org/pdf/2505.12854", "abs": "https://arxiv.org/abs/2505.12854", "authors": ["Anna Maschek", "David C. Schedl"], "title": "The Way Up: A Dataset for Hold Usage Detection in Sport Climbing", "categories": ["cs.CV"], "comment": "accepted at the International Workshop on Computer Vision in Sports\n  (CVsports) at CVPR 2025", "summary": "Detecting an athlete's position on a route and identifying hold usage are\ncrucial in various climbing-related applications. However, no climbing dataset\nwith detailed hold usage annotations exists to our knowledge. To address this\nissue, we introduce a dataset of 22 annotated climbing videos, providing\nground-truth labels for hold locations, usage order, and time of use.\nFurthermore, we explore the application of keypoint-based 2D pose-estimation\nmodels for detecting hold usage in sport climbing. We determine usage by\nanalyzing the key points of certain joints and the corresponding overlap with\nclimbing holds. We evaluate multiple state-of-the-art models and analyze their\naccuracy on our dataset, identifying and highlighting climbing-specific\nchallenges. Our dataset and results highlight key challenges in\nclimbing-specific pose estimation and establish a foundation for future\nresearch toward AI-assisted systems for sports climbing."}
{"id": "2505.12860", "pdf": "https://arxiv.org/pdf/2505.12860", "abs": "https://arxiv.org/abs/2505.12860", "authors": ["Wenbo Yang", "Zhongling Wang", "Zhou Wang"], "title": "Towards a Universal Image Degradation Model via Content-Degradation Disentanglement", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Image degradation synthesis is highly desirable in a wide variety of\napplications ranging from image restoration to simulating artistic effects.\nExisting models are designed to generate one specific or a narrow set of\ndegradations, which often require user-provided degradation parameters. As a\nresult, they lack the generalizability to synthesize degradations beyond their\ninitial design or adapt to other applications. Here we propose the first\nuniversal degradation model that can synthesize a broad spectrum of complex and\nrealistic degradations containing both homogeneous (global) and inhomogeneous\n(spatially varying) components. Our model automatically extracts and\ndisentangles homogeneous and inhomogeneous degradation features, which are\nlater used for degradation synthesis without user intervention. A\ndisentangle-by-compression method is proposed to separate degradation\ninformation from images. Two novel modules for extracting and incorporating\ninhomogeneous degradations are created to model inhomogeneous components in\ncomplex degradations. We demonstrate the model's accuracy and adaptability in\nfilm-grain simulation and blind image restoration tasks. The demo video, code,\nand dataset of this project will be released upon publication at\ngithub.com/yangwenbo99/content-degradation-disentanglement."}
{"id": "2505.12861", "pdf": "https://arxiv.org/pdf/2505.12861", "abs": "https://arxiv.org/abs/2505.12861", "authors": ["Jiaqi Tan", "Xu Zheng", "Yang Liu"], "title": "Robust Multimodal Segmentation with Representation Regularization and Hybrid Prototype Distillation", "categories": ["cs.CV"], "comment": null, "summary": "Multi-modal semantic segmentation (MMSS) faces significant challenges in\nreal-world scenarios due to dynamic environments, sensor failures, and noise\ninterference, creating a gap between theoretical models and practical\nperformance. To address this, we propose a two-stage framework called\nRobustSeg, which enhances multi-modal robustness through two key components:\nthe Hybrid Prototype Distillation Module (HPDM) and the Representation\nRegularization Module (RRM). In the first stage, RobustSeg pre-trains a\nmulti-modal teacher model using complete modalities. In the second stage, a\nstudent model is trained with random modality dropout while learning from the\nteacher via HPDM and RRM. HPDM transforms features into compact prototypes,\nenabling cross-modal hybrid knowledge distillation and mitigating bias from\nmissing modalities. RRM reduces representation discrepancies between the\nteacher and student by optimizing functional entropy through the log-Sobolev\ninequality. Extensive experiments on three public benchmarks demonstrate that\nRobustSeg outperforms previous state-of-the-art methods, achieving improvements\nof +2.76%, +4.56%, and +0.98%, respectively. Code is available at:\nhttps://github.com/RobustSeg/RobustSeg."}
{"id": "2505.12890", "pdf": "https://arxiv.org/pdf/2505.12890", "abs": "https://arxiv.org/abs/2505.12890", "authors": ["Ege zsoy", "Chantal Pellegrini", "David Bani-Harouni", "Kun Yuan", "Matthias Keicher", "Nassir Navab"], "title": "ORQA: A Benchmark and Foundation Model for Holistic Operating Room Modeling", "categories": ["cs.CV"], "comment": null, "summary": "The real-world complexity of surgeries necessitates surgeons to have deep and\nholistic comprehension to ensure precision, safety, and effective\ninterventions. Computational systems are required to have a similar level of\ncomprehension within the operating room. Prior works, limited to single-task\nefforts like phase recognition or scene graph generation, lack scope and\ngeneralizability. In this work, we introduce ORQA, a novel OR question\nanswering benchmark and foundational multimodal model to advance OR\nintelligence. By unifying all four public OR datasets into a comprehensive\nbenchmark, we enable our approach to concurrently address a diverse range of OR\nchallenges. The proposed multimodal large language model fuses diverse OR\nsignals such as visual, auditory, and structured data, for a holistic modeling\nof the OR. Finally, we propose a novel, progressive knowledge distillation\nparadigm, to generate a family of models optimized for different speed and\nmemory requirements. We show the strong performance of ORQA on our proposed\nbenchmark, and its zero-shot generalization, paving the way for scalable,\nunified OR modeling and significantly advancing multimodal surgical\nintelligence. We will release our code and data upon acceptance."}
{"id": "2505.12897", "pdf": "https://arxiv.org/pdf/2505.12897", "abs": "https://arxiv.org/abs/2505.12897", "authors": ["Piotr Borycki", "Magdalena Trdowicz", "Szymon Janusz", "Jacek Tabor", "Przemysaw Spurek", "Arkadiusz Lewicki", "ukasz Struski"], "title": "EPIC: Explanation of Pretrained Image Classification Networks via Prototype", "categories": ["cs.CV"], "comment": null, "summary": "Explainable AI (XAI) methods generally fall into two categories. Post-hoc\napproaches generate explanations for pre-trained models and are compatible with\nvarious neural network architectures. These methods often use feature\nimportance visualizations, such as saliency maps, to indicate which input\nregions influenced the model's prediction. Unfortunately, they typically offer\na coarse understanding of the model's decision-making process. In contrast,\nante-hoc (inherently explainable) methods rely on specially designed model\narchitectures trained from scratch. A notable subclass of these methods\nprovides explanations through prototypes, representative patches extracted from\nthe training data. However, prototype-based approaches have limitations: they\nrequire dedicated architectures, involve specialized training procedures, and\nperform well only on specific datasets. In this work, we propose EPIC\n(Explanation of Pretrained Image Classification), a novel approach that bridges\nthe gap between these two paradigms. Like post-hoc methods, EPIC operates on\npre-trained models without architectural modifications. Simultaneously, it\ndelivers intuitive, prototype-based explanations inspired by ante-hoc\ntechniques. To the best of our knowledge, EPIC is the first post-hoc method\ncapable of fully replicating the core explanatory power of inherently\ninterpretable models. We evaluate EPIC on benchmark datasets commonly used in\nprototype-based explanations, such as CUB-200-2011 and Stanford Cars, alongside\nlarge-scale datasets like ImageNet, typically employed by post-hoc methods.\nEPIC uses prototypes to explain model decisions, providing a flexible and\neasy-to-understand tool for creating clear, high-quality explanations."}
{"id": "2505.12903", "pdf": "https://arxiv.org/pdf/2505.12903", "abs": "https://arxiv.org/abs/2505.12903", "authors": ["Shiao Wang", "Xiao Wang", "Liye Jin", "Bo Jiang", "Lin Zhu", "Lan Chen", "Yonghong Tian", "Bin Luo"], "title": "Towards Low-Latency Event Stream-based Visual Object Tracking: A Slow-Fast Approach", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Existing tracking algorithms typically rely on low-frame-rate RGB cameras\ncoupled with computationally intensive deep neural network architectures to\nachieve effective tracking. However, such frame-based methods inherently face\nchallenges in achieving low-latency performance and often fail in\nresource-constrained environments. Visual object tracking using bio-inspired\nevent cameras has emerged as a promising research direction in recent years,\noffering distinct advantages for low-latency applications. In this paper, we\npropose a novel Slow-Fast Tracking paradigm that flexibly adapts to different\noperational requirements, termed SFTrack. The proposed framework supports two\ncomplementary modes, i.e., a high-precision slow tracker for scenarios with\nsufficient computational resources, and an efficient fast tracker tailored for\nlatency-aware, resource-constrained environments. Specifically, our framework\nfirst performs graph-based representation learning from\nhigh-temporal-resolution event streams, and then integrates the learned\ngraph-structured information into two FlashAttention-based vision backbones,\nyielding the slow and fast trackers, respectively. The fast tracker achieves\nlow latency through a lightweight network design and by producing multiple\nbounding box outputs in a single forward pass. Finally, we seamlessly combine\nboth trackers via supervised fine-tuning and further enhance the fast tracker's\nperformance through a knowledge distillation strategy. Extensive experiments on\npublic benchmarks, including FE240, COESOT, and EventVOT, demonstrate the\neffectiveness and efficiency of our proposed method across different real-world\nscenarios. The source code has been released on\nhttps://github.com/Event-AHU/SlowFast_Event_Track."}
{"id": "2505.12908", "pdf": "https://arxiv.org/pdf/2505.12908", "abs": "https://arxiv.org/abs/2505.12908", "authors": ["Xiao Wang", "Yu Jin", "Lan Chen", "Bo Jiang", "Lin Zhu", "Yonghong Tian", "Jin Tang", "Bin Luo"], "title": "Dynamic Graph Induced Contour-aware Heat Conduction Network for Event-based Object Detection", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Event-based Vision Sensors (EVS) have demonstrated significant advantages\nover traditional RGB frame-based cameras in low-light conditions, high-speed\nmotion capture, and low latency. Consequently, object detection based on EVS\nhas attracted increasing attention from researchers. Current event stream\nobject detection algorithms are typically built upon Convolutional Neural\nNetworks (CNNs) or Transformers, which either capture limited local features\nusing convolutional filters or incur high computational costs due to the\nutilization of self-attention. Recently proposed vision heat conduction\nbackbone networks have shown a good balance between efficiency and accuracy;\nhowever, these models are not specifically designed for event stream data. They\nexhibit weak capability in modeling object contour information and fail to\nexploit the benefits of multi-scale features. To address these issues, this\npaper proposes a novel dynamic graph induced contour-aware heat conduction\nnetwork for event stream based object detection, termed CvHeat-DET. The\nproposed model effectively leverages the clear contour information inherent in\nevent streams to predict the thermal diffusivity coefficients within the heat\nconduction model, and integrates hierarchical structural graph features to\nenhance feature learning across multiple scales. Extensive experiments on three\nbenchmark datasets for event stream-based object detection fully validated the\neffectiveness of the proposed model. The source code of this paper will be\nreleased on https://github.com/Event-AHU/OpenEvDET."}
{"id": "2505.12911", "pdf": "https://arxiv.org/pdf/2505.12911", "abs": "https://arxiv.org/abs/2505.12911", "authors": ["Simone Alberto Peirone", "Francesca Pistilli", "Giuseppe Averta"], "title": "HiERO: understanding the hierarchy of human behavior enhances reasoning on egocentric videos", "categories": ["cs.CV"], "comment": "Project page https://github.com/sapeirone/hiero", "summary": "Human activities are particularly complex and variable, and this makes\nchallenging for deep learning models to reason about them. However, we note\nthat such variability does have an underlying structure, composed of a\nhierarchy of patterns of related actions. We argue that such structure can\nemerge naturally from unscripted videos of human activities, and can be\nleveraged to better reason about their content. We present HiERO, a\nweakly-supervised method to enrich video segments features with the\ncorresponding hierarchical activity threads. By aligning video clips with their\nnarrated descriptions, HiERO infers contextual, semantic and temporal reasoning\nwith an hierarchical architecture. We prove the potential of our enriched\nfeatures with multiple video-text alignment benchmarks (EgoMCQ, EgoNLQ) with\nminimal additional training, and in zero-shot for procedure learning tasks\n(EgoProceL and Ego4D Goal-Step). Notably, HiERO achieves state-of-the-art\nperformance in all the benchmarks, and for procedure learning tasks it\noutperforms fully-supervised methods by a large margin (+12.5% F1 on EgoProceL)\nin zero shot. Our results prove the relevance of using knowledge of the\nhierarchy of human activities for multiple reasoning tasks in egocentric\nvision."}
{"id": "2505.12912", "pdf": "https://arxiv.org/pdf/2505.12912", "abs": "https://arxiv.org/abs/2505.12912", "authors": ["Kazuki Adachi", "Shin'ya Yamaguchi", "Tomoki Hamagami"], "title": "Uniformity First: Uniformity-aware Test-time Adaptation of Vision-language Models against Image Corruption", "categories": ["cs.CV"], "comment": "Code is available at https://github.com/kzkadc/uninfo", "summary": "Pre-trained vision-language models such as contrastive language-image\npre-training (CLIP) have demonstrated a remarkable generalizability, which has\nenabled a wide range of applications represented by zero-shot classification.\nHowever, vision-language models still suffer when they face datasets with large\ngaps from training ones, i.e., distribution shifts. We found that CLIP is\nespecially vulnerable to sensor degradation, a type of realistic distribution\nshift caused by sensor conditions such as weather, light, or noise. Collecting\na new dataset from a test distribution for fine-tuning highly costs since\nsensor degradation occurs unexpectedly and has a range of variety. Thus, we\ninvestigate test-time adaptation (TTA) of zero-shot classification, which\nenables on-the-fly adaptation to the test distribution with unlabeled test\ndata. Existing TTA methods for CLIP mainly focus on modifying image and text\nembeddings or predictions to address distribution shifts. Although these\nmethods can adapt to domain shifts, such as fine-grained labels spaces or\ndifferent renditions in input images, they fail to adapt to distribution shifts\ncaused by sensor degradation. We found that this is because image embeddings\nare \"corrupted\" in terms of uniformity, a measure related to the amount of\ninformation. To make models robust to sensor degradation, we propose a novel\nmethod called uniformity-aware information-balanced TTA (UnInfo). To address\nthe corruption of image embeddings, we introduce uniformity-aware confidence\nmaximization, information-aware loss balancing, and knowledge distillation from\nthe exponential moving average (EMA) teacher. Through experiments, we\ndemonstrate that our UnInfo improves accuracy under sensor degradation by\nretaining information in terms of uniformity."}
{"id": "2505.12935", "pdf": "https://arxiv.org/pdf/2505.12935", "abs": "https://arxiv.org/abs/2505.12935", "authors": ["Di You", "Daniel Siromani", "Pier Luigi Dragotti"], "title": "LatentINDIGO: An INN-Guided Latent Diffusion Algorithm for Image Restoration", "categories": ["cs.CV"], "comment": "Submitted to IEEE Transactions on Image Processing (TIP)", "summary": "There is a growing interest in the use of latent diffusion models (LDMs) for\nimage restoration (IR) tasks due to their ability to model effectively the\ndistribution of natural images. While significant progress has been made, there\nare still key challenges that need to be addressed. First, many approaches\ndepend on a predefined degradation operator, making them ill-suited for complex\nor unknown degradations that deviate from standard analytical models. Second,\nmany methods struggle to provide a stable guidance in the latent space and\nfinally most methods convert latent representations back to the pixel domain\nfor guidance at every sampling iteration, which significantly increases\ncomputational and memory overhead. To overcome these limitations, we introduce\na wavelet-inspired invertible neural network (INN) that simulates degradations\nthrough a forward transform and reconstructs lost details via the inverse\ntransform. We further integrate this design into a latent diffusion pipeline\nthrough two proposed approaches: LatentINDIGO-PixelINN, which operates in the\npixel domain, and LatentINDIGO-LatentINN, which stays fully in the latent space\nto reduce complexity. Both approaches alternate between updating intermediate\nlatent variables under the guidance of our INN and refining the INN forward\nmodel to handle unknown degradations. In addition, a regularization step\npreserves the proximity of latent variables to the natural image manifold.\nExperiments demonstrate that our algorithm achieves state-of-the-art\nperformance on synthetic and real-world low-quality images, and can be readily\nadapted to arbitrary output sizes."}
{"id": "2505.12966", "pdf": "https://arxiv.org/pdf/2505.12966", "abs": "https://arxiv.org/abs/2505.12966", "authors": ["Zihan Xiong", "Xiaohua Wu", "Lei Chen", "Fangqi Lou"], "title": "Multiscale Adaptive Conflict-Balancing Model For Multimedia Deepfake Detection", "categories": ["cs.CV", "cs.AI"], "comment": "9 pages,ICMR accepted", "summary": "Advances in computer vision and deep learning have blurred the line between\ndeepfakes and authentic media, undermining multimedia credibility through\naudio-visual forgery. Current multimodal detection methods remain limited by\nunbalanced learning between modalities. To tackle this issue, we propose an\nAudio-Visual Joint Learning Method (MACB-DF) to better mitigate modality\nconflicts and neglect by leveraging contrastive learning to assist in\nmulti-level and cross-modal fusion, thereby fully balancing and exploiting\ninformation from each modality. Additionally, we designed an\northogonalization-multimodal pareto module that preserves unimodal information\nwhile addressing gradient conflicts in audio-video encoders caused by differing\noptimization targets of the loss functions. Extensive experiments and ablation\nstudies conducted on mainstream deepfake datasets demonstrate consistent\nperformance gains of our model across key evaluation metrics, achieving an\naverage accuracy of 95.5% across multiple datasets. Notably, our method\nexhibits superior cross-dataset generalization capabilities, with absolute\nimprovements of 8.0% and 7.7% in ACC scores over the previous best-performing\napproach when trained on DFDC and tested on DefakeAVMiT and FakeAVCeleb\ndatasets."}
{"id": "2505.12998", "pdf": "https://arxiv.org/pdf/2505.12998", "abs": "https://arxiv.org/abs/2505.12998", "authors": ["Vinkle Srivastav", "Juliette Puel", "Jonathan Vappou", "Elijah Van Houten", "Paolo Cabras", "Nicolas Padoy"], "title": "A Skull-Adaptive Framework for AI-Based 3D Transcranial Focused Ultrasound Simulation", "categories": ["cs.CV"], "comment": "The project page is available at\n  https://github.com/CAMMA-public/TFUScapes", "summary": "Transcranial focused ultrasound (tFUS) is an emerging modality for\nnon-invasive brain stimulation and therapeutic intervention, offering\nmillimeter-scale spatial precision and the ability to target deep brain\nstructures. However, the heterogeneous and anisotropic nature of the human\nskull introduces significant distortions to the propagating ultrasound\nwavefront, which require time-consuming patient-specific planning and\ncorrections using numerical solvers for accurate targeting. To enable\ndata-driven approaches in this domain, we introduce TFUScapes, the first\nlarge-scale, high-resolution dataset of tFUS simulations through anatomically\nrealistic human skulls derived from T1-weighted MRI images. We have developed a\nscalable simulation engine pipeline using the k-Wave pseudo-spectral solver,\nwhere each simulation returns a steady-state pressure field generated by a\nfocused ultrasound transducer placed at realistic scalp locations. In addition\nto the dataset, we present DeepTFUS, a deep learning model that estimates\nnormalized pressure fields directly from input 3D CT volumes and transducer\nposition. The model extends a U-Net backbone with transducer-aware\nconditioning, incorporating Fourier-encoded position embeddings and MLP layers\nto create global transducer embeddings. These embeddings are fused with U-Net\nencoder features via feature-wise modulation, dynamic convolutions, and\ncross-attention mechanisms. The model is trained using a combination of\nspatially weighted and gradient-sensitive loss functions, enabling it to\napproximate high-fidelity wavefields. The TFUScapes dataset is publicly\nreleased to accelerate research at the intersection of computational acoustics,\nneurotechnology, and deep learning. The project page is available at\nhttps://github.com/CAMMA-public/TFUScapes."}
{"id": "2505.13023", "pdf": "https://arxiv.org/pdf/2505.13023", "abs": "https://arxiv.org/abs/2505.13023", "authors": ["Yimao Guo", "Zuomin Qu", "Wei Lu", "Xiangyang Luo"], "title": "Anti-Inpainting: A Proactive Defense against Malicious Diffusion-based Inpainters under Unknown Conditions", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": null, "summary": "As diffusion-based malicious image manipulation becomes increasingly\nprevalent, multiple proactive defense methods are developed to safeguard images\nagainst unauthorized tampering. However, most proactive defense methods only\ncan safeguard images against manipulation under known conditions, and fail to\nprotect images from manipulations guided by tampering conditions crafted by\nmalicious users. To tackle this issue, we propose Anti-Inpainting, a proactive\ndefense method that achieves adequate protection under unknown conditions\nthrough a triple mechanism to address this challenge. Specifically, a\nmulti-level deep feature extractor is presented to obtain intricate features\nduring the diffusion denoising process to improve protective effectiveness. We\ndesign multi-scale semantic-preserving data augmentation to enhance the\ntransferability of adversarial perturbations across unknown conditions by\nmulti-scale transformations while preserving semantic integrity. In addition,\nwe propose a selection-based distribution deviation optimization strategy to\nimprove the protection of adversarial perturbation against manipulation under\ndiverse random seeds. Extensive experiments indicate the proactive defensive\nperformance of Anti-Inpainting against diffusion-based inpainters guided by\nunknown conditions in InpaintGuardBench and CelebA-HQ. At the same time, we\nalso demonstrate the proposed approach's robustness under various image\npurification methods and its transferability across different versions of\ndiffusion models."}
{"id": "2505.13039", "pdf": "https://arxiv.org/pdf/2505.13039", "abs": "https://arxiv.org/abs/2505.13039", "authors": ["Xiao Wu", "Xiaoqing Zhang", "Zunjie Xiao", "Lingxi Hu", "Risa Higashita", "Jiang Liu"], "title": "Expert-Like Reparameterization of Heterogeneous Pyramid Receptive Fields in Efficient CNNs for Fair Medical Image Classification", "categories": ["cs.CV"], "comment": null, "summary": "Efficient convolutional neural network (CNN) architecture designs have\nattracted growing research interests. However, they usually apply single\nreceptive field (RF), small asymmetric RFs, or pyramid RFs to learn different\nfeature representations, still encountering two significant challenges in\nmedical image classification tasks: 1) They have limitations in capturing\ndiverse lesion characteristics efficiently, e.g., tiny, coordination, small and\nsalient, which have unique roles on results, especially imbalanced medical\nimage classification. 2) The predictions generated by those CNNs are often\nunfair/biased, bringing a high risk by employing them to real-world medical\ndiagnosis conditions. To tackle these issues, we develop a new concept,\nExpert-Like Reparameterization of Heterogeneous Pyramid Receptive Fields\n(ERoHPRF), to simultaneously boost medical image classification performance and\nfairness. This concept aims to mimic the multi-expert consultation mode by\napplying the well-designed heterogeneous pyramid RF bags to capture different\nlesion characteristics effectively via convolution operations with multiple\nheterogeneous kernel sizes. Additionally, ERoHPRF introduces an expert-like\nstructural reparameterization technique to merge its parameters with the\ntwo-stage strategy, ensuring competitive computation cost and inference speed\nthrough comparisons to a single RF. To manifest the effectiveness and\ngeneralization ability of ERoHPRF, we incorporate it into mainstream efficient\nCNN architectures. The extensive experiments show that our method maintains a\nbetter trade-off than state-of-the-art methods in terms of medical image\nclassification, fairness, and computation overhead. The codes of this paper\nwill be released soon."}
{"id": "2505.13043", "pdf": "https://arxiv.org/pdf/2505.13043", "abs": "https://arxiv.org/abs/2505.13043", "authors": ["Hao-Ran Yang", "Xiaohui Chen", "Chuan-Xian Ren"], "title": "A Generalized Label Shift Perspective for Cross-Domain Gaze Estimation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Aiming to generalize the well-trained gaze estimation model to new target\ndomains, Cross-domain Gaze Estimation (CDGE) is developed for real-world\napplication scenarios. Existing CDGE methods typically extract the\ndomain-invariant features to mitigate domain shift in feature space, which is\nproved insufficient by Generalized Label Shift (GLS) theory. In this paper, we\nintroduce a novel GLS perspective to CDGE and modelize the cross-domain problem\nby label and conditional shift problem. A GLS correction framework is presented\nand a feasible realization is proposed, in which a importance reweighting\nstrategy based on truncated Gaussian distribution is introduced to overcome the\ncontinuity challenges in label shift correction. To embed the reweighted source\ndistribution to conditional invariant learning, we further derive a\nprobability-aware estimation of conditional operator discrepancy. Extensive\nexperiments on standard CDGE tasks with different backbone models validate the\nsuperior generalization capability across domain and applicability on various\nmodels of proposed method."}
{"id": "2505.13050", "pdf": "https://arxiv.org/pdf/2505.13050", "abs": "https://arxiv.org/abs/2505.13050", "authors": ["Beibei Lin", "Zifeng Yuan", "Tingting Chen"], "title": "RGB-to-Polarization Estimation: A New Task and Benchmark Study", "categories": ["cs.CV"], "comment": null, "summary": "Polarization images provide rich physical information that is fundamentally\nabsent from standard RGB images, benefiting a wide range of computer vision\napplications such as reflection separation and material classification.\nHowever, the acquisition of polarization images typically requires additional\noptical components, which increases both the cost and the complexity of the\napplications. To bridge this gap, we introduce a new task: RGB-to-polarization\nimage estimation, which aims to infer polarization information directly from\nRGB images. In this work, we establish the first comprehensive benchmark for\nthis task by leveraging existing polarization datasets and evaluating a diverse\nset of state-of-the-art deep learning models, including both\nrestoration-oriented and generative architectures. Through extensive\nquantitative and qualitative analysis, our benchmark not only establishes the\ncurrent performance ceiling of RGB-to-polarization estimation, but also\nsystematically reveals the respective strengths and limitations of different\nmodel families -- such as direct reconstruction versus generative synthesis,\nand task-specific training versus large-scale pre-training. In addition, we\nprovide some potential directions for future research on polarization\nestimation. This benchmark is intended to serve as a foundational resource to\nfacilitate the design and evaluation of future methods for polarization\nestimation from standard RGB inputs."}
{"id": "2505.13061", "pdf": "https://arxiv.org/pdf/2505.13061", "abs": "https://arxiv.org/abs/2505.13061", "authors": ["CHengtang Yao", "Zhidan Liu", "Jiaxi Zeng", "Lidong Yu", "Yuwei Wu", "Yunde Jia"], "title": "3D Visual Illusion Depth Estimation", "categories": ["cs.CV"], "comment": "Project:\n  https://github.com/YaoChengTang/3D-Visual-Illusion-Depth-Estimation", "summary": "3D visual illusion is a perceptual phenomenon where a two-dimensional plane\nis manipulated to simulate three-dimensional spatial relationships, making a\nflat artwork or object look three-dimensional in the human visual system. In\nthis paper, we reveal that the machine visual system is also seriously fooled\nby 3D visual illusions, including monocular and binocular depth estimation. In\norder to explore and analyze the impact of 3D visual illusion on depth\nestimation, we collect a large dataset containing almost 3k scenes and 200k\nimages to train and evaluate SOTA monocular and binocular depth estimation\nmethods. We also propose a robust depth estimation framework that uses common\nsense from a vision-language model to adaptively select reliable depth from\nbinocular disparity and monocular depth. Experiments show that SOTA monocular,\nbinocular, and multi-view depth estimation approaches are all fooled by various\n3D visual illusions, while our method achieves SOTA performance."}
{"id": "2505.13088", "pdf": "https://arxiv.org/pdf/2505.13088", "abs": "https://arxiv.org/abs/2505.13088", "authors": ["Zhaoyi Wang", "Shengyu Huang", "Jemil Avers Butt", "Yuanzhou Cai", "Matej Varga", "Andreas Wieser"], "title": "Cross-modal feature fusion for robust point cloud registration with ambiguous geometry", "categories": ["cs.CV", "cs.LG"], "comment": "To appear in the ISPRS Journal of Photogrammetry and Remote Sensing.\n  19 pages, 14 figures", "summary": "Point cloud registration has seen significant advancements with the\napplication of deep learning techniques. However, existing approaches often\noverlook the potential of integrating radiometric information from RGB images.\nThis limitation reduces their effectiveness in aligning point clouds pairs,\nespecially in regions where geometric data alone is insufficient. When used\neffectively, radiometric information can enhance the registration process by\nproviding context that is missing from purely geometric data. In this paper, we\npropose CoFF, a novel Cross-modal Feature Fusion method that utilizes both\npoint cloud geometry and RGB images for pairwise point cloud registration.\nAssuming that the co-registration between point clouds and RGB images is\navailable, CoFF explicitly addresses the challenges where geometric information\nalone is unclear, such as in regions with symmetric similarity or planar\nstructures, through a two-stage fusion of 3D point cloud features and 2D image\nfeatures. It incorporates a cross-modal feature fusion module that assigns\npixel-wise image features to 3D input point clouds to enhance learned 3D point\nfeatures, and integrates patch-wise image features with superpoint features to\nimprove the quality of coarse matching. This is followed by a coarse-to-fine\nmatching module that accurately establishes correspondences using the fused\nfeatures. We extensively evaluate CoFF on four common datasets: 3DMatch,\n3DLoMatch, IndoorLRS, and the recently released ScanNet++ datasets. In\naddition, we assess CoFF on specific subset datasets containing geometrically\nambiguous cases. Our experimental results demonstrate that CoFF achieves\nstate-of-the-art registration performance across all benchmarks, including\nremarkable registration recalls of 95.9% and 81.6% on the widely-used 3DMatch\nand 3DLoMatch datasets, respectively...(Truncated to fit arXiv abstract length)"}
{"id": "2505.13091", "pdf": "https://arxiv.org/pdf/2505.13091", "abs": "https://arxiv.org/abs/2505.13091", "authors": ["Yuanbo Wang", "Zhaoxuan Zhang", "Jiajin Qiu", "Dilong Sun", "Zhengyu Meng", "Xiaopeng Wei", "Xin Yang"], "title": "Touch2Shape: Touch-Conditioned 3D Diffusion for Shape Exploration and Reconstruction", "categories": ["cs.CV"], "comment": "10 pages, 6 figures", "summary": "Diffusion models have made breakthroughs in 3D generation tasks. Current 3D\ndiffusion models focus on reconstructing target shape from images or a set of\npartial observations. While excelling in global context understanding, they\nstruggle to capture the local details of complex shapes and limited to the\nocclusion and lighting conditions. To overcome these limitations, we utilize\ntactile images to capture the local 3D information and propose a Touch2Shape\nmodel, which leverages a touch-conditioned diffusion model to explore and\nreconstruct the target shape from touch. For shape reconstruction, we have\ndeveloped a touch embedding module to condition the diffusion model in creating\na compact representation and a touch shape fusion module to refine the\nreconstructed shape. For shape exploration, we combine the diffusion model with\nreinforcement learning to train a policy. This involves using the generated\nlatent vector from the diffusion model to guide the touch exploration policy\ntraining through a novel reward design. Experiments validate the reconstruction\nquality thorough both qualitatively and quantitative analysis, and our touch\nexploration policy further boosts reconstruction performance."}
{"id": "2505.13099", "pdf": "https://arxiv.org/pdf/2505.13099", "abs": "https://arxiv.org/abs/2505.13099", "authors": ["Shinichi Mae", "Ryosuke Yamada", "Hirokatsu Kataoka"], "title": "Industry-focused Synthetic Segmentation Pre-training", "categories": ["cs.CV"], "comment": null, "summary": "Pre-training on real-image datasets has been widely proven effective for\nimproving instance segmentation. However, industrial applications face two key\nchallenges: (1) legal and ethical restrictions, such as ImageNet's prohibition\nof commercial use, and (2) limited transferability due to the domain gap\nbetween web images and industrial imagery. Even recent vision foundation\nmodels, including the segment anything model (SAM), show notable performance\ndegradation in industrial settings. These challenges raise critical questions:\nCan we build a vision foundation model for industrial applications without\nrelying on real images or manual annotations? And can such models outperform\neven fine-tuned SAM on industrial datasets? To address these questions, we\npropose the Instance Core Segmentation Dataset (InsCore), a synthetic\npre-training dataset based on formula-driven supervised learning (FDSL).\nInsCore generates fully annotated instance segmentation images that reflect key\ncharacteristics of industrial data, including complex occlusions, dense\nhierarchical masks, and diverse non-rigid shapes, distinct from typical web\nimagery. Unlike previous methods, InsCore requires neither real images nor\nhuman annotations. Experiments on five industrial datasets show that models\npre-trained with InsCore outperform those trained on COCO and ImageNet-21k, as\nwell as fine-tuned SAM, achieving an average improvement of 6.2 points in\ninstance segmentation performance. This result is achieved using only 100k\nsynthetic images, more than 100 times fewer than the 11 million images in SAM's\nSA-1B dataset, demonstrating the data efficiency of our approach. These\nfindings position InsCore as a practical and license-free vision foundation\nmodel for industrial applications."}
{"id": "2505.13101", "pdf": "https://arxiv.org/pdf/2505.13101", "abs": "https://arxiv.org/abs/2505.13101", "authors": ["Shaowu Wu", "Liting Zeng", "Wei Lu", "Xiangyang Luo"], "title": "ARIW-Framework: Adaptive Robust Iterative Watermarking Framework", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages, 4 figures", "summary": "With the rapid rise of large models, copyright protection for generated image\ncontent has become a critical security challenge. Although deep learning\nwatermarking techniques offer an effective solution for digital image copyright\nprotection, they still face limitations in terms of visual quality, robustness\nand generalization. To address these issues, this paper proposes an adaptive\nrobust iterative watermarking framework (ARIW-Framework) that achieves\nhigh-quality watermarked images while maintaining exceptional robustness and\ngeneralization performance. Specifically, we introduce an iterative approach to\noptimize the encoder for generating robust residuals. The encoder incorporates\nnoise layers and a decoder to compute robustness weights for residuals under\nvarious noise attacks. By employing a parallel optimization strategy, the\nframework enhances robustness against multiple types of noise attacks.\nFurthermore, we leverage image gradients to determine the embedding strength at\neach pixel location, significantly improving the visual quality of the\nwatermarked images. Extensive experiments demonstrate that the proposed method\nachieves superior visual quality while exhibiting remarkable robustness and\ngeneralization against noise attacks."}
{"id": "2505.13123", "pdf": "https://arxiv.org/pdf/2505.13123", "abs": "https://arxiv.org/abs/2505.13123", "authors": ["Snehashis Majhi", "Giacomo D'Amicantonio", "Antitza Dantcheva", "Quan Kong", "Lorenzo Garattoni", "Gianpiero Francesca", "Egor Bondarev", "Francois Bremond"], "title": "Just Dance with $$! A Poly-modal Inductor for Weakly-supervised Video Anomaly Detection", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Weakly-supervised methods for video anomaly detection (VAD) are\nconventionally based merely on RGB spatio-temporal features, which continues to\nlimit their reliability in real-world scenarios. This is due to the fact that\nRGB-features are not sufficiently distinctive in setting apart categories such\nas shoplifting from visually similar events. Therefore, towards robust complex\nreal-world VAD, it is essential to augment RGB spatio-temporal features by\nadditional modalities. Motivated by this, we introduce the Poly-modal Induced\nframework for VAD: \"PI-VAD\", a novel approach that augments RGB representations\nby five additional modalities. Specifically, the modalities include sensitivity\nto fine-grained motion (Pose), three dimensional scene and entity\nrepresentation (Depth), surrounding objects (Panoptic masks), global motion\n(optical flow), as well as language cues (VLM). Each modality represents an\naxis of a polygon, streamlined to add salient cues to RGB. PI-VAD includes two\nplug-in modules, namely Pseudo-modality Generation module and Cross Modal\nInduction module, which generate modality-specific prototypical representation\nand, thereby, induce multi-modal information into RGB cues. These modules\noperate by performing anomaly-aware auxiliary tasks and necessitate five\nmodality backbones -- only during training. Notably, PI-VAD achieves\nstate-of-the-art accuracy on three prominent VAD datasets encompassing\nreal-world scenarios, without requiring the computational overhead of five\nmodality backbones at inference."}
{"id": "2505.13130", "pdf": "https://arxiv.org/pdf/2505.13130", "abs": "https://arxiv.org/abs/2505.13130", "authors": ["Muhammad Awais Amin", "Adama Ilboudo", "Abdul Samad bin Shahid", "Amjad Ali", "Waqas Haider Khan Bangyal"], "title": "Adaptive Image Restoration for Video Surveillance: A Real-Time Approach", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "One of the major challenges in the field of computer vision especially for\ndetection, segmentation, recognition, monitoring, and automated solutions, is\nthe quality of images. Image degradation, often caused by factors such as rain,\nfog, lighting, etc., has a negative impact on automated\ndecision-making.Furthermore, several image restoration solutions exist,\nincluding restoration models for single degradation and restoration models for\nmultiple degradations. However, these solutions are not suitable for real-time\nprocessing. In this study, the aim was to develop a real-time image restoration\nsolution for video surveillance. To achieve this, using transfer learning with\nResNet_50, we developed a model for automatically identifying the types of\ndegradation present in an image to reference the necessary treatment(s) for\nimage restoration. Our solution has the advantage of being flexible and\nscalable."}
{"id": "2505.13137", "pdf": "https://arxiv.org/pdf/2505.13137", "abs": "https://arxiv.org/abs/2505.13137", "authors": ["Robert-Jan Bruintjes", "Jan van Gemert"], "title": "Learning to Adapt to Position Bias in Vision Transformer Classifiers", "categories": ["cs.CV"], "comment": null, "summary": "How discriminative position information is for image classification depends\non the data. On the one hand, the camera position is arbitrary and objects can\nappear anywhere in the image, arguing for translation invariance. At the same\ntime, position information is key for exploiting capture/center bias, and scene\nlayout, e.g.: the sky is up. We show that position bias, the level to which a\ndataset is more easily solved when positional information on input features is\nused, plays a crucial role in the performance of Vision Transformers image\nclassifiers. To investigate, we propose Position-SHAP, a direct measure of\nposition bias by extending SHAP to work with position embeddings. We show\nvarious levels of position bias in different datasets, and find that the\noptimal choice of position embedding depends on the position bias apparent in\nthe dataset. We therefore propose Auto-PE, a single-parameter position\nembedding extension, which allows the position embedding to modulate its norm,\nenabling the unlearning of position information. Auto-PE combines with existing\nPEs to match or improve accuracy on classification datasets."}
{"id": "2505.13140", "pdf": "https://arxiv.org/pdf/2505.13140", "abs": "https://arxiv.org/abs/2505.13140", "authors": ["Takahiro Maeda", "Jinkun Cao", "Norimichi Ukita", "Kris Kitani"], "title": "CacheFlow: Fast Human Motion Prediction by Cached Normalizing Flow", "categories": ["cs.CV"], "comment": null, "summary": "Many density estimation techniques for 3D human motion prediction require a\nsignificant amount of inference time, often exceeding the duration of the\npredicted time horizon. To address the need for faster density estimation for\n3D human motion prediction, we introduce a novel flow-based method for human\nmotion prediction called CacheFlow. Unlike previous conditional generative\nmodels that suffer from time efficiency, CacheFlow takes advantage of an\nunconditional flow-based generative model that transforms a Gaussian mixture\ninto the density of future motions. The results of the computation of the\nflow-based generative model can be precomputed and cached. Then, for\nconditional prediction, we seek a mapping from historical trajectories to\nsamples in the Gaussian mixture. This mapping can be done by a much more\nlightweight model, thus saving significant computation overhead compared to a\ntypical conditional flow model. In such a two-stage fashion and by caching\nresults from the slow flow model computation, we build our CacheFlow without\nloss of prediction accuracy and model expressiveness. This inference process is\ncompleted in approximately one millisecond, making it 4 times faster than\nprevious VAE methods and 30 times faster than previous diffusion-based methods\non standard benchmarks such as Human3.6M and AMASS datasets. Furthermore, our\nmethod demonstrates improved density estimation accuracy and comparable\nprediction accuracy to a SOTA method on Human3.6M. Our code and models will be\npublicly available."}
{"id": "2505.13174", "pdf": "https://arxiv.org/pdf/2505.13174", "abs": "https://arxiv.org/abs/2505.13174", "authors": ["Alp Eren Sari", "Paolo Favaro"], "title": "FlowCut: Unsupervised Video Instance Segmentation via Temporal Mask Matching", "categories": ["cs.CV"], "comment": null, "summary": "We propose FlowCut, a simple and capable method for unsupervised video\ninstance segmentation consisting of a three-stage framework to construct a\nhigh-quality video dataset with pseudo labels. To our knowledge, our work is\nthe first attempt to curate a video dataset with pseudo-labels for unsupervised\nvideo instance segmentation. In the first stage, we generate pseudo-instance\nmasks by exploiting the affinities of features from both images and optical\nflows. In the second stage, we construct short video segments containing\nhigh-quality, consistent pseudo-instance masks by temporally matching them\nacross the frames. In the third stage, we use the YouTubeVIS-2021 video dataset\nto extract our training instance segmentation set, and then train a video\nsegmentation model. FlowCut achieves state-of-the-art performance on the\nYouTubeVIS-2019, YouTubeVIS-2021, DAVIS-2017, and DAVIS-2017 Motion benchmarks."}
{"id": "2505.13191", "pdf": "https://arxiv.org/pdf/2505.13191", "abs": "https://arxiv.org/abs/2505.13191", "authors": ["Pengcheng Pan", "Yonekura Shogo", "Yasuo Kuniyoshi"], "title": "Emergence of Fixational and Saccadic Movements in a Multi-Level Recurrent Attention Model for Vision", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Inspired by foveal vision, hard attention models promise interpretability and\nparameter economy. However, existing models like the Recurrent Model of Visual\nAttention (RAM) and Deep Recurrent Attention Model (DRAM) failed to model the\nhierarchy of human vision system, that compromise on the visual exploration\ndynamics. As a result, they tend to produce attention that are either overly\nfixational or excessively saccadic, diverging from human eye movement behavior.\nIn this paper, we propose a Multi-Level Recurrent Attention Model (MRAM), a\nnovel hard attention framework that explicitly models the neural hierarchy of\nhuman visual processing. By decoupling the function of glimpse location\ngeneration and task execution in two recurrent layers, MRAM emergent a balanced\nbehavior between fixation and saccadic movement. Our results show that MRAM not\nonly achieves more human-like attention dynamics, but also consistently\noutperforms CNN, RAM and DRAM baselines on standard image classification\nbenchmarks."}
{"id": "2505.13201", "pdf": "https://arxiv.org/pdf/2505.13201", "abs": "https://arxiv.org/abs/2505.13201", "authors": ["Yuzhen Chen", "Hojun Son", "Arpan Kusari"], "title": "MatPredict: a dataset and benchmark for learning material properties of diverse indoor objects", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Determining material properties from camera images can expand the ability to\nidentify complex objects in indoor environments, which is valuable for consumer\nrobotics applications. To support this, we introduce MatPredict, a dataset that\ncombines the high-quality synthetic objects from Replica dataset with MatSynth\ndataset's material properties classes - to create objects with diverse material\nproperties. We select 3D meshes of specific foreground objects and render them\nwith different material properties. In total, we generate \\textbf{18} commonly\noccurring objects with \\textbf{14} different materials. We showcase how we\nprovide variability in terms of lighting and camera placement for these\nobjects. Next, we provide a benchmark for inferring material properties from\nvisual images using these perturbed models in the scene, discussing the\nspecific neural network models involved and their performance based on\ndifferent image comparison metrics. By accurately simulating light interactions\nwith different materials, we can enhance realism, which is crucial for training\nmodels effectively through large-scale simulations. This research aims to\nrevolutionize perception in consumer robotics. The dataset is provided\n\\href{https://huggingface.co/datasets/UMTRI/MatPredict}{here} and the code is\nprovided \\href{https://github.com/arpan-kusari/MatPredict}{here}."}
{"id": "2505.13211", "pdf": "https://arxiv.org/pdf/2505.13211", "abs": "https://arxiv.org/abs/2505.13211", "authors": ["Sand. ai", "Hansi Teng", "Hongyu Jia", "Lei Sun", "Lingzhi Li", "Maolin Li", "Mingqiu Tang", "Shuai Han", "Tianning Zhang", "W. Q. Zhang", "Weifeng Luo", "Xiaoyang Kang", "Yuchen Sun", "Yue Cao", "Yunpeng Huang", "Yutong Lin", "Yuxin Fang", "Zewei Tao", "Zheng Zhang", "Zhongshu Wang", "Zixun Liu", "Dai Shi", "Guoli Su", "Hanwen Sun", "Hong Pan", "Jie Wang", "Jiexin Sheng", "Min Cui", "Min Hu", "Ming Yan", "Shucheng Yin", "Siran Zhang", "Tingting Liu", "Xianping Yin", "Xiaoyu Yang", "Xin Song", "Xuan Hu", "Yankai Zhang", "Yuqiao Li"], "title": "MAGI-1: Autoregressive Video Generation at Scale", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We present MAGI-1, a world model that generates videos by autoregressively\npredicting a sequence of video chunks, defined as fixed-length segments of\nconsecutive frames. Trained to denoise per-chunk noise that increases\nmonotonically over time, MAGI-1 enables causal temporal modeling and naturally\nsupports streaming generation. It achieves strong performance on image-to-video\n(I2V) tasks conditioned on text instructions, providing high temporal\nconsistency and scalability, which are made possible by several algorithmic\ninnovations and a dedicated infrastructure stack. MAGI-1 facilitates\ncontrollable generation via chunk-wise prompting and supports real-time,\nmemory-efficient deployment by maintaining constant peak inference cost,\nregardless of video length. The largest variant of MAGI-1 comprises 24 billion\nparameters and supports context lengths of up to 4 million tokens,\ndemonstrating the scalability and robustness of our approach. The code and\nmodels are available at https://github.com/SandAI-org/MAGI-1 and\nhttps://github.com/SandAI-org/MagiAttention. The product can be accessed at\nhttps://sand.ai."}
{"id": "2505.13212", "pdf": "https://arxiv.org/pdf/2505.13212", "abs": "https://arxiv.org/abs/2505.13212", "authors": ["Qingling Shu", "Sibao Chen", "Zhihui You", "Wei Lu", "Jin Tang", "Bin Luo"], "title": "RB-SCD: A New Benchmark for Semantic Change Detection of Roads and Bridges in Traffic Scenes", "categories": ["cs.CV"], "comment": null, "summary": "Accurate detection of changes in roads and bridges, such as construction,\nrenovation, and demolition, is essential for urban planning and traffic\nmanagement. However, existing methods often struggle to extract fine-grained\nsemantic change information due to the lack of high-quality annotated datasets\nin traffic scenarios. To address this, we introduce the Road and Bridge\nSemantic Change Detection (RB-SCD) dataset, a comprehensive benchmark\ncomprising 260 pairs of high-resolution remote sensing images from diverse\ncities and countries. RB-SCD captures 11 types of semantic changes across\nvaried road and bridge structures, enabling detailed structural and functional\nanalysis. Building on this dataset, we propose a novel framework, Multimodal\nFrequency-Driven Change Detector (MFDCD), which integrates multimodal features\nin the frequency domain. MFDCD includes a Dynamic Frequency Coupler (DFC) that\nfuses hierarchical visual features with wavelet-based frequency components, and\na Textual Frequency Filter (TFF) that transforms CLIP-derived textual features\ninto the frequency domain and applies graph-based filtering. Experimental\nresults on RB-SCD and three public benchmarks demonstrate the effectiveness of\nour approach."}
{"id": "2505.13215", "pdf": "https://arxiv.org/pdf/2505.13215", "abs": "https://arxiv.org/abs/2505.13215", "authors": ["Seungjun Oh", "Younggeun Lee", "Hyejin Jeon", "Eunbyung Park"], "title": "Hybrid 3D-4D Gaussian Splatting for Fast Dynamic Scene Representation", "categories": ["cs.CV"], "comment": "https://ohsngjun.github.io/3D-4DGS/", "summary": "Recent advancements in dynamic 3D scene reconstruction have shown promising\nresults, enabling high-fidelity 3D novel view synthesis with improved temporal\nconsistency. Among these, 4D Gaussian Splatting (4DGS) has emerged as an\nappealing approach due to its ability to model high-fidelity spatial and\ntemporal variations. However, existing methods suffer from substantial\ncomputational and memory overhead due to the redundant allocation of 4D\nGaussians to static regions, which can also degrade image quality. In this\nwork, we introduce hybrid 3D-4D Gaussian Splatting (3D-4DGS), a novel framework\nthat adaptively represents static regions with 3D Gaussians while reserving 4D\nGaussians for dynamic elements. Our method begins with a fully 4D Gaussian\nrepresentation and iteratively converts temporally invariant Gaussians into 3D,\nsignificantly reducing the number of parameters and improving computational\nefficiency. Meanwhile, dynamic Gaussians retain their full 4D representation,\ncapturing complex motions with high fidelity. Our approach achieves\nsignificantly faster training times compared to baseline 4D Gaussian Splatting\nmethods while maintaining or improving the visual quality."}
{"id": "2505.13219", "pdf": "https://arxiv.org/pdf/2505.13219", "abs": "https://arxiv.org/abs/2505.13219", "authors": ["Jiafu Wu", "Yabiao Wang", "Jian Li", "Jinlong Peng", "Yun Cao", "Chengjie Wang", "Jiangning Zhang"], "title": "Swin DiT: Diffusion Transformer using Pseudo Shifted Windows", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion Transformers (DiTs) achieve remarkable performance within the\ndomain of image generation through the incorporation of the transformer\narchitecture. Conventionally, DiTs are constructed by stacking serial isotropic\nglobal information modeling transformers, which face significant computational\ncost when processing high-resolution images. We empirically analyze that latent\nspace image generation does not exhibit a strong dependence on global\ninformation as traditionally assumed. Most of the layers in the model\ndemonstrate redundancy in global computation. In addition, conventional\nattention mechanisms exhibit low-frequency inertia issues. To address these\nissues, we propose \\textbf{P}seudo \\textbf{S}hifted \\textbf{W}indow\n\\textbf{A}ttention (PSWA), which fundamentally mitigates global model\nredundancy. PSWA achieves intermediate global-local information interaction\nthrough window attention, while employing a high-frequency bridging branch to\nsimulate shifted window operations, supplementing appropriate global and\nhigh-frequency information. Furthermore, we propose the Progressive Coverage\nChannel Allocation(PCCA) strategy that captures high-order attention similarity\nwithout additional computational cost. Building upon all of them, we propose a\nseries of Pseudo \\textbf{S}hifted \\textbf{Win}dow DiTs (\\textbf{Swin DiT}),\naccompanied by extensive experiments demonstrating their superior performance.\nFor example, our proposed Swin-DiT-L achieves a 54%$\\uparrow$ FID improvement\nover DiT-XL/2 while requiring less computational.\nhttps://github.com/wujiafu007/Swin-DiT"}
{"id": "2505.13225", "pdf": "https://arxiv.org/pdf/2505.13225", "abs": "https://arxiv.org/abs/2505.13225", "authors": ["David Levin", "Gonen Singer"], "title": "Automatic Complementary Separation Pruning Toward Lightweight CNNs", "categories": ["cs.CV"], "comment": null, "summary": "In this paper, we present Automatic Complementary Separation Pruning (ACSP),\na novel and fully automated pruning method for convolutional neural networks.\nACSP integrates the strengths of both structured pruning and activation-based\npruning, enabling the efficient removal of entire components such as neurons\nand channels while leveraging activations to identify and retain the most\nrelevant components. Our approach is designed specifically for supervised\nlearning tasks, where we construct a graph space that encodes the separation\ncapabilities of each component with respect to all class pairs. By employing\ncomplementary selection principles and utilizing a clustering algorithm, ACSP\nensures that the selected components maintain diverse and complementary\nseparation capabilities, reducing redundancy and maintaining high network\nperformance. The method automatically determines the optimal subset of\ncomponents in each layer, utilizing a knee-finding algorithm to select the\nminimal subset that preserves performance without requiring user-defined\npruning volumes. Extensive experiments on multiple architectures, including\nVGG-16, ResNet-50, and MobileNet-V2, across datasets like CIFAR-10, CIFAR-100,\nand ImageNet-1K, demonstrate that ACSP achieves competitive accuracy compared\nto other methods while significantly reducing computational costs. This fully\nautomated approach not only enhances scalability but also makes ACSP especially\npractical for real-world deployment by eliminating the need for manually\ndefining the pruning volume."}
{"id": "2505.13233", "pdf": "https://arxiv.org/pdf/2505.13233", "abs": "https://arxiv.org/abs/2505.13233", "authors": ["Lincan Cai", "Jingxuan Kang", "Shuang Li", "Wenxuan Ma", "Binhui Xie", "Zhida Qin", "Jian Liang"], "title": "From Local Details to Global Context: Advancing Vision-Language Models with Attention-Based Selection", "categories": ["cs.CV"], "comment": null, "summary": "Pretrained vision-language models (VLMs), e.g., CLIP, demonstrate impressive\nzero-shot capabilities on downstream tasks. Prior research highlights the\ncrucial role of visual augmentation techniques, like random cropping, in\nalignment with fine-grained class descriptions generated by large language\nmodels (LLMs), significantly enhancing zero-shot performance by incorporating\nmulti-view information. However, the inherent randomness of these augmentations\ncan inevitably introduce background artifacts and cause models to overly focus\non local details, compromising global semantic understanding. To address these\nissues, we propose an \\textbf{A}ttention-\\textbf{B}ased \\textbf{S}election\n(\\textbf{ABS}) method from local details to global context, which applies\nattention-guided cropping in both raw images and feature space, supplement\nglobal semantic information through strategic feature selection. Additionally,\nwe introduce a soft matching technique to effectively filter LLM descriptions\nfor better alignment. \\textbf{ABS} achieves state-of-the-art performance on\nout-of-distribution generalization and zero-shot classification tasks. Notably,\n\\textbf{ABS} is training-free and even rivals few-shot and test-time adaptation\nmethods. Our code is available at\n\\href{https://github.com/BIT-DA/ABS}{\\textcolor{darkgreen}{https://github.com/BIT-DA/ABS}}."}
{"id": "2505.13235", "pdf": "https://arxiv.org/pdf/2505.13235", "abs": "https://arxiv.org/abs/2505.13235", "authors": ["Dang Hoai Nam", "Huynh Tong Dang Khoa", "Vo Nguyen Le Duy"], "title": "WriteViT: Handwritten Text Generation with Vision Transformer", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Humans can quickly generalize handwriting styles from a single example by\nintuitively separating content from style. Machines, however, struggle with\nthis task, especially in low-data settings, often missing subtle spatial and\nstylistic cues. Motivated by this gap, we introduce WriteViT, a one-shot\nhandwritten text synthesis framework that incorporates Vision Transformers\n(ViT), a family of models that have shown strong performance across various\ncomputer vision tasks. WriteViT integrates a ViT-based Writer Identifier for\nextracting style embeddings, a multi-scale generator built with Transformer\nencoder-decoder blocks enhanced by conditional positional encoding (CPE), and a\nlightweight ViT-based recognizer. While previous methods typically rely on CNNs\nor CRNNs, our design leverages transformers in key components to better capture\nboth fine-grained stroke details and higher-level style information. Although\nhandwritten text synthesis has been widely explored, its application to\nVietnamese -- a language rich in diacritics and complex typography -- remains\nlimited. Experiments on Vietnamese and English datasets demonstrate that\nWriteViT produces high-quality, style-consistent handwriting while maintaining\nstrong recognition performance in low-resource scenarios. These results\nhighlight the promise of transformer-based designs for multilingual handwriting\ngeneration and efficient style adaptation."}
{"id": "2505.13250", "pdf": "https://arxiv.org/pdf/2505.13250", "abs": "https://arxiv.org/abs/2505.13250", "authors": ["Hashan K. Weerasooriya", "Prateek Chennuri", "Weijian Zhang", "Istvan Gyongy", "Stanley H. Chan"], "title": "Joint Depth and Reflectivity Estimation using Single-Photon LiDAR", "categories": ["cs.CV"], "comment": null, "summary": "Single-Photon Light Detection and Ranging (SP-LiDAR is emerging as a leading\ntechnology for long-range, high-precision 3D vision tasks. In SP-LiDAR,\ntimestamps encode two complementary pieces of information: pulse travel time\n(depth) and the number of photons reflected by the object (reflectivity).\nExisting SP-LiDAR reconstruction methods typically recover depth and\nreflectivity separately or sequentially use one modality to estimate the other.\nMoreover, the conventional 3D histogram construction is effective mainly for\nslow-moving or stationary scenes. In dynamic scenes, however, it is more\nefficient and effective to directly process the timestamps. In this paper, we\nintroduce an estimation method to simultaneously recover both depth and\nreflectivity in fast-moving scenes. We offer two contributions: (1) A\ntheoretical analysis demonstrating the mutual correlation between depth and\nreflectivity and the conditions under which joint estimation becomes\nbeneficial. (2) A novel reconstruction method, \"SPLiDER\", which exploits the\nshared information to enhance signal recovery. On both synthetic and real\nSP-LiDAR data, our method outperforms existing approaches, achieving superior\njoint reconstruction quality."}
{"id": "2505.13261", "pdf": "https://arxiv.org/pdf/2505.13261", "abs": "https://arxiv.org/abs/2505.13261", "authors": ["Mingrui Chen", "Haogeng Liu", "Hao Liang", "Huaibo Huang", "Wentao Zhang", "Ran He"], "title": "Unlocking the Potential of Difficulty Prior in RL-based Multimodal Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "In this work, we investigate how explicitly modeling problem's difficulty\nprior information shapes the effectiveness of reinforcement learning based\nfine-tuning for multimodal reasoning. Our exploration mainly comprises of\nfollowing three perspective: First, through offline data curation, we analyze\nthe U-shaped difficulty distribution of two given datasets using the base model\nby multi-round sampling, and then filter out prompts that are either too simple\nor extremely difficult to provide meaningful gradients and perform subsequent\ntwo-stage training. Second, we implement an online advantage differentiation,\ncomputing group-wise empirical accuracy as a difficulty proxy to adaptively\nreweight advantages estimation, providing stronger learning signals for more\nchallenging problems. Finally, we introduce difficulty hints as explicit\nprompts for more complex samples in the second training stage, encouraging the\nmodel to calibrate its reasoning depth and perform reflective validation\nchecks. Our comprehensive approach demonstrates significant performances across\nvarious multi-modal mathematical reasoning benchmarks with only 2K+0.6K\ntwo-stage training data."}
{"id": "2505.13266", "pdf": "https://arxiv.org/pdf/2505.13266", "abs": "https://arxiv.org/abs/2505.13266", "authors": ["Yehao Liu", "Xiaosu Xu", "Zijian Wang", "Yiqing Yao"], "title": "DB3D-L: Depth-aware BEV Feature Transformation for Accurate 3D Lane Detection", "categories": ["cs.CV"], "comment": null, "summary": "3D Lane detection plays an important role in autonomous driving. Recent\nadvances primarily build Birds-Eye-View (BEV) feature from front-view (FV)\nimages to perceive 3D information of Lane more effectively. However,\nconstructing accurate BEV information from FV image is limited due to the\nlacking of depth information, causing previous works often rely heavily on the\nassumption of a flat ground plane. Leveraging monocular depth estimation to\nassist in constructing BEV features is less constrained, but existing methods\nstruggle to effectively integrate the two tasks. To address the above issue, in\nthis paper, an accurate 3D lane detection method based on depth-aware BEV\nfeature transtormation is proposed. In detail, an effective feature extraction\nmodule is designed, in which a Depth Net is integrated to obtain the vital\ndepth information for 3D perception, thereby simplifying the complexity of view\ntransformation. Subquently a feature reduce module is proposed to reduce height\ndimension of FV features and depth features, thereby enables effective fusion\nof crucial FV features and depth features. Then a fusion module is designed to\nbuild BEV feature from prime FV feature and depth information. The proposed\nmethod performs comparably with state-of-the-art methods on both synthetic\nApollo, realistic OpenLane datasets."}
{"id": "2505.13279", "pdf": "https://arxiv.org/pdf/2505.13279", "abs": "https://arxiv.org/abs/2505.13279", "authors": ["Zhiqiang Yan", "Jianhao Jiao", "Zhengxue Wang", "Gim Hee Lee"], "title": "Event-Driven Dynamic Scene Depth Completion", "categories": ["cs.CV"], "comment": "9 pages", "summary": "Depth completion in dynamic scenes poses significant challenges due to rapid\nego-motion and object motion, which can severely degrade the quality of input\nmodalities such as RGB images and LiDAR measurements. Conventional RGB-D\nsensors often struggle to align precisely and capture reliable depth under such\nconditions. In contrast, event cameras with their high temporal resolution and\nsensitivity to motion at the pixel level provide complementary cues that are\n%particularly beneficial in dynamic environments.To this end, we propose\nEventDC, the first event-driven depth completion framework. It consists of two\nkey components: Event-Modulated Alignment (EMA) and Local Depth Filtering\n(LDF). Both modules adaptively learn the two fundamental components of\nconvolution operations: offsets and weights conditioned on motion-sensitive\nevent streams. In the encoder, EMA leverages events to modulate the sampling\npositions of RGB-D features to achieve pixel redistribution for improved\nalignment and fusion. In the decoder, LDF refines depth estimations around\nmoving objects by learning motion-aware masks from events. Additionally,\nEventDC incorporates two loss terms to further benefit global alignment and\nenhance local depth recovery. Moreover, we establish the first benchmark for\nevent-based depth completion comprising one real-world and two synthetic\ndatasets to facilitate future research. Extensive experiments on this benchmark\ndemonstrate the superiority of our EventDC."}
{"id": "2505.13281", "pdf": "https://arxiv.org/pdf/2505.13281", "abs": "https://arxiv.org/abs/2505.13281", "authors": ["Zekun Wang", "Sashank Varma"], "title": "Computer Vision Models Show Human-Like Sensitivity to Geometric and Topological Concepts", "categories": ["cs.CV"], "comment": "10 pages, 4 figures, CosSci 2025", "summary": "With the rapid improvement of machine learning (ML) models, cognitive\nscientists are increasingly asking about their alignment with how humans think.\nHere, we ask this question for computer vision models and human sensitivity to\ngeometric and topological (GT) concepts. Under the core knowledge account,\nthese concepts are innate and supported by dedicated neural circuitry. In this\nwork, we investigate an alternative explanation, that GT concepts are learned\n``for free'' through everyday interaction with the environment. We do so using\ncomputer visions models, which are trained on large image datasets. We build on\nprior studies to investigate the overall performance and human alignment of\nthree classes of models -- convolutional neural networks (CNNs),\ntransformer-based models, and vision-language models -- on an odd-one-out task\ntesting 43 GT concepts spanning seven classes. Transformer-based models achieve\nthe highest overall accuracy, surpassing that of young children. They also show\nstrong alignment with children's performance, finding the same classes of\nconcepts easy vs. difficult. By contrast, vision-language models underperform\ntheir vision-only counterparts and deviate further from human profiles,\nindicating that na\\\"ive multimodality might compromise abstract geometric\nsensitivity. These findings support the use of computer vision models to\nevaluate the sufficiency of the learning account for explaining human\nsensitivity to GT concepts, while also suggesting that integrating linguistic\nand visual representations might have unpredicted deleterious consequences."}
{"id": "2505.13300", "pdf": "https://arxiv.org/pdf/2505.13300", "abs": "https://arxiv.org/abs/2505.13300", "authors": ["Zekai Li", "Xinhao Zhong", "Samir Khaki", "Zhiyuan Liang", "Yuhao Zhou", "Mingjia Shi", "Ziqiao Wang", "Xuanlei Zhao", "Wangbo Zhao", "Ziheng Qin", "Mengxuan Wu", "Pengfei Zhou", "Haonan Wang", "David Junhao Zhang", "Jia-Wei Liu", "Shaobo Wang", "Dai Liu", "Linfeng Zhang", "Guang Li", "Kun Wang", "Zheng Zhu", "Zhiheng Ma", "Joey Tianyi Zhou", "Jiancheng Lv", "Yaochu Jin", "Peihao Wang", "Kaipeng Zhang", "Lingjuan Lyu", "Yiran Huang", "Zeynep Akata", "Zhiwei Deng", "Xindi Wu", "George Cazenavette", "Yuzhang Shang", "Justin Cui", "Jindong Gu", "Qian Zheng", "Hao Ye", "Shuo Wang", "Xiaobo Wang", "Yan Yan", "Angela Yao", "Mike Zheng Shou", "Tianlong Chen", "Hakan Bilen", "Baharan Mirzasoleiman", "Manolis Kellis", "Konstantinos N. Plataniotis", "Zhangyang Wang", "Bo Zhao", "Yang You", "Kai Wang"], "title": "DD-Ranking: Rethinking the Evaluation of Dataset Distillation", "categories": ["cs.CV"], "comment": "20 pages, 4 figures", "summary": "In recent years, dataset distillation has provided a reliable solution for\ndata compression, where models trained on the resulting smaller synthetic\ndatasets achieve performance comparable to those trained on the original\ndatasets. To further improve the performance of synthetic datasets, various\ntraining pipelines and optimization objectives have been proposed, greatly\nadvancing the field of dataset distillation. Recent decoupled dataset\ndistillation methods introduce soft labels and stronger data augmentation\nduring the post-evaluation phase and scale dataset distillation up to larger\ndatasets (e.g., ImageNet-1K). However, this raises a question: Is accuracy\nstill a reliable metric to fairly evaluate dataset distillation methods? Our\nempirical findings suggest that the performance improvements of these methods\noften stem from additional techniques rather than the inherent quality of the\nimages themselves, with even randomly sampled images achieving superior\nresults. Such misaligned evaluation settings severely hinder the development of\nDD. Therefore, we propose DD-Ranking, a unified evaluation framework, along\nwith new general evaluation metrics to uncover the true performance\nimprovements achieved by different methods. By refocusing on the actual\ninformation enhancement of distilled datasets, DD-Ranking provides a more\ncomprehensive and fair evaluation standard for future research advancements."}
{"id": "2505.13306", "pdf": "https://arxiv.org/pdf/2505.13306", "abs": "https://arxiv.org/abs/2505.13306", "authors": ["Chengsong Sun", "Weiping Li", "Xiang Li", "Yuankun Liu", "Lianlei Shan"], "title": "GMM-Based Comprehensive Feature Extraction and Relative Distance Preservation For Few-Shot Cross-Modal Retrieval", "categories": ["cs.CV", "cs.IR"], "comment": null, "summary": "Few-shot cross-modal retrieval focuses on learning cross-modal\nrepresentations with limited training samples, enabling the model to handle\nunseen classes during inference. Unlike traditional cross-modal retrieval\ntasks, which assume that both training and testing data share the same class\ndistribution, few-shot retrieval involves data with sparse representations\nacross modalities. Existing methods often fail to adequately model the\nmulti-peak distribution of few-shot cross-modal data, resulting in two main\nbiases in the latent semantic space: intra-modal bias, where sparse samples\nfail to capture intra-class diversity, and inter-modal bias, where\nmisalignments between image and text distributions exacerbate the semantic gap.\nThese biases hinder retrieval accuracy. To address these issues, we propose a\nnovel method, GCRDP, for few-shot cross-modal retrieval. This approach\neffectively captures the complex multi-peak distribution of data using a\nGaussian Mixture Model (GMM) and incorporates a multi-positive sample\ncontrastive learning mechanism for comprehensive feature modeling.\nAdditionally, we introduce a new strategy for cross-modal semantic alignment,\nwhich constrains the relative distances between image and text feature\ndistributions, thereby improving the accuracy of cross-modal representations.\nWe validate our approach through extensive experiments on four benchmark\ndatasets, demonstrating superior performance over six state-of-the-art methods."}
{"id": "2505.13309", "pdf": "https://arxiv.org/pdf/2505.13309", "abs": "https://arxiv.org/abs/2505.13309", "authors": ["Jad Mansour", "Sebastian Realpe", "Hayat Rajani", "Michele Grimaldi", "Rafael Garcia", "Nuno Gracias"], "title": "eStonefish-scenes: A synthetically generated dataset for underwater event-based optical flow prediction tasks", "categories": ["cs.CV", "I.2.5; I.2.6; I.2.9; I.2.10"], "comment": "Submitted to IJRR", "summary": "The combined use of event-based vision and Spiking Neural Networks (SNNs) is\nexpected to significantly impact robotics, particularly in tasks like visual\nodometry and obstacle avoidance. While existing real-world event-based datasets\nfor optical flow prediction, typically captured with Unmanned Aerial Vehicles\n(UAVs), offer valuable insights, they are limited in diversity, scalability,\nand are challenging to collect. Moreover, there is a notable lack of labelled\ndatasets for underwater applications, which hinders the integration of\nevent-based vision with Autonomous Underwater Vehicles (AUVs). To address this,\nsynthetic datasets could provide a scalable solution while bridging the gap\nbetween simulation and reality. In this work, we introduce eStonefish-scenes, a\nsynthetic event-based optical flow dataset based on the Stonefish simulator.\nAlong with the dataset, we present a data generation pipeline that enables the\ncreation of customizable underwater environments. This pipeline allows for\nsimulating dynamic scenarios, such as biologically inspired schools of fish\nexhibiting realistic motion patterns, including obstacle avoidance and reactive\nnavigation around corals. Additionally, we introduce a scene generator that can\nbuild realistic reef seabeds by randomly distributing coral across the terrain.\nTo streamline data accessibility, we present eWiz, a comprehensive library\ndesigned for processing event-based data, offering tools for data loading,\naugmentation, visualization, encoding, and training data generation, along with\nloss functions and performance metrics."}
{"id": "2505.13316", "pdf": "https://arxiv.org/pdf/2505.13316", "abs": "https://arxiv.org/abs/2505.13316", "authors": ["Gabriele Spadaro", "Alberto Presta", "Jhony H. Giraldo", "Marco Grangetto", "Wei Hu", "Giuseppe Valenzise", "Attilio Fiandrotti", "Enzo Tartaglione"], "title": "Denoising Diffusion Probabilistic Model for Point Cloud Compression at Low Bit-Rates", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "6 pages, 5 figures, accepted at ICME 2025", "summary": "Efficient compression of low-bit-rate point clouds is critical for\nbandwidth-constrained applications. However, existing techniques mainly focus\non high-fidelity reconstruction, requiring many bits for compression. This\npaper proposes a \"Denoising Diffusion Probabilistic Model\" (DDPM) architecture\nfor point cloud compression (DDPM-PCC) at low bit-rates. A PointNet encoder\nproduces the condition vector for the generation, which is then quantized via a\nlearnable vector quantizer. This configuration allows to achieve a low bitrates\nwhile preserving quality. Experiments on ShapeNet and ModelNet40 show improved\nrate-distortion at low rates compared to standardized and state-of-the-art\napproaches. We publicly released the code at\nhttps://github.com/EIDOSLAB/DDPM-PCC."}
{"id": "2505.13318", "pdf": "https://arxiv.org/pdf/2505.13318", "abs": "https://arxiv.org/abs/2505.13318", "authors": ["Paula Feldman", "Martin Sinnona", "Viviana Siless", "Claudio Delrieux", "Emmanuel Iarussi"], "title": "VesselGPT: Autoregressive Modeling of Vascular Geometry", "categories": ["cs.CV", "cs.LG", "eess.IV"], "comment": null, "summary": "Anatomical trees are critical for clinical diagnosis and treatment planning,\nyet their complex and diverse geometry make accurate representation a\nsignificant challenge. Motivated by the latest advances in large language\nmodels, we introduce an autoregressive method for synthesizing anatomical\ntrees. Our approach first embeds vessel structures into a learned discrete\nvocabulary using a VQ-VAE architecture, then models their generation\nautoregressively with a GPT-2 model. This method effectively captures intricate\ngeometries and branching patterns, enabling realistic vascular tree synthesis.\nComprehensive qualitative and quantitative evaluations reveal that our\ntechnique achieves high-fidelity tree reconstruction with compact discrete\nrepresentations. Moreover, our B-spline representation of vessel cross-sections\npreserves critical morphological details that are often overlooked in previous'\nmethods parameterizations. To the best of our knowledge, this work is the first\nto generate blood vessels in an autoregressive manner. Code, data, and trained\nmodels will be made available."}
{"id": "2505.13327", "pdf": "https://arxiv.org/pdf/2505.13327", "abs": "https://arxiv.org/abs/2505.13327", "authors": ["Ajian Liu", "Haocheng Yuan", "Xiao Guo", "Hui Ma", "Wanyi Zhuang", "Changtao Miao", "Yan Hong", "Chuanbiao Song", "Jun Lan", "Qi Chu", "Tao Gong", "Yanyan Liang", "Weiqiang Wang", "Jun Wan", "Xiaoming Liu", "Zhen Lei"], "title": "Benchmarking Unified Face Attack Detection via Hierarchical Prompt Tuning", "categories": ["cs.CV"], "comment": null, "summary": "Presentation Attack Detection and Face Forgery Detection are designed to\nprotect face data from physical media-based Presentation Attacks and digital\nediting-based DeepFakes respectively. But separate training of these two models\nmakes them vulnerable to unknown attacks and burdens deployment environments.\nThe lack of a Unified Face Attack Detection model to handle both types of\nattacks is mainly due to two factors. First, there's a lack of adequate\nbenchmarks for models to explore. Existing UAD datasets have limited attack\ntypes and samples, restricting the model's ability to address advanced threats.\nTo address this, we propose UniAttackDataPlus (UniAttackData+), the most\nextensive and sophisticated collection of forgery techniques to date. It\nincludes 2,875 identities and their 54 kinds of falsified samples, totaling\n697,347 videos. Second, there's a lack of a reliable classification criterion.\nCurrent methods try to find an arbitrary criterion within the same semantic\nspace, which fails when encountering diverse attacks. So, we present a novel\nVisual-Language Model-based Hierarchical Prompt Tuning Framework (HiPTune) that\nadaptively explores multiple classification criteria from different semantic\nspaces. We build a Visual Prompt Tree to explore various classification rules\nhierarchically. Then, by adaptively pruning the prompts, the model can select\nthe most suitable prompts to guide the encoder to extract discriminative\nfeatures at different levels in a coarse-to-fine way. Finally, to help the\nmodel understand the classification criteria in visual space, we propose a\nDynamically Prompt Integration module to project the visual prompts to the text\nencoder for more accurate semantics. Experiments on 12 datasets have shown the\npotential to inspire further innovations in the UAD field."}
{"id": "2505.13344", "pdf": "https://arxiv.org/pdf/2505.13344", "abs": "https://arxiv.org/abs/2505.13344", "authors": ["Ahmet Berke Gokmen", "Yigit Ekin", "Bahri Batuhan Bilecen", "Aysegul Dundar"], "title": "RoPECraft: Training-Free Motion Transfer with Trajectory-Guided RoPE Optimization on Diffusion Transformers", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "https://berkegokmen1.github.io/RoPECraft/", "summary": "We propose RoPECraft, a training-free video motion transfer method for\ndiffusion transformers that operates solely by modifying their rotary\npositional embeddings (RoPE). We first extract dense optical flow from a\nreference video, and utilize the resulting motion offsets to warp the\ncomplex-exponential tensors of RoPE, effectively encoding motion into the\ngeneration process. These embeddings are then further optimized during\ndenoising time steps via trajectory alignment between the predicted and target\nvelocities using a flow-matching objective. To keep the output faithful to the\ntext prompt and prevent duplicate generations, we incorporate a regularization\nterm based on the phase components of the reference video's Fourier transform,\nprojecting the phase angles onto a smooth manifold to suppress high-frequency\nartifacts. Experiments on benchmarks reveal that RoPECraft outperforms all\nrecently published methods, both qualitatively and quantitatively."}
{"id": "2505.13389", "pdf": "https://arxiv.org/pdf/2505.13389", "abs": "https://arxiv.org/abs/2505.13389", "authors": ["Peiyuan Zhang", "Haofeng Huang", "Yongqi Chen", "Will Lin", "Zhengzhong Liu", "Ion Stoica", "Eric P. Xing", "Hao Zhang"], "title": "Faster Video Diffusion with Trainable Sparse Attention", "categories": ["cs.CV"], "comment": null, "summary": "Scaling video diffusion transformers (DiTs) is limited by their quadratic 3D\nattention, even though most of the attention mass concentrates on a small\nsubset of positions. We turn this observation into VSA, a trainable,\nhardware-efficient sparse attention that replaces full attention at \\emph{both}\ntraining and inference. In VSA, a lightweight coarse stage pools tokens into\ntiles and identifies high-weight \\emph{critical tokens}; a fine stage computes\ntoken-level attention only inside those tiles subjecting to block computing\nlayout to ensure hard efficiency. This leads to a single differentiable kernel\nthat trains end-to-end, requires no post-hoc profiling, and sustains 85\\% of\nFlashAttention3 MFU. We perform a large sweep of ablation studies and\nscaling-law experiments by pretraining DiTs from 60M to 1.4B parameters. VSA\nreaches a Pareto point that cuts training FLOPS by 2.53$\\times$ with no drop in\ndiffusion loss. Retrofitting the open-source Wan-2.1 model speeds up attention\ntime by 6$\\times$ and lowers end-to-end generation time from 31s to 18s with\ncomparable quality. These results establish trainable sparse attention as a\npractical alternative to full attention and a key enabler for further scaling\nof video diffusion models."}
{"id": "2505.13419", "pdf": "https://arxiv.org/pdf/2505.13419", "abs": "https://arxiv.org/abs/2505.13419", "authors": ["Zhuozhao Hu", "Kaishen Yuan", "Xin Liu", "Zitong Yu", "Yuan Zong", "Jingang Shi", "Huanjing Yue", "Jingyu Yang"], "title": "FEALLM: Advancing Facial Emotion Analysis in Multimodal Large Language Models with Emotional Synergy and Reasoning", "categories": ["cs.CV"], "comment": "10 pages, 7 figures", "summary": "Facial Emotion Analysis (FEA) plays a crucial role in visual affective\ncomputing, aiming to infer a person's emotional state based on facial data.\nScientifically, facial expressions (FEs) result from the coordinated movement\nof facial muscles, which can be decomposed into specific action units (AUs)\nthat provide detailed emotional insights. However, traditional methods often\nstruggle with limited interpretability, constrained generalization and\nreasoning abilities. Recently, Multimodal Large Language Models (MLLMs) have\nshown exceptional performance in various visual tasks, while they still face\nsignificant challenges in FEA due to the lack of specialized datasets and their\ninability to capture the intricate relationships between FEs and AUs. To\naddress these issues, we introduce a novel FEA Instruction Dataset that\nprovides accurate and aligned FE and AU descriptions and establishes causal\nreasoning relationships between them, followed by constructing a new benchmark,\nFEABench. Moreover, we propose FEALLM, a novel MLLM architecture designed to\ncapture more detailed facial information, enhancing its capability in FEA\ntasks. Our model demonstrates strong performance on FEABench and impressive\ngeneralization capability through zero-shot evaluation on various datasets,\nincluding RAF-DB, AffectNet, BP4D, and DISFA, showcasing its robustness and\neffectiveness in FEA tasks. The dataset and code will be available at\nhttps://github.com/953206211/FEALLM."}
{"id": "2505.13426", "pdf": "https://arxiv.org/pdf/2505.13426", "abs": "https://arxiv.org/abs/2505.13426", "authors": ["Liang Chen", "Hongcheng Gao", "Tianyu Liu", "Zhiqi Huang", "Flood Sung", "Xinyu Zhou", "Yuxin Wu", "Baobao Chang"], "title": "G1: Bootstrapping Perception and Reasoning Abilities of Vision-Language Model via Reinforcement Learning", "categories": ["cs.CV"], "comment": "21 pages, 14 figures, code released at\n  https://github.com/chenllliang/G1", "summary": "Vision-Language Models (VLMs) excel in many direct multimodal tasks but\nstruggle to translate this prowess into effective decision-making within\ninteractive, visually rich environments like games. This ``knowing-doing'' gap\nsignificantly limits their potential as autonomous agents, as leading VLMs\noften performing badly in simple games. To address this, we introduce VLM-Gym,\na curated reinforcement learning (RL) environment featuring diverse visual\ngames with unified interfaces and adjustable, compositional difficulty,\nspecifically designed for scalable multi-game parallel training. Leveraging\nVLM-Gym, we train G0 models using pure RL-driven self-evolution, which\ndemonstrate emergent perception and reasoning patterns. To further mitigate\nchallenges arising from game diversity, we develop G1 models. G1 incorporates a\nperception-enhanced cold start prior to RL fine-tuning. Our resulting G1 models\nconsistently surpass their teacher across all games and outperform leading\nproprietary models like Claude-3.7-Sonnet-Thinking. Systematic analysis reveals\nan intriguing finding: perception and reasoning abilities mutually bootstrap\neach other throughout the RL training process. Source code including VLM-Gym\nand RL training are released at https://github.com/chenllliang/G1 to foster\nfuture research in advancing VLMs as capable interactive agents."}
{"id": "2505.13429", "pdf": "https://arxiv.org/pdf/2505.13429", "abs": "https://arxiv.org/abs/2505.13429", "authors": ["Cristobal Eyzaguirre", "Igor Vasiljevic", "Achal Dave", "Jiajun Wu", "Rares Andrei Ambrus", "Thomas Kollar", "Juan Carlos Niebles", "Pavel Tokmakov"], "title": "Understanding Complexity in VideoQA via Visual Program Generation", "categories": ["cs.CV"], "comment": null, "summary": "We propose a data-driven approach to analyzing query complexity in Video\nQuestion Answering (VideoQA). Previous efforts in benchmark design have relied\non human expertise to design challenging questions, yet we experimentally show\nthat humans struggle to predict which questions are difficult for machine\nlearning models. Our automatic approach leverages recent advances in code\ngeneration for visual question answering, using the complexity of generated\ncode as a proxy for question difficulty. We demonstrate that this measure\ncorrelates significantly better with model performance than human estimates. To\noperationalize this insight, we propose an algorithm for estimating question\ncomplexity from code. It identifies fine-grained primitives that correlate with\nthe hardest questions for any given set of models, making it easy to scale to\nnew approaches in the future. Finally, to further illustrate the utility of our\nmethod, we extend it to automatically generate complex questions, constructing\na new benchmark that is 1.9 times harder than the popular NExT-QA."}
{"id": "2505.13436", "pdf": "https://arxiv.org/pdf/2505.13436", "abs": "https://arxiv.org/abs/2505.13436", "authors": ["R. James Cotton"], "title": "KinTwin: Imitation Learning with Torque and Muscle Driven Biomechanical Models Enables Precise Replication of Able-Bodied and Impaired Movement from Markerless Motion Capture", "categories": ["cs.CV"], "comment": null, "summary": "Broader access to high-quality movement analysis could greatly benefit\nmovement science and rehabilitation, such as allowing more detailed\ncharacterization of movement impairments and responses to interventions, or\neven enabling early detection of new neurological conditions or fall risk.\nWhile emerging technologies are making it easier to capture kinematics with\nbiomechanical models, or how joint angles change over time, inferring the\nunderlying physics that give rise to these movements, including ground reaction\nforces, joint torques, or even muscle activations, is still challenging. Here\nwe explore whether imitation learning applied to a biomechanical model from a\nlarge dataset of movements from able-bodied and impaired individuals can learn\nto compute these inverse dynamics. Although imitation learning in human pose\nestimation has seen great interest in recent years, our work differences in\nseveral ways: we focus on using an accurate biomechanical model instead of\nmodels adopted for computer vision, we test it on a dataset that contains\nparticipants with impaired movements, we reported detailed tracking metrics\nrelevant for the clinical measurement of movement including joint angles and\nground contact events, and finally we apply imitation learning to a\nmuscle-driven neuromusculoskeletal model. We show that our imitation learning\npolicy, KinTwin, can accurately replicate the kinematics of a wide range of\nmovements, including those with assistive devices or therapist assistance, and\nthat it can infer clinically meaningful differences in joint torques and muscle\nactivations. Our work demonstrates the potential for using imitation learning\nto enable high-quality movement analysis in clinical practice."}
{"id": "2505.13437", "pdf": "https://arxiv.org/pdf/2505.13437", "abs": "https://arxiv.org/abs/2505.13437", "authors": ["Dian Shao", "Mingfei Shi", "Shengda Xu", "Haodong Chen", "Yongle Huang", "Binglu Wang"], "title": "FinePhys: Fine-grained Human Action Generation by Explicitly Incorporating Physical Laws for Effective Skeletal Guidance", "categories": ["cs.CV", "cs.AI"], "comment": "CVPR 2025", "summary": "Despite significant advances in video generation, synthesizing physically\nplausible human actions remains a persistent challenge, particularly in\nmodeling fine-grained semantics and complex temporal dynamics. For instance,\ngenerating gymnastics routines such as \"switch leap with 0.5 turn\" poses\nsubstantial difficulties for current methods, often yielding unsatisfactory\nresults. To bridge this gap, we propose FinePhys, a Fine-grained human action\ngeneration framework that incorporates Physics to obtain effective skeletal\nguidance. Specifically, FinePhys first estimates 2D poses in an online manner\nand then performs 2D-to-3D dimension lifting via in-context learning. To\nmitigate the instability and limited interpretability of purely data-driven 3D\nposes, we further introduce a physics-based motion re-estimation module\ngoverned by Euler-Lagrange equations, calculating joint accelerations via\nbidirectional temporal updating. The physically predicted 3D poses are then\nfused with data-driven ones, offering multi-scale 2D heatmap guidance for the\ndiffusion process. Evaluated on three fine-grained action subsets from FineGym\n(FX-JUMP, FX-TURN, and FX-SALTO), FinePhys significantly outperforms\ncompetitive baselines. Comprehensive qualitative results further demonstrate\nFinePhys's ability to generate more natural and plausible fine-grained human\nactions."}
{"id": "2505.13439", "pdf": "https://arxiv.org/pdf/2505.13439", "abs": "https://arxiv.org/abs/2505.13439", "authors": ["Huawei Lin", "Tong Geng", "Zhaozhuo Xu", "Weijie Zhao"], "title": "VTBench: Evaluating Visual Tokenizers for Autoregressive Image Generation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "24 pages, 13 figures, 3 tables", "summary": "Autoregressive (AR) models have recently shown strong performance in image\ngeneration, where a critical component is the visual tokenizer (VT) that maps\ncontinuous pixel inputs to discrete token sequences. The quality of the VT\nlargely defines the upper bound of AR model performance. However, current\ndiscrete VTs fall significantly behind continuous variational autoencoders\n(VAEs), leading to degraded image reconstructions and poor preservation of\ndetails and text. Existing benchmarks focus on end-to-end generation quality,\nwithout isolating VT performance. To address this gap, we introduce VTBench, a\ncomprehensive benchmark that systematically evaluates VTs across three core\ntasks: Image Reconstruction, Detail Preservation, and Text Preservation, and\ncovers a diverse range of evaluation scenarios. We systematically assess\nstate-of-the-art VTs using a set of metrics to evaluate the quality of\nreconstructed images. Our findings reveal that continuous VAEs produce superior\nvisual representations compared to discrete VTs, particularly in retaining\nspatial structure and semantic detail. In contrast, the degraded\nrepresentations produced by discrete VTs often lead to distorted\nreconstructions, loss of fine-grained textures, and failures in preserving text\nand object integrity. Furthermore, we conduct experiments on GPT-4o image\ngeneration and discuss its potential AR nature, offering new insights into the\nrole of visual tokenization. We release our benchmark and codebase publicly to\nsupport further research and call on the community to develop strong,\ngeneral-purpose open-source VTs."}
{"id": "2505.13440", "pdf": "https://arxiv.org/pdf/2505.13440", "abs": "https://arxiv.org/abs/2505.13440", "authors": ["Ruoyu Wang", "Yi Ma", "Shenghua Gao"], "title": "Recollection from Pensieve: Novel View Synthesis via Learning from Uncalibrated Videos", "categories": ["cs.CV"], "comment": "13 pages, 4 figures", "summary": "Currently almost all state-of-the-art novel view synthesis and reconstruction\nmodels rely on calibrated cameras or additional geometric priors for training.\nThese prerequisites significantly limit their applicability to massive\nuncalibrated data. To alleviate this requirement and unlock the potential for\nself-supervised training on large-scale uncalibrated videos, we propose a novel\ntwo-stage strategy to train a view synthesis model from only raw video frames\nor multi-view images, without providing camera parameters or other priors. In\nthe first stage, we learn to reconstruct the scene implicitly in a latent space\nwithout relying on any explicit 3D representation. Specifically, we predict\nper-frame latent camera and scene context features, and employ a view synthesis\nmodel as a proxy for explicit rendering. This pretraining stage substantially\nreduces the optimization complexity and encourages the network to learn the\nunderlying 3D consistency in a self-supervised manner. The learned latent\ncamera and implicit scene representation have a large gap compared with the\nreal 3D world. To reduce this gap, we introduce the second stage training by\nexplicitly predicting 3D Gaussian primitives. We additionally apply explicit\nGaussian Splatting rendering loss and depth projection loss to align the\nlearned latent representations with physically grounded 3D geometry. In this\nway, Stage 1 provides a strong initialization and Stage 2 enforces 3D\nconsistency - the two stages are complementary and mutually beneficial.\nExtensive experiments demonstrate the effectiveness of our approach, achieving\nhigh-quality novel view synthesis and accurate camera pose estimation, compared\nto methods that employ supervision with calibration, pose, or depth\ninformation. The code is available at https://github.com/Dwawayu/Pensieve."}
{"id": "2505.11518", "pdf": "https://arxiv.org/pdf/2505.11518", "abs": "https://arxiv.org/abs/2505.11518", "authors": ["Merham Fouladvand", "Peuroly Batra"], "title": "Deep Unrolled Meta-Learning for Multi-Coil and Multi-Modality MRI with Adaptive Optimization", "categories": ["math.OC", "cs.CV"], "comment": null, "summary": "We propose a unified deep meta-learning framework for accelerated magnetic\nresonance imaging (MRI) that jointly addresses multi-coil reconstruction and\ncross-modality synthesis. Motivated by the limitations of conventional methods\nin handling undersampled data and missing modalities, our approach unrolls a\nprovably convergent optimization algorithm into a structured neural network\narchitecture. Each phase of the network mimics a step of an adaptive\nforward-backward scheme with extrapolation, enabling the model to incorporate\nboth data fidelity and nonconvex regularization in a principled manner. To\nenhance generalization across different acquisition settings, we integrate\nmeta-learning, which enables the model to rapidly adapt to unseen sampling\npatterns and modality combinations using task-specific meta-knowledge. The\nproposed method is evaluated on the open source datasets, showing significant\nimprovements in PSNR and SSIM over conventional supervised learning, especially\nunder aggressive undersampling and domain shifts. Our results demonstrate the\nsynergy of unrolled optimization, task-aware meta-learning, and modality\nfusion, offering a scalable and generalizable solution for real-world clinical\nMRI reconstruction."}
{"id": "2505.11535", "pdf": "https://arxiv.org/pdf/2505.11535", "abs": "https://arxiv.org/abs/2505.11535", "authors": ["Yuhang Wang", "Hao Zhou"], "title": "Bridging Human Oversight and Black-box Driver Assistance: Vision-Language Models for Predictive Alerting in Lane Keeping Assist Systems", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": null, "summary": "Lane Keeping Assist systems, while increasingly prevalent, often suffer from\nunpredictable real-world failures, largely due to their opaque, black-box\nnature, which limits driver anticipation and trust. To bridge the gap between\nautomated assistance and effective human oversight, we present LKAlert, a novel\nsupervisory alert system that leverages VLM to forecast potential LKA risk 1-3\nseconds in advance. LKAlert processes dash-cam video and CAN data, integrating\nsurrogate lane segmentation features from a parallel interpretable model as\nautomated guiding attention. Unlike traditional binary classifiers, LKAlert\nissues both predictive alert and concise natural language explanation,\nenhancing driver situational awareness and trust. To support the development\nand evaluation of such systems, we introduce OpenLKA-Alert, the first benchmark\ndataset designed for predictive and explainable LKA failure warnings. It\ncontains synchronized multimodal inputs and human-authored justifications\nacross annotated temporal windows. We further contribute a generalizable\nmethodological framework for VLM-based black-box behavior prediction, combining\nsurrogate feature guidance with LoRA. This framework enables VLM to reason over\nstructured visual context without altering its vision backbone, making it\nbroadly applicable to other complex, opaque systems requiring interpretable\noversight. Empirical results correctly predicts upcoming LKA failures with\n69.8% accuracy and a 58.6\\% F1-score. The system also generates high-quality\ntextual explanations for drivers (71.7 ROUGE-L) and operates efficiently at\napproximately 2 Hz, confirming its suitability for real-time, in-vehicle use.\nOur findings establish LKAlert as a practical solution for enhancing the safety\nand usability of current ADAS and offer a scalable paradigm for applying VLMs\nto human-centered supervision of black-box automation."}
{"id": "2505.11538", "pdf": "https://arxiv.org/pdf/2505.11538", "abs": "https://arxiv.org/abs/2505.11538", "authors": ["Jiacheng Hou", "Zhenjie Song", "Ercan Engin Kuruoglu"], "title": "BrainNetMLP: An Efficient and Effective Baseline for Functional Brain Network Classification", "categories": ["q-bio.NC", "cs.CV"], "comment": "V1.0", "summary": "Recent studies have made great progress in functional brain network\nclassification by modeling the brain as a network of Regions of Interest (ROIs)\nand leveraging their connections to understand brain functionality and diagnose\nmental disorders. Various deep learning architectures, including Convolutional\nNeural Networks, Graph Neural Networks, and the recent Transformer, have been\ndeveloped. However, despite the increasing complexity of these models, the\nperformance gain has not been as salient. This raises a question: Does\nincreasing model complexity necessarily lead to higher classification accuracy?\nIn this paper, we revisit the simplest deep learning architecture, the\nMulti-Layer Perceptron (MLP), and propose a pure MLP-based method, named\nBrainNetMLP, for functional brain network classification, which capitalizes on\nthe advantages of MLP, including efficient computation and fewer parameters.\nMoreover, BrainNetMLP incorporates a dual-branch structure to jointly capture\nboth spatial connectivity and spectral information, enabling precise\nspatiotemporal feature fusion. We evaluate our proposed BrainNetMLP on two\npublic and popular brain network classification datasets, the Human Connectome\nProject (HCP) and the Autism Brain Imaging Data Exchange (ABIDE). Experimental\nresults demonstrate pure MLP-based methods can achieve state-of-the-art\nperformance, revealing the potential of MLP-based models as more efficient yet\neffective alternatives in functional brain network classification. The code\nwill be available at https://github.com/JayceonHo/BrainNetMLP."}
{"id": "2505.11576", "pdf": "https://arxiv.org/pdf/2505.11576", "abs": "https://arxiv.org/abs/2505.11576", "authors": ["Shuchen Wu", "Stephan Alaniz", "Shyamgopal Karthik", "Peter Dayan", "Eric Schulz", "Zeynep Akata"], "title": "Concept-Guided Interpretability via Neural Chunking", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "35 pages, 32 figures. arXiv admin note: text overlap with\n  arXiv:2502.01803", "summary": "Neural networks are often black boxes, reflecting the significant challenge\nof understanding their internal workings. We propose a different perspective\nthat challenges the prevailing view: rather than being inscrutable, neural\nnetworks exhibit patterns in their raw population activity that mirror\nregularities in the training data. We refer to this as the Reflection\nHypothesis and provide evidence for this phenomenon in both simple recurrent\nneural networks (RNNs) and complex large language models (LLMs). Building on\nthis insight, we propose to leverage cognitively-inspired methods of chunking\nto segment high-dimensional neural population dynamics into interpretable units\nthat reflect underlying concepts. We propose three methods to extract these\nemerging entities, complementing each other based on label availability and\ndimensionality. Discrete sequence chunking (DSC) creates a dictionary of\nentities; population averaging (PA) extracts recurring entities that correspond\nto known labels; and unsupervised chunk discovery (UCD) can be used when labels\nare absent. We demonstrate the effectiveness of these methods in extracting\nentities across varying model sizes, ranging from inducing compositionality in\nRNNs to uncovering recurring neural population states in large models with\ndiverse architectures, and illustrate their advantage over other methods.\nThroughout, we observe a robust correspondence between the extracted entities\nand concrete or abstract concepts. Artificially inducing the extracted entities\nin neural populations effectively alters the network's generation of associated\nconcepts. Our work points to a new direction for interpretability, one that\nharnesses both cognitive principles and the structure of naturalistic data to\nreveal the hidden computations of complex learning systems, gradually\ntransforming them from black boxes into systems we can begin to understand."}
{"id": "2505.11594", "pdf": "https://arxiv.org/pdf/2505.11594", "abs": "https://arxiv.org/abs/2505.11594", "authors": ["Jintao Zhang", "Jia Wei", "Pengle Zhang", "Xiaoming Xu", "Haofeng Huang", "Haoxu Wang", "Kai Jiang", "Jun Zhu", "Jianfei Chen"], "title": "SageAttention3: Microscaling FP4 Attention for Inference and An Exploration of 8-Bit Training", "categories": ["cs.LG", "cs.AI", "cs.AR", "cs.CV", "cs.PF"], "comment": null, "summary": "The efficiency of attention is important due to its quadratic time\ncomplexity. We enhance the efficiency of attention through two key\ncontributions: First, we leverage the new FP4 Tensor Cores in Blackwell GPUs to\naccelerate attention computation. Our implementation achieves 1038 TOPS on\nRTX5090, which is a 5x speedup over the fastest FlashAttention on RTX5090.\nExperiments show that our FP4 attention can accelerate inference of various\nmodels in a plug-and-play way. Second, we pioneer low-bit attention to training\ntasks. Existing low-bit attention works like FlashAttention3 and SageAttention\nfocus only on inference. However, the efficiency of training large models is\nalso important. To explore whether low-bit attention can be effectively applied\nto training tasks, we design an accurate and efficient 8-bit attention for both\nforward and backward propagation. Experiments indicate that 8-bit attention\nachieves lossless performance in fine-tuning tasks but exhibits slower\nconvergence in pretraining tasks. The code will be available at\nhttps://github.com/thu-ml/SageAttention."}
{"id": "2505.11645", "pdf": "https://arxiv.org/pdf/2505.11645", "abs": "https://arxiv.org/abs/2505.11645", "authors": ["Jinzhou Cao", "Xiangxu Wang", "Jiashi Chen", "Wei Tu", "Zhenhui Li", "Xindong Yang", "Tianhong Zhao", "Qingquan Li"], "title": "Urban Representation Learning for Fine-grained Economic Mapping: A Semi-supervised Graph-based Approach", "categories": ["cs.LG", "cs.CV"], "comment": "Accepted for publication in International Society Journal of\n  Photogrammetry and Remote Sensing (ISPRS). 70 pages, 10 Figures, 15 Tables", "summary": "Fine-grained economic mapping through urban representation learning has\nemerged as a crucial tool for evidence-based economic decisions. While existing\nmethods primarily rely on supervised or unsupervised approaches, they often\noverlook semi-supervised learning in data-scarce scenarios and lack unified\nmulti-task frameworks for comprehensive sectoral economic analysis. To address\nthese gaps, we propose SemiGTX, an explainable semi-supervised graph learning\nframework for sectoral economic mapping. The framework is designed with\ndedicated fusion encoding modules for various geospatial data modalities,\nseamlessly integrating them into a cohesive graph structure. It introduces a\nsemi-information loss function that combines spatial self-supervision with\nlocally masked supervised regression, enabling more informative and effective\nregion representations. Through multi-task learning, SemiGTX concurrently maps\nGDP across primary, secondary, and tertiary sectors within a unified model.\nExtensive experiments conducted in the Pearl River Delta region of China\ndemonstrate the model's superior performance compared to existing methods,\nachieving R2 scores of 0.93, 0.96, and 0.94 for the primary, secondary and\ntertiary sectors, respectively. Cross-regional experiments in Beijing and\nChengdu further illustrate its generality. Systematic analysis reveals how\ndifferent data modalities influence model predictions, enhancing explainability\nwhile providing valuable insights for regional development planning. This\nrepresentation learning framework advances regional economic monitoring through\ndiverse urban data integration, providing a robust foundation for precise\neconomic forecasting."}
{"id": "2505.11651", "pdf": "https://arxiv.org/pdf/2505.11651", "abs": "https://arxiv.org/abs/2505.11651", "authors": ["Radek Osmulsk", "Gabriel de Souza P. Moreira", "Ronay Ak", "Mengyao Xu", "Benedikt Schifferer", "Even Oldridge"], "title": "MIRACL-VISION: A Large, multilingual, visual document retrieval benchmark", "categories": ["cs.IR", "cs.CV"], "comment": null, "summary": "Document retrieval is an important task for search and Retrieval-Augmented\nGeneration (RAG) applications. Large Language Models (LLMs) have contributed to\nimproving the accuracy of text-based document retrieval. However, documents\nwith complex layout and visual elements like tables, charts and infographics\nare not perfectly represented in textual format. Recently, image-based document\nretrieval pipelines have become popular, which use visual large language models\n(VLMs) to retrieve relevant page images given a query. Current evaluation\nbenchmarks on visual document retrieval are limited, as they primarily focus\nonly English language, rely on synthetically generated questions and offer a\nsmall corpus size. Therefore, we introduce MIRACL-VISION, a multilingual visual\ndocument retrieval evaluation benchmark. MIRACL-VISION covers 18 languages, and\nis an extension of the MIRACL dataset, a popular benchmark to evaluate\ntext-based multilingual retrieval pipelines. MIRACL was built using a\nhuman-intensive annotation process to generate high-quality questions. In order\nto reduce MIRACL-VISION corpus size to make evaluation more compute friendly\nwhile keeping the datasets challenging, we have designed a method for\neliminating the \"easy\" negatives from the corpus. We conducted extensive\nexperiments comparing MIRACL-VISION with other benchmarks, using popular public\ntext and image models. We observe a gap in state-of-the-art VLM-based embedding\nmodels on multilingual capabilities, with up to 59.7% lower retrieval accuracy\nthan a text-based retrieval models. Even for the English language, the visual\nmodels retrieval accuracy is 12.1% lower compared to text-based models.\nMIRACL-VISION is a challenging, representative, multilingual evaluation\nbenchmark for visual retrieval pipelines and will help the community build\nrobust models for document retrieval."}
{"id": "2505.11717", "pdf": "https://arxiv.org/pdf/2505.11717", "abs": "https://arxiv.org/abs/2505.11717", "authors": ["Xilong Wang", "John Bloch", "Zedian Shao", "Yuepeng Hu", "Shuyan Zhou", "Neil Zhenqiang Gong"], "title": "EnvInjection: Environmental Prompt Injection Attack to Multi-modal Web Agents", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Multi-modal large language model (MLLM)-based web agents interact with\nwebpage environments by generating actions based on screenshots of the\nwebpages. Environmental prompt injection attacks manipulate the environment to\ninduce the web agent to perform a specific, attacker-chosen action--referred to\nas the target action. However, existing attacks suffer from limited\neffectiveness or stealthiness, or are impractical in real-world settings. In\nthis work, we propose EnvInjection, a new attack that addresses these\nlimitations. Our attack adds a perturbation to the raw pixel values of the\nrendered webpage, which can be implemented by modifying the webpage's source\ncode. After these perturbed pixels are mapped into a screenshot, the\nperturbation induces the web agent to perform the target action. We formulate\nthe task of finding the perturbation as an optimization problem. A key\nchallenge in solving this problem is that the mapping between raw pixel values\nand screenshot is non-differentiable, making it difficult to backpropagate\ngradients to the perturbation. To overcome this, we train a neural network to\napproximate the mapping and apply projected gradient descent to solve the\nreformulated optimization problem. Extensive evaluation on multiple webpage\ndatasets shows that EnvInjection is highly effective and significantly\noutperforms existing baselines."}
{"id": "2505.11797", "pdf": "https://arxiv.org/pdf/2505.11797", "abs": "https://arxiv.org/abs/2505.11797", "authors": ["Hancan Zhu", "Jinhao Chen", "Guanghua He"], "title": "MedVKAN: Efficient Feature Extraction with Mamba and KAN for Medical Image Segmentation", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Medical image segmentation relies heavily on convolutional neural networks\n(CNNs) and Transformer-based models. However, CNNs are constrained by limited\nreceptive fields, while Transformers suffer from scalability challenges due to\ntheir quadratic computational complexity. To address these limitations, recent\nadvances have explored alternative architectures. The state-space model Mamba\noffers near-linear complexity while capturing long-range dependencies, and the\nKolmogorov-Arnold Network (KAN) enhances nonlinear expressiveness by replacing\nfixed activation functions with learnable ones. Building on these strengths, we\npropose MedVKAN, an efficient feature extraction model integrating Mamba and\nKAN. Specifically, we introduce the EFC-KAN module, which enhances KAN with\nconvolutional operations to improve local pixel interaction. We further design\nthe VKAN module, integrating Mamba with EFC-KAN as a replacement for\nTransformer modules, significantly improving feature extraction. Extensive\nexperiments on five public medical image segmentation datasets show that\nMedVKAN achieves state-of-the-art performance on four datasets and ranks second\non the remaining one. These results validate the potential of Mamba and KAN for\nmedical image segmentation while introducing an innovative and computationally\nefficient feature extraction framework. The code is available at:\nhttps://github.com/beginner-cjh/MedVKAN."}
{"id": "2505.11832", "pdf": "https://arxiv.org/pdf/2505.11832", "abs": "https://arxiv.org/abs/2505.11832", "authors": ["Yuxiang Lai", "Jike Zhong", "Vanessa Su", "Xiaofeng Yang"], "title": "Patient-Specific Autoregressive Models for Organ Motion Prediction in Radiotherapy", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Radiotherapy often involves a prolonged treatment period. During this time,\npatients may experience organ motion due to breathing and other physiological\nfactors. Predicting and modeling this motion before treatment is crucial for\nensuring precise radiation delivery. However, existing pre-treatment organ\nmotion prediction methods primarily rely on deformation analysis using\nprincipal component analysis (PCA), which is highly dependent on registration\nquality and struggles to capture periodic temporal dynamics for motion\nmodeling.In this paper, we observe that organ motion prediction closely\nresembles an autoregressive process, a technique widely used in natural\nlanguage processing (NLP). Autoregressive models predict the next token based\non previous inputs, naturally aligning with our objective of predicting future\norgan motion phases. Building on this insight, we reformulate organ motion\nprediction as an autoregressive process to better capture patient-specific\nmotion patterns. Specifically, we acquire 4D CT scans for each patient before\ntreatment, with each sequence comprising multiple 3D CT phases. These phases\nare fed into the autoregressive model to predict future phases based on prior\nphase motion patterns. We evaluate our method on a real-world test set of 4D CT\nscans from 50 patients who underwent radiotherapy at our institution and a\npublic dataset containing 4D CT scans from 20 patients (some with multiple\nscans), totaling over 1,300 3D CT phases. The performance in predicting the\nmotion of the lung and heart surpasses existing benchmarks, demonstrating its\neffectiveness in capturing motion dynamics from CT images. These results\nhighlight the potential of our method to improve pre-treatment planning in\nradiotherapy, enabling more precise and adaptive radiation delivery."}
{"id": "2505.11865", "pdf": "https://arxiv.org/pdf/2505.11865", "abs": "https://arxiv.org/abs/2505.11865", "authors": ["Teli Ma", "Jia Zheng", "Zifan Wang", "Ziyao Gao", "Jiaming Zhou", "Junwei Liang"], "title": "GLOVER++: Unleashing the Potential of Affordance Learning from Human Behaviors for Robotic Manipulation", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Learning manipulation skills from human demonstration videos offers a\npromising path toward generalizable and interpretable robotic\nintelligence-particularly through the lens of actionable affordances. However,\ntransferring such knowledge remains challenging due to: 1) a lack of\nlarge-scale datasets with precise affordance annotations, and 2) insufficient\nexploration of affordances in diverse manipulation contexts. To address these\ngaps, we introduce HOVA-500K, a large-scale, affordance-annotated dataset\ncomprising 500,000 images across 1,726 object categories and 675 actions. We\nalso release a standardized benchmarking suite for multi-modal affordance\nreasoning. Built upon HOVA-500K, we present GLOVER++, a global-to-local\naffordance training framework that effectively transfers actionable affordance\nknowledge from human demonstrations to downstream open-vocabulary reasoning\ntasks. GLOVER++ achieves state-of-the-art results on the HOVA-500K benchmark\nand demonstrates strong generalization across diverse downstream robotic\nmanipulation tasks. By explicitly modeling actionable affordances, GLOVER++\nfacilitates robust transfer across scenes, modalities, and tasks. We hope that\nHOVA-500K and the GLOVER++ framework will serve as valuable resources for\nbridging the gap between human demonstrations and robotic manipulation\ncapabilities."}
{"id": "2505.11879", "pdf": "https://arxiv.org/pdf/2505.11879", "abs": "https://arxiv.org/abs/2505.11879", "authors": ["Reihaneh Yourdkhani", "Arash Tavoosian", "Navid Asadi Khomami", "Mehdi Tale Masouleh"], "title": "Experimental Study on Automatically Assembling Custom Catering Packages With a 3-DOF Delta Robot Using Deep Learning Methods", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "This paper introduces a pioneering experimental study on the automated\npacking of a catering package using a two-fingered gripper affixed to a\n3-degree-of-freedom Delta parallel robot. A distinctive contribution lies in\nthe application of a deep learning approach to tackle this challenge. A custom\ndataset, comprising 1,500 images, is meticulously curated for this endeavor,\nrepresenting a noteworthy initiative as the first dataset focusing on\nPersian-manufactured products. The study employs the YOLOV5 model for object\ndetection, followed by segmentation using the FastSAM model. Subsequently,\nrotation angle calculation is facilitated with segmentation masks, and a\nrotated rectangle encapsulating the object is generated. This rectangle forms\nthe basis for calculating two grasp points using a novel geometrical approach\ninvolving eigenvectors. An extensive experimental study validates the proposed\nmodel, where all pertinent information is seamlessly transmitted to the 3-DOF\nDelta parallel robot. The proposed algorithm ensures real-time detection,\ncalibration, and the fully autonomous packing process of a catering package,\nboasting an impressive over 80\\% success rate in automatic grasping. This study\nmarks a significant stride in advancing the capabilities of robotic systems for\npractical applications in packaging automation."}
{"id": "2505.11883", "pdf": "https://arxiv.org/pdf/2505.11883", "abs": "https://arxiv.org/abs/2505.11883", "authors": ["Zihuan Qiu", "Yi Xu", "Chiyuan He", "Fanman Meng", "Linfeng Xu", "Qingbo Wu", "Hongliang Li"], "title": "MINGLE: Mixtures of Null-Space Gated Low-Rank Experts for Test-Time Continual Model Merging", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Continual model merging integrates independently fine-tuned models\nsequentially without access to original training data, providing a scalable and\nefficient solution to continual learning. However, current methods still face\ncritical challenges, notably parameter interference among tasks and limited\nadaptability to evolving test distributions. The former causes catastrophic\nforgetting of integrated tasks, while the latter hinders effective adaptation\nto new tasks. To address these, we propose MINGLE, a novel framework for\ntest-time continual model merging, which leverages test-time adaptation using a\nsmall set of unlabeled test samples from the current task to dynamically guide\nthe merging process. MINGLE employs a mixture-of-experts architecture composed\nof parameter-efficient, low-rank experts, enabling efficient adaptation and\nimproving robustness to distribution shifts. To mitigate catastrophic\nforgetting, we propose Null-Space Constrained Gating, which restricts gating\nupdates to subspaces orthogonal to prior task representations. This suppresses\nactivations on old task inputs and preserves model behavior on past tasks. To\nfurther balance stability and adaptability, we design an Adaptive Relaxation\nStrategy, which dynamically adjusts the constraint strength based on\ninterference signals captured during test-time adaptation. Extensive\nexperiments on standard continual merging benchmarks demonstrate that MINGLE\nachieves robust generalization, reduces forgetting significantly, and\nconsistently surpasses previous state-of-the-art methods by 7-9\\% on average\nacross diverse task orders."}
{"id": "2505.11909", "pdf": "https://arxiv.org/pdf/2505.11909", "abs": "https://arxiv.org/abs/2505.11909", "authors": ["Pengfei Lyu", "Pak-Hei Yeung", "Xiaosheng Yu", "Jing Xia", "Jianning Chi", "Chengdong Wu", "Jagath C. Rajapakse"], "title": "Bridging the Inter-Domain Gap through Low-Level Features for Cross-Modal Medical Image Segmentation", "categories": ["eess.IV", "cs.CV"], "comment": "11 pages, 2 figures", "summary": "This paper addresses the task of cross-modal medical image segmentation by\nexploring unsupervised domain adaptation (UDA) approaches. We propose a\nmodel-agnostic UDA framework, LowBridge, which builds on a simple observation\nthat cross-modal images share some similar low-level features (e.g., edges) as\nthey are depicting the same structures. Specifically, we first train a\ngenerative model to recover the source images from their edge features,\nfollowed by training a segmentation model on the generated source images,\nseparately. At test time, edge features from the target images are input to the\npretrained generative model to generate source-style target domain images,\nwhich are then segmented using the pretrained segmentation network. Despite its\nsimplicity, extensive experiments on various publicly available datasets\ndemonstrate that \\proposed achieves state-of-the-art performance, outperforming\neleven existing UDA approaches under different settings. Notably, further\nablation studies show that \\proposed is agnostic to different types of\ngenerative and segmentation models, suggesting its potential to be seamlessly\nplugged with the most advanced models to achieve even more outstanding results\nin the future. The code is available at https://github.com/JoshuaLPF/LowBridge."}
{"id": "2505.11913", "pdf": "https://arxiv.org/pdf/2505.11913", "abs": "https://arxiv.org/abs/2505.11913", "authors": ["Sven Dummer", "Puru Vaish", "Christoph Brune"], "title": "Joint Manifold Learning and Optimal Transport for Dynamic Imaging", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Dynamic imaging is critical for understanding and visualizing dynamic\nbiological processes in medicine and cell biology. These applications often\nencounter the challenge of a limited amount of time series data and time\npoints, which hinders learning meaningful patterns. Regularization methods\nprovide valuable prior knowledge to address this challenge, enabling the\nextraction of relevant information despite the scarcity of time-series data and\ntime points. In particular, low-dimensionality assumptions on the image\nmanifold address sample scarcity, while time progression models, such as\noptimal transport (OT), provide priors on image development to mitigate the\nlack of time points. Existing approaches using low-dimensionality assumptions\ndisregard a temporal prior but leverage information from multiple time series.\nOT-prior methods, however, incorporate the temporal prior but regularize only\nindividual time series, ignoring information from other time series of the same\nimage modality. In this work, we investigate the effect of integrating a\nlow-dimensionality assumption of the underlying image manifold with an OT\nregularizer for time-evolving images. In particular, we propose a latent model\nrepresentation of the underlying image manifold and promote consistency between\nthis representation, the time series data, and the OT prior on the\ntime-evolving images. We discuss the advantages of enriching OT interpolations\nwith latent models and integrating OT priors into latent models."}
{"id": "2505.11998", "pdf": "https://arxiv.org/pdf/2505.11998", "abs": "https://arxiv.org/abs/2505.11998", "authors": ["Prashant Shivaram Bhat", "Shakib Yazdani", "Elahe Arani", "Bahram Zonooz"], "title": "Parameter Efficient Continual Learning with Dynamic Low-Rank Adaptation", "categories": ["cs.LG", "cs.CV"], "comment": "27 pages, 5 figures", "summary": "Catastrophic forgetting has remained a critical challenge for deep neural\nnetworks in Continual Learning (CL) as it undermines consolidated knowledge\nwhen learning new tasks. Parameter efficient fine tuning CL techniques are\ngaining traction for their effectiveness in addressing catastrophic forgetting\nwith a lightweight training schedule while avoiding degradation of consolidated\nknowledge in pre-trained models. However, low rank adapters (LoRA) in these\napproaches are highly sensitive to rank selection which can lead to sub-optimal\nresource allocation and performance. To this end, we introduce PEARL, a\nrehearsal-free CL framework that entails dynamic rank allocation for LoRA\ncomponents during CL training. Specifically, PEARL leverages reference task\nweights and adaptively determines the rank of task-specific LoRA components\nbased on the current tasks' proximity to reference task weights in parameter\nspace. To demonstrate the versatility of PEARL, we evaluate it across three\nvision architectures (ResNet, Separable Convolutional Network and Vision\nTransformer) and a multitude of CL scenarios, and show that PEARL outperforms\nall considered baselines by a large margin."}
{"id": "2505.12051", "pdf": "https://arxiv.org/pdf/2505.12051", "abs": "https://arxiv.org/abs/2505.12051", "authors": ["Yinghui Zhang", "Tailin Chen", "Yuchen Zhang", "Zeyu Fu"], "title": "Enhanced Multimodal Hate Video Detection via Channel-wise and Modality-wise Fusion", "categories": ["cs.MM", "cs.AI", "cs.CV"], "comment": "ICDMW 2024, Github: https://github.com/EvelynZ10/cmfusion", "summary": "The rapid rise of video content on platforms such as TikTok and YouTube has\ntransformed information dissemination, but it has also facilitated the spread\nof harmful content, particularly hate videos. Despite significant efforts to\ncombat hate speech, detecting these videos remains challenging due to their\noften implicit nature. Current detection methods primarily rely on unimodal\napproaches, which inadequately capture the complementary features across\ndifferent modalities. While multimodal techniques offer a broader perspective,\nmany fail to effectively integrate temporal dynamics and modality-wise\ninteractions essential for identifying nuanced hate content. In this paper, we\npresent CMFusion, an enhanced multimodal hate video detection model utilizing a\nnovel Channel-wise and Modality-wise Fusion Mechanism. CMFusion first extracts\nfeatures from text, audio, and video modalities using pre-trained models and\nthen incorporates a temporal cross-attention mechanism to capture dependencies\nbetween video and audio streams. The learned features are then processed by\nchannel-wise and modality-wise fusion modules to obtain informative\nrepresentations of videos. Our extensive experiments on a real-world dataset\ndemonstrate that CMFusion significantly outperforms five widely used baselines\nin terms of accuracy, precision, recall, and F1 score. Comprehensive ablation\nstudies and parameter analyses further validate our design choices,\nhighlighting the model's effectiveness in detecting hate videos. The source\ncodes will be made publicly available at https://github.com/EvelynZ10/cmfusion."}
{"id": "2505.12061", "pdf": "https://arxiv.org/pdf/2505.12061", "abs": "https://arxiv.org/abs/2505.12061", "authors": ["Samuel T. M. Ball"], "title": "Bayesian Deep Learning Approaches for Uncertainty-Aware Retinal OCT Image Segmentation for Multiple Sclerosis", "categories": ["eess.IV", "cs.CV", "68U10, 92C55", "I.2.10; I.4.6; J.3"], "comment": null, "summary": "Optical Coherence Tomography (OCT) provides valuable insights in\nophthalmology, cardiology, and neurology due to high-resolution,\ncross-sectional images of the retina. One critical task for ophthalmologists\nusing OCT is delineation of retinal layers within scans. This process is\ntime-consuming and prone to human bias, affecting the accuracy and reliability\nof diagnoses. Previous efforts to automate delineation using deep learning face\nchallenges in uptake from clinicians and statisticians due to the absence of\nuncertainty estimation, leading to \"confidently wrong\" models via\nhallucinations. In this study, we address these challenges by applying Bayesian\nconvolutional neural networks (BCNNs) to segment an openly available OCT\nimaging dataset containing 35 human retina OCTs split between healthy controls\nand patients with multiple sclerosis. Our findings demonstrate that Bayesian\nmodels can be used to provide uncertainty maps of the segmentation, which can\nfurther be used to identify highly uncertain samples that exhibit recording\nartefacts such as noise or miscalibration at inference time. Our method also\nallows for uncertainty-estimation for important secondary measurements such as\nlayer thicknesses, that are medically relevant for patients. We show that these\nfeatures come in addition to greater performance compared to similar work over\nall delineations; with an overall Dice score of 95.65%. Our work brings greater\nclinical applicability, statistical robustness, and performance to retinal OCT\nsegmentation."}
{"id": "2505.12089", "pdf": "https://arxiv.org/pdf/2505.12089", "abs": "https://arxiv.org/abs/2505.12089", "authors": ["Sangmin Lee", "Eunpil Park", "Angel Canelo", "Hyunhee Park", "Youngjo Kim", "Hyung-Ju Chun", "Xin Jin", "Chongyi Li", "Chun-Le Guo", "Radu Timofte", "Qi Wu", "Tianheng Qiu", "Yuchun Dong", "Shenglin Ding", "Guanghua Pan", "Weiyu Zhou", "Tao Hu", "Yixu Feng", "Duwei Dai", "Yu Cao", "Peng Wu", "Wei Dong", "Yanning Zhang", "Qingsen Yan", "Simon J. Larsen", "Ruixuan Jiang", "Senyan Xu", "Xingbo Wang", "Xin Lu", "Marcos V. Conde", "Javier Abad-Hernandez", "Alvaro Garca-Lara", "Daniel Feijoo", "Alvaro Garca", "Zeyu Xiao", "Zhuoyuan Li"], "title": "NTIRE 2025 Challenge on Efficient Burst HDR and Restoration: Datasets, Methods, and Results", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "This paper reviews the NTIRE 2025 Efficient Burst HDR and Restoration\nChallenge, which aims to advance efficient multi-frame high dynamic range (HDR)\nand restoration techniques. The challenge is based on a novel RAW multi-frame\nfusion dataset, comprising nine noisy and misaligned RAW frames with various\nexposure levels per scene. Participants were tasked with developing solutions\ncapable of effectively fusing these frames while adhering to strict efficiency\nconstraints: fewer than 30 million model parameters and a computational budget\nunder 4.0 trillion FLOPs. A total of 217 participants registered, with six\nteams finally submitting valid solutions. The top-performing approach achieved\na PSNR of 43.22 dB, showcasing the potential of novel methods in this domain.\nThis paper provides a comprehensive overview of the challenge, compares the\nproposed solutions, and serves as a valuable reference for researchers and\npractitioners in efficient burst HDR and restoration."}
{"id": "2505.12114", "pdf": "https://arxiv.org/pdf/2505.12114", "abs": "https://arxiv.org/abs/2505.12114", "authors": ["Dena F. Mujtaba", "Nihar R. Mahapatra"], "title": "Behind the Screens: Uncovering Bias in AI-Driven Video Interview Assessments Using Counterfactuals", "categories": ["cs.HC", "cs.CV", "eess.IV"], "comment": null, "summary": "AI-enhanced personality assessments are increasingly shaping hiring\ndecisions, using affective computing to predict traits from the Big Five\n(OCEAN) model. However, integrating AI into these assessments raises ethical\nconcerns, especially around bias amplification rooted in training data. These\nbiases can lead to discriminatory outcomes based on protected attributes like\ngender, ethnicity, and age. To address this, we introduce a\ncounterfactual-based framework to systematically evaluate and quantify bias in\nAI-driven personality assessments. Our approach employs generative adversarial\nnetworks (GANs) to generate counterfactual representations of job applicants by\naltering protected attributes, enabling fairness analysis without access to the\nunderlying model. Unlike traditional bias assessments that focus on unimodal or\nstatic data, our method supports multimodal evaluation-spanning visual, audio,\nand textual features. This comprehensive approach is particularly important in\nhigh-stakes applications like hiring, where third-party vendors often provide\nAI systems as black boxes. Applied to a state-of-the-art personality prediction\nmodel, our method reveals significant disparities across demographic groups. We\nalso validate our framework using a protected attribute classifier to confirm\nthe effectiveness of our counterfactual generation. This work provides a\nscalable tool for fairness auditing of commercial AI hiring platforms,\nespecially in black-box settings where training data and model internals are\ninaccessible. Our results highlight the importance of counterfactual approaches\nin improving ethical transparency in affective computing."}
{"id": "2505.12120", "pdf": "https://arxiv.org/pdf/2505.12120", "abs": "https://arxiv.org/abs/2505.12120", "authors": ["Dmitry Nechaev", "Alexey Pchelnikov", "Ekaterina Ivanova"], "title": "HISTAI: An Open-Source, Large-Scale Whole Slide Image Dataset for Computational Pathology", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Recent advancements in Digital Pathology (DP), particularly through\nartificial intelligence and Foundation Models, have underscored the importance\nof large-scale, diverse, and richly annotated datasets. Despite their critical\nrole, publicly available Whole Slide Image (WSI) datasets often lack sufficient\nscale, tissue diversity, and comprehensive clinical metadata, limiting the\nrobustness and generalizability of AI models. In response, we introduce the\nHISTAI dataset, a large, multimodal, open-access WSI collection comprising over\n60,000 slides from various tissue types. Each case in the HISTAI dataset is\naccompanied by extensive clinical metadata, including diagnosis, demographic\ninformation, detailed pathological annotations, and standardized diagnostic\ncoding. The dataset aims to fill gaps identified in existing resources,\npromoting innovation, reproducibility, and the development of clinically\nrelevant computational pathology solutions. The dataset can be accessed at\nhttps://github.com/HistAI/HISTAI."}
{"id": "2505.12203", "pdf": "https://arxiv.org/pdf/2505.12203", "abs": "https://arxiv.org/abs/2505.12203", "authors": ["Zhiting Zheng", "Shuqi Wu", "Wen Ding"], "title": "CTLformer: A Hybrid Denoising Model Combining Convolutional Layers and Self-Attention for Enhanced CT Image Reconstruction", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Low-dose CT (LDCT) images are often accompanied by significant noise, which\nnegatively impacts image quality and subsequent diagnostic accuracy. To address\nthe challenges of multi-scale feature fusion and diverse noise distribution\npatterns in LDCT denoising, this paper introduces an innovative model,\nCTLformer, which combines convolutional structures with transformer\narchitecture. Two key innovations are proposed: a multi-scale attention\nmechanism and a dynamic attention control mechanism. The multi-scale attention\nmechanism, implemented through the Token2Token mechanism and self-attention\ninteraction modules, effectively captures both fine details and global\nstructures at different scales, enhancing relevant features and suppressing\nnoise. The dynamic attention control mechanism adapts the attention\ndistribution based on the noise characteristics of the input image, focusing on\nhigh-noise regions while preserving details in low-noise areas, thereby\nenhancing robustness and improving denoising performance. Furthermore,\nCTLformer integrates convolutional layers for efficient feature extraction and\nuses overlapping inference to mitigate boundary artifacts, further\nstrengthening its denoising capability. Experimental results on the 2016\nNational Institutes of Health AAPM Mayo Clinic LDCT Challenge dataset\ndemonstrate that CTLformer significantly outperforms existing methods in both\ndenoising performance and model efficiency, greatly improving the quality of\nLDCT images. The proposed CTLformer not only provides an efficient solution for\nLDCT denoising but also shows broad potential in medical image analysis,\nespecially for clinical applications dealing with complex noise patterns."}
{"id": "2505.12233", "pdf": "https://arxiv.org/pdf/2505.12233", "abs": "https://arxiv.org/abs/2505.12233", "authors": ["Yeonkyung Lee", "Woojung Han", "Youngjun Jun", "Hyeonmin Kim", "Jungkyung Cho", "Seong Jae Hwang"], "title": "PRETI: Patient-Aware Retinal Foundation Model via Metadata-Guided Representation Learning", "categories": ["eess.IV", "cs.CV"], "comment": "MICCAI2025 early accept", "summary": "Retinal foundation models have significantly advanced retinal image analysis\nby leveraging self-supervised learning to reduce dependence on labeled data\nwhile achieving strong generalization. Many recent approaches enhance retinal\nimage understanding using report supervision, but obtaining clinical reports is\noften costly and challenging. In contrast, metadata (e.g., age, gender) is\nwidely available and serves as a valuable resource for analyzing disease\nprogression. To effectively incorporate patient-specific information, we\npropose PRETI, a retinal foundation model that integrates metadata-aware\nlearning with robust self-supervised representation learning. We introduce\nLearnable Metadata Embedding (LME), which dynamically refines metadata\nrepresentations. Additionally, we construct patient-level data pairs,\nassociating images from the same individual to improve robustness against\nnon-clinical variations. To further optimize retinal image representation, we\npropose Retina-Aware Adaptive Masking (RAAM), a strategy that selectively\napplies masking within the retinal region and dynamically adjusts the masking\nratio during training. PRETI captures both global structures and fine-grained\npathological details, resulting in superior diagnostic performance. Extensive\nexperiments demonstrate that PRETI achieves state-of-the-art results across\ndiverse diseases and biomarker predictions using in-house and public data,\nindicating the importance of metadata-guided foundation models in retinal\ndisease analysis. Our code and pretrained model are available at\nhttps://github.com/MICV-yonsei/PRETI"}
{"id": "2505.12261", "pdf": "https://arxiv.org/pdf/2505.12261", "abs": "https://arxiv.org/abs/2505.12261", "authors": ["Hanchen Wang", "Yixuan Wu", "Yinan Feng", "Peng Jin", "Shihang Feng", "Yiming Mao", "James Wiskin", "Baris Turkbey", "Peter A. Pinto", "Bradford J. Wood", "Songting Luo", "Yinpeng Chen", "Emad Boctor", "Youzuo Lin"], "title": "OpenPros: A Large-Scale Dataset for Limited View Prostate Ultrasound Computed Tomography", "categories": ["physics.med-ph", "cs.CV"], "comment": null, "summary": "Prostate cancer is one of the most common and lethal cancers among men,\nmaking its early detection critically important. Although ultrasound imaging\noffers greater accessibility and cost-effectiveness compared to MRI,\ntraditional transrectal ultrasound methods suffer from low sensitivity,\nespecially in detecting anteriorly located tumors. Ultrasound computed\ntomography provides quantitative tissue characterization, but its clinical\nimplementation faces significant challenges, particularly under anatomically\nconstrained limited-angle acquisition conditions specific to prostate imaging.\nTo address these unmet needs, we introduce OpenPros, the first large-scale\nbenchmark dataset explicitly developed for limited-view prostate USCT. Our\ndataset includes over 280,000 paired samples of realistic 2D speed-of-sound\n(SOS) phantoms and corresponding ultrasound full-waveform data, generated from\nanatomically accurate 3D digital prostate models derived from real clinical\nMRI/CT scans and ex vivo ultrasound measurements, annotated by medical experts.\nSimulations are conducted under clinically realistic configurations using\nadvanced finite-difference time-domain and Runge-Kutta acoustic wave solvers,\nboth provided as open-source components. Through comprehensive baseline\nexperiments, we demonstrate that state-of-the-art deep learning methods surpass\ntraditional physics-based approaches in both inference efficiency and\nreconstruction accuracy. Nevertheless, current deep learning models still fall\nshort of delivering clinically acceptable high-resolution images with\nsufficient accuracy. By publicly releasing OpenPros, we aim to encourage the\ndevelopment of advanced machine learning algorithms capable of bridging this\nperformance gap and producing clinically usable, high-resolution, and highly\naccurate prostate ultrasound images. The dataset is publicly accessible at\nhttps://open-pros.github.io/."}
{"id": "2505.12278", "pdf": "https://arxiv.org/pdf/2505.12278", "abs": "https://arxiv.org/abs/2505.12278", "authors": ["Zhengyi Luo", "Chen Tessler", "Toru Lin", "Ye Yuan", "Tairan He", "Wenli Xiao", "Yunrong Guo", "Gal Chechik", "Kris Kitani", "Linxi Fan", "Yuke Zhu"], "title": "Emergent Active Perception and Dexterity of Simulated Humanoids from Visual Reinforcement Learning", "categories": ["cs.RO", "cs.CV"], "comment": "Project page: https://zhengyiluo.github.io/PDC", "summary": "Human behavior is fundamentally shaped by visual perception -- our ability to\ninteract with the world depends on actively gathering relevant information and\nadapting our movements accordingly. Behaviors like searching for objects,\nreaching, and hand-eye coordination naturally emerge from the structure of our\nsensory system. Inspired by these principles, we introduce Perceptive Dexterous\nControl (PDC), a framework for vision-driven dexterous whole-body control with\nsimulated humanoids. PDC operates solely on egocentric vision for task\nspecification, enabling object search, target placement, and skill selection\nthrough visual cues, without relying on privileged state information (e.g., 3D\nobject positions and geometries). This perception-as-interface paradigm enables\nlearning a single policy to perform multiple household tasks, including\nreaching, grasping, placing, and articulated object manipulation. We also show\nthat training from scratch with reinforcement learning can produce emergent\nbehaviors such as active search. These results demonstrate how vision-driven\ncontrol and complex tasks induce human-like behaviors and can serve as the key\ningredients in closing the perception-action loop for animation, robotics, and\nembodied AI."}
{"id": "2505.12298", "pdf": "https://arxiv.org/pdf/2505.12298", "abs": "https://arxiv.org/abs/2505.12298", "authors": ["Amal Lahchim", "Lazar Davic"], "title": "Attention-Enhanced U-Net for Accurate Segmentation of COVID-19 Infected Lung Regions in CT Scans", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "14 pages, 9 figures, created using Google Colab and PyTorch. Compares\n  segmentation models for COVID-19 CT data", "summary": "In this study, we propose a robust methodology for automatic segmentation of\ninfected lung regions in COVID-19 CT scans using convolutional neural networks.\nThe approach is based on a modified U-Net architecture enhanced with attention\nmechanisms, data augmentation, and postprocessing techniques. It achieved a\nDice coefficient of 0.8658 and mean IoU of 0.8316, outperforming other methods.\nThe dataset was sourced from public repositories and augmented for diversity.\nResults demonstrate superior segmentation performance. Future work includes\nexpanding the dataset, exploring 3D segmentation, and preparing the model for\nclinical deployment."}
{"id": "2505.12322", "pdf": "https://arxiv.org/pdf/2505.12322", "abs": "https://arxiv.org/abs/2505.12322", "authors": ["Ali Gholamzadeh", "Noor Sajid"], "title": "Model alignment using inter-modal bridges", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Foundation models have demonstrated remarkable performance across modalities\nsuch as language and vision. However, model reuse across distinct modalities\n(e.g., text and vision) remains limited due to the difficulty of aligning\ninternal representations. Existing methods require extensive paired training\ndata or are constrained to specific domains. We introduce a semi-supervised\napproach for model alignment via conditional flow matching. The conditional\nflow between latent spaces of different modalities (e.g., text-to-image or\nbiological-to-artificial neuronal activity) can be learned in two settings:\n($1$) solving a (balanced or unbalanced) optimal transport problem with an\ninter-space bridge cost, and ($2$) performing memory-efficient alignment using\nlabelled exemplars. Despite being constrained by the original models' capacity,\nour method--under both settings--matches downstream task performance of\nend-to-end trained models on object recognition and image generation tasks\nacross MNIST, ImageNet, and \\cite{majaj2015simple} datasets, particularly when\nlabelled training data is scarce ($<20\\%$). Our method provides a\ndata-efficient solution for inter-modal model alignment with minimal\nsupervision."}
{"id": "2505.12332", "pdf": "https://arxiv.org/pdf/2505.12332", "abs": "https://arxiv.org/abs/2505.12332", "authors": ["Qianyue Hu", "Junyan Wu", "Wei Lu", "Xiangyang Luo"], "title": "VoiceCloak: A Multi-Dimensional Defense Framework against Unauthorized Diffusion-based Voice Cloning", "categories": ["cs.SD", "cs.AI", "cs.CV", "cs.MM", "eess.AS"], "comment": null, "summary": "Diffusion Models (DMs) have achieved remarkable success in realistic voice\ncloning (VC), while they also increase the risk of malicious misuse. Existing\nproactive defenses designed for traditional VC models aim to disrupt the\nforgery process, but they have been proven incompatible with DMs due to the\nintricate generative mechanisms of diffusion. To bridge this gap, we introduce\nVoiceCloak, a multi-dimensional proactive defense framework with the goal of\nobfuscating speaker identity and degrading perceptual quality in potential\nunauthorized VC. To achieve these goals, we conduct a focused analysis to\nidentify specific vulnerabilities within DMs, allowing VoiceCloak to disrupt\nthe cloning process by introducing adversarial perturbations into the reference\naudio. Specifically, to obfuscate speaker identity, VoiceCloak first targets\nspeaker identity by distorting representation learning embeddings to maximize\nidentity variation, which is guided by auditory perception principles.\nAdditionally, VoiceCloak disrupts crucial conditional guidance processes,\nparticularly attention context, thereby preventing the alignment of vocal\ncharacteristics that are essential for achieving convincing cloning. Then, to\naddress the second objective, VoiceCloak introduces score magnitude\namplification to actively steer the reverse trajectory away from the generation\nof high-quality speech. Noise-guided semantic corruption is further employed to\ndisrupt structural speech semantics captured by DMs, degrading output quality.\nExtensive experiments highlight VoiceCloak's outstanding defense success rate\nagainst unauthorized diffusion-based voice cloning. Audio samples of VoiceCloak\nare available at https://voice-cloak.github.io/VoiceCloak/."}
{"id": "2505.12337", "pdf": "https://arxiv.org/pdf/2505.12337", "abs": "https://arxiv.org/abs/2505.12337", "authors": ["Junlin Song", "Miguel Olivares-Mendez"], "title": "Structureless VIO", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Visual odometry (VO) is typically considered as a chicken-and-egg problem, as\nthe localization and mapping modules are tightly-coupled. The estimation of\nvisual map relies on accurate localization information. Meanwhile, localization\nrequires precise map points to provide motion constraints. This classical\ndesign principle is naturally inherited by visual-inertial odometry (VIO).\nEfficient localization solution that does not require a map has not been fully\ninvestigated. To this end, we propose a novel structureless VIO, where the\nvisual map is removed from the odometry framework. Experimental results\ndemonstrated that, compared to the structure-based VIO baseline, our\nstructureless VIO not only substantially improves computational efficiency but\nalso has advantages in accuracy."}
{"id": "2505.12343", "pdf": "https://arxiv.org/pdf/2505.12343", "abs": "https://arxiv.org/abs/2505.12343", "authors": ["Kai Tang", "Jinhao You", "Xiuqi Ge", "Hanze Li", "Yichen Guo", "Xiande Huang"], "title": "Mitigating Hallucinations via Inter-Layer Consistency Aggregation in Large Vision-Language Models", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Despite the impressive capabilities of Large Vision-Language Models (LVLMs),\nthey remain susceptible to hallucinations-generating content that is\ninconsistent with the input image. Existing training-free hallucination\nmitigation methods often suffer from unstable performance and high sensitivity\nto hyperparameter settings, limiting their practicality and broader adoption.\nIn this paper, we propose a novel decoding mechanism, Decoding with Inter-layer\nConsistency via Layer Aggregation (DCLA), which requires no retraining,\nfine-tuning, or access to external knowledge bases. Specifically, our approach\nconstructs a dynamic semantic reference by aggregating representations from\nprevious layers, and corrects semantically deviated layers to enforce\ninter-layer consistency. The method allows DCLA to robustly mitigate\nhallucinations across multiple LVLMs. Experiments on hallucination benchmarks\nsuch as MME and POPE demonstrate that DCLA effectively reduces hallucinations\nwhile enhancing the reliability and performance of LVLMs."}
{"id": "2505.12359", "pdf": "https://arxiv.org/pdf/2505.12359", "abs": "https://arxiv.org/abs/2505.12359", "authors": ["Yichen Guo", "Hanze Li", "Zonghao Zhang", "Jinhao You", "Kai Tang", "Xiande Huang"], "title": "STAR: Stage-Wise Attention-Guided Token Reduction for Efficient Large Vision-Language Models Inference", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Although large vision-language models (LVLMs) leverage rich visual token\nrepresentations to achieve strong performance on multimodal tasks, these tokens\nalso introduce significant computational overhead during inference. Existing\ntraining-free token pruning methods typically adopt a single-stage strategy,\nfocusing either on visual self-attention or visual-textual cross-attention.\nHowever, such localized perspectives often overlook the broader information\nflow across the model, leading to substantial performance degradation,\nespecially under high pruning ratios. In this work, we propose STAR (Stage-wise\nAttention-guided token Reduction), a training-free, plug-and-play framework\nthat approaches token pruning from a global perspective. Instead of pruning at\na single point, STAR performs attention-guided reduction in two complementary\nstages: an early-stage pruning based on visual self-attention to remove\nredundant low-level features, and a later-stage pruning guided by cross-modal\nattention to discard task-irrelevant tokens. This holistic approach allows STAR\nto significantly reduce computational cost while better preserving\ntask-critical information. Extensive experiments across multiple LVLM\narchitectures and benchmarks show that STAR achieves strong acceleration while\nmaintaining comparable, and in some cases even improved performance."}
{"id": "2505.12373", "pdf": "https://arxiv.org/pdf/2505.12373", "abs": "https://arxiv.org/abs/2505.12373", "authors": ["Kapil Dev"], "title": "Modeling Aesthetic Preferences in 3D Shapes: A Large-Scale Paired Comparison Study Across Object Categories", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": "11 pages, 8 figures, submitted to IEEE Transactions on Visualization\n  and Computer Graphics (TVCG)", "summary": "Human aesthetic preferences for 3D shapes are central to industrial design,\nvirtual reality, and consumer product development. However, most computational\nmodels of 3D aesthetics lack empirical grounding in large-scale human\njudgments, limiting their practical relevance. We present a large-scale study\nof human preferences. We collected 22,301 pairwise comparisons across five\nobject categories (chairs, tables, mugs, lamps, and dining chairs) via Amazon\nMechanical Turk. Building on a previously published\ndataset~\\cite{dev2020learning}, we introduce new non-linear modeling and\ncross-category analysis to uncover the geometric drivers of aesthetic\npreference. We apply the Bradley-Terry model to infer latent aesthetic scores\nand use Random Forests with SHAP analysis to identify and interpret the most\ninfluential geometric features (e.g., symmetry, curvature, compactness). Our\ncross-category analysis reveals both universal principles and domain-specific\ntrends in aesthetic preferences. We focus on human interpretable geometric\nfeatures to ensure model transparency and actionable design insights, rather\nthan relying on black-box deep learning approaches. Our findings bridge\ncomputational aesthetics and cognitive science, providing practical guidance\nfor designers and a publicly available dataset to support reproducibility. This\nwork advances the understanding of 3D shape aesthetics through a human-centric,\ndata-driven framework."}
{"id": "2505.12418", "pdf": "https://arxiv.org/pdf/2505.12418", "abs": "https://arxiv.org/abs/2505.12418", "authors": ["Yuanpeng He", "Yali Bi", "Lijian Li", "Chi-Man Pun", "Wenpin Jiao", "Zhi Jin"], "title": "Mutual Evidential Deep Learning for Medical Image Segmentation", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Existing semi-supervised medical segmentation co-learning frameworks have\nrealized that model performance can be diminished by the biases in model\nrecognition caused by low-quality pseudo-labels. Due to the averaging nature of\ntheir pseudo-label integration strategy, they fail to explore the reliability\nof pseudo-labels from different sources. In this paper, we propose a mutual\nevidential deep learning (MEDL) framework that offers a potentially viable\nsolution for pseudo-label generation in semi-supervised learning from two\nperspectives. First, we introduce networks with different architectures to\ngenerate complementary evidence for unlabeled samples and adopt an improved\nclass-aware evidential fusion to guide the confident synthesis of evidential\npredictions sourced from diverse architectural networks. Second, utilizing the\nuncertainty in the fused evidence, we design an asymptotic Fisher\ninformation-based evidential learning strategy. This strategy enables the model\nto initially focus on unlabeled samples with more reliable pseudo-labels,\ngradually shifting attention to samples with lower-quality pseudo-labels while\navoiding over-penalization of mislabeled classes in high data uncertainty\nsamples. Additionally, for labeled data, we continue to adopt an\nuncertainty-driven asymptotic learning strategy, gradually guiding the model to\nfocus on challenging voxels. Extensive experiments on five mainstream datasets\nhave demonstrated that MEDL achieves state-of-the-art performance."}
{"id": "2505.12432", "pdf": "https://arxiv.org/pdf/2505.12432", "abs": "https://arxiv.org/abs/2505.12432", "authors": ["Zirun Guo", "Minjie Hong", "Tao Jin"], "title": "Observe-R1: Unlocking Reasoning Abilities of MLLMs with Dynamic Progressive Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Reinforcement Learning (RL) has shown promise in improving the reasoning\nabilities of Large Language Models (LLMs). However, the specific challenges of\nadapting RL to multimodal data and formats remain relatively unexplored. In\nthis work, we present Observe-R1, a novel framework aimed at enhancing the\nreasoning capabilities of multimodal large language models (MLLMs). We draw\ninspirations from human learning progression--from simple to complex and easy\nto difficult, and propose a gradual learning paradigm for MLLMs. To this end,\nwe construct the NeuraLadder dataset, which is organized and sampled according\nto the difficulty and complexity of data samples for RL training. To tackle\nmultimodal tasks, we introduce a multimodal format constraint that encourages\ncareful observation of images, resulting in enhanced visual abilities and\nclearer and more structured responses. Additionally, we implement a bonus\nreward system that favors concise, correct answers within a length constraint,\nalongside a dynamic weighting mechanism that prioritizes uncertain and\nmedium-difficulty problems, ensuring that more informative samples have a\ngreater impact on training. Our experiments with the Qwen2.5-VL-3B and\nQwen2.5-VL-7B models on 20k samples from the NeuraLadder dataset show that\nObserve-R1 outperforms a series of larger reasoning models on both reasoning\nand general benchmarks, achieving superior clarity and conciseness in reasoning\nchains. Ablation studies validate the effectiveness of our strategies,\nhighlighting the robustness and generalization of our approach. The dataset and\ncode will be released at https://github.com/zrguo/Observe-R1."}
{"id": "2505.12477", "pdf": "https://arxiv.org/pdf/2505.12477", "abs": "https://arxiv.org/abs/2505.12477", "authors": ["Hugues Van Assel", "Mark Ibrahim", "Tommaso Biancalani", "Aviv Regev", "Randall Balestriero"], "title": "Joint Embedding vs Reconstruction: Provable Benefits of Latent Space Prediction for Self Supervised Learning", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "33 pages, 9 figures", "summary": "Reconstruction and joint embedding have emerged as two leading paradigms in\nSelf Supervised Learning (SSL). Reconstruction methods focus on recovering the\noriginal sample from a different view in input space. On the other hand, joint\nembedding methods align the representations of different views in latent space.\nBoth approaches offer compelling advantages, yet practitioners lack clear\nguidelines for choosing between them. In this work, we unveil the core\nmechanisms that distinguish each paradigm. By leveraging closed form solutions\nfor both approaches, we precisely characterize how the view generation process,\ne.g. data augmentation, impacts the learned representations. We then\ndemonstrate that, unlike supervised learning, both SSL paradigms require a\nminimal alignment between augmentations and irrelevant features to achieve\nasymptotic optimality with increasing sample size. Our findings indicate that\nin scenarios where these irrelevant features have a large magnitude, joint\nembedding methods are preferable because they impose a strictly weaker\nalignment condition compared to reconstruction based methods. These results not\nonly clarify the trade offs between the two paradigms but also substantiate the\nempirical success of joint embedding approaches on real world challenging\ndatasets."}
{"id": "2505.12512", "pdf": "https://arxiv.org/pdf/2505.12512", "abs": "https://arxiv.org/abs/2505.12512", "authors": ["Truman Hickok"], "title": "Scalable Strategies for Continual Learning with Replay", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Future deep learning models will be distinguished by systems that perpetually\nlearn through interaction, imagination, and cooperation, blurring the line\nbetween training and inference. This makes continual learning a critical\nchallenge, as methods that efficiently maximize bidirectional transfer across\nlearning trajectories will be essential. Replay is on track to play a\nfoundational role in continual learning, allowing models to directly reconcile\nnew information with past knowledge. In practice, however, replay is quite\nunscalable, doubling the cost of continual learning when applied naively.\nMoreover, the continual learning literature has not fully synchronized with the\nmulti-task fine-tuning literature, having not fully integrated highly scalable\ntechniques like model merging and low rank adaptation into a replay-enabled\ntoolset that can produce a unified model in the face of many sequential tasks.\nIn this paper, we begin by applying and analyzing low rank adaptation in a\ncontinual learning setting. Next, we introduce consolidation, a phasic approach\nto replay which leads to up to 55\\% less replay samples being needed for a\ngiven performance target. Then, we propose sequential merging, an offshoot of\ntask arithmetic which is tailored to the continual learning setting and is\nshown to work well in combination with replay. Finally, we demonstrate that the\ndeveloped strategies can operate synergistically, resulting in a highly\nscalable toolset that outperforms standalone variants."}
{"id": "2505.12552", "pdf": "https://arxiv.org/pdf/2505.12552", "abs": "https://arxiv.org/abs/2505.12552", "authors": ["Junliang Ye", "Lei Wang", "Md Zakir Hossain"], "title": "FreqSelect: Frequency-Aware fMRI-to-Image Reconstruction", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": "Research report", "summary": "Reconstructing natural images from functional magnetic resonance imaging\n(fMRI) data remains a core challenge in natural decoding due to the mismatch\nbetween the richness of visual stimuli and the noisy, low resolution nature of\nfMRI signals. While recent two-stage models, combining deep variational\nautoencoders (VAEs) with diffusion models, have advanced this task, they treat\nall spatial-frequency components of the input equally. This uniform treatment\nforces the model to extract meaning features and suppress irrelevant noise\nsimultaneously, limiting its effectiveness. We introduce FreqSelect, a\nlightweight, adaptive module that selectively filters spatial-frequency bands\nbefore encoding. By dynamically emphasizing frequencies that are most\npredictive of brain activity and suppressing those that are uninformative,\nFreqSelect acts as a content-aware gate between image features and natural\ndata. It integrates seamlessly into standard very deep VAE-diffusion pipelines\nand requires no additional supervision. Evaluated on the Natural Scenes\ndataset, FreqSelect consistently improves reconstruction quality across both\nlow- and high-level metrics. Beyond performance gains, the learned\nfrequency-selection patterns offer interpretable insights into how different\nvisual frequencies are represented in the brain. Our method generalizes across\nsubjects and scenes, and holds promise for extension to other neuroimaging\nmodalities, offering a principled approach to enhancing both decoding accuracy\nand neuroscientific interpretability."}
{"id": "2505.12581", "pdf": "https://arxiv.org/pdf/2505.12581", "abs": "https://arxiv.org/abs/2505.12581", "authors": ["Lucas M. Dorneles", "Luan Fonseca Garcia", "Joel Lus Carbonera"], "title": "An approach based on class activation maps for investigating the effects of data augmentation on neural networks for image classification", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Neural networks have become increasingly popular in the last few years as an\neffective tool for the task of image classification due to the impressive\nperformance they have achieved on this task. In image classification tasks, it\nis common to use data augmentation strategies to increase the robustness of\ntrained networks to changes in the input images and to avoid overfitting.\nAlthough data augmentation is a widely adopted technique, the literature lacks\na body of research analyzing the effects data augmentation methods have on the\npatterns learned by neural network models working on complex datasets. The\nprimary objective of this work is to propose a methodology and set of metrics\nthat may allow a quantitative approach to analyzing the effects of data\naugmentation in convolutional networks applied to image classification. An\nimportant tool used in the proposed approach lies in the concept of class\nactivation maps for said models, which allow us to identify and measure the\nimportance these models assign to each individual pixel in an image when\nexecuting the classification task. From these maps, we may then extract metrics\nover the similarities and differences between maps generated by these models\ntrained on a given dataset with different data augmentation strategies.\nExperiments made using this methodology suggest that the effects of these data\naugmentation techniques not only can be analyzed in this way but also allow us\nto identify different impact profiles over the trained models."}
{"id": "2505.12642", "pdf": "https://arxiv.org/pdf/2505.12642", "abs": "https://arxiv.org/abs/2505.12642", "authors": ["Jung Hoon Lee", "Sujith Vijayan"], "title": "Two out of Three (ToT): using self-consistency to make robust predictions", "categories": ["cs.LG", "cs.CV"], "comment": "12 pages, 7 main figures, 1 supplementary table and 2 supplementary\n  figures", "summary": "Deep learning (DL) can automatically construct intelligent agents, deep\nneural networks (alternatively, DL models), that can outperform humans in\ncertain tasks. However, the operating principles of DL remain poorly\nunderstood, making its decisions incomprehensible. As a result, it poses a\ngreat risk to deploy DL in high-stakes domains in which mistakes or errors may\nlead to critical consequences. Here, we aim to develop an algorithm that can\nhelp DL models make more robust decisions by allowing them to abstain from\nanswering when they are uncertain. Our algorithm, named `Two out of Three\n(ToT)', is inspired by the sensitivity of the human brain to conflicting\ninformation. ToT creates two alternative predictions in addition to the\noriginal model prediction and uses the alternative predictions to decide\nwhether it should provide an answer or not."}
{"id": "2505.12681", "pdf": "https://arxiv.org/pdf/2505.12681", "abs": "https://arxiv.org/abs/2505.12681", "authors": ["Hana Satou", "Alan Mitkiy"], "title": "On the Mechanisms of Adversarial Data Augmentation for Robust and Adaptive Transfer Learning", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Transfer learning across domains with distribution shift remains a\nfundamental challenge in building robust and adaptable machine learning\nsystems. While adversarial perturbations are traditionally viewed as threats\nthat expose model vulnerabilities, recent studies suggest that they can also\nserve as constructive tools for data augmentation. In this work, we\nsystematically investigate the role of adversarial data augmentation (ADA) in\nenhancing both robustness and adaptivity in transfer learning settings. We\nanalyze how adversarial examples, when used strategically during training,\nimprove domain generalization by enriching decision boundaries and reducing\noverfitting to source-domain-specific features. We further propose a unified\nframework that integrates ADA with consistency regularization and\ndomain-invariant representation learning. Extensive experiments across multiple\nbenchmark datasets -- including VisDA, DomainNet, and Office-Home --\ndemonstrate that our method consistently improves target-domain performance\nunder both unsupervised and few-shot domain adaptation settings. Our results\nhighlight a constructive perspective of adversarial learning, transforming\nperturbation from a destructive attack into a regularizing force for\ncross-domain transferability."}
{"id": "2505.12748", "pdf": "https://arxiv.org/pdf/2505.12748", "abs": "https://arxiv.org/abs/2505.12748", "authors": ["Hangyu Li", "Qin Zhao", "Haoran Xu", "Xinyu Jiang", "Qingwei Ben", "Feiyu Jia", "Haoyu Zhao", "Liang Xu", "Jia Zeng", "Hanqing Wang", "Bo Dai", "Junting Dong", "Jiangmiao Pang"], "title": "TeleOpBench: A Simulator-Centric Benchmark for Dual-Arm Dexterous Teleoperation", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "13 pages", "summary": "Teleoperation is a cornerstone of embodied-robot learning, and bimanual\ndexterous teleoperation in particular provides rich demonstrations that are\ndifficult to obtain with fully autonomous systems. While recent studies have\nproposed diverse hardware pipelines-ranging from inertial motion-capture gloves\nto exoskeletons and vision-based interfaces-there is still no unified benchmark\nthat enables fair, reproducible comparison of these systems. In this paper, we\nintroduce TeleOpBench, a simulator-centric benchmark tailored to bimanual\ndexterous teleoperation. TeleOpBench contains 30 high-fidelity task\nenvironments that span pick-and-place, tool use, and collaborative\nmanipulation, covering a broad spectrum of kinematic and force-interaction\ndifficulty. Within this benchmark we implement four representative\nteleoperation modalities-(i) MoCap, (ii) VR device, (iii) arm-hand\nexoskeletons, and (iv) monocular vision tracking-and evaluate them with a\ncommon protocol and metric suite. To validate that performance in simulation is\npredictive of real-world behavior, we conduct mirrored experiments on a\nphysical dual-arm platform equipped with two 6-DoF dexterous hands. Across 10\nheld-out tasks we observe a strong correlation between simulator and hardware\nperformance, confirming the external validity of TeleOpBench. TeleOpBench\nestablishes a common yardstick for teleoperation research and provides an\nextensible platform for future algorithmic and hardware innovation."}
{"id": "2505.12751", "pdf": "https://arxiv.org/pdf/2505.12751", "abs": "https://arxiv.org/abs/2505.12751", "authors": ["Filippo Leveni"], "title": "Structure-based Anomaly Detection and Clustering", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "comment": "Doctoral dissertation at Politecnico di Milano", "summary": "Anomaly detection is a fundamental problem in domains such as healthcare,\nmanufacturing, and cybersecurity. This thesis proposes new unsupervised methods\nfor anomaly detection in both structured and streaming data settings. In the\nfirst part, we focus on structure-based anomaly detection, where normal data\nfollows low-dimensional manifolds while anomalies deviate from them. We\nintroduce Preference Isolation Forest (PIF), which embeds data into a\nhigh-dimensional preference space via manifold fitting, and isolates outliers\nusing two variants: Voronoi-iForest, based on geometric distances, and\nRuzHash-iForest, leveraging Locality Sensitive Hashing for scalability. We also\npropose Sliding-PIF, which captures local manifold information for streaming\nscenarios. Our methods outperform existing techniques on synthetic and real\ndatasets. We extend this to structure-based clustering with MultiLink, a novel\nmethod for recovering multiple geometric model families in noisy data.\nMultiLink merges clusters via a model-aware linkage strategy, enabling robust\nmulti-class structure recovery. It offers key advantages over existing\napproaches, such as speed, reduced sensitivity to thresholds, and improved\nrobustness to poor initial sampling. The second part of the thesis addresses\nonline anomaly detection in evolving data streams. We propose Online Isolation\nForest (Online-iForest), which uses adaptive, multi-resolution histograms and\ndynamically updates tree structures to track changes over time. It avoids\nretraining while achieving accuracy comparable to offline models, with superior\nefficiency for real-time applications. Finally, we tackle anomaly detection in\ncybersecurity via open-set recognition for malware classification. We enhance a\nGradient Boosting classifier with MaxLogit to detect unseen malware families, a\nmethod now integrated into Cleafy's production system."}
{"id": "2505.12774", "pdf": "https://arxiv.org/pdf/2505.12774", "abs": "https://arxiv.org/abs/2505.12774", "authors": ["Zichen Geng", "Zeeshan Hayder", "Wei Liu", "Ajmal Mian"], "title": "UniHM: Universal Human Motion Generation with Object Interactions in Indoor Scenes", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": null, "summary": "Human motion synthesis in complex scenes presents a fundamental challenge,\nextending beyond conventional Text-to-Motion tasks by requiring the integration\nof diverse modalities such as static environments, movable objects, natural\nlanguage prompts, and spatial waypoints. Existing language-conditioned motion\nmodels often struggle with scene-aware motion generation due to limitations in\nmotion tokenization, which leads to information loss and fails to capture the\ncontinuous, context-dependent nature of 3D human movement. To address these\nissues, we propose UniHM, a unified motion language model that leverages\ndiffusion-based generation for synthesizing scene-aware human motion. UniHM is\nthe first framework to support both Text-to-Motion and Text-to-Human-Object\nInteraction (HOI) in complex 3D scenes. Our approach introduces three key\ncontributions: (1) a mixed-motion representation that fuses continuous 6DoF\nmotion with discrete local motion tokens to improve motion realism; (2) a novel\nLook-Up-Free Quantization VAE (LFQ-VAE) that surpasses traditional VQ-VAEs in\nboth reconstruction accuracy and generative performance; and (3) an enriched\nversion of the Lingo dataset augmented with HumanML3D annotations, providing\nstronger supervision for scene-specific motion learning. Experimental results\ndemonstrate that UniHM achieves comparative performance on the OMOMO benchmark\nfor text-to-HOI synthesis and yields competitive results on HumanML3D for\ngeneral text-conditioned motion generation."}
{"id": "2505.12782", "pdf": "https://arxiv.org/pdf/2505.12782", "abs": "https://arxiv.org/abs/2505.12782", "authors": ["Kai Zhang", "Xingyu Chen", "Xiaofeng Zhang"], "title": "AdaToken-3D: Dynamic Spatial Gating for Efficient 3D Large Multimodal-Models Reasoning", "categories": ["cs.GR", "cs.CV", "cs.IR", "cs.IT", "math.IT"], "comment": null, "summary": "Large Multimodal Models (LMMs) have become a pivotal research focus in deep\nlearning, demonstrating remarkable capabilities in 3D scene understanding.\nHowever, current 3D LMMs employing thousands of spatial tokens for multimodal\nreasoning suffer from critical inefficiencies: excessive computational overhead\nand redundant information flows. Unlike 2D VLMs processing single images, 3D\nLMMs exhibit inherent architectural redundancy due to the heterogeneous\nmechanisms between spatial tokens and visual tokens. To address this challenge,\nwe propose AdaToken-3D, an adaptive spatial token optimization framework that\ndynamically prunes redundant tokens through spatial contribution analysis. Our\nmethod automatically tailors pruning strategies to different 3D LMM\narchitectures by quantifying token-level information flows via attention\npattern mining. Extensive experiments on LLaVA-3D (a 7B parameter 3D-LMM)\ndemonstrate that AdaToken-3D achieves 21\\% faster inference speed and 63\\%\nFLOPs reduction while maintaining original task accuracy. Beyond efficiency\ngains, this work systematically investigates redundancy patterns in multimodal\nspatial information flows through quantitative token interaction analysis. Our\nfindings reveal that over 60\\% of spatial tokens contribute minimally ($<$5\\%)\nto the final predictions, establishing theoretical foundations for efficient 3D\nmultimodal learning."}
{"id": "2505.12835", "pdf": "https://arxiv.org/pdf/2505.12835", "abs": "https://arxiv.org/abs/2505.12835", "authors": ["Hengxing Cai", "Jinhan Dong", "Jingjun Tan", "Jingcheng Deng", "Sihang Li", "Zhifeng Gao", "Haidong Wang", "Zicheng Su", "Agachai Sumalee", "Renxin Zhong"], "title": "FlightGPT: Towards Generalizable and Interpretable UAV Vision-and-Language Navigation with Vision-Language Models", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Unmanned Aerial Vehicle (UAV) Vision-and-Language Navigation (VLN) is vital\nfor applications such as disaster response, logistics delivery, and urban\ninspection. However, existing methods often struggle with insufficient\nmultimodal fusion, weak generalization, and poor interpretability. To address\nthese challenges, we propose FlightGPT, a novel UAV VLN framework built upon\nVision-Language Models (VLMs) with powerful multimodal perception capabilities.\nWe design a two-stage training pipeline: first, Supervised Fine-Tuning (SFT)\nusing high-quality demonstrations to improve initialization and structured\nreasoning; then, Group Relative Policy Optimization (GRPO) algorithm, guided by\na composite reward that considers goal accuracy, reasoning quality, and format\ncompliance, to enhance generalization and adaptability. Furthermore, FlightGPT\nintroduces a Chain-of-Thought (CoT)-based reasoning mechanism to improve\ndecision interpretability. Extensive experiments on the city-scale dataset\nCityNav demonstrate that FlightGPT achieves state-of-the-art performance across\nall scenarios, with a 9.22\\% higher success rate than the strongest baseline in\nunseen environments. Our implementation is publicly available."}
{"id": "2505.12836", "pdf": "https://arxiv.org/pdf/2505.12836", "abs": "https://arxiv.org/abs/2505.12836", "authors": ["Muhamed Kuric", "Martin Zach", "Andreas Habring", "Michael Unser", "Thomas Pock"], "title": "The Gaussian Latent Machine: Efficient Prior and Posterior Sampling for Inverse Problems", "categories": ["eess.IV", "cs.CV", "cs.LG", "stat.ML", "65C40, 65C05, 68U10, 65C60"], "comment": null, "summary": "We consider the problem of sampling from a product-of-experts-type model that\nencompasses many standard prior and posterior distributions commonly found in\nBayesian imaging. We show that this model can be easily lifted into a novel\nlatent variable model, which we refer to as a Gaussian latent machine. This\nleads to a general sampling approach that unifies and generalizes many existing\nsampling algorithms in the literature. Most notably, it yields a highly\nefficient and effective two-block Gibbs sampling approach in the general case,\nwhile also specializing to direct sampling algorithms in particular cases.\nFinally, we present detailed numerical experiments that demonstrate the\nefficiency and effectiveness of our proposed sampling approach across a wide\nrange of prior and posterior sampling problems from Bayesian imaging."}
{"id": "2505.12863", "pdf": "https://arxiv.org/pdf/2505.12863", "abs": "https://arxiv.org/abs/2505.12863", "authors": ["Jongmin Jung", "Dongmin Kim", "Sihun Lee", "Seola Cho", "Hyungjoon Soh", "Irmak Bukey", "Chris Donahue", "Dasaem Jeong"], "title": "Unified Cross-modal Translation of Score Images, Symbolic Music, and Performance Audio", "categories": ["cs.SD", "cs.AI", "cs.CV", "eess.AS"], "comment": "Submitted to IEEE Transactions on Audio, Speech and Language\n  Processing (TASLPRO)", "summary": "Music exists in various modalities, such as score images, symbolic scores,\nMIDI, and audio. Translations between each modality are established as core\ntasks of music information retrieval, such as automatic music transcription\n(audio-to-MIDI) and optical music recognition (score image to symbolic score).\nHowever, most past work on multimodal translation trains specialized models on\nindividual translation tasks. In this paper, we propose a unified approach,\nwhere we train a general-purpose model on many translation tasks\nsimultaneously. Two key factors make this unified approach viable: a new\nlarge-scale dataset and the tokenization of each modality. Firstly, we propose\na new dataset that consists of more than 1,300 hours of paired audio-score\nimage data collected from YouTube videos, which is an order of magnitude larger\nthan any existing music modal translation datasets. Secondly, our unified\ntokenization framework discretizes score images, audio, MIDI, and MusicXML into\na sequence of tokens, enabling a single encoder-decoder Transformer to tackle\nmultiple cross-modal translation as one coherent sequence-to-sequence task.\nExperimental results confirm that our unified multitask model improves upon\nsingle-task baselines in several key areas, notably reducing the symbol error\nrate for optical music recognition from 24.58% to a state-of-the-art 13.67%,\nwhile similarly substantial improvements are observed across the other\ntranslation tasks. Notably, our approach achieves the first successful\nscore-image-conditioned audio generation, marking a significant breakthrough in\ncross-modal music generation."}
{"id": "2505.12884", "pdf": "https://arxiv.org/pdf/2505.12884", "abs": "https://arxiv.org/abs/2505.12884", "authors": ["Yuanze Hu", "Zhaoxin Fan", "Xinyu Wang", "Gen Li", "Ye Qiu", "Zhichao Yang", "Wenjun Wu", "Kejian Wu", "Yifan Sun", "Xiaotie Deng", "Jin Dong"], "title": "TinyAlign: Boosting Lightweight Vision-Language Models by Mitigating Modal Alignment Bottlenecks", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Lightweight Vision-Language Models (VLMs) are indispensable for\nresource-constrained applications. The prevailing approach to aligning vision\nand language models involves freezing both the vision encoder and the language\nmodel while training small connector modules. However, this strategy heavily\ndepends on the intrinsic capabilities of the language model, which can be\nsuboptimal for lightweight models with limited representational capacity. In\nthis work, we investigate this alignment bottleneck through the lens of mutual\ninformation, demonstrating that the constrained capacity of the language model\ninherently limits the Effective Mutual Information (EMI) between multimodal\ninputs and outputs, thereby compromising alignment quality. To address this\nchallenge, we propose TinyAlign, a novel framework inspired by\nRetrieval-Augmented Generation, which strategically retrieves relevant context\nfrom a memory bank to enrich multimodal inputs and enhance their alignment.\nExtensive empirical evaluations reveal that TinyAlign significantly reduces\ntraining loss, accelerates convergence, and enhances task performance.\nRemarkably, it allows models to achieve baseline-level performance with only\n40\\% of the fine-tuning data, highlighting exceptional data efficiency. Our\nwork thus offers a practical pathway for developing more capable lightweight\nVLMs while introducing a fresh theoretical lens to better understand and\naddress alignment bottlenecks in constrained multimodal systems."}
{"id": "2505.12887", "pdf": "https://arxiv.org/pdf/2505.12887", "abs": "https://arxiv.org/abs/2505.12887", "authors": ["Junzhi Ning", "Cheng Tang", "Kaijin Zhou", "Diping Song", "Lihao Liu", "Ming Hu", "Wei Li", "Yanzhou Su", "Tianbing Li", "Jiyao Liu", "Yejin", "Sheng Zhang", "Yuanfeng Ji", "Junjun He"], "title": "RetinaLogos: Fine-Grained Synthesis of High-Resolution Retinal Images Through Captions", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "The scarcity of high-quality, labelled retinal imaging data, which presents a\nsignificant challenge in the development of machine learning models for\nophthalmology, hinders progress in the field. To synthesise Colour Fundus\nPhotographs (CFPs), existing methods primarily relying on predefined disease\nlabels face significant limitations. However, current methods remain limited,\nthus failing to generate images for broader categories with diverse and\nfine-grained anatomical structures. To overcome these challenges, we first\nintroduce an innovative pipeline that creates a large-scale, synthetic\nCaption-CFP dataset comprising 1.4 million entries, called RetinaLogos-1400k.\nSpecifically, RetinaLogos-1400k uses large language models (LLMs) to describe\nretinal conditions and key structures, such as optic disc configuration,\nvascular distribution, nerve fibre layers, and pathological features.\nFurthermore, based on this dataset, we employ a novel three-step training\nframework, called RetinaLogos, which enables fine-grained semantic control over\nretinal images and accurately captures different stages of disease progression,\nsubtle anatomical variations, and specific lesion types. Extensive experiments\ndemonstrate state-of-the-art performance across multiple datasets, with 62.07%\nof text-driven synthetic images indistinguishable from real ones by\nophthalmologists. Moreover, the synthetic data improves accuracy by 10%-25% in\ndiabetic retinopathy grading and glaucoma detection, thereby providing a\nscalable solution to augment ophthalmic datasets."}
{"id": "2505.12944", "pdf": "https://arxiv.org/pdf/2505.12944", "abs": "https://arxiv.org/abs/2505.12944", "authors": ["Jan Hagnberger", "Daniel Musekamp", "Mathias Niepert"], "title": "CALM-PDE: Continuous and Adaptive Convolutions for Latent Space Modeling of Time-dependent PDEs", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.NE", "physics.comp-ph"], "comment": null, "summary": "Solving time-dependent Partial Differential Equations (PDEs) using a densely\ndiscretized spatial domain is a fundamental problem in various scientific and\nengineering disciplines, including modeling climate phenomena and fluid\ndynamics. However, performing these computations directly in the physical space\noften incurs significant computational costs. To address this issue, several\nneural surrogate models have been developed that operate in a compressed latent\nspace to solve the PDE. While these approaches reduce computational complexity,\nthey often use Transformer-based attention mechanisms to handle irregularly\nsampled domains, resulting in increased memory consumption. In contrast,\nconvolutional neural networks allow memory-efficient encoding and decoding but\nare limited to regular discretizations. Motivated by these considerations, we\npropose CALM-PDE, a model class that efficiently solves arbitrarily discretized\nPDEs in a compressed latent space. We introduce a novel continuous\nconvolution-based encoder-decoder architecture that uses an\nepsilon-neighborhood-constrained kernel and learns to apply the convolution\noperator to adaptive and optimized query points. We demonstrate the\neffectiveness of CALM-PDE on a diverse set of PDEs with both regularly and\nirregularly sampled spatial domains. CALM-PDE is competitive with or\noutperforms existing baseline methods while offering significant improvements\nin memory and inference time efficiency compared to Transformer-based methods."}
{"id": "2505.12963", "pdf": "https://arxiv.org/pdf/2505.12963", "abs": "https://arxiv.org/abs/2505.12963", "authors": ["Maksim I. Ivanov", "Olga E. Mendybaeva", "Yuri E. Karyakin", "Igor N. Glukhikh", "Aleksey V. Lebedev"], "title": "Segmentation of temporomandibular joint structures on mri images using neural networks for diagnosis of pathologies", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "10 pages, 10 figures", "summary": "This article explores the use of artificial intelligence for the diagnosis of\npathologies of the temporomandibular joint (TMJ), in particular, for the\nsegmentation of the articular disc on MRI images. The relevance of the work is\ndue to the high prevalence of TMJ pathologies, as well as the need to improve\nthe accuracy and speed of diagnosis in medical institutions. During the study,\nthe existing solutions (Diagnocat, MandSeg) were analyzed, which, as a result,\nare not suitable for studying the articular disc due to the orientation towards\nbone structures. To solve the problem, an original dataset was collected from\n94 images with the classes \"temporomandibular joint\" and \"jaw\". To increase the\namount of data, augmentation methods were used. After that, the models of\nU-Net, YOLOv8n, YOLOv11n and Roboflow neural networks were trained and\ncompared. The evaluation was carried out according to the Dice Score,\nPrecision, Sensitivity, Specificity, and Mean Average Precision metrics. The\nresults confirm the potential of using the Roboflow model for segmentation of\nthe temporomandibular joint. In the future, it is planned to develop an\nalgorithm for measuring the distance between the jaws and determining the\nposition of the articular disc, which will improve the diagnosis of TMJ\npathologies."}
{"id": "2505.12978", "pdf": "https://arxiv.org/pdf/2505.12978", "abs": "https://arxiv.org/abs/2505.12978", "authors": ["Yinzhe Wu", "Jiahao Huang", "Fanwen Wang", "Mengze Gao", "Congyu Liao", "Guang Yang", "Kawin Setsompop"], "title": "Enhancing Diffusion-Weighted Images (DWI) for Diffusion MRI: Is it Enough without Non-Diffusion-Weighted B=0 Reference?", "categories": ["eess.IV", "cs.CV"], "comment": "IEEE ISBI 2025", "summary": "Diffusion MRI (dMRI) is essential for studying brain microstructure, but\nhigh-resolution imaging remains challenging due to the inherent trade-offs\nbetween acquisition time and signal-to-noise ratio (SNR). Conventional methods\noften optimize only the diffusion-weighted images (DWIs) without considering\ntheir relationship with the non-diffusion-weighted (b=0) reference images.\nHowever, calculating diffusion metrics, such as the apparent diffusion\ncoefficient (ADC) and diffusion tensor with its derived metrics like fractional\nanisotropy (FA) and mean diffusivity (MD), relies on the ratio between each DWI\nand the b=0 image, which is crucial for clinical observation and diagnostics.\nIn this study, we demonstrate that solely enhancing DWIs using a conventional\npixel-wise mean squared error (MSE) loss is insufficient, as the error in ratio\nbetween generated DWIs and b=0 diverges. We propose a novel ratio loss, defined\nas the MSE loss between the predicted and ground-truth log of DWI/b=0 ratios.\nOur results show that incorporating the ratio loss significantly improves the\nconvergence of this ratio error, achieving lower ratio MSE and slightly\nenhancing the peak signal-to-noise ratio (PSNR) of generated DWIs. This leads\nto improved dMRI super-resolution and better preservation of b=0 ratio-based\nfeatures for the derivation of diffusion metrics."}
{"id": "2505.12999", "pdf": "https://arxiv.org/pdf/2505.12999", "abs": "https://arxiv.org/abs/2505.12999", "authors": ["Lorena Garcia-Foncillas Macias", "Aaron Kujawa", "Aya Elshalakany", "Jonathan Shapey", "Tom Vercauteren"], "title": "A generalisable head MRI defacing pipeline: Evaluation on 2,566 meningioma scans", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Reliable MRI defacing techniques to safeguard patient privacy while\npreserving brain anatomy are critical for research collaboration. Existing\nmethods often struggle with incomplete defacing or degradation of brain tissue\nregions. We present a robust, generalisable defacing pipeline for\nhigh-resolution MRI that integrates atlas-based registration with brain\nmasking. Our method was evaluated on 2,566 heterogeneous clinical scans for\nmeningioma and achieved a 99.92 per cent success rate (2,564/2,566) upon visual\ninspection. Excellent anatomical preservation is demonstrated with a Dice\nsimilarity coefficient of 0.9975 plus or minus 0.0023 between brain masks\nautomatically extracted from the original and defaced volumes. Source code is\navailable at https://github.com/cai4cai/defacing_pipeline."}
{"id": "2505.13081", "pdf": "https://arxiv.org/pdf/2505.13081", "abs": "https://arxiv.org/abs/2505.13081", "authors": ["Xiaoyu Yang", "Jie Lu", "En Yu"], "title": "Walking the Tightrope: Disentangling Beneficial and Detrimental Drifts in Non-Stationary Custom-Tuning", "categories": ["cs.LG", "cs.CV"], "comment": "17 pages, 5figures", "summary": "This paper uncovers a critical yet overlooked phenomenon in multi-modal large\nlanguage models (MLLMs): detrimental concept drift within chain-of-thought\n(CoT) reasoning during non-stationary reinforcement fine-tuning (RFT), where\nreasoning token distributions evolve unpredictably, thereby introducing\nsignificant biases in final predictions. To address this, we are pioneers in\nestablishing the theoretical bridge between concept drift theory and RFT\nprocesses by formalizing CoT's autoregressive token streams as non-stationary\ndistributions undergoing arbitrary temporal shifts. Leveraging this framework,\nwe propose a novel counterfact-aware RFT that systematically decouples\nbeneficial distribution adaptation from harmful concept drift through concept\ngraph-empowered LLM experts generating counterfactual reasoning trajectories.\nOur solution, Counterfactual Preference Optimization (CPO), enables stable RFT\nin non-stationary environments, particularly within the medical domain, through\ncustom-tuning of counterfactual-aware preference alignment. Extensive\nexperiments demonstrate our superior performance of robustness, generalization\nand coordination within RFT. Besides, we also contributed a large-scale dataset\nCXR-CounterFact (CCF), comprising 320,416 meticulously curated counterfactual\nreasoning trajectories derived from MIMIC-CXR. Our code and data are public."}
{"id": "2505.13152", "pdf": "https://arxiv.org/pdf/2505.13152", "abs": "https://arxiv.org/abs/2505.13152", "authors": ["Jonas Brenig", "Radu Timofte"], "title": "Higher fidelity perceptual image and video compression with a latent conditioned residual denoising diffusion model", "categories": ["eess.IV", "cs.CV"], "comment": "Accepted at AIM Workshop 2024 at ECCV 2024", "summary": "Denoising diffusion models achieved impressive results on several image\ngeneration tasks often outperforming GAN based models. Recently, the generative\ncapabilities of diffusion models have been employed for perceptual image\ncompression, such as in CDC. A major drawback of these diffusion-based methods\nis that, while producing impressive perceptual quality images they are dropping\nin fidelity/increasing the distortion to the original uncompressed images when\ncompared with other traditional or learned image compression schemes aiming for\nfidelity. In this paper, we propose a hybrid compression scheme optimized for\nperceptual quality, extending the approach of the CDC model with a decoder\nnetwork in order to reduce the impact on distortion metrics such as PSNR. After\nusing the decoder network to generate an initial image, optimized for\ndistortion, the latent conditioned diffusion model refines the reconstruction\nfor perceptual quality by predicting the residual. On standard benchmarks, we\nachieve up to +2dB PSNR fidelity improvements while maintaining comparable\nLPIPS and FID perceptual scores when compared with CDC. Additionally, the\napproach is easily extensible to video compression, where we achieve similar\nresults."}
{"id": "2505.13227", "pdf": "https://arxiv.org/pdf/2505.13227", "abs": "https://arxiv.org/abs/2505.13227", "authors": ["Tianbao Xie", "Jiaqi Deng", "Xiaochuan Li", "Junlin Yang", "Haoyuan Wu", "Jixuan Chen", "Wenjing Hu", "Xinyuan Wang", "Yuhui Xu", "Zekun Wang", "Yiheng Xu", "Junli Wang", "Doyen Sahoo", "Tao Yu", "Caiming Xiong"], "title": "Scaling Computer-Use Grounding via User Interface Decomposition and Synthesis", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC"], "comment": "49 pages, 13 figures", "summary": "Graphical user interface (GUI) grounding, the ability to map natural language\ninstructions to specific actions on graphical user interfaces, remains a\ncritical bottleneck in computer use agent development. Current benchmarks\noversimplify grounding tasks as short referring expressions, failing to capture\nthe complexity of real-world interactions that require software commonsense,\nlayout understanding, and fine-grained manipulation capabilities. To address\nthese limitations, we introduce OSWorld-G, a comprehensive benchmark comprising\n564 finely annotated samples across diverse task types including text matching,\nelement recognition, layout understanding, and precise manipulation.\nAdditionally, we synthesize and release the largest computer use grounding\ndataset Jedi, which contains 4 million examples through multi-perspective\ndecoupling of tasks. Our multi-scale models trained on Jedi demonstrate its\neffectiveness by outperforming existing approaches on ScreenSpot-v2,\nScreenSpot-Pro, and our OSWorld-G. Furthermore, we demonstrate that improved\ngrounding with Jedi directly enhances agentic capabilities of general\nfoundation models on complex computer tasks, improving from 5% to 27% on\nOSWorld. Through detailed ablation studies, we identify key factors\ncontributing to grounding performance and verify that combining specialized\ndata for different interface elements enables compositional generalization to\nnovel interfaces. All benchmark, data, checkpoints, and code are open-sourced\nand available at https://osworld-grounding.github.io."}
{"id": "2505.13232", "pdf": "https://arxiv.org/pdf/2505.13232", "abs": "https://arxiv.org/abs/2505.13232", "authors": ["Younghyun Kim", "Jongheon Jeong", "Sangkyung Kwak", "Kyungmin Lee", "Juho Lee", "Jinwoo Shin"], "title": "StarFT: Robust Fine-tuning of Zero-shot Models via Spuriosity Alignment", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "Learning robust representations from data often requires scale, which has led\nto the success of recent zero-shot models such as CLIP. However, the obtained\nrobustness can easily be deteriorated when these models are fine-tuned on other\ndownstream tasks (e.g., of smaller scales). Previous works often interpret this\nphenomenon in the context of domain shift, developing fine-tuning methods that\naim to preserve the original domain as much as possible. However, in a\ndifferent context, fine-tuned models with limited data are also prone to\nlearning features that are spurious to humans, such as background or texture.\nIn this paper, we propose StarFT (Spurious Textual Alignment Regularization), a\nnovel framework for fine-tuning zero-shot models to enhance robustness by\npreventing them from learning spuriosity. We introduce a regularization that\naligns the output distribution for spuriosity-injected labels with the original\nzero-shot model, ensuring that the model is not induced to extract irrelevant\nfeatures further from these descriptions.We leverage recent language models to\nget such spuriosity-injected labels by generating alternative textual\ndescriptions that highlight potentially confounding features.Extensive\nexperiments validate the robust generalization of StarFT and its emerging\nproperties: zero-shot group robustness and improved zero-shot classification.\nNotably, StarFT boosts both worst-group and average accuracy by 14.30% and\n3.02%, respectively, in the Waterbirds group shift scenario, where other robust\nfine-tuning baselines show even degraded performance."}
{"id": "2505.13289", "pdf": "https://arxiv.org/pdf/2505.13289", "abs": "https://arxiv.org/abs/2505.13289", "authors": ["Alonso Urbano", "David W. Romero", "Max Zimmer", "Sebastian Pokutta"], "title": "RECON: Robust symmetry discovery via Explicit Canonical Orientation Normalization", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Real-world data often exhibits unknown or approximate symmetries, yet\nexisting equivariant networks must commit to a fixed transformation group prior\nto training, e.g., continuous $SO(2)$ rotations. This mismatch degrades\nperformance when the actual data symmetries differ from those in the\ntransformation group. We introduce RECON, a framework to discover each input's\nintrinsic symmetry distribution from unlabeled data. RECON leverages class-pose\ndecompositions and applies a data-driven normalization to align arbitrary\nreference frames into a common natural pose, yielding directly comparable and\ninterpretable symmetry descriptors. We demonstrate effective symmetry discovery\non 2D image benchmarks and -- for the first time -- extend it to 3D\ntransformation groups, paving the way towards more flexible equivariant\nmodeling."}
{"id": "2505.13307", "pdf": "https://arxiv.org/pdf/2505.13307", "abs": "https://arxiv.org/abs/2505.13307", "authors": ["Qiguang Chen", "Libo Qin", "Jinhao Liu", "Yue Liao", "Jiaqi Wang", "Jingxuan Zhou", "Wanxiang Che"], "title": "RBF++: Quantifying and Optimizing Reasoning Boundaries across Measurable and Unmeasurable Capabilities for Chain-of-Thought Reasoning", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "Manuscript", "summary": "Chain-of-Thought (CoT) reasoning has proven effective in enhancing large\nlanguage models (LLMs) on complex tasks, spurring research into its underlying\nmechanisms. However, two primary challenges remain for real-world applications:\n(1) the lack of quantitative metrics and actionable guidelines for evaluating\nand optimizing measurable boundaries of CoT capability, and (2) the absence of\nmethods to assess boundaries of unmeasurable CoT capability, such as multimodal\nperception. To address these gaps, we introduce the Reasoning Boundary\nFramework++ (RBF++). To tackle the first challenge, we define the reasoning\nboundary (RB) as the maximum limit of CoT performance. We also propose a\ncombination law for RBs, enabling quantitative analysis and offering actionable\nguidance across various CoT tasks. For the second challenge, particularly in\nmultimodal scenarios, we introduce a constant assumption, which replaces\nunmeasurable RBs with scenario-specific constants. Additionally, we propose the\nreasoning boundary division mechanism, which divides unmeasurable RBs into two\nsub-boundaries, facilitating the quantification and optimization of both\nunmeasurable domain knowledge and multimodal perception capabilities. Extensive\nexperiments involving 38 models across 13 tasks validate the feasibility of our\nframework in cross-modal settings. Additionally, we evaluate 10 CoT strategies,\noffer insights into optimization and decay from two complementary perspectives,\nand expand evaluation benchmarks for measuring RBs in LLM reasoning. We hope\nthis work advances the understanding of RBs and optimization strategies in\nLLMs. Code and data are available at\nhttps://github.com/LightChen233/reasoning-boundary."}
{"id": "2505.13391", "pdf": "https://arxiv.org/pdf/2505.13391", "abs": "https://arxiv.org/abs/2505.13391", "authors": ["Mikoaj Makiski", "Jacek Madziuk"], "title": "Advancing Generalization Across a Variety of Abstract Visual Reasoning Tasks", "categories": ["cs.AI", "cs.CV", "cs.LG"], "comment": "Accepted to the 34th International Joint Conference on Artificial\n  Intelligence (IJCAI 2025)", "summary": "The abstract visual reasoning (AVR) domain presents a diverse suite of\nanalogy-based tasks devoted to studying model generalization. Recent years have\nbrought dynamic progress in the field, particularly in i.i.d. scenarios, in\nwhich models are trained and evaluated on the same data distributions.\nNevertheless, o.o.d. setups that assess model generalization to new test\ndistributions remain challenging even for the most recent models. To advance\ngeneralization in AVR tasks, we present the Pathways of Normalized Group\nConvolution model (PoNG), a novel neural architecture that features group\nconvolution, normalization, and a parallel design. We consider a wide set of\nAVR benchmarks, including Raven's Progressive Matrices and visual analogy\nproblems with both synthetic and real-world images. The experiments demonstrate\nstrong generalization capabilities of the proposed model, which in several\nsettings outperforms the existing literature methods."}
{"id": "2505.13414", "pdf": "https://arxiv.org/pdf/2505.13414", "abs": "https://arxiv.org/abs/2505.13414", "authors": ["Yaqian Chen", "Hanxue Gu", "Haoyu Dong", "Qihang Li", "Yuwen Chen", "Nicholas Konz", "Lin Li", "Maciej A. Mazurowski"], "title": "GuidedMorph: Two-Stage Deformable Registration for Breast MRI", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Accurately registering breast MR images from different time points enables\nthe alignment of anatomical structures and tracking of tumor progression,\nsupporting more effective breast cancer detection, diagnosis, and treatment\nplanning. However, the complexity of dense tissue and its highly non-rigid\nnature pose challenges for conventional registration methods, which primarily\nfocus on aligning general structures while overlooking intricate internal\ndetails. To address this, we propose \\textbf{GuidedMorph}, a novel two-stage\nregistration framework designed to better align dense tissue. In addition to a\nsingle-scale network for global structure alignment, we introduce a framework\nthat utilizes dense tissue information to track breast movement. The learned\ntransformation fields are fused by introducing the Dual Spatial Transformer\nNetwork (DSTN), improving overall alignment accuracy. A novel warping method\nbased on the Euclidean distance transform (EDT) is also proposed to accurately\nwarp the registered dense tissue and breast masks, preserving fine structural\ndetails during deformation. The framework supports paradigms that require\nexternal segmentation models and with image data only. It also operates\neffectively with the VoxelMorph and TransMorph backbones, offering a versatile\nsolution for breast registration. We validate our method on ISPY2 and internal\ndataset, demonstrating superior performance in dense tissue, overall breast\nalignment, and breast structural similarity index measure (SSIM), with notable\nimprovements by over 13.01% in dense tissue Dice, 3.13% in breast Dice, and\n1.21% in breast SSIM compared to the best learning-based baseline."}
{"id": "2505.13427", "pdf": "https://arxiv.org/pdf/2505.13427", "abs": "https://arxiv.org/abs/2505.13427", "authors": ["Lingxiao Du", "Fanqing Meng", "Zongkai Liu", "Zhixiang Zhou", "Ping Luo", "Qiaosheng Zhang", "Wenqi Shao"], "title": "MM-PRM: Enhancing Multimodal Mathematical Reasoning with Scalable Step-Level Supervision", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "While Multimodal Large Language Models (MLLMs) have achieved impressive\nprogress in vision-language understanding, they still struggle with complex\nmulti-step reasoning, often producing logically inconsistent or partially\ncorrect solutions. A key limitation lies in the lack of fine-grained\nsupervision over intermediate reasoning steps. To address this, we propose\nMM-PRM, a process reward model trained within a fully automated, scalable\nframework. We first build MM-Policy, a strong multimodal model trained on\ndiverse mathematical reasoning data. Then, we construct MM-K12, a curated\ndataset of 10,000 multimodal math problems with verifiable answers, which\nserves as seed data. Leveraging a Monte Carlo Tree Search (MCTS)-based\npipeline, we generate over 700k step-level annotations without human labeling.\nThe resulting PRM is used to score candidate reasoning paths in the Best-of-N\ninference setup and achieves significant improvements across both in-domain\n(MM-K12 test set) and out-of-domain (OlympiadBench, MathVista, etc.)\nbenchmarks. Further analysis confirms the effectiveness of soft labels, smaller\nlearning rates, and path diversity in optimizing PRM performance. MM-PRM\ndemonstrates that process supervision is a powerful tool for enhancing the\nlogical robustness of multimodal reasoning systems. We release all our codes\nand data at https://github.com/ModalMinds/MM-PRM."}
{"id": "2505.13430", "pdf": "https://arxiv.org/pdf/2505.13430", "abs": "https://arxiv.org/abs/2505.13430", "authors": ["Sifeng Shang", "Jiayi Zhou", "Chenyu Lin", "Minxian Li", "Kaiyang Zhou"], "title": "Fine-tuning Quantized Neural Networks with Zeroth-order Optimization", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": null, "summary": "As the size of large language models grows exponentially, GPU memory has\nbecome a bottleneck for adapting these models to downstream tasks. In this\npaper, we aim to push the limits of memory-efficient training by minimizing\nmemory usage on model weights, gradients, and optimizer states, within a\nunified framework. Our idea is to eliminate both gradients and optimizer states\nusing zeroth-order optimization, which approximates gradients by perturbing\nweights during forward passes to identify gradient directions. To minimize\nmemory usage on weights, we employ model quantization, e.g., converting from\nbfloat16 to int4. However, directly applying zeroth-order optimization to\nquantized weights is infeasible due to the precision gap between discrete\nweights and continuous gradients, which would otherwise require de-quantization\nand re-quantization. To overcome this challenge, we propose Quantized\nZeroth-order Optimization (QZO), a novel approach that perturbs the continuous\nquantization scale for gradient estimation and uses a directional derivative\nclipping method to stabilize training. QZO is orthogonal to both scalar-based\nand codebook-based post-training quantization methods. Compared to\nfull-parameter fine-tuning in bfloat16, QZO can reduce the total memory cost by\nmore than 18$\\times$ for 4-bit LLMs, and enables fine-tuning Llama-2-13B and\nStable Diffusion 3.5 Large within a single 24GB GPU."}
{"id": "2505.13444", "pdf": "https://arxiv.org/pdf/2505.13444", "abs": "https://arxiv.org/abs/2505.13444", "authors": ["Liyan Tang", "Grace Kim", "Xinyu Zhao", "Thom Lake", "Wenxuan Ding", "Fangcong Yin", "Prasann Singhal", "Manya Wadhwa", "Zeyu Leo Liu", "Zayne Sprague", "Ramya Namuduri", "Bodun Hu", "Juan Diego Rodriguez", "Puyuan Peng", "Greg Durrett"], "title": "ChartMuseum: Testing Visual Reasoning Capabilities of Large Vision-Language Models", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Chart understanding presents a unique challenge for large vision-language\nmodels (LVLMs), as it requires the integration of sophisticated textual and\nvisual reasoning capabilities. However, current LVLMs exhibit a notable\nimbalance between these skills, falling short on visual reasoning that is\ndifficult to perform in text. We conduct a case study using a synthetic dataset\nsolvable only through visual reasoning and show that model performance degrades\nsignificantly with increasing visual complexity, while human performance\nremains robust. We then introduce ChartMuseum, a new Chart Question Answering\n(QA) benchmark containing 1,162 expert-annotated questions spanning multiple\nreasoning types, curated from real-world charts across 184 sources,\nspecifically built to evaluate complex visual and textual reasoning. Unlike\nprior chart understanding benchmarks -- where frontier models perform similarly\nand near saturation -- our benchmark exposes a substantial gap between model\nand human performance, while effectively differentiating model capabilities:\nalthough humans achieve 93% accuracy, the best-performing model Gemini-2.5-Pro\nattains only 63.0%, and the leading open-source LVLM Qwen2.5-VL-72B-Instruct\nachieves only 38.5%. Moreover, on questions requiring primarily visual\nreasoning, all models experience a 35%-55% performance drop from\ntext-reasoning-heavy question performance. Lastly, our qualitative error\nanalysis reveals specific categories of visual reasoning that are challenging\nfor current LVLMs."}
{"id": "2505.13447", "pdf": "https://arxiv.org/pdf/2505.13447", "abs": "https://arxiv.org/abs/2505.13447", "authors": ["Zhengyang Geng", "Mingyang Deng", "Xingjian Bai", "J. Zico Kolter", "Kaiming He"], "title": "Mean Flows for One-step Generative Modeling", "categories": ["cs.LG", "cs.CV"], "comment": "Tech report", "summary": "We propose a principled and effective framework for one-step generative\nmodeling. We introduce the notion of average velocity to characterize flow\nfields, in contrast to instantaneous velocity modeled by Flow Matching methods.\nA well-defined identity between average and instantaneous velocities is derived\nand used to guide neural network training. Our method, termed the MeanFlow\nmodel, is self-contained and requires no pre-training, distillation, or\ncurriculum learning. MeanFlow demonstrates strong empirical performance: it\nachieves an FID of 3.43 with a single function evaluation (1-NFE) on ImageNet\n256x256 trained from scratch, significantly outperforming previous\nstate-of-the-art one-step diffusion/flow models. Our study substantially\nnarrows the gap between one-step diffusion/flow models and their multi-step\npredecessors, and we hope it will motivate future research to revisit the\nfoundations of these powerful models."}
