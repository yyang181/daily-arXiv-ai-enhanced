{"id": "2506.02010", "pdf": "https://arxiv.org/pdf/2506.02010", "abs": "https://arxiv.org/abs/2506.02010", "authors": ["Zehua Liu", "Xiaolou Li", "Chen Chen", "Lantian Li", "Dong Wang"], "title": "CNVSRC 2024: The Second Chinese Continuous Visual Speech Recognition Challenge", "categories": ["cs.CV", "cs.SD", "eess.AS"], "comment": "to be published in INTERSPEECH 2025", "summary": "This paper presents the second Chinese Continuous Visual Speech Recognition\nChallenge (CNVSRC 2024), which builds on CNVSRC 2023 to advance research in\nChinese Large Vocabulary Continuous Visual Speech Recognition (LVC-VSR). The\nchallenge evaluates two test scenarios: reading in recording studios and\nInternet speech. CNVSRC 2024 uses the same datasets as its predecessor CNVSRC\n2023, which involves CN-CVS for training and CNVSRC-Single/Multi for\ndevelopment and evaluation. However, CNVSRC 2024 introduced two key\nimprovements: (1) a stronger baseline system, and (2) an additional dataset,\nCN-CVS2-P1, for open tracks to improve data volume and diversity. The new\nchallenge has demonstrated several important innovations in data preprocessing,\nfeature extraction, model design, and training strategies, further pushing the\nstate-of-the-art in Chinese LVC-VSR. More details and resources are available\nat the official website."}
{"id": "2506.02011", "pdf": "https://arxiv.org/pdf/2506.02011", "abs": "https://arxiv.org/abs/2506.02011", "authors": ["Minjae Lee", "Minhyuk Seo", "Tingyu Qu", "Tinne Tuytelaars", "Jonghyun Choi"], "title": "OASIS: Online Sample Selection for Continual Visual Instruction Tuning", "categories": ["cs.CV"], "comment": null, "summary": "In continual visual instruction tuning (CVIT) scenarios, where multi-modal\ndata continuously arrive in an online streaming manner, training delays from\nlarge-scale data significantly hinder real-time adaptation. While existing data\nselection strategies reduce training overheads, they rely on pre-trained\nreference models, which are impractical in CVIT setups due to unknown future\ndata. Recent reference model-free online sample selection methods address this\nissue but typically select a fixed number of samples per batch (e.g., top-k),\ncausing them to suffer from distribution shifts where informativeness varies\nacross batches. To address these limitations, we propose OASIS, an adaptive\nonline sample selection approach for CVIT that: (1) dynamically adjusts\nselected samples per batch based on relative inter-batch informativeness, and\n(2) minimizes redundancy of selected samples through iterative selection score\nupdates. Empirical results across various MLLMs, such as LLaVA-1.5 and\nQwen-VL-2.5, show that OASIS achieves comparable performance to full-data\ntraining using only 25% of the data and outperforms the state-of-the-art."}
{"id": "2506.02012", "pdf": "https://arxiv.org/pdf/2506.02012", "abs": "https://arxiv.org/abs/2506.02012", "authors": ["Zehua Liu", "Xiaolou Li", "Li Guo", "Lantian Li", "Dong Wang"], "title": "Leveraging Large Language Models in Visual Speech Recognition: Model Scaling, Context-Aware Decoding, and Iterative Polishing", "categories": ["cs.CV", "cs.SD", "eess.AS"], "comment": null, "summary": "Visual Speech Recognition (VSR) transcribes speech by analyzing lip\nmovements. Recently, Large Language Models (LLMs) have been integrated into VSR\nsystems, leading to notable performance improvements. However, the potential of\nLLMs has not been extensively studied, and how to effectively utilize LLMs in\nVSR tasks remains unexplored. This paper systematically explores how to better\nleverage LLMs for VSR tasks and provides three key contributions: (1) Scaling\nTest: We study how the LLM size affects VSR performance, confirming a scaling\nlaw in the VSR task. (2) Context-Aware Decoding: We add contextual text to\nguide the LLM decoding, improving recognition accuracy. (3) Iterative\nPolishing: We propose iteratively refining LLM outputs, progressively reducing\nrecognition errors. Extensive experiments demonstrate that by these designs,\nthe great potential of LLMs can be largely harnessed, leading to significant\nVSR performance improvement."}
{"id": "2506.02014", "pdf": "https://arxiv.org/pdf/2506.02014", "abs": "https://arxiv.org/abs/2506.02014", "authors": ["Wang Mengjie", "Zhu Huiping", "Li Jian", "Shi Wenxiu", "Zhang Song"], "title": "Research on Driving Scenario Technology Based on Multimodal Large Lauguage Model Optimization", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "With the advancement of autonomous and assisted driving technologies, higher\ndemands are placed on the ability to understand complex driving scenarios.\nMultimodal general large models have emerged as a solution for this challenge.\nHowever, applying these models in vertical domains involves difficulties such\nas data collection, model training, and deployment optimization. This paper\nproposes a comprehensive method for optimizing multimodal models in driving\nscenarios, including cone detection, traffic light recognition, speed limit\nrecommendation, and intersection alerts. The method covers key aspects such as\ndynamic prompt optimization, dataset construction, model training, and\ndeployment. Specifically, the dynamic prompt optimization adjusts the prompts\nbased on the input image content to focus on objects affecting the ego vehicle,\nenhancing the model's task-specific focus and judgment capabilities. The\ndataset is constructed by combining real and synthetic data to create a\nhigh-quality and diverse multimodal training dataset, improving the model's\ngeneralization in complex driving environments. In model training, advanced\ntechniques like knowledge distillation, dynamic fine-tuning, and quantization\nare integrated to reduce storage and computational costs while boosting\nperformance. Experimental results show that this systematic optimization method\nnot only significantly improves the model's accuracy in key tasks but also\nachieves efficient resource utilization, providing strong support for the\npractical application of driving scenario perception technologies."}
{"id": "2506.02015", "pdf": "https://arxiv.org/pdf/2506.02015", "abs": "https://arxiv.org/abs/2506.02015", "authors": ["Yoonjin Oh", "Yongjin Kim", "Hyomin Kim", "Donghwan Chi", "Sungwoong Kim"], "title": "Object-centric Self-improving Preference Optimization for Text-to-Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly improved both image understanding and generation capabilities.\nDespite these improvements, MLLMs still struggle with fine-grained visual\ncomprehension, particularly in text-to-image generation tasks. While preference\noptimization methods have been explored to address these limitations in image\nunderstanding tasks, their application to image generation remains largely\nunderexplored. To address this gap, we propose an Object-centric Self-improving\nPreference Optimization (OSPO) framework designed for text-to-image generation\nby MLLMs. OSPO leverages the intrinsic reasoning abilities of MLLMs without\nrequiring any external datasets or models. OSPO emphasizes the importance of\nhigh-quality preference pair data, which is critical for effective preference\noptimization. To achieve this, it introduces a self-improving mechanism that\nautonomously constructs object-level contrastive preference pairs through\nobject-centric prompt perturbation, densification and VQA scoring. This process\neliminates ambiguous or disproportionate variations commonly found in naively\ngenerated preference pairs, thereby enhancing the effectiveness of preference\noptimization. We validate OSPO on three representative compositional\ntext-to-image benchmarks, demonstrating substantial performance gains over\nbaseline models."}
{"id": "2506.02016", "pdf": "https://arxiv.org/pdf/2506.02016", "abs": "https://arxiv.org/abs/2506.02016", "authors": ["Nuolin Sun", "Linyuan Wang", "Dongyang Li", "Bin Yan", "Lei Li"], "title": "Are classical deep neural networks weakly adversarially robust?", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Adversarial attacks have received increasing attention and it has been widely\nrecognized that classical DNNs have weak adversarial robustness. The most\ncommonly used adversarial defense method, adversarial training, improves the\nadversarial accuracy of DNNs by generating adversarial examples and retraining\nthe model. However, adversarial training requires a significant computational\noverhead. In this paper, inspired by existing studies focusing on the\nclustering properties of DNN output features at each layer and the Progressive\nFeedforward Collapse phenomenon, we propose a method for adversarial example\ndetection and image recognition that uses layer-wise features to construct\nfeature paths and computes the correlation between the examples feature paths\nand the class-centered feature paths. Experimental results show that the\nrecognition method achieves 82.77% clean accuracy and 44.17% adversarial\naccuracy on the ResNet-20 with PFC. Compared to the adversarial training method\nwith 77.64% clean accuracy and 52.94% adversarial accuracy, our method exhibits\na trade-off without relying on computationally expensive defense strategies.\nFurthermore, on the standard ResNet-18, our method maintains this advantage\nwith respective metrics of 80.01% and 46.1%. This result reveals inherent\nadversarial robustness in DNNs, challenging the conventional understanding of\nthe weak adversarial robustness in DNNs."}
{"id": "2506.02017", "pdf": "https://arxiv.org/pdf/2506.02017", "abs": "https://arxiv.org/abs/2506.02017", "authors": ["Camilla Quaresmini", "Giacomo Zanotti"], "title": "Fairness through Feedback: Addressing Algorithmic Misgendering in Automatic Gender Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Automatic Gender Recognition (AGR) systems are an increasingly widespread\napplication in the Machine Learning (ML) landscape. While these systems are\ntypically understood as detecting gender, they often classify datapoints based\non observable features correlated at best with either male or female sex. In\naddition to questionable binary assumptions, from an epistemological point of\nview, this is problematic for two reasons. First, there exists a gap between\nthe categories the system is meant to predict (woman versus man) and those onto\nwhich their output reasonably maps (female versus male). What is more, gender\ncannot be inferred on the basis of such observable features. This makes AGR\ntools often unreliable, especially in the case of non-binary and gender\nnon-conforming people. We suggest a theoretical and practical rethinking of AGR\nsystems. To begin, distinctions are made between sex, gender, and gender\nexpression. Then, we build upon the observation that, unlike algorithmic\nmisgendering, human-human misgendering is open to the possibility of\nre-evaluation and correction. We suggest that analogous dynamics should be\nrecreated in AGR, giving users the possibility to correct the system's output.\nWhile implementing such a feedback mechanism could be regarded as diminishing\nthe system's autonomy, it represents a way to significantly increase fairness\nlevels in AGR. This is consistent with the conceptual change of paradigm that\nwe advocate for AGR systems, which should be understood as tools respecting\nindividuals' rights and capabilities of self-expression and determination."}
{"id": "2506.02020", "pdf": "https://arxiv.org/pdf/2506.02020", "abs": "https://arxiv.org/abs/2506.02020", "authors": ["Youze Xue", "Dian Li", "Gang Liu"], "title": "Improve Multi-Modal Embedding Learning via Explicit Hard Negative Gradient Amplifying", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "With the rapid advancement of multi-modal large language models (MLLMs) in\nrecent years, the foundational Contrastive Language-Image Pretraining (CLIP)\nframework has been successfully extended to MLLMs, enabling more powerful and\nuniversal multi-modal embeddings for a wide range of retrieval tasks. Despite\nthese developments, the core contrastive learning paradigm remains largely\nunchanged from CLIP-style models to MLLMs. Within this framework, the effective\nmining of hard negative samples continues to be a critical factor for enhancing\nperformance. Prior works have introduced both offline and online strategies for\nhard negative mining to improve the efficiency of contrastive learning. While\nthese approaches have led to improved multi-modal embeddings, the specific\ncontribution of each hard negative sample to the learning process has not been\nthoroughly investigated. In this work, we conduct a detailed analysis of the\ngradients of the info-NCE loss with respect to the query, positive, and\nnegative samples, elucidating the role of hard negatives in updating model\nparameters. Building upon this analysis, we propose to explicitly amplify the\ngradients associated with hard negative samples, thereby encouraging the model\nto learn more discriminative embeddings. Our multi-modal embedding model,\ntrained with the proposed Explicit Gradient Amplifier and based on the\nLLaVA-OneVision-7B architecture, achieves state-of-the-art performance on the\nMMEB benchmark compared to previous methods utilizing the same MLLM backbone.\nFurthermore, when integrated with our self-developed MLLM, QQMM, our approach\nattains the top rank on the MMEB leaderboard. Code and models are released on\nhttps://github.com/QQ-MM/QQMM-embed."}
{"id": "2506.02021", "pdf": "https://arxiv.org/pdf/2506.02021", "abs": "https://arxiv.org/abs/2506.02021", "authors": ["Yinjie Zhao", "Heng Zhao", "Bihan Wen", "Yew-Soon Ong", "Joey Tianyi Zhou"], "title": "Dynamic-Aware Video Distillation: Optimizing Temporal Resolution Based on Video Semantics", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "With the rapid development of vision tasks and the scaling on datasets and\nmodels, redundancy reduction in vision datasets has become a key area of\nresearch. To address this issue, dataset distillation (DD) has emerged as a\npromising approach to generating highly compact synthetic datasets with\nsignificantly less redundancy while preserving essential information. However,\nwhile DD has been extensively studied for image datasets, DD on video datasets\nremains underexplored. Video datasets present unique challenges due to the\npresence of temporal information and varying levels of redundancy across\ndifferent classes. Existing DD approaches assume a uniform level of temporal\nredundancy across all different video semantics, which limits their\neffectiveness on video datasets. In this work, we propose Dynamic-Aware Video\nDistillation (DAViD), a Reinforcement Learning (RL) approach to predict the\noptimal Temporal Resolution of the synthetic videos. A teacher-in-the-loop\nreward function is proposed to update the RL agent policy. To the best of our\nknowledge, this is the first study to introduce adaptive temporal resolution\nbased on video semantics in video dataset distillation. Our approach\nsignificantly outperforms existing DD methods, demonstrating substantial\nimprovements in performance. This work paves the way for future research on\nmore efficient and semantic-adaptive video dataset distillation research."}
{"id": "2506.02022", "pdf": "https://arxiv.org/pdf/2506.02022", "abs": "https://arxiv.org/abs/2506.02022", "authors": ["Aditya Kanade", "Tanuja Ganu"], "title": "Do You See Me : A Multidimensional Benchmark for Evaluating Visual Perception in Multimodal LLMs", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) show reasoning promise, yet their\nvisual perception is a critical bottleneck. Strikingly, MLLMs can produce\ncorrect answers even while misinterpreting crucial visual elements, masking\nthese underlying failures. Our preliminary study on a joint\nperception-reasoning dataset revealed that for one leading MLLM, 29% of its\ncorrect answers to reasoning questions still exhibited visual perception\nerrors. To systematically address this, we introduce \"Do You See Me\", a\nscalable benchmark with 1,758 images and 2,612 questions. It spans seven\nhuman-psychology inspired subtasks in 2D and 3D, featuring controllable\ncomplexity to rigorously evaluate MLLM visual skills. Our findings on 3 leading\nclosed-source and 5 major open-source models reveal a stark deficit: humans\nachieve 96.49% accuracy, while top MLLMs average below 50%. This performance\ngap widens rapidly with increased task complexity (e.g., from 12% to 45% in the\nvisual form constancy subtask). Further analysis into the root causes suggests\nthat failures stem from challenges like misallocated visual attention and the\ninstability of internal representations for fine-grained details, especially at\nor below encoder patch resolution. This underscores an urgent need for MLLMs\nwith truly robust visual perception. The benchmark dataset, source code and\nevaluation scripts are available at https://github.com/microsoft/Do-You-See-Me."}
{"id": "2506.02095", "pdf": "https://arxiv.org/pdf/2506.02095", "abs": "https://arxiv.org/abs/2506.02095", "authors": ["Hyojin Bahng", "Caroline Chan", "Fredo Durand", "Phillip Isola"], "title": "Cycle Consistency as Reward: Learning Image-Text Alignment without Human Preferences", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Learning alignment between language and vision is a fundamental challenge,\nespecially as multimodal data becomes increasingly detailed and complex.\nExisting methods often rely on collecting human or AI preferences, which can be\ncostly and time-intensive. We propose an alternative approach that leverages\ncycle consistency as a supervisory signal. Given an image and generated text,\nwe map the text back to image space using a text-to-image model and compute the\nsimilarity between the original image and its reconstruction. Analogously, for\ntext-to-image generation, we measure the textual similarity between an input\ncaption and its reconstruction through the cycle. We use the cycle consistency\nscore to rank candidates and construct a preference dataset of 866K comparison\npairs. The reward model trained on our dataset outperforms state-of-the-art\nalignment metrics on detailed captioning, with superior inference-time\nscalability when used as a verifier for Best-of-N sampling. Furthermore,\nperforming DPO and Diffusion DPO using our dataset enhances performance across\na wide range of vision-language tasks and text-to-image generation. Our\ndataset, model, and code are at https://cyclereward.github.io"}
{"id": "2506.02112", "pdf": "https://arxiv.org/pdf/2506.02112", "abs": "https://arxiv.org/abs/2506.02112", "authors": ["Xuweiyi Chen", "Tian Xia", "Sihan Xu", "Jianing Yang", "Joyce Chai", "Zezhou Cheng"], "title": "SAB3R: Semantic-Augmented Backbone in 3D Reconstruction", "categories": ["cs.CV"], "comment": "Project page: https://uva-computer-vision-lab.github.io/sab3r/", "summary": "We introduce a new task, Map and Locate, which unifies the traditionally\ndistinct objectives of open-vocabulary segmentation - detecting and segmenting\nobject instances based on natural language queries - and 3D reconstruction, the\nprocess of estimating a scene's 3D structure from visual inputs. Specifically,\nMap and Locate involves generating a point cloud from an unposed video and\nsegmenting object instances based on open-vocabulary queries. This task serves\nas a critical step toward real-world embodied AI applications and introduces a\npractical task that bridges reconstruction, recognition and reorganization. To\ntackle this task, we introduce a simple yet effective baseline, which we denote\nas SAB3R. Our approach builds upon MASt3R, a recent breakthrough in 3D computer\nvision, and incorporates a lightweight distillation strategy. This method\ntransfers dense, per-pixel semantic features from 2D vision backbones (eg, CLIP\nand DINOv2) to enhance MASt3R's capabilities. Without introducing any auxiliary\nfrozen networks, our model generates per-pixel semantic features and constructs\ncohesive point maps in a single forward pass. Compared to separately deploying\nMASt3R and CLIP, our unified model, SAB3R, achieves superior performance on the\nMap and Locate benchmark. Furthermore, we evaluate SAB3R on both 2D semantic\nsegmentation and 3D tasks to comprehensively validate its effectiveness."}
{"id": "2506.02150", "pdf": "https://arxiv.org/pdf/2506.02150", "abs": "https://arxiv.org/abs/2506.02150", "authors": ["Stefano Fogarollo", "Gregor Laimer", "Reto Bale", "Matthias Harders"], "title": "Implicit Deformable Medical Image Registration with Learnable Kernels", "categories": ["cs.CV", "cs.AI"], "comment": "MICCAI 2025 Provisional Accept", "summary": "Deformable medical image registration is an essential task in\ncomputer-assisted interventions. This problem is particularly relevant to\noncological treatments, where precise image alignment is necessary for tracking\ntumor growth, assessing treatment response, and ensuring accurate delivery of\ntherapies. Recent AI methods can outperform traditional techniques in accuracy\nand speed, yet they often produce unreliable deformations that limit their\nclinical adoption. In this work, we address this challenge and introduce a\nnovel implicit registration framework that can predict accurate and reliable\ndeformations. Our insight is to reformulate image registration as a signal\nreconstruction problem: we learn a kernel function that can recover the dense\ndisplacement field from sparse keypoint correspondences. We integrate our\nmethod in a novel hierarchical architecture, and estimate the displacement\nfield in a coarse-to-fine manner. Our formulation also allows for efficient\nrefinement at test time, permitting clinicians to easily adjust registrations\nwhen needed. We validate our method on challenging intra-patient thoracic and\nabdominal zero-shot registration tasks, using public and internal datasets from\nthe local University Hospital. Our method not only shows competitive accuracy\nto state-of-the-art approaches, but also bridges the generalization gap between\nimplicit and explicit registration techniques. In particular, our method\ngenerates deformations that better preserve anatomical relationships and\nmatches the performance of specialized commercial systems, underscoring its\npotential for clinical adoption."}
{"id": "2506.02161", "pdf": "https://arxiv.org/pdf/2506.02161", "abs": "https://arxiv.org/abs/2506.02161", "authors": ["Xinyu Wei", "Jinrui Zhang", "Zeqing Wang", "Hongyang Wei", "Zhen Guo", "Lei Zhang"], "title": "TIIF-Bench: How Does Your T2I Model Follow Your Instructions?", "categories": ["cs.CV"], "comment": "23 pages, 12 figures, 11 tables", "summary": "The rapid advancements of Text-to-Image (T2I) models have ushered in a new\nphase of AI-generated content, marked by their growing ability to interpret and\nfollow user instructions. However, existing T2I model evaluation benchmarks\nfall short in limited prompt diversity and complexity, as well as coarse\nevaluation metrics, making it difficult to evaluate the fine-grained alignment\nperformance between textual instructions and generated images. In this paper,\nwe present TIIF-Bench (Text-to-Image Instruction Following Benchmark), aiming\nto systematically assess T2I models' ability in interpreting and following\nintricate textual instructions. TIIF-Bench comprises a set of 5000 prompts\norganized along multiple dimensions, which are categorized into three levels of\ndifficulties and complexities. To rigorously evaluate model robustness to\nvarying prompt lengths, we provide a short and a long version for each prompt\nwith identical core semantics. Two critical attributes, i.e., text rendering\nand style control, are introduced to evaluate the precision of text synthesis\nand the aesthetic coherence of T2I models. In addition, we collect 100\nhigh-quality designer level prompts that encompass various scenarios to\ncomprehensively assess model performance. Leveraging the world knowledge\nencoded in large vision language models, we propose a novel computable\nframework to discern subtle variations in T2I model outputs. Through meticulous\nbenchmarking of mainstream T2I models on TIIF-Bench, we analyze the pros and\ncons of current T2I models and reveal the limitations of current T2I\nbenchmarks. Project Page: https://a113n-w3i.github.io/TIIF_Bench/."}
{"id": "2506.02164", "pdf": "https://arxiv.org/pdf/2506.02164", "abs": "https://arxiv.org/abs/2506.02164", "authors": ["Yu", "Qian", "Wilson S. Geisler", "Xue-Xin Wei"], "title": "Quantifying task-relevant representational similarity using decision variable correlation", "categories": ["cs.CV", "cs.LG", "q-bio.NC", "q-bio.QM"], "comment": null, "summary": "Previous studies have compared the brain and deep neural networks trained on\nimage classification. Intriguingly, while some suggest that their\nrepresentations are highly similar, others argued the opposite. Here, we\npropose a new approach to characterize the similarity of the decision\nstrategies of two observers (models or brains) using decision variable\ncorrelation (DVC). DVC quantifies the correlation between decoded decisions on\nindividual samples in a classification task and thus can capture task-relevant\ninformation rather than general representational alignment. We evaluate this\nmethod using monkey V4/IT recordings and models trained on image classification\ntasks.\n  We find that model--model similarity is comparable to monkey--monkey\nsimilarity, whereas model--monkey similarity is consistently lower and,\nsurprisingly, decreases with increasing ImageNet-1k performance. While\nadversarial training enhances robustness, it does not improve model--monkey\nsimilarity in task-relevant dimensions; however, it markedly increases\nmodel--model similarity. Similarly, pre-training on larger datasets does not\nimprove model--monkey similarity. These results suggest a fundamental\ndivergence between the task-relevant representations in monkey V4/IT and those\nlearned by models trained on image classification tasks."}
{"id": "2506.02167", "pdf": "https://arxiv.org/pdf/2506.02167", "abs": "https://arxiv.org/abs/2506.02167", "authors": ["Aditi Tiwari", "Farzaneh Masoud", "Dac Trong Nguyen", "Jill Kraft", "Heng Ji", "Klara Nahrstedt"], "title": "Fire360: A Benchmark for Robust Perception and Episodic Memory in Degraded 360-Degree Firefighting Videos", "categories": ["cs.CV", "cs.AI"], "comment": "20 pages, 9 figures, 6 tables", "summary": "Modern AI systems struggle most in environments where reliability is critical\n- scenes with smoke, poor visibility, and structural deformation. Each year,\ntens of thousands of firefighters are injured on duty, often due to breakdowns\nin situational perception. We introduce Fire360, a benchmark for evaluating\nperception and reasoning in safety-critical firefighting scenarios. The dataset\nincludes 228 360-degree videos from professional training sessions under\ndiverse conditions (e.g., low light, thermal distortion), annotated with action\nsegments, object locations, and degradation metadata. Fire360 supports five\ntasks: Visual Question Answering, Temporal Action Captioning, Object\nLocalization, Safety-Critical Reasoning, and Transformed Object Retrieval\n(TOR). TOR tests whether models can match pristine exemplars to fire-damaged\ncounterparts in unpaired scenes, evaluating transformation-invariant\nrecognition. While human experts achieve 83.5% on TOR, models like GPT-4o lag\nsignificantly, exposing failures in reasoning under degradation. By releasing\nFire360 and its evaluation suite, we aim to advance models that not only see,\nbut also remember, reason, and act under uncertainty. The dataset is available\nat: https://uofi.box.com/v/fire360dataset."}
{"id": "2506.02221", "pdf": "https://arxiv.org/pdf/2506.02221", "abs": "https://arxiv.org/abs/2506.02221", "authors": ["Johannes Schusterbauer", "Ming Gui", "Frank Fundel", "Bj√∂rn Ommer"], "title": "Diff2Flow: Training Flow Matching Models via Diffusion Model Alignment", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted by CVPR 2025", "summary": "Diffusion models have revolutionized generative tasks through high-fidelity\noutputs, yet flow matching (FM) offers faster inference and empirical\nperformance gains. However, current foundation FM models are computationally\nprohibitive for finetuning, while diffusion models like Stable Diffusion\nbenefit from efficient architectures and ecosystem support. This work addresses\nthe critical challenge of efficiently transferring knowledge from pre-trained\ndiffusion models to flow matching. We propose Diff2Flow, a novel framework that\nsystematically bridges diffusion and FM paradigms by rescaling timesteps,\naligning interpolants, and deriving FM-compatible velocity fields from\ndiffusion predictions. This alignment enables direct and efficient FM\nfinetuning of diffusion priors with no extra computation overhead. Our\nexperiments demonstrate that Diff2Flow outperforms na\\\"ive FM and diffusion\nfinetuning particularly under parameter-efficient constraints, while achieving\nsuperior or competitive performance across diverse downstream tasks compared to\nstate-of-the-art methods. We will release our code at\nhttps://github.com/CompVis/diff2flow."}
{"id": "2506.02229", "pdf": "https://arxiv.org/pdf/2506.02229", "abs": "https://arxiv.org/abs/2506.02229", "authors": ["Manas Mehta", "Yimu Pan", "Kelly Gallagher", "Alison D. Gernand", "Jeffery A. Goldstein", "Delia Mwinyelle", "Leena Mithal", "James Z. Wang"], "title": "VLCD: Vision-Language Contrastive Distillation for Accurate and Efficient Automatic Placenta Analysis", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "Proceedings of the 9th International Workshop on Health Intelligence,\n  in conjunction with the Annual AAAI Conference on Artificial Intelligence,\n  Philadelphia, Pennsylvania, March 2025", "summary": "Pathological examination of the placenta is an effective method for detecting\nand mitigating health risks associated with childbirth. Recent advancements in\nAI have enabled the use of photographs of the placenta and pathology reports\nfor detecting and classifying signs of childbirth-related pathologies. However,\nexisting automated methods are computationally extensive, which limits their\ndeployability. We propose two modifications to vision-language contrastive\nlearning (VLC) frameworks to enhance their accuracy and efficiency: (1)\ntext-anchored vision-language contrastive knowledge distillation (VLCD)-a new\nknowledge distillation strategy for medical VLC pretraining, and (2)\nunsupervised predistillation using a large natural images dataset for improved\ninitialization. Our approach distills efficient neural networks that match or\nsurpass the teacher model in performance while achieving model compression and\nacceleration. Our results showcase the value of unsupervised predistillation in\nimproving the performance and robustness of our approach, specifically for\nlower-quality images. VLCD serves as an effective way to improve the efficiency\nand deployability of medical VLC approaches, making AI-based healthcare\nsolutions more accessible, especially in resource-constrained environments."}
{"id": "2506.02244", "pdf": "https://arxiv.org/pdf/2506.02244", "abs": "https://arxiv.org/abs/2506.02244", "authors": ["Bowen Xue", "Giuseppe Claudio Guarnera", "Shuang Zhao", "Zahra Montazeri"], "title": "Motion aware video generative model", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advances in diffusion-based video generation have yielded\nunprecedented quality in visual content and semantic coherence. However,\ncurrent approaches predominantly rely on statistical learning from vast\ndatasets without explicitly modeling the underlying physics of motion,\nresulting in subtle yet perceptible non-physical artifacts that diminish the\nrealism of generated videos. This paper introduces a physics-informed frequency\ndomain approach to enhance the physical plausibility of generated videos. We\nfirst conduct a systematic analysis of the frequency-domain characteristics of\ndiverse physical motions (translation, rotation, scaling), revealing that each\nmotion type exhibits distinctive and identifiable spectral signatures. Building\non this theoretical foundation, we propose two complementary components: (1) a\nphysical motion loss function that quantifies and optimizes the conformity of\ngenerated videos to ideal frequency-domain motion patterns, and (2) a frequency\ndomain enhancement module that progressively learns to adjust video features to\nconform to physical motion constraints while preserving original network\nfunctionality through a zero-initialization strategy. Experiments across\nmultiple video diffusion architectures demonstrate that our approach\nsignificantly enhances motion quality and physical plausibility without\ncompromising visual quality or semantic alignment. Our frequency-domain\nphysical motion framework generalizes effectively across different video\ngeneration architectures, offering a principled approach to incorporating\nphysical constraints into deep learning-based video synthesis pipelines. This\nwork seeks to establish connections between data-driven models and\nphysics-based motion models."}
{"id": "2506.02247", "pdf": "https://arxiv.org/pdf/2506.02247", "abs": "https://arxiv.org/abs/2506.02247", "authors": ["Yu Wang", "Juhyung Ha", "David J. Crandall"], "title": "PAIR-Net: Enhancing Egocentric Speaker Detection via Pretrained Audio-Visual Fusion and Alignment Loss", "categories": ["cs.CV"], "comment": "4 pages, 1 figure, and 1 table", "summary": "Active speaker detection (ASD) in egocentric videos presents unique\nchallenges due to unstable viewpoints, motion blur, and off-screen speech\nsources - conditions under which traditional visual-centric methods degrade\nsignificantly. We introduce PAIR-Net (Pretrained Audio-Visual Integration with\nRegularization Network), an effective model that integrates a partially frozen\nWhisper audio encoder with a fine-tuned AV-HuBERT visual backbone to robustly\nfuse cross-modal cues. To counteract modality imbalance, we introduce an\ninter-modal alignment loss that synchronizes audio and visual representations,\nenabling more consistent convergence across modalities. Without relying on\nmulti-speaker context or ideal frontal views, PAIR-Net achieves\nstate-of-the-art performance on the Ego4D ASD benchmark with 76.6% mAP,\nsurpassing LoCoNet and STHG by 8.2% and 12.9% mAP, respectively. Our results\nhighlight the value of pretrained audio priors and alignment-based fusion for\nrobust ASD under real-world egocentric conditions."}
{"id": "2506.02265", "pdf": "https://arxiv.org/pdf/2506.02265", "abs": "https://arxiv.org/abs/2506.02265", "authors": ["Samuel Li", "Pujith Kachana", "Prajwal Chidananda", "Saurabh Nair", "Yasutaka Furukawa", "Matthew Brown"], "title": "Rig3R: Rig-Aware Conditioning for Learned 3D Reconstruction", "categories": ["cs.CV"], "comment": null, "summary": "Estimating agent pose and 3D scene structure from multi-camera rigs is a\ncentral task in embodied AI applications such as autonomous driving. Recent\nlearned approaches such as DUSt3R have shown impressive performance in\nmultiview settings. However, these models treat images as unstructured\ncollections, limiting effectiveness in scenarios where frames are captured from\nsynchronized rigs with known or inferable structure.\n  To this end, we introduce Rig3R, a generalization of prior multiview\nreconstruction models that incorporates rig structure when available, and\nlearns to infer it when not. Rig3R conditions on optional rig metadata\nincluding camera ID, time, and rig poses to develop a rig-aware latent space\nthat remains robust to missing information. It jointly predicts pointmaps and\ntwo types of raymaps: a pose raymap relative to a global frame, and a rig\nraymap relative to a rig-centric frame consistent across time. Rig raymaps\nallow the model to infer rig structure directly from input images when metadata\nis missing.\n  Rig3R achieves state-of-the-art performance in 3D reconstruction, camera pose\nestimation, and rig discovery, outperforming both traditional and learned\nmethods by 17-45% mAA across diverse real-world rig datasets, all in a single\nforward pass without post-processing or iterative refinement."}
{"id": "2506.02291", "pdf": "https://arxiv.org/pdf/2506.02291", "abs": "https://arxiv.org/abs/2506.02291", "authors": ["Cristian-Ioan Blaga", "Paul Suganthan", "Sahil Dua", "Krishna Srinivasan", "Enrique Alfonseca", "Peter Dornbach", "Tom Duerig", "Imed Zitouni", "Zhe Dong"], "title": "Entity Image and Mixed-Modal Image Retrieval Datasets", "categories": ["cs.CV", "cs.IR"], "comment": null, "summary": "Despite advances in multimodal learning, challenging benchmarks for\nmixed-modal image retrieval that combines visual and textual information are\nlacking. This paper introduces a novel benchmark to rigorously evaluate image\nretrieval that demands deep cross-modal contextual understanding. We present\ntwo new datasets: the Entity Image Dataset (EI), providing canonical images for\nWikipedia entities, and the Mixed-Modal Image Retrieval Dataset (MMIR), derived\nfrom the WIT dataset. The MMIR benchmark features two challenging query types\nrequiring models to ground textual descriptions in the context of provided\nvisual entities: single entity-image queries (one entity image with descriptive\ntext) and multi-entity-image queries (multiple entity images with relational\ntext). We empirically validate the benchmark's utility as both a training\ncorpus and an evaluation set for mixed-modal retrieval. The quality of both\ndatasets is further affirmed through crowd-sourced human annotations. The\ndatasets are accessible through the GitHub page:\nhttps://github.com/google-research-datasets/wit-retrieval."}
{"id": "2506.02294", "pdf": "https://arxiv.org/pdf/2506.02294", "abs": "https://arxiv.org/abs/2506.02294", "authors": ["Niclas Popp", "Kevin Alexander Laube", "Matthias Hein", "Lukas Schott"], "title": "Improving Knowledge Distillation Under Unknown Covariate Shift Through Confidence-Guided Data Augmentation", "categories": ["cs.CV"], "comment": null, "summary": "Large foundation models trained on extensive datasets demonstrate strong\nzero-shot capabilities in various domains. To replicate their success when data\nand model size are constrained, knowledge distillation has become an\nestablished tool for transferring knowledge from foundation models to small\nstudent networks. However, the effectiveness of distillation is critically\nlimited by the available training data. This work addresses the common\npractical issue of covariate shift in knowledge distillation, where spurious\nfeatures appear during training but not at test time. We ask the question: when\nthese spurious features are unknown, yet a robust teacher is available, is it\npossible for a student to also become robust to them? We address this problem\nby introducing a novel diffusion-based data augmentation strategy that\ngenerates images by maximizing the disagreement between the teacher and the\nstudent, effectively creating challenging samples that the student struggles\nwith. Experiments demonstrate that our approach significantly improves worst\ngroup and mean group accuracy on CelebA and SpuCo Birds as well as the spurious\nmAUC on spurious ImageNet under covariate shift, outperforming state-of-the-art\ndiffusion-based data augmentation baselines"}
{"id": "2506.02295", "pdf": "https://arxiv.org/pdf/2506.02295", "abs": "https://arxiv.org/abs/2506.02295", "authors": ["Ahmed Wasfy", "Omer Nacar", "Abdelakreem Elkhateb", "Mahmoud Reda", "Omar Elshehy", "Adel Ammar", "Wadii Boulila"], "title": "QARI-OCR: High-Fidelity Arabic Text Recognition through Multimodal Large Language Model Adaptation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The inherent complexities of Arabic script; its cursive nature, diacritical\nmarks (tashkeel), and varied typography, pose persistent challenges for Optical\nCharacter Recognition (OCR). We present Qari-OCR, a series of vision-language\nmodels derived from Qwen2-VL-2B-Instruct, progressively optimized for Arabic\nthrough iterative fine-tuning on specialized synthetic datasets. Our leading\nmodel, QARI v0.2, establishes a new open-source state-of-the-art with a Word\nError Rate (WER) of 0.160, Character Error Rate (CER) of 0.061, and BLEU score\nof 0.737 on diacritically-rich texts. Qari-OCR demonstrates superior handling\nof tashkeel, diverse fonts, and document layouts, alongside impressive\nperformance on low-resolution images. Further explorations (QARI v0.3) showcase\nstrong potential for structural document understanding and handwritten text.\nThis work delivers a marked improvement in Arabic OCR accuracy and efficiency,\nwith all models and datasets released to foster further research."}
{"id": "2506.02327", "pdf": "https://arxiv.org/pdf/2506.02327", "abs": "https://arxiv.org/abs/2506.02327", "authors": ["Yijun Yang", "Zhao-Yang Wang", "Qiuping Liu", "Shuwen Sun", "Kang Wang", "Rama Chellappa", "Zongwei Zhou", "Alan Yuille", "Lei Zhu", "Yu-Dong Zhang", "Jieneng Chen"], "title": "Medical World Model: Generative Simulation of Tumor Evolution for Treatment Planning", "categories": ["cs.CV"], "comment": null, "summary": "Providing effective treatment and making informed clinical decisions are\nessential goals of modern medicine and clinical care. We are interested in\nsimulating disease dynamics for clinical decision-making, leveraging recent\nadvances in large generative models. To this end, we introduce the Medical\nWorld Model (MeWM), the first world model in medicine that visually predicts\nfuture disease states based on clinical decisions. MeWM comprises (i)\nvision-language models to serve as policy models, and (ii) tumor generative\nmodels as dynamics models. The policy model generates action plans, such as\nclinical treatments, while the dynamics model simulates tumor progression or\nregression under given treatment conditions. Building on this, we propose the\ninverse dynamics model that applies survival analysis to the simulated\npost-treatment tumor, enabling the evaluation of treatment efficacy and the\nselection of the optimal clinical action plan. As a result, the proposed MeWM\nsimulates disease dynamics by synthesizing post-treatment tumors, with\nstate-of-the-art specificity in Turing tests evaluated by radiologists.\nSimultaneously, its inverse dynamics model outperforms medical-specialized GPTs\nin optimizing individualized treatment protocols across all metrics. Notably,\nMeWM improves clinical decision-making for interventional physicians, boosting\nF1-score in selecting the optimal TACE protocol by 13%, paving the way for\nfuture integration of medical world models as the second readers."}
{"id": "2506.02334", "pdf": "https://arxiv.org/pdf/2506.02334", "abs": "https://arxiv.org/abs/2506.02334", "authors": ["Duo Liu", "Zhiquan Tan", "Linglan Zhao", "Zhongqiang Zhang", "Xiangzhong Fang", "Weiran Huang"], "title": "Generalized Category Discovery via Reciprocal Learning and Class-Wise Distribution Regularization", "categories": ["cs.CV"], "comment": "ICML2025 Poster", "summary": "Generalized Category Discovery (GCD) aims to identify unlabeled samples by\nleveraging the base knowledge from labeled ones, where the unlabeled set\nconsists of both base and novel classes. Since clustering methods are\ntime-consuming at inference, parametric-based approaches have become more\npopular. However, recent parametric-based methods suffer from inferior base\ndiscrimination due to unreliable self-supervision. To address this issue, we\npropose a Reciprocal Learning Framework (RLF) that introduces an auxiliary\nbranch devoted to base classification. During training, the main branch filters\nthe pseudo-base samples to the auxiliary branch. In response, the auxiliary\nbranch provides more reliable soft labels for the main branch, leading to a\nvirtuous cycle. Furthermore, we introduce Class-wise Distribution\nRegularization (CDR) to mitigate the learning bias towards base classes. CDR\nessentially increases the prediction confidence of the unlabeled data and\nboosts the novel class performance. Combined with both components, our proposed\nmethod, RLCD, achieves superior performance in all classes with negligible\nextra computation. Comprehensive experiments across seven GCD datasets validate\nits superiority. Our codes are available at https://github.com/APORduo/RLCD."}
{"id": "2506.02354", "pdf": "https://arxiv.org/pdf/2506.02354", "abs": "https://arxiv.org/abs/2506.02354", "authors": ["Junjie Li", "Nan Zhang", "Xiaoyang Qu", "Kai Lu", "Guokuan Li", "Jiguang Wan", "Jianzong Wang"], "title": "RATE-Nav: Region-Aware Termination Enhancement for Zero-shot Object Navigation with Vision-Language Models", "categories": ["cs.CV"], "comment": "Accepted by the 63rd Annual Meeting of the Association for\n  Computational Linguistics (ACL 2025)", "summary": "Object Navigation (ObjectNav) is a fundamental task in embodied artificial\nintelligence. Although significant progress has been made in semantic map\nconstruction and target direction prediction in current research, redundant\nexploration and exploration failures remain inevitable. A critical but\nunderexplored direction is the timely termination of exploration to overcome\nthese challenges. We observe a diminishing marginal effect between exploration\nsteps and exploration rates and analyze the cost-benefit relationship of\nexploration. Inspired by this, we propose RATE-Nav, a Region-Aware\nTermination-Enhanced method. It includes a geometric predictive region\nsegmentation algorithm and region-Based exploration estimation algorithm for\nexploration rate calculation. By leveraging the visual question answering\ncapabilities of visual language models (VLMs) and exploration rates enables\nefficient termination.RATE-Nav achieves a success rate of 67.8% and an SPL of\n31.3% on the HM3D dataset. And on the more challenging MP3D dataset, RATE-Nav\nshows approximately 10% improvement over previous zero-shot methods."}
{"id": "2506.02356", "pdf": "https://arxiv.org/pdf/2506.02356", "abs": "https://arxiv.org/abs/2506.02356", "authors": ["Woojeong Jin", "Seongchan Kim", "Seungryong Kim"], "title": "InterRVOS: Interaction-aware Referring Video Object Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Referring video object segmentation aims to segment the object in a video\ncorresponding to a given natural language expression. While prior works have\nexplored various referring scenarios, including motion-centric or\nmulti-instance expressions, most approaches still focus on localizing a single\ntarget object in isolation. However, in comprehensive video understanding, an\nobject's role is often defined by its interactions with other entities, which\nare largely overlooked in existing datasets and models. In this work, we\nintroduce Interaction-aware referring video object sgementation (InterRVOS), a\nnew task that requires segmenting both actor and target entities involved in an\ninteraction. Each interactoin is described through a pair of complementary\nexpressions from different semantic perspectives, enabling fine-grained\nmodeling of inter-object relationships. To tackle this task, we propose\nInterRVOS-8K, the large-scale and automatically constructed dataset containing\ndiverse interaction-aware expressions with corresponding masks, including\nchallenging cases such as motion-only multi-instance expressions. We also\npresent a baseline architecture, ReVIOSa, designed to handle actor-target\nsegmentation from a single expression, achieving strong performance in both\nstandard and interaction-focused settings. Furthermore, we introduce an\nactor-target-aware evalaution setting that enables a more targeted assessment\nof interaction understanding. Experimental results demonstrate that our\napproach outperforms prior methods in modeling complex object interactions for\nreferring video object segmentation task, establishing a strong foundation for\nfuture research in interaction-centric video understanding. Our project page is\navailable at\n\\href{https://cvlab-kaist.github.io/InterRVOS}{https://cvlab-kaist.github.io/InterRVOS}."}
{"id": "2506.02358", "pdf": "https://arxiv.org/pdf/2506.02358", "abs": "https://arxiv.org/abs/2506.02358", "authors": ["Tianze Wang", "Zhang Zhang", "Chao Sun"], "title": "RoadFormer : Local-Global Feature Fusion for Road Surface Classification in Autonomous Driving", "categories": ["cs.CV"], "comment": null, "summary": "The classification of the type of road surface (RSC) aims to utilize pavement\nfeatures to identify the roughness, wet and dry conditions, and material\ninformation of the road surface. Due to its ability to effectively enhance road\nsafety and traffic management, it has received widespread attention in recent\nyears. In autonomous driving, accurate RSC allows vehicles to better understand\nthe road environment, adjust driving strategies, and ensure a safer and more\nefficient driving experience. For a long time, vision-based RSC has been\nfavored. However, existing visual classification methods have overlooked the\nexploration of fine-grained classification of pavement types (such as similar\npavement textures). In this work, we propose a pure vision-based fine-grained\nRSC method for autonomous driving scenarios, which fuses local and global\nfeature information through the stacking of convolutional and transformer\nmodules. We further explore the stacking strategies of local and global feature\nextraction modules to find the optimal feature extraction strategy. In\naddition, since fine-grained tasks also face the challenge of relatively large\nintra-class differences and relatively small inter-class differences, we\npropose a Foreground-Background Module (FBM) that effectively extracts\nfine-grained context features of the pavement, enhancing the classification\nability for complex pavements. Experiments conducted on a large-scale pavement\ndataset containing one million samples and a simplified dataset reorganized\nfrom this dataset achieved Top-1 classification accuracies of 92.52% and\n96.50%, respectively, improving by 5.69% to 12.84% compared to SOTA methods.\nThese results demonstrate that RoadFormer outperforms existing methods in RSC\ntasks, providing significant progress in improving the reliability of pavement\nperception in autonomous driving systems."}
{"id": "2506.02359", "pdf": "https://arxiv.org/pdf/2506.02359", "abs": "https://arxiv.org/abs/2506.02359", "authors": ["Brent A. Griffin", "Manushree Gangwar", "Jacob Sela", "Jason J. Corso"], "title": "Auto-Labeling Data for Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "Great labels make great models. However, traditional labeling approaches for\ntasks like object detection have substantial costs at scale. Furthermore,\nalternatives to fully-supervised object detection either lose functionality or\nrequire larger models with prohibitive computational costs for inference at\nscale. To that end, this paper addresses the problem of training standard\nobject detection models without any ground truth labels. Instead, we configure\npreviously-trained vision-language foundation models to generate\napplication-specific pseudo \"ground truth\" labels. These auto-generated labels\ndirectly integrate with existing model training frameworks, and we subsequently\ntrain lightweight detection models that are computationally efficient. In this\nway, we avoid the costs of traditional labeling, leverage the knowledge of\nvision-language models, and keep the efficiency of lightweight models for\npractical application. We perform exhaustive experiments across multiple\nlabeling configurations, downstream inference models, and datasets to establish\nbest practices and set an extensive auto-labeling benchmark. From our results,\nwe find that our approach is a viable alternative to standard labeling in that\nit maintains competitive performance on multiple datasets and substantially\nreduces labeling time and costs."}
{"id": "2506.02364", "pdf": "https://arxiv.org/pdf/2506.02364", "abs": "https://arxiv.org/abs/2506.02364", "authors": ["Liang Li", "Jianli Zhao", "Sheng Fang", "Siyu Chen", "Hui Sun"], "title": "A TRPCA-Inspired Deep Unfolding Network for Hyperspectral Image Denoising via Thresholded t-SVD and Top-K Sparse Transformer", "categories": ["cs.CV"], "comment": "11 pages,6 figures", "summary": "Hyperspectral images (HSIs) are often degraded by complex mixed noise during\nacquisition and transmission, making effective denoising essential for\nsubsequent analysis. Recent hybrid approaches that bridge model-driven and\ndata-driven paradigms have shown great promise. However, most of these\napproaches lack effective alternation between different priors or modules,\nresulting in loosely coupled regularization and insufficient exploitation of\ntheir complementary strengths. Inspired by tensor robust principal component\nanalysis (TRPCA), we propose a novel deep unfolding network (DU-TRPCA) that\nenforces stage-wise alternation between two tightly integrated modules:\nlow-rank and sparse. The low-rank module employs thresholded tensor singular\nvalue decomposition (t-SVD), providing a widely adopted convex surrogate for\ntensor low-rankness and has been demonstrated to effectively capture the global\nspatial-spectral structure of HSIs. The Top-K sparse transformer module\nadaptively imposes sparse constraints, directly matching the sparse\nregularization in TRPCA and enabling effective removal of localized outliers\nand complex noise. This tightly coupled architecture preserves the stage-wise\nalternation between low-rank approximation and sparse refinement inherent in\nTRPCA, while enhancing representational capacity through attention mechanisms.\nExtensive experiments on synthetic and real-world HSIs demonstrate that\nDU-TRPCA surpasses state-of-the-art methods under severe mixed noise, while\noffering interpretability benefits and stable denoising dynamics inspired by\niterative optimization. Code is available at\nhttps://github.com/liangli97/TRPCA-Deep-Unfolding-HSI-Denoising."}
{"id": "2506.02366", "pdf": "https://arxiv.org/pdf/2506.02366", "abs": "https://arxiv.org/abs/2506.02366", "authors": ["Qin Xie", "Qinghua Zhang", "Shuyin Xia"], "title": "Approximate Borderline Sampling using Granular-Ball for Classification Tasks", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Data sampling enhances classifier efficiency and robustness through data\ncompression and quality improvement. Recently, the sampling method based on\ngranular-ball (GB) has shown promising performance in generality and noisy\nclassification tasks. However, some limitations remain, including the absence\nof borderline sampling strategies and issues with class boundary blurring or\nshrinking due to overlap between GBs. In this paper, an approximate borderline\nsampling method using GBs is proposed for classification tasks. First, a\nrestricted diffusion-based GB generation (RD-GBG) method is proposed, which\nprevents GB overlaps by constrained expansion, preserving precise geometric\nrepresentation of GBs via redefined ones. Second, based on the concept of\nheterogeneous nearest neighbor, a GB-based approximate borderline sampling\n(GBABS) method is proposed, which is the first general sampling method capable\nof both borderline sampling and improving the quality of class noise datasets.\nAdditionally, since RD-GBG incorporates noise detection and GBABS focuses on\nborderline samples, GBABS performs outstandingly on class noise datasets\nwithout the need for an optimal purity threshold. Experimental results\ndemonstrate that the proposed methods outperform the GB-based sampling method\nand several representative sampling methods. Our source code is publicly\navailable at https://github.com/CherylTse/GBABS."}
{"id": "2506.02367", "pdf": "https://arxiv.org/pdf/2506.02367", "abs": "https://arxiv.org/abs/2506.02367", "authors": ["Jiayi Su", "Dequan Jin"], "title": "ViTNF: Leveraging Neural Fields to Boost Vision Transformers in Generalized Category Discovery", "categories": ["cs.CV", "68T07", "I.5.1"], "comment": "22 pages, 3 figures", "summary": "Generalized category discovery (GCD) is a highly popular task in open-world\nrecognition, aiming to identify unknown class samples using known class data.\nBy leveraging pre-training, meta-training, and fine-tuning, ViT achieves\nexcellent few-shot learning capabilities. Its MLP head is a feedforward\nnetwork, trained synchronously with the entire network in the same process,\nincreasing the training cost and difficulty without fully leveraging the power\nof the feature extractor. This paper proposes a new architecture by replacing\nthe MLP head with a neural field-based one. We first present a new static\nneural field function to describe the activity distribution of the neural field\nand then use two static neural field functions to build an efficient few-shot\nclassifier. This neural field-based (NF) classifier consists of two coupled\nstatic neural fields. It stores the feature information of support samples by\nits elementary field, the known categories by its high-level field, and the\ncategory information of support samples by its cross-field connections. We\nreplace the MLP head with the proposed NF classifier, resulting in a novel\narchitecture ViTNF, and simplify the three-stage training mode by pre-training\nthe feature extractor on source tasks and training the NF classifier with\nsupport samples in meta-testing separately, significantly reducing ViT's demand\nfor training samples and the difficulty of model training. To enhance the\nmodel's capability in identifying new categories, we provide an effective\nalgorithm to determine the lateral interaction scale of the elementary field.\nExperimental results demonstrate that our model surpasses existing\nstate-of-the-art methods on CIFAR-100, ImageNet-100, CUB-200, and Standard\nCars, achieving dramatic accuracy improvements of 19\\% and 16\\% in new and all\nclasses, respectively, indicating a notable advantage in GCD."}
{"id": "2506.02382", "pdf": "https://arxiv.org/pdf/2506.02382", "abs": "https://arxiv.org/abs/2506.02382", "authors": ["Seulgi Kim", "Ghazal Kaviani", "Mohit Prabhushankar", "Ghassan AlRegib"], "title": "Multi-level and Multi-modal Action Anticipation", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted in 2025 IEEE International Conference on Image Processing\n  (ICIP)", "summary": "Action anticipation, the task of predicting future actions from partially\nobserved videos, is crucial for advancing intelligent systems. Unlike action\nrecognition, which operates on fully observed videos, action anticipation must\nhandle incomplete information. Hence, it requires temporal reasoning, and\ninherent uncertainty handling. While recent advances have been made,\ntraditional methods often focus solely on visual modalities, neglecting the\npotential of integrating multiple sources of information. Drawing inspiration\nfrom human behavior, we introduce \\textit{Multi-level and Multi-modal Action\nAnticipation (m\\&m-Ant)}, a novel multi-modal action anticipation approach that\ncombines both visual and textual cues, while explicitly modeling hierarchical\nsemantic information for more accurate predictions. To address the challenge of\ninaccurate coarse action labels, we propose a fine-grained label generator\npaired with a specialized temporal consistency loss function to optimize\nperformance. Extensive experiments on widely used datasets, including\nBreakfast, 50 Salads, and DARai, demonstrate the effectiveness of our approach,\nachieving state-of-the-art results with an average anticipation accuracy\nimprovement of 3.08\\% over existing methods. This work underscores the\npotential of multi-modal and hierarchical modeling in advancing action\nanticipation and establishes a new benchmark for future research in the field.\nOur code is available at: https://github.com/olivesgatech/mM-ant."}
{"id": "2506.02393", "pdf": "https://arxiv.org/pdf/2506.02393", "abs": "https://arxiv.org/abs/2506.02393", "authors": ["Yongxian Liu", "Boyang Li", "Ting Liu", "Zaiping Lin", "Wei An"], "title": "RRCANet: Recurrent Reusable-Convolution Attention Network for Infrared Small Target Detection", "categories": ["cs.CV"], "comment": null, "summary": "Infrared small target detection is a challenging task due to its unique\ncharacteristics (e.g., small, dim, shapeless and changeable). Recently\npublished CNN-based methods have achieved promising performance with heavy\nfeature extraction and fusion modules. To achieve efficient and effective\ndetection, we propose a recurrent reusable-convolution attention network\n(RRCA-Net) for infrared small target detection. Specifically, RRCA-Net\nincorporates reusable-convolution block (RuCB) in a recurrent manner without\nintroducing extra parameters. With the help of the repetitive iteration in\nRuCB, the high-level information of small targets in the deep layers can be\nwell maintained and further refined. Then, a dual interactive attention\naggregation module (DIAAM) is proposed to promote the mutual enhancement and\nfusion of refined information. In this way, RRCA-Net can both achieve\nhigh-level feature refinement and enhance the correlation of contextual\ninformation between adjacent layers. Moreover, to achieve steady convergence,\nwe design a target characteristic inspired loss function (DpT-k loss) by\nintegrating physical and mathematical constraints. Experimental results on\nthree benchmark datasets (e.g. NUAA-SIRST, IRSTD-1k, DenseSIRST) demonstrate\nthat our RRCA-Net can achieve comparable performance to the state-of-the-art\nmethods while maintaining a small number of parameters, and act as a plug and\nplay module to introduce consistent performance improvement for several popular\nIRSTD methods. Our code will be available at https://github.com/yongxianLiu/\nsoon."}
{"id": "2506.02395", "pdf": "https://arxiv.org/pdf/2506.02395", "abs": "https://arxiv.org/abs/2506.02395", "authors": ["Xiaofeng Cong", "Yu-Xin Zhang", "Haoran Wei", "Yeying Jin", "Junming Hou", "Jie Gui", "Jing Zhang", "Dacheng Tao"], "title": "The Devil is in the Darkness: Diffusion-Based Nighttime Dehazing Anchored in Brightness Perception", "categories": ["cs.CV"], "comment": null, "summary": "While nighttime image dehazing has been extensively studied, converting\nnighttime hazy images to daytime-equivalent brightness remains largely\nunaddressed. Existing methods face two critical limitations: (1) datasets\noverlook the brightness relationship between day and night, resulting in the\nbrightness mapping being inconsistent with the real world during image\nsynthesis; and (2) models do not explicitly incorporate daytime brightness\nknowledge, limiting their ability to reconstruct realistic lighting. To address\nthese challenges, we introduce the Diffusion-Based Nighttime Dehazing (DiffND)\nframework, which excels in both data synthesis and lighting reconstruction. Our\napproach starts with a data synthesis pipeline that simulates severe\ndistortions while enforcing brightness consistency between synthetic and\nreal-world scenes, providing a strong foundation for learning night-to-day\nbrightness mapping. Next, we propose a restoration model that integrates a\npre-trained diffusion model guided by a brightness perception network. This\ndesign harnesses the diffusion model's generative ability while adapting it to\nnighttime dehazing through brightness-aware optimization. Experiments validate\nour dataset's utility and the model's superior performance in joint haze\nremoval and brightness mapping."}
{"id": "2506.02396", "pdf": "https://arxiv.org/pdf/2506.02396", "abs": "https://arxiv.org/abs/2506.02396", "authors": ["Longyu Yang", "Ping Hu", "Shangbo Yuan", "Lu Zhang", "Jun Liu", "Hengtao Shen", "Xiaofeng Zhu"], "title": "Towards Explicit Geometry-Reflectance Collaboration for Generalized LiDAR Segmentation in Adverse Weather", "categories": ["cs.CV"], "comment": null, "summary": "Existing LiDAR semantic segmentation models often suffer from decreased\naccuracy when exposed to adverse weather conditions. Recent methods addressing\nthis issue focus on enhancing training data through weather simulation or\nuniversal augmentation techniques. However, few works have studied the negative\nimpacts caused by the heterogeneous domain shifts in the geometric structure\nand reflectance intensity of point clouds. In this paper, we delve into this\nchallenge and address it with a novel Geometry-Reflectance Collaboration (GRC)\nframework that explicitly separates feature extraction for geometry and\nreflectance. Specifically, GRC employs a dual-branch architecture designed to\nindependently process geometric and reflectance features initially, thereby\ncapitalizing on their distinct characteristic. Then, GRC adopts a robust\nmulti-level feature collaboration module to suppress redundant and unreliable\ninformation from both branches. Consequently, without complex simulation or\naugmentation, our method effectively extracts intrinsic information about the\nscene while suppressing interference, thus achieving better robustness and\ngeneralization in adverse weather conditions. We demonstrate the effectiveness\nof GRC through comprehensive experiments on challenging benchmarks, showing\nthat our method outperforms previous approaches and establishes new\nstate-of-the-art results."}
{"id": "2506.02405", "pdf": "https://arxiv.org/pdf/2506.02405", "abs": "https://arxiv.org/abs/2506.02405", "authors": ["Zhiya Tan", "Xin Zhang", "Joey Tianyi Zhou"], "title": "Modelship Attribution: Tracing Multi-Stage Manipulations Across Generative Models", "categories": ["cs.CV"], "comment": null, "summary": "As generative techniques become increasingly accessible, authentic visuals\nare frequently subjected to iterative alterations by various individuals\nemploying a variety of tools. Currently, to avoid misinformation and ensure\naccountability, a lot of research on detection and attribution is emerging.\nAlthough these methods demonstrate promise in single-stage manipulation\nscenarios, they fall short when addressing complex real-world iterative\nmanipulation. In this paper, we are the first, to the best of our knowledge, to\nsystematically model this real-world challenge and introduce a novel method to\nsolve it. We define a task called \"Modelship Attribution\", which aims to trace\nthe evolution of manipulated images by identifying the generative models\ninvolved and reconstructing the sequence of edits they performed. To\nrealistically simulate this scenario, we utilize three generative models,\nStyleMapGAN, DiffSwap, and FacePartsSwap, that sequentially modify distinct\nregions of the same image. This process leads to the creation of the first\nmodelship dataset, comprising 83,700 images (16,740 images*5). Given that later\nedits often overwrite the fingerprints of earlier models, the focus shifts from\nextracting blended fingerprints to characterizing each model's distinctive\nediting patterns. To tackle this challenge, we introduce the modelship\nattribution transformer (MAT), a purpose-built framework designed to\neffectively recognize and attribute the contributions of various models within\ncomplex, multi-stage manipulation workflows. Through extensive experiments and\ncomparative analysis with other related methods, our results, including\ncomprehensive ablation studies, demonstrate that the proposed approach is a\nhighly effective solution for modelship attribution."}
{"id": "2506.02408", "pdf": "https://arxiv.org/pdf/2506.02408", "abs": "https://arxiv.org/abs/2506.02408", "authors": ["Wenhao Tang", "Rong Qin", "Heng Fang", "Fengtao Zhou", "Hao Chen", "Xiang Li", "Ming-Ming Cheng"], "title": "Revisiting End-to-End Learning with Slide-level Supervision in Computational Pathology", "categories": ["cs.CV"], "comment": null, "summary": "Pre-trained encoders for offline feature extraction followed by multiple\ninstance learning (MIL) aggregators have become the dominant paradigm in\ncomputational pathology (CPath), benefiting cancer diagnosis and prognosis.\nHowever, performance limitations arise from the absence of encoder fine-tuning\nfor downstream tasks and disjoint optimization with MIL. While slide-level\nsupervised end-to-end (E2E) learning is an intuitive solution to this issue, it\nfaces challenges such as high computational demands and suboptimal results.\nThese limitations motivate us to revisit E2E learning. We argue that prior work\nneglects inherent E2E optimization challenges, leading to performance\ndisparities compared to traditional two-stage methods. In this paper, we\npioneer the elucidation of optimization challenge caused by sparse-attention\nMIL and propose a novel MIL called ABMILX. It mitigates this problem through\nglobal correlation-based attention refinement and multi-head mechanisms. With\nthe efficient multi-scale random patch sampling strategy, an E2E trained ResNet\nwith ABMILX surpasses SOTA foundation models under the two-stage paradigm\nacross multiple challenging benchmarks, while remaining computationally\nefficient (<10 RTX3090 hours). We show the potential of E2E learning in CPath\nand calls for greater research focus in this area. The code is\nhttps://github.com/DearCaat/E2E-WSI-ABMILX."}
{"id": "2506.02419", "pdf": "https://arxiv.org/pdf/2506.02419", "abs": "https://arxiv.org/abs/2506.02419", "authors": ["Nurislam Tursynbek", "Hastings Greer", "Basar Demir", "Marc Niethammer"], "title": "Guiding Registration with Emergent Similarity from Pre-Trained Diffusion Models", "categories": ["cs.CV"], "comment": "MICCAI 2025", "summary": "Diffusion models, while trained for image generation, have emerged as\npowerful foundational feature extractors for downstream tasks. We find that\noff-the-shelf diffusion models, trained exclusively to generate natural RGB\nimages, can identify semantically meaningful correspondences in medical images.\nBuilding on this observation, we propose to leverage diffusion model features\nas a similarity measure to guide deformable image registration networks. We\nshow that common intensity-based similarity losses often fail in challenging\nscenarios, such as when certain anatomies are visible in one image but absent\nin another, leading to anatomically inaccurate alignments. In contrast, our\nmethod identifies true semantic correspondences, aligning meaningful structures\nwhile disregarding those not present across images. We demonstrate superior\nperformance of our approach on two tasks: multimodal 2D registration (DXA to\nX-Ray) and monomodal 3D registration (brain-extracted to non-brain-extracted\nMRI). Code: https://github.com/uncbiag/dgir"}
{"id": "2506.02433", "pdf": "https://arxiv.org/pdf/2506.02433", "abs": "https://arxiv.org/abs/2506.02433", "authors": ["Weiheng Yao", "Xuhang Chen", "Shuqiang Wang"], "title": "Empowering Functional Neuroimaging: A Pre-trained Generative Framework for Unified Representation of Neural Signals", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal functional neuroimaging enables systematic analysis of brain\nmechanisms and provides discriminative representations for brain-computer\ninterface (BCI) decoding. However, its acquisition is constrained by high costs\nand feasibility limitations. Moreover, underrepresentation of specific groups\nundermines fairness of BCI decoding model. To address these challenges, we\npropose a unified representation framework for multimodal functional\nneuroimaging via generative artificial intelligence (AI). By mapping multimodal\nfunctional neuroimaging into a unified representation space, the proposed\nframework is capable of generating data for acquisition-constrained modalities\nand underrepresented groups. Experiments show that the framework can generate\ndata consistent with real brain activity patterns, provide insights into brain\nmechanisms, and improve performance on downstream tasks. More importantly, it\ncan enhance model fairness by augmenting data for underrepresented groups.\nOverall, the framework offers a new paradigm for decreasing the cost of\nacquiring multimodal functional neuroimages and enhancing the fairness of BCI\ndecoding models."}
{"id": "2506.02439", "pdf": "https://arxiv.org/pdf/2506.02439", "abs": "https://arxiv.org/abs/2506.02439", "authors": ["Shuang Li", "Jiaxu Leng", "Changjiang Kuang", "Mingpi Tan", "Xinbo Gao"], "title": "Video-Level Language-Driven Video-Based Visible-Infrared Person Re-Identification", "categories": ["cs.CV"], "comment": "Accepted by IEEE TIFS", "summary": "Video-based Visible-Infrared Person Re-Identification (VVI-ReID) aims to\nmatch pedestrian sequences across modalities by extracting modality-invariant\nsequence-level features. As a high-level semantic representation, language\nprovides a consistent description of pedestrian characteristics in both\ninfrared and visible modalities. Leveraging the Contrastive Language-Image\nPre-training (CLIP) model to generate video-level language prompts and guide\nthe learning of modality-invariant sequence-level features is theoretically\nfeasible. However, the challenge of generating and utilizing modality-shared\nvideo-level language prompts to address modality gaps remains a critical\nproblem. To address this problem, we propose a simple yet powerful framework,\nvideo-level language-driven VVI-ReID (VLD), which consists of two core modules:\ninvariant-modality language prompting (IMLP) and spatial-temporal prompting\n(STP). IMLP employs a joint fine-tuning strategy for the visual encoder and the\nprompt learner to effectively generate modality-shared text prompts and align\nthem with visual features from different modalities in CLIP's multimodal space,\nthereby mitigating modality differences. Additionally, STP models\nspatiotemporal information through two submodules, the spatial-temporal hub\n(STH) and spatial-temporal aggregation (STA), which further enhance IMLP by\nincorporating spatiotemporal information into text prompts. The STH aggregates\nand diffuses spatiotemporal information into the [CLS] token of each frame\nacross the vision transformer (ViT) layers, whereas STA introduces dedicated\nidentity-level loss and specialized multihead attention to ensure that the STH\nfocuses on identity-relevant spatiotemporal feature aggregation. The VLD\nframework achieves state-of-the-art results on two VVI-ReID benchmarks. The\ncode will be released at https://github.com/Visuang/VLD."}
{"id": "2506.02444", "pdf": "https://arxiv.org/pdf/2506.02444", "abs": "https://arxiv.org/abs/2506.02444", "authors": ["Lingwei Dang", "Ruizhi Shao", "Hongwen Zhang", "Wei Min", "Yebin Liu", "Qingyao Wu"], "title": "SViMo: Synchronized Diffusion for Video and Motion Generation in Hand-object Interaction Scenarios", "categories": ["cs.CV"], "comment": null, "summary": "Hand-Object Interaction (HOI) generation has significant application\npotential. However, current 3D HOI motion generation approaches heavily rely on\npredefined 3D object models and lab-captured motion data, limiting\ngeneralization capabilities. Meanwhile, HOI video generation methods prioritize\npixel-level visual fidelity, often sacrificing physical plausibility.\nRecognizing that visual appearance and motion patterns share fundamental\nphysical laws in the real world, we propose a novel framework that combines\nvisual priors and dynamic constraints within a synchronized diffusion process\nto generate the HOI video and motion simultaneously. To integrate the\nheterogeneous semantics, appearance, and motion features, our method implements\ntri-modal adaptive modulation for feature aligning, coupled with 3D\nfull-attention for modeling inter- and intra-modal dependencies. Furthermore,\nwe introduce a vision-aware 3D interaction diffusion model that generates\nexplicit 3D interaction sequences directly from the synchronized diffusion\noutputs, then feeds them back to establish a closed-loop feedback cycle. This\narchitecture eliminates dependencies on predefined object models or explicit\npose guidance while significantly enhancing video-motion consistency.\nExperimental results demonstrate our method's superiority over state-of-the-art\napproaches in generating high-fidelity, dynamically plausible HOI sequences,\nwith notable generalization capabilities in unseen real-world scenarios.\nProject page at\n\\href{https://github.com/Droliven}{https://github.com/Droliven}."}
{"id": "2506.02448", "pdf": "https://arxiv.org/pdf/2506.02448", "abs": "https://arxiv.org/abs/2506.02448", "authors": ["Baoyu Liang", "Qile Su", "Shoutai Zhu", "Yuchen Liang", "Chao Tong"], "title": "VidEvent: A Large Dataset for Understanding Dynamic Evolution of Events in Videos", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Despite the significant impact of visual events on human cognition,\nunderstanding events in videos remains a challenging task for AI due to their\ncomplex structures, semantic hierarchies, and dynamic evolution. To address\nthis, we propose the task of video event understanding that extracts event\nscripts and makes predictions with these scripts from videos. To support this\ntask, we introduce VidEvent, a large-scale dataset containing over 23,000\nwell-labeled events, featuring detailed event structures, broad hierarchies,\nand logical relations extracted from movie recap videos. The dataset was\ncreated through a meticulous annotation process, ensuring high-quality and\nreliable event data. We also provide comprehensive baseline models offering\ndetailed descriptions of their architecture and performance metrics. These\nmodels serve as benchmarks for future research, facilitating comparisons and\nimprovements. Our analysis of VidEvent and the baseline models highlights the\ndataset's potential to advance video event understanding and encourages the\nexploration of innovative algorithms and models. The dataset and related\nresources are publicly available at www.videvent.top."}
{"id": "2506.02452", "pdf": "https://arxiv.org/pdf/2506.02452", "abs": "https://arxiv.org/abs/2506.02452", "authors": ["Wenshuo Chen", "Kuimou Yu", "Haozhe Jia", "Kaishen Yuan", "Bowen Tian", "Songning Lai", "Hongru Xiao", "Erhang Zhang", "Lei Wang", "Yutao Yue"], "title": "ANT: Adaptive Neural Temporal-Aware Text-to-Motion Model", "categories": ["cs.CV"], "comment": null, "summary": "While diffusion models advance text-to-motion generation, their static\nsemantic conditioning ignores temporal-frequency demands: early denoising\nrequires structural semantics for motion foundations while later stages need\nlocalized details for text alignment. This mismatch mirrors biological\nmorphogenesis where developmental phases demand distinct genetic programs.\nInspired by epigenetic regulation governing morphological specialization, we\npropose **(ANT)**, an **A**daptive **N**eural **T**emporal-Aware architecture.\nANT orchestrates semantic granularity through: **(i) Semantic Temporally\nAdaptive (STA) Module:** Automatically partitions denoising into low-frequency\nstructural planning and high-frequency refinement via spectral analysis. **(ii)\nDynamic Classifier-Free Guidance scheduling (DCFG):** Adaptively adjusts\nconditional to unconditional ratio enhancing efficiency while maintaining\nfidelity. **(iii) Temporal-semantic reweighting:** Quantitatively aligns text\ninfluence with phase requirements. Extensive experiments show that ANT can be\napplied to various baselines, significantly improving model performance, and\nachieving state-of-the-art semantic alignment on StableMoFusion."}
{"id": "2506.02453", "pdf": "https://arxiv.org/pdf/2506.02453", "abs": "https://arxiv.org/abs/2506.02453", "authors": ["Kunyu Wang", "Xueyang Fu", "Yunfei Bao", "Chengjie Ge", "Chengzhi Cao", "Wei Zhai", "Zheng-Jun Zha"], "title": "PAID: Pairwise Angular-Invariant Decomposition for Continual Test-Time Adaptation", "categories": ["cs.CV"], "comment": null, "summary": "Continual Test-Time Adaptation (CTTA) aims to online adapt a pre-trained\nmodel to changing environments during inference. Most existing methods focus on\nexploiting target data, while overlooking another crucial source of\ninformation, the pre-trained weights, which encode underutilized\ndomain-invariant priors. This paper takes the geometric attributes of\npre-trained weights as a starting point, systematically analyzing three key\ncomponents: magnitude, absolute angle, and pairwise angular structure. We find\nthat the pairwise angular structure remains stable across diverse corrupted\ndomains and encodes domain-invariant semantic information, suggesting it should\nbe preserved during adaptation. Based on this insight, we propose PAID\n(Pairwise Angular-Invariant Decomposition), a prior-driven CTTA method that\ndecomposes weight into magnitude and direction, and introduces a learnable\northogonal matrix via Householder reflections to globally rotate direction\nwhile preserving the pairwise angular structure. During adaptation, only the\nmagnitudes and the orthogonal matrices are updated. PAID achieves consistent\nimprovements over recent SOTA methods on four widely used CTTA benchmarks,\ndemonstrating that preserving pairwise angular structure offers a simple yet\neffective principle for CTTA."}
{"id": "2506.02459", "pdf": "https://arxiv.org/pdf/2506.02459", "abs": "https://arxiv.org/abs/2506.02459", "authors": ["Martin JJ. Bucher", "Iro Armeni"], "title": "ReSpace: Text-Driven 3D Scene Synthesis and Editing with Preference Alignment", "categories": ["cs.CV", "I.2.10; I.2.7"], "comment": "20 pages, 17 figures (incl. appendix)", "summary": "Scene synthesis and editing has emerged as a promising direction in computer\ngraphics. Current trained approaches for 3D indoor scenes either oversimplify\nobject semantics through one-hot class encodings (e.g., 'chair' or 'table'),\nrequire masked diffusion for editing, ignore room boundaries, or rely on floor\nplan renderings that fail to capture complex layouts. In contrast, LLM-based\nmethods enable richer semantics via natural language (e.g., 'modern studio with\nlight wood furniture') but do not support editing, remain limited to\nrectangular layouts or rely on weak spatial reasoning from implicit world\nmodels. We introduce ReSpace, a generative framework for text-driven 3D indoor\nscene synthesis and editing using autoregressive language models. Our approach\nfeatures a compact structured scene representation with explicit room\nboundaries that frames scene editing as a next-token prediction task. We\nleverage a dual-stage training approach combining supervised fine-tuning and\npreference alignment, enabling a specially trained language model for object\naddition that accounts for user instructions, spatial geometry, object\nsemantics, and scene-level composition. For scene editing, we employ a\nzero-shot LLM to handle object removal and prompts for addition. We further\nintroduce a novel voxelization-based evaluation that captures fine-grained\ngeometry beyond 3D bounding boxes. Experimental results surpass\nstate-of-the-art on object addition while maintaining competitive results on\nfull scene synthesis."}
{"id": "2506.02462", "pdf": "https://arxiv.org/pdf/2506.02462", "abs": "https://arxiv.org/abs/2506.02462", "authors": ["Kunyu Wang", "Xueyang Fu", "Xin Lu", "Chengjie Ge", "Chengzhi Cao", "Wei Zhai", "Zheng-Jun Zha"], "title": "Efficient Test-time Adaptive Object Detection via Sensitivity-Guided Pruning", "categories": ["cs.CV"], "comment": "Accepted as CVPR 2025 oral paper", "summary": "Continual test-time adaptive object detection (CTTA-OD) aims to online adapt\na source pre-trained detector to ever-changing environments during inference\nunder continuous domain shifts. Most existing CTTA-OD methods prioritize\neffectiveness while overlooking computational efficiency, which is crucial for\nresource-constrained scenarios. In this paper, we propose an efficient CTTA-OD\nmethod via pruning. Our motivation stems from the observation that not all\nlearned source features are beneficial; certain domain-sensitive feature\nchannels can adversely affect target domain performance. Inspired by this, we\nintroduce a sensitivity-guided channel pruning strategy that quantifies each\nchannel based on its sensitivity to domain discrepancies at both image and\ninstance levels. We apply weighted sparsity regularization to selectively\nsuppress and prune these sensitive channels, focusing adaptation efforts on\ninvariant ones. Additionally, we introduce a stochastic channel reactivation\nmechanism to restore pruned channels, enabling recovery of potentially useful\nfeatures and mitigating the risks of early pruning. Extensive experiments on\nthree benchmarks show that our method achieves superior adaptation performance\nwhile reducing computational overhead by 12% in FLOPs compared to the recent\nSOTA method."}
{"id": "2506.02472", "pdf": "https://arxiv.org/pdf/2506.02472", "abs": "https://arxiv.org/abs/2506.02472", "authors": ["Halil Ismail Helvaci", "Justin Philip Huber", "Jihye Bae", "Sen-ching Samson Cheung"], "title": "HRTR: A Single-stage Transformer for Fine-grained Sub-second Action Segmentation in Stroke Rehabilitation", "categories": ["cs.CV"], "comment": null, "summary": "Stroke rehabilitation often demands precise tracking of patient movements to\nmonitor progress, with complexities of rehabilitation exercises presenting two\ncritical challenges: fine-grained and sub-second (under one-second) action\ndetection. In this work, we propose the High Resolution Temporal Transformer\n(HRTR), to time-localize and classify high-resolution (fine-grained),\nsub-second actions in a single-stage transformer, eliminating the need for\nmulti-stage methods and post-processing. Without any refinements, HRTR\noutperforms state-of-the-art systems on both stroke related and general\ndatasets, achieving Edit Score (ES) of 70.1 on StrokeRehab Video, 69.4 on\nStrokeRehab IMU, and 88.4 on 50Salads."}
{"id": "2506.02473", "pdf": "https://arxiv.org/pdf/2506.02473", "abs": "https://arxiv.org/abs/2506.02473", "authors": ["Xinran Nicole Han", "Ko Nishino", "Todd Zickler"], "title": "Generative Perception of Shape and Material from Differential Motion", "categories": ["cs.CV"], "comment": null, "summary": "Perceiving the shape and material of an object from a single image is\ninherently ambiguous, especially when lighting is unknown and unconstrained.\nDespite this, humans can often disentangle shape and material, and when they\nare uncertain, they often move their head slightly or rotate the object to help\nresolve the ambiguities. Inspired by this behavior, we introduce a novel\nconditional denoising-diffusion model that generates samples of\nshape-and-material maps from a short video of an object undergoing differential\nmotions. Our parameter-efficient architecture allows training directly in\npixel-space, and it generates many disentangled attributes of an object\nsimultaneously. Trained on a modest number of synthetic object-motion videos\nwith supervision on shape and material, the model exhibits compelling emergent\nbehavior: For static observations, it produces diverse, multimodal predictions\nof plausible shape-and-material maps that capture the inherent ambiguities; and\nwhen objects move, the distributions quickly converge to more accurate\nexplanations. The model also produces high-quality shape-and-material estimates\nfor less ambiguous, real-world objects. By moving beyond single-view to\ncontinuous motion observations, our work suggests a generative perception\napproach for improving visual reasoning in physically-embodied systems."}
{"id": "2506.02477", "pdf": "https://arxiv.org/pdf/2506.02477", "abs": "https://arxiv.org/abs/2506.02477", "authors": ["Kunyu Wang", "Xueyang Fu", "Chengzhi Cao", "Chengjie Ge", "Wei Zhai", "Zheng-Jun Zha"], "title": "Towards Better De-raining Generalization via Rainy Characteristics Memorization and Replay", "categories": ["cs.CV"], "comment": null, "summary": "Current image de-raining methods primarily learn from a limited dataset,\nleading to inadequate performance in varied real-world rainy conditions. To\ntackle this, we introduce a new framework that enables networks to\nprogressively expand their de-raining knowledge base by tapping into a growing\npool of datasets, significantly boosting their adaptability. Drawing\ninspiration from the human brain's ability to continuously absorb and\ngeneralize from ongoing experiences, our approach borrow the mechanism of the\ncomplementary learning system. Specifically, we first deploy Generative\nAdversarial Networks (GANs) to capture and retain the unique features of new\ndata, mirroring the hippocampus's role in learning and memory. Then, the\nde-raining network is trained with both existing and GAN-synthesized data,\nmimicking the process of hippocampal replay and interleaved learning.\nFurthermore, we employ knowledge distillation with the replayed data to\nreplicate the synergy between the neocortex's activity patterns triggered by\nhippocampal replays and the pre-existing neocortical knowledge. This\ncomprehensive framework empowers the de-raining network to amass knowledge from\nvarious datasets, continually enhancing its performance on previously unseen\nrainy scenes. Our testing on three benchmark de-raining networks confirms the\nframework's effectiveness. It not only facilitates continuous knowledge\naccumulation across six datasets but also surpasses state-of-the-art methods in\ngeneralizing to new real-world scenarios."}
{"id": "2506.02488", "pdf": "https://arxiv.org/pdf/2506.02488", "abs": "https://arxiv.org/abs/2506.02488", "authors": ["Hongtao Huang", "Xiaojun Chang", "Lina Yao"], "title": "Flexiffusion: Training-Free Segment-Wise Neural Architecture Search for Efficient Diffusion Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Diffusion models (DMs) are powerful generative models capable of producing\nhigh-fidelity images but are constrained by high computational costs due to\niterative multi-step inference. While Neural Architecture Search (NAS) can\noptimize DMs, existing methods are hindered by retraining requirements,\nexponential search complexity from step-wise optimization, and slow evaluation\nrelying on massive image generation. To address these challenges, we propose\nFlexiffusion, a training-free NAS framework that jointly optimizes generation\nschedules and model architectures without modifying pre-trained parameters. Our\nkey insight is to decompose the generation process into flexible segments of\nequal length, where each segment dynamically combines three step types: full\n(complete computation), partial (cache-reused computation), and null (skipped\ncomputation). This segment-wise search space reduces the candidate pool\nexponentially compared to step-wise NAS while preserving architectural\ndiversity. Further, we introduce relative FID (rFID), a lightweight evaluation\nmetric for NAS that measures divergence from a teacher model's outputs instead\nof ground truth, slashing evaluation time by over $90\\%$. In practice,\nFlexiffusion achieves at least $2\\times$ acceleration across LDMs, Stable\nDiffusion, and DDPMs on ImageNet and MS-COCO, with FID degradation under $5\\%$,\noutperforming prior NAS and caching methods. Notably, it attains $5.1\\times$\nspeedup on Stable Diffusion with near-identical CLIP scores. Our work pioneers\na resource-efficient paradigm for searching high-speed DMs without sacrificing\nquality."}
{"id": "2506.02492", "pdf": "https://arxiv.org/pdf/2506.02492", "abs": "https://arxiv.org/abs/2506.02492", "authors": ["Yuanpeng He", "Lijian Li", "Tianxiang Zhan", "Chi-Man Pun", "Wenpin Jiao", "Zhi Jin"], "title": "Co-Evidential Fusion with Information Volume for Medical Image Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Although existing semi-supervised image segmentation methods have achieved\ngood performance, they cannot effectively utilize multiple sources of\nvoxel-level uncertainty for targeted learning. Therefore, we propose two main\nimprovements. First, we introduce a novel pignistic co-evidential fusion\nstrategy using generalized evidential deep learning, extended by traditional\nD-S evidence theory, to obtain a more precise uncertainty measure for each\nvoxel in medical samples. This assists the model in learning mixed labeled\ninformation and establishing semantic associations between labeled and\nunlabeled data. Second, we introduce the concept of information volume of mass\nfunction (IVUM) to evaluate the constructed evidence, implementing two\nevidential learning schemes. One optimizes evidential deep learning by\ncombining the information volume of the mass function with original uncertainty\nmeasures. The other integrates the learning pattern based on the co-evidential\nfusion strategy, using IVUM to design a new optimization objective. Experiments\non four datasets demonstrate the competitive performance of our method."}
{"id": "2506.02493", "pdf": "https://arxiv.org/pdf/2506.02493", "abs": "https://arxiv.org/abs/2506.02493", "authors": ["Jiachen Liu", "Rui Yu", "Sili Chen", "Sharon X. Huang", "Hengkai Guo"], "title": "Towards In-the-wild 3D Plane Reconstruction from a Single Image", "categories": ["cs.CV"], "comment": "CVPR 2025 Highlighted Paper", "summary": "3D plane reconstruction from a single image is a crucial yet challenging\ntopic in 3D computer vision. Previous state-of-the-art (SOTA) methods have\nfocused on training their system on a single dataset from either indoor or\noutdoor domain, limiting their generalizability across diverse testing data. In\nthis work, we introduce a novel framework dubbed ZeroPlane, a Transformer-based\nmodel targeting zero-shot 3D plane detection and reconstruction from a single\nimage, over diverse domains and environments. To enable data-driven models\nacross multiple domains, we have curated a large-scale planar benchmark,\ncomprising over 14 datasets and 560,000 high-resolution, dense planar\nannotations for diverse indoor and outdoor scenes. To address the challenge of\nachieving desirable planar geometry on multi-dataset training, we propose to\ndisentangle the representation of plane normal and offset, and employ an\nexemplar-guided, classification-then-regression paradigm to learn plane and\noffset respectively. Additionally, we employ advanced backbones as image\nencoder, and present an effective pixel-geometry-enhanced plane embedding\nmodule to further facilitate planar reconstruction. Extensive experiments\nacross multiple zero-shot evaluation datasets have demonstrated that our\napproach significantly outperforms previous methods on both reconstruction\naccuracy and generalizability, especially over in-the-wild data. Our code and\ndata are available at: https://github.com/jcliu0428/ZeroPlane."}
{"id": "2506.02497", "pdf": "https://arxiv.org/pdf/2506.02497", "abs": "https://arxiv.org/abs/2506.02497", "authors": ["Jiahao Chen", "Hangjie Yuan", "Yichen Qian", "Jingyun Liang", "Jiazheng Xing", "Pengwei Liu", "Weihua Chen", "Fan Wang", "Bing Su"], "title": "LumosFlow: Motion-Guided Long Video Generation", "categories": ["cs.CV"], "comment": null, "summary": "Long video generation has gained increasing attention due to its widespread\napplications in fields such as entertainment and simulation. Despite advances,\nsynthesizing temporally coherent and visually compelling long sequences remains\na formidable challenge. Conventional approaches often synthesize long videos by\nsequentially generating and concatenating short clips, or generating key frames\nand then interpolate the intermediate frames in a hierarchical manner. However,\nboth of them still remain significant challenges, leading to issues such as\ntemporal repetition or unnatural transitions. In this paper, we revisit the\nhierarchical long video generation pipeline and introduce LumosFlow, a\nframework introduce motion guidance explicitly. Specifically, we first employ\nthe Large Motion Text-to-Video Diffusion Model (LMTV-DM) to generate key frames\nwith larger motion intervals, thereby ensuring content diversity in the\ngenerated long videos. Given the complexity of interpolating contextual\ntransitions between key frames, we further decompose the intermediate frame\ninterpolation into motion generation and post-hoc refinement. For each pair of\nkey frames, the Latent Optical Flow Diffusion Model (LOF-DM) synthesizes\ncomplex and large-motion optical flows, while MotionControlNet subsequently\nrefines the warped results to enhance quality and guide intermediate frame\ngeneration. Compared with traditional video frame interpolation, we achieve 15x\ninterpolation, ensuring reasonable and continuous motion between adjacent\nframes. Experiments show that our method can generate long videos with\nconsistent motion and appearance. Code and models will be made publicly\navailable upon acceptance. Our project page:\nhttps://jiahaochen1.github.io/LumosFlow/"}
{"id": "2506.02528", "pdf": "https://arxiv.org/pdf/2506.02528", "abs": "https://arxiv.org/abs/2506.02528", "authors": ["Yan Gong", "Yiren Song", "Yicheng Li", "Chenglin Li", "Yin Zhang"], "title": "RelationAdapter: Learning and Transferring Visual Relation with Diffusion Transformers", "categories": ["cs.CV"], "comment": null, "summary": "Inspired by the in-context learning mechanism of large language models\n(LLMs), a new paradigm of generalizable visual prompt-based image editing is\nemerging. Existing single-reference methods typically focus on style or\nappearance adjustments and struggle with non-rigid transformations. To address\nthese limitations, we propose leveraging source-target image pairs to extract\nand transfer content-aware editing intent to novel query images. To this end,\nwe introduce RelationAdapter, a lightweight module that enables Diffusion\nTransformer (DiT) based models to effectively capture and apply visual\ntransformations from minimal examples. We also introduce Relation252K, a\ncomprehensive dataset comprising 218 diverse editing tasks, to evaluate model\ngeneralization and adaptability in visual prompt-driven scenarios. Experiments\non Relation252K show that RelationAdapter significantly improves the model's\nability to understand and transfer editing intent, leading to notable gains in\ngeneration quality and overall editing performance."}
{"id": "2506.02534", "pdf": "https://arxiv.org/pdf/2506.02534", "abs": "https://arxiv.org/abs/2506.02534", "authors": ["Sining Chen", "Yilei Shi", "Xiao Xiang Zhu"], "title": "Enhancing Monocular Height Estimation via Weak Supervision from Imperfect Labels", "categories": ["cs.CV"], "comment": null, "summary": "Monocular height estimation is considered the most efficient and\ncost-effective means of 3D perception in remote sensing, and it has attracted\nmuch attention since the emergence of deep learning. While training neural\nnetworks requires a large amount of data, data with perfect labels are scarce\nand only available within developed regions. The trained models therefore lack\ngeneralizability, which limits the potential for large-scale application of\nexisting methods. We tackle this problem for the first time, by introducing\ndata with imperfect labels into training pixel-wise height estimation networks,\nincluding labels that are incomplete, inexact, and inaccurate compared to\nhigh-quality labels. We propose an ensemble-based pipeline compatible with any\nmonocular height estimation network. Taking the challenges of noisy labels,\ndomain shift, and long-tailed distribution of height values into consideration,\nwe carefully design the architecture and loss functions to leverage the\ninformation concealed in imperfect labels using weak supervision through\nbalanced soft losses and ordinal constraints. We conduct extensive experiments\non two datasets with different resolutions, DFC23 (0.5 to 1 m) and GBH (3 m).\nThe results indicate that the proposed pipeline outperforms baselines by\nachieving more balanced performance across various domains, leading to\nimprovements of average root mean square errors up to 22.94 %, and 18.62 % on\nDFC23 and GBH, respectively. The efficacy of each design component is validated\nthrough ablation studies. Code is available at\nhttps://github.com/zhu-xlab/weakim2h."}
{"id": "2506.02535", "pdf": "https://arxiv.org/pdf/2506.02535", "abs": "https://arxiv.org/abs/2506.02535", "authors": ["Juntong Li", "Lingwei Dang", "Yukun Su", "Yun Hao", "Qingxin Xiao", "Yongwei Nie", "Qingyao Wu"], "title": "MemoryOut: Learning Principal Features via Multimodal Sparse Filtering Network for Semi-supervised Video Anomaly Detection", "categories": ["cs.CV"], "comment": null, "summary": "Video Anomaly Detection (VAD) methods based on reconstruction or prediction\nface two critical challenges: (1) strong generalization capability often\nresults in accurate reconstruction or prediction of abnormal events, making it\ndifficult to distinguish normal from abnormal patterns; (2) reliance only on\nlow-level appearance and motion cues limits their ability to identify\nhigh-level semantic in abnormal events from complex scenes. To address these\nlimitations, we propose a novel VAD framework with two key innovations. First,\nto suppress excessive generalization, we introduce the Sparse Feature Filtering\nModule (SFFM) that employs bottleneck filters to dynamically and adaptively\nremove abnormal information from features. Unlike traditional memory modules,\nit does not need to memorize the normal prototypes across the training dataset.\nFurther, we design the Mixture of Experts (MoE) architecture for SFFM. Each\nexpert is responsible for extracting specialized principal features during\nrunning time, and different experts are selectively activated to ensure the\ndiversity of the learned principal features. Second, to overcome the neglect of\nsemantics in existing methods, we integrate a Vision-Language Model (VLM) to\ngenerate textual descriptions for video clips, enabling comprehensive joint\nmodeling of semantic, appearance, and motion cues. Additionally, we enforce\nmodality consistency through semantic similarity constraints and motion\nframe-difference contrastive loss. Extensive experiments on multiple public\ndatasets validate the effectiveness of our multimodal joint modeling framework\nand sparse feature filtering paradigm. Project page at\nhttps://qzfm.github.io/sfn_vad_project_page/."}
{"id": "2506.02537", "pdf": "https://arxiv.org/pdf/2506.02537", "abs": "https://arxiv.org/abs/2506.02537", "authors": ["Hao Yan", "Handong Zheng", "Hao Wang", "Liang Yin", "Xingchen Liu", "Zhenbiao Cao", "Xinxing Su", "Zihao Chen", "Jihao Wu", "Minghui Liao", "Chao Weng", "Wei Chen", "Yuliang Liu", "Xiang Bai"], "title": "VisuRiddles: Fine-grained Perception is a Primary Bottleneck for Multimodal Large Language Models in Abstract Visual Reasoning", "categories": ["cs.CV", "cs.AI"], "comment": "13 pages, 4 figures", "summary": "Recent strides in multimodal large language models (MLLMs) have significantly\nadvanced their performance in many reasoning tasks. However, Abstract Visual\nReasoning (AVR) remains a critical challenge, primarily due to limitations in\nperceiving abstract graphics. To tackle this issue, we investigate the\nbottlenecks in current MLLMs and synthesize training data to improve their\nabstract visual perception. First, we propose VisuRiddles, a benchmark for AVR,\nfeaturing tasks meticulously constructed to assess models' reasoning capacities\nacross five core dimensions and two high-level reasoning categories. Second, we\nintroduce the Perceptual Riddle Synthesizer (PRS), an automated framework for\ngenerating riddles with fine-grained perceptual descriptions. PRS not only\ngenerates valuable training data for abstract graphics but also provides\nfine-grained perceptual description, crucially allowing for supervision over\nintermediate reasoning stages and thereby improving both training efficacy and\nmodel interpretability. Our extensive experimental results on VisuRiddles\nempirically validate that fine-grained visual perception is the principal\nbottleneck and our synthesis framework markedly enhances the performance of\ncontemporary MLLMs on these challenging tasks. Our code and dataset will be\nreleased at https://github.com/yh-hust/VisuRiddles"}
{"id": "2506.02547", "pdf": "https://arxiv.org/pdf/2506.02547", "abs": "https://arxiv.org/abs/2506.02547", "authors": ["Andreu Girbau-Xalabarder", "Jun Nagata", "Shinichi Sumiyoshi"], "title": "Probabilistic Online Event Downsampling", "categories": ["cs.CV", "cs.ET"], "comment": "Accepted at CVPR 2025 Event-Vision workshop", "summary": "Event cameras capture scene changes asynchronously on a per-pixel basis,\nenabling extremely high temporal resolution. However, this advantage comes at\nthe cost of high bandwidth, memory, and computational demands. To address this,\nprior work has explored event downsampling, but most approaches rely on fixed\nheuristics or threshold-based strategies, limiting their adaptability. Instead,\nwe propose a probabilistic framework, POLED, that models event importance\nthrough an event-importance probability density function (ePDF), which can be\narbitrarily defined and adapted to different applications. Our approach\noperates in a purely online setting, estimating event importance on-the-fly\nfrom raw event streams, enabling scene-specific adaptation. Additionally, we\nintroduce zero-shot event downsampling, where downsampled events must remain\nusable for models trained on the original event stream, without task-specific\nadaptation. We design a contour-preserving ePDF that prioritizes structurally\nimportant events and evaluate our method across four datasets and tasks--object\nclassification, image interpolation, surface normal estimation, and object\ndetection--demonstrating that intelligent sampling is crucial for maintaining\nperformance under event-budget constraints."}
{"id": "2506.02550", "pdf": "https://arxiv.org/pdf/2506.02550", "abs": "https://arxiv.org/abs/2506.02550", "authors": ["Qiaohui Chu", "Haoyu Zhang", "Yisen Feng", "Meng Liu", "Weili Guan", "Yaowei Wang", "Liqiang Nie"], "title": "Technical Report for Ego4D Long-Term Action Anticipation Challenge 2025", "categories": ["cs.CV", "cs.AI"], "comment": "The champion solution for the Ego4D Long-Term Action Anticipation\n  Challenge at the CVPR EgoVis Workshop 2025", "summary": "In this report, we present a novel three-stage framework developed for the\nEgo4D Long-Term Action Anticipation (LTA) task. Inspired by recent advances in\nfoundation models, our method consists of three stages: feature extraction,\naction recognition, and long-term action anticipation. First, visual features\nare extracted using a high-performance visual encoder. The features are then\nfed into a Transformer to predict verbs and nouns, with a verb-noun\nco-occurrence matrix incorporated to enhance recognition accuracy. Finally, the\npredicted verb-noun pairs are formatted as textual prompts and input into a\nfine-tuned large language model (LLM) to anticipate future action sequences.\nOur framework achieves first place in this challenge at CVPR 2025, establishing\na new state-of-the-art in long-term action prediction. Our code will be\nreleased at https://github.com/CorrineQiu/Ego4D-LTA-Challenge-2025."}
{"id": "2506.02555", "pdf": "https://arxiv.org/pdf/2506.02555", "abs": "https://arxiv.org/abs/2506.02555", "authors": ["Zhitao Zeng", "Zhu Zhuo", "Xiaojun Jia", "Erli Zhang", "Junde Wu", "Jiaan Zhang", "Yuxuan Wang", "Chang Han Low", "Jian Jiang", "Zilong Zheng", "Xiaochun Cao", "Yutong Ban", "Qi Dou", "Yang Liu", "Yueming Jin"], "title": "SurgVLM: A Large Vision-Language Model and Systematic Evaluation Benchmark for Surgical Intelligence", "categories": ["cs.CV", "68T45", "I.2.10"], "comment": "29 pages, 5 figures", "summary": "Foundation models have achieved transformative success across biomedical\ndomains by enabling holistic understanding of multimodal data. However, their\napplication in surgery remains underexplored. Surgical intelligence presents\nunique challenges - requiring surgical visual perception, temporal analysis,\nand reasoning. Existing general-purpose vision-language models fail to address\nthese needs due to insufficient domain-specific supervision and the lack of a\nlarge-scale high-quality surgical database. To bridge this gap, we propose\nSurgVLM, one of the first large vision-language foundation models for surgical\nintelligence, where this single universal model can tackle versatile surgical\ntasks. To enable this, we construct a large-scale multimodal surgical database,\nSurgVLM-DB, comprising over 1.81 million frames with 7.79 million\nconversations, spanning more than 16 surgical types and 18 anatomical\nstructures. We unify and reorganize 23 public datasets across 10 surgical\ntasks, followed by standardizing labels and doing hierarchical vision-language\nalignment to facilitate comprehensive coverage of gradually finer-grained\nsurgical tasks, from visual perception, temporal analysis, to high-level\nreasoning. Building upon this comprehensive dataset, we propose SurgVLM, which\nis built upon Qwen2.5-VL, and undergoes instruction tuning to 10+ surgical\ntasks. We further construct a surgical multimodal benchmark, SurgVLM-Bench, for\nmethod evaluation. SurgVLM-Bench consists of 6 popular and widely-used datasets\nin surgical domain, covering several crucial downstream tasks. Based on\nSurgVLM-Bench, we evaluate the performance of our SurgVLM (3 SurgVLM variants:\nSurgVLM-7B, SurgVLM-32B, and SurgVLM-72B), and conduct comprehensive\ncomparisons with 14 mainstream commercial VLMs (e.g., GPT-4o, Gemini 2.0 Flash,\nQwen2.5-Max)."}
{"id": "2506.02557", "pdf": "https://arxiv.org/pdf/2506.02557", "abs": "https://arxiv.org/abs/2506.02557", "authors": ["Shizhan Gong", "Yankai Jiang", "Qi Dou", "Farzan Farnia"], "title": "Kernel-based Unsupervised Embedding Alignment for Enhanced Visual Representation in Vision-language Models", "categories": ["cs.CV"], "comment": "ICML 2025", "summary": "Vision-language models, such as CLIP, have achieved significant success in\naligning visual and textual representations, becoming essential components of\nmany multi-modal large language models (MLLMs) like LLaVA and OpenFlamingo.\nHowever, numerous studies have identified CLIP's limited fine-grained\nperception as a critical drawback, leading to substantial failures in\ndownstream MLLMs. In contrast, vision-centric foundation models like DINOv2\ndemonstrate remarkable capabilities in capturing fine details from images. In\nthis work, we propose a novel kernel-based method to align CLIP's visual\nrepresentation with that of DINOv2, ensuring that the resulting embeddings\nmaintain compatibility with text embeddings while enhancing perceptual\ncapabilities. Our alignment objective is designed for efficient stochastic\noptimization. Following this image-only alignment fine-tuning, the visual\nencoder retains compatibility with the frozen text encoder and exhibits\nsignificant improvements in zero-shot object recognition, fine-grained spatial\nreasoning, and localization. By integrating the aligned visual encoder,\ndownstream MLLMs also demonstrate enhanced performance."}
{"id": "2506.02560", "pdf": "https://arxiv.org/pdf/2506.02560", "abs": "https://arxiv.org/abs/2506.02560", "authors": ["Zixiang Li", "Haoyu Wang", "Wei Wang", "Chuangchuang Tan", "Yunchao Wei", "Yao Zhao"], "title": "DCI: Dual-Conditional Inversion for Boosting Diffusion-Based Image Editing", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion models have achieved remarkable success in image generation and\nediting tasks. Inversion within these models aims to recover the latent noise\nrepresentation for a real or generated image, enabling reconstruction, editing,\nand other downstream tasks. However, to date, most inversion approaches suffer\nfrom an intrinsic trade-off between reconstruction accuracy and editing\nflexibility. This limitation arises from the difficulty of maintaining both\nsemantic alignment and structural consistency during the inversion process. In\nthis work, we introduce Dual-Conditional Inversion (DCI), a novel framework\nthat jointly conditions on the source prompt and reference image to guide the\ninversion process. Specifically, DCI formulates the inversion process as a\ndual-condition fixed-point optimization problem, minimizing both the latent\nnoise gap and the reconstruction error under the joint guidance. This design\nanchors the inversion trajectory in both semantic and visual space, leading to\nmore accurate and editable latent representations. Our novel setup brings new\nunderstanding to the inversion process. Extensive experiments demonstrate that\nDCI achieves state-of-the-art performance across multiple editing tasks,\nsignificantly improving both reconstruction quality and editing precision.\nFurthermore, we also demonstrate that our method achieves strong results in\nreconstruction tasks, implying a degree of robustness and generalizability\napproaching the ultimate goal of the inversion process."}
{"id": "2506.02571", "pdf": "https://arxiv.org/pdf/2506.02571", "abs": "https://arxiv.org/abs/2506.02571", "authors": ["Abhishek Vivekanandan", "Christian Hubschneider", "J. Marius Z√∂llner"], "title": "Contrast & Compress: Learning Lightweight Embeddings for Short Trajectories", "categories": ["cs.CV"], "comment": "Submitted for peer review", "summary": "The ability to retrieve semantically and directionally similar short-range\ntrajectories with both accuracy and efficiency is foundational for downstream\napplications such as motion forecasting and autonomous navigation. However,\nprevailing approaches often depend on computationally intensive heuristics or\nlatent anchor representations that lack interpretability and controllability.\nIn this work, we propose a novel framework for learning fixed-dimensional\nembeddings for short trajectories by leveraging a Transformer encoder trained\nwith a contrastive triplet loss that emphasize the importance of discriminative\nfeature spaces for trajectory data. We analyze the influence of Cosine and\nFFT-based similarity metrics within the contrastive learning paradigm, with a\nfocus on capturing the nuanced directional intent that characterizes short-term\nmaneuvers. Our empirical evaluation on the Argoverse 2 dataset demonstrates\nthat embeddings shaped by Cosine similarity objectives yield superior\nclustering of trajectories by both semantic and directional attributes,\noutperforming FFT-based baselines in retrieval tasks. Notably, we show that\ncompact Transformer architectures, even with low-dimensional embeddings (e.g.,\n16 dimensions, but qualitatively down to 4), achieve a compelling balance\nbetween retrieval performance (minADE, minFDE) and computational overhead,\naligning with the growing demand for scalable and interpretable motion priors\nin real-time systems. The resulting embeddings provide a compact, semantically\nmeaningful, and efficient representation of trajectory data, offering a robust\nalternative to heuristic similarity measures and paving the way for more\ntransparent and controllable motion forecasting pipelines."}
{"id": "2506.02587", "pdf": "https://arxiv.org/pdf/2506.02587", "abs": "https://arxiv.org/abs/2506.02587", "authors": ["Weiduo Yuan", "Jerry Li", "Justin Yue", "Divyank Shah", "Konstantinos Karydis", "Hang Qiu"], "title": "BEVCALIB: LiDAR-Camera Calibration via Geometry-Guided Bird's-Eye View Representations", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Accurate LiDAR-camera calibration is fundamental to fusing multi-modal\nperception in autonomous driving and robotic systems. Traditional calibration\nmethods require extensive data collection in controlled environments and cannot\ncompensate for the transformation changes during the vehicle/robot movement. In\nthis paper, we propose the first model that uses bird's-eye view (BEV) features\nto perform LiDAR camera calibration from raw data, termed BEVCALIB. To achieve\nthis, we extract camera BEV features and LiDAR BEV features separately and fuse\nthem into a shared BEV feature space. To fully utilize the geometric\ninformation from the BEV feature, we introduce a novel feature selector to\nfilter the most important features in the transformation decoder, which reduces\nmemory consumption and enables efficient training. Extensive evaluations on\nKITTI, NuScenes, and our own dataset demonstrate that BEVCALIB establishes a\nnew state of the art. Under various noise conditions, BEVCALIB outperforms the\nbest baseline in the literature by an average of (47.08%, 82.32%) on KITTI\ndataset, and (78.17%, 68.29%) on NuScenes dataset, in terms of (translation,\nrotation), respectively. In the open-source domain, it improves the best\nreproducible baseline by one order of magnitude. Our code and demo results are\navailable at https://cisl.ucr.edu/BEVCalib."}
{"id": "2506.02601", "pdf": "https://arxiv.org/pdf/2506.02601", "abs": "https://arxiv.org/abs/2506.02601", "authors": ["Shiyu Shen", "Bin Pan", "Ziye Zhang", "Zhenwei Shi"], "title": "Hyperspectral Image Generation with Unmixing Guided Diffusion Model", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Recently, hyperspectral image generation has received increasing attention,\nbut existing generative models rely on conditional generation schemes, which\nlimits the diversity of generated images. Diffusion models are popular for\ntheir ability to generate high-quality samples, but adapting these models from\nRGB to hyperspectral data presents the challenge of high dimensionality and\nphysical constraints. To address these challenges, we propose a novel diffusion\nmodel guided by hyperspectral unmixing. Our model comprises two key modules: an\nunmixing autoencoder module and an abundance diffusion module. The unmixing\nautoencoder module leverages unmixing guidance to shift the generative task\nfrom the image space to the low-dimensional abundance space, significantly\nreducing computational complexity while preserving high fidelity. The abundance\ndiffusion module generates samples that satisfy the constraints of\nnon-negativity and unity, ensuring the physical consistency of the\nreconstructed HSIs. Additionally, we introduce two evaluation metrics tailored\nto hyperspectral data. Empirical results, evaluated using both traditional\nmetrics and our proposed metrics, indicate that our model is capable of\ngenerating high-quality and diverse hyperspectral images, offering an\nadvancement in hyperspectral data generation."}
{"id": "2506.02604", "pdf": "https://arxiv.org/pdf/2506.02604", "abs": "https://arxiv.org/abs/2506.02604", "authors": ["Tian Chunwei", "Song Mingjian", "Zuo Wangmeng", "Du Bo", "Zhang Yanning", "Zhang Shichao"], "title": "Application of convolutional neural networks in image super-resolution", "categories": ["cs.CV", "eess.IV"], "comment": "It has been accepted by CAAI transactions on intelligent systems, in\n  Chinese language", "summary": "Due to strong learning abilities of convolutional neural networks (CNNs),\nthey have become mainstream methods for image super-resolution. However, there\nare big differences of different deep learning methods with different types.\nThere is little literature to summarize relations and differences of different\nmethods in image super-resolution. Thus, summarizing these literatures are\nimportant, according to loading capacity and execution speed of devices. This\npaper first introduces principles of CNNs in image super-resolution, then\nintroduces CNNs based bicubic interpolation, nearest neighbor interpolation,\nbilinear interpolation, transposed convolution, sub-pixel layer, meta\nup-sampling for image super-resolution to analyze differences and relations of\ndifferent CNNs based interpolations and modules, and compare performance of\nthese methods by experiments. Finally, this paper gives potential research\npoints and drawbacks and summarizes the whole paper, which can facilitate\ndevelopments of CNNs in image super-resolution."}
{"id": "2506.02605", "pdf": "https://arxiv.org/pdf/2506.02605", "abs": "https://arxiv.org/abs/2506.02605", "authors": ["Xue Wu", "Jingwei Xin", "Zhijun Tu", "Jie Hu", "Jie Li", "Nannan Wang", "Xinbo Gao"], "title": "One-Step Diffusion-based Real-World Image Super-Resolution with Visual Perception Distillation", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion-based models have been widely used in various visual generation\ntasks, showing promising results in image super-resolution (SR), while\ntypically being limited by dozens or even hundreds of sampling steps. Although\nexisting methods aim to accelerate the inference speed of multi-step\ndiffusion-based SR methods through knowledge distillation, their generated\nimages exhibit insufficient semantic alignment with real images, resulting in\nsuboptimal perceptual quality reconstruction, specifically reflected in the\nCLIPIQA score. These methods still have many challenges in perceptual quality\nand semantic fidelity. Based on the challenges, we propose VPD-SR, a novel\nvisual perception diffusion distillation framework specifically designed for\nSR, aiming to construct an effective and efficient one-step SR model.\nSpecifically, VPD-SR consists of two components: Explicit Semantic-aware\nSupervision (ESS) and High-Frequency Perception (HFP) loss. Firstly, the ESS\nleverages the powerful visual perceptual understanding capabilities of the CLIP\nmodel to extract explicit semantic supervision, thereby enhancing semantic\nconsistency. Then, Considering that high-frequency information contributes to\nthe visual perception quality of images, in addition to the vanilla\ndistillation loss, the HFP loss guides the student model to restore the missing\nhigh-frequency details in degraded images that are critical for enhancing\nperceptual quality. Lastly, we expand VPD-SR in adversarial training manner to\nfurther enhance the authenticity of the generated content. Extensive\nexperiments conducted on synthetic and real-world datasets demonstrate that the\nproposed VPD-SR achieves superior performance compared to both previous\nstate-of-the-art methods and the teacher model with just one-step sampling."}
{"id": "2506.02614", "pdf": "https://arxiv.org/pdf/2506.02614", "abs": "https://arxiv.org/abs/2506.02614", "authors": ["Guohang Zhuang", "Weixi Song", "Jinyang Huang", "Chenwei Yang", "Yan Lu"], "title": "High Performance Space Debris Tracking in Complex Skylight Backgrounds with a Large-Scale Dataset", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "With the rapid development of space exploration, space debris has attracted\nmore attention due to its potential extreme threat, leading to the need for\nreal-time and accurate debris tracking. However, existing methods are mainly\nbased on traditional signal processing, which cannot effectively process the\ncomplex background and dense space debris. In this paper, we propose a deep\nlearning-based Space Debris Tracking Network~(SDT-Net) to achieve highly\naccurate debris tracking. SDT-Net effectively represents the feature of debris,\nenhancing the efficiency and stability of end-to-end model learning. To train\nand evaluate this model effectively, we also produce a large-scale dataset\nSpace Debris Tracking Dataset (SDTD) by a novel observation-based data\nsimulation scheme. SDTD contains 18,040 video sequences with a total of 62,562\nframes and covers 250,000 synthetic space debris. Extensive experiments\nvalidate the effectiveness of our model and the challenging of our dataset.\nFurthermore, we test our model on real data from the Antarctic Station,\nachieving a MOTA score of 70.6%, which demonstrates its strong transferability\nto real-world scenarios. Our dataset and code will be released soon."}
{"id": "2506.02615", "pdf": "https://arxiv.org/pdf/2506.02615", "abs": "https://arxiv.org/abs/2506.02615", "authors": ["Safaa Abdullahi Moallim Mohamud", "Minjin Baek", "Dong Seog Han"], "title": "Hierarchical Question-Answering for Driving Scene Understanding Using Vision-Language Models", "categories": ["cs.CV", "cs.AI"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "In this paper, we present a hierarchical question-answering (QA) approach for\nscene understanding in autonomous vehicles, balancing cost-efficiency with\ndetailed visual interpretation. The method fine-tunes a compact vision-language\nmodel (VLM) on a custom dataset specific to the geographical area in which the\nvehicle operates to capture key driving-related visual elements. At the\ninference stage, the hierarchical QA strategy decomposes the scene\nunderstanding task into high-level and detailed sub-questions. Instead of\ngenerating lengthy descriptions, the VLM navigates a structured question tree,\nwhere answering high-level questions (e.g., \"Is it possible for the ego vehicle\nto turn left at the intersection?\") triggers more detailed sub-questions (e.g.,\n\"Is there a vehicle approaching the intersection from the opposite\ndirection?\"). To optimize inference time, questions are dynamically skipped\nbased on previous answers, minimizing computational overhead. The extracted\nanswers are then synthesized using handcrafted templates to ensure coherent,\ncontextually accurate scene descriptions. We evaluate the proposed approach on\nthe custom dataset using GPT reference-free scoring, demonstrating its\ncompetitiveness with state-of-the-art methods like GPT-4o in capturing key\nscene details while achieving significantly lower inference time. Moreover,\nqualitative results from real-time deployment highlight the proposed approach's\ncapacity to capture key driving elements with minimal latency."}
{"id": "2506.02626", "pdf": "https://arxiv.org/pdf/2506.02626", "abs": "https://arxiv.org/abs/2506.02626", "authors": ["Ada Sawilska", "Mateusz Trokielewicz"], "title": "Synthetic Iris Image Databases and Identity Leakage: Risks and Mitigation Strategies", "categories": ["cs.CV"], "comment": null, "summary": "This paper presents a comprehensive overview of iris image synthesis methods,\nwhich can alleviate the issues associated with gathering large, diverse\ndatasets of biometric data from living individuals, which are considered\npivotal for biometric methods development. These methods for synthesizing iris\ndata range from traditional, hand crafted image processing-based techniques,\nthrough various iterations of GAN-based image generators, variational\nautoencoders (VAEs), as well as diffusion models. The potential and fidelity in\niris image generation of each method is discussed and examples of inferred\npredictions are provided. Furthermore, the risks of individual biometric\nfeatures leakage from the training sets are considered, together with possible\nstrategies for preventing them, which have to be implemented should these\ngenerative methods be considered a valid replacement of real-world biometric\ndatasets."}
{"id": "2506.02633", "pdf": "https://arxiv.org/pdf/2506.02633", "abs": "https://arxiv.org/abs/2506.02633", "authors": ["Cheng Yang", "Lijing Liang", "Zhixun Su"], "title": "ControlMambaIR: Conditional Controls with State-Space Model for Image Restoration", "categories": ["cs.CV"], "comment": null, "summary": "This paper proposes ControlMambaIR, a novel image restoration method designed\nto address perceptual challenges in image deraining, deblurring, and denoising\ntasks. By integrating the Mamba network architecture with the diffusion model,\nthe condition network achieves refined conditional control, thereby enhancing\nthe control and optimization of the image generation process. To evaluate the\nrobustness and generalization capability of our method across various image\ndegradation conditions, extensive experiments were conducted on several\nbenchmark datasets, including Rain100H, Rain100L, GoPro, and SSID. The results\ndemonstrate that our proposed approach consistently surpasses existing methods\nin perceptual quality metrics, such as LPIPS and FID, while maintaining\ncomparable performance in image distortion metrics, including PSNR and SSIM,\nhighlighting its effectiveness and adaptability. Notably, ablation experiments\nreveal that directly noise prediction in the diffusion process achieves better\nperformance, effectively balancing noise suppression and detail preservation.\nFurthermore, the findings indicate that the Mamba architecture is particularly\nwell-suited as a conditional control network for diffusion models,\noutperforming both CNN- and Attention-based approaches in this context.\nOverall, these results highlight the flexibility and effectiveness of\nControlMambaIR in addressing a range of image restoration perceptual\nchallenges."}
{"id": "2506.02671", "pdf": "https://arxiv.org/pdf/2506.02671", "abs": "https://arxiv.org/abs/2506.02671", "authors": ["Xiao Chen", "Jiazhen Huang", "Qinting Jiang", "Fanding Huang", "Xianghua Fu", "Jingyan Jiang", "Zhi Wang"], "title": "Small Aid, Big Leap: Efficient Test-Time Adaptation for Vision-Language Models with AdaptNet", "categories": ["cs.CV"], "comment": null, "summary": "Test-time adaptation (TTA) has emerged as a critical technique for enhancing\nthe generalization capability of vision-language models (VLMs) during\ninference. However, existing approaches often incur substantial computational\ncosts and exhibit poor scalability, primarily due to sample-wise adaptation\ngranularity and reliance on costly auxiliary designs such as data augmentation.\nTo address these limitations, we introduce SAIL (Small Aid, Big Leap), a novel\nadapter-based TTA framework that leverages a lightweight, learnable AdaptNet to\nenable efficient and scalable model adaptation. As SAIL's core, a frozen\npre-trained VLM collaborates with AdaptNet through a confidence-based\ninterpolation weight, generating robust predictions during inference. These\npredictions serve as self-supervised targets to align AdaptNet's outputs\nthrough efficient batch-wise processing, dramatically reducing computational\ncosts without modifying the VLM or requiring memory caches. To mitigate\ncatastrophic forgetting during continual adaptation, we propose a\ngradient-aware reset strategy driven by a gradient drift indicator (GDI), which\ndynamically detects domain transitions and strategically resets AdaptNet for\nstable adaptation. Extensive experiments across diverse benchmarks on two\nscenarios demonstrate that SAIL achieves state-of-the-art performance while\nmaintaining low computational costs. These results highlight SAIL's\neffectiveness, efficiency and scalability for real-world deployment. The code\nwill be released upon acceptance."}
{"id": "2506.02677", "pdf": "https://arxiv.org/pdf/2506.02677", "abs": "https://arxiv.org/abs/2506.02677", "authors": ["Jintao Tong", "Yixiong Zou", "Guangyao Chen", "Yuhua Li", "Ruixuan Li"], "title": "Self-Disentanglement and Re-Composition for Cross-Domain Few-Shot Segmentation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Accepted by ICML 2025", "summary": "Cross-Domain Few-Shot Segmentation (CD-FSS) aims to transfer knowledge from a\nsource-domain dataset to unseen target-domain datasets with limited\nannotations. Current methods typically compare the distance between training\nand testing samples for mask prediction. However, we find an entanglement\nproblem exists in this widely adopted method, which tends to bind sourcedomain\npatterns together and make each of them hard to transfer. In this paper, we aim\nto address this problem for the CD-FSS task. We first find a natural\ndecomposition of the ViT structure, based on which we delve into the\nentanglement problem for an interpretation. We find the decomposed ViT\ncomponents are crossly compared between images in distance calculation, where\nthe rational comparisons are entangled with those meaningless ones by their\nequal importance, leading to the entanglement problem. Based on this\ninterpretation, we further propose to address the entanglement problem by\nlearning to weigh for all comparisons of ViT components, which learn\ndisentangled features and re-compose them for the CD-FSS task, benefiting both\nthe generalization and finetuning. Experiments show that our model outperforms\nthe state-of-the-art CD-FSS method by 1.92% and 1.88% in average accuracy under\n1-shot and 5-shot settings, respectively."}
{"id": "2506.02680", "pdf": "https://arxiv.org/pdf/2506.02680", "abs": "https://arxiv.org/abs/2506.02680", "authors": ["Julius Erbach", "Dominik Narnhofer", "Andreas Dombos", "Bernt Schiele", "Jan Eric Lenssen", "Konrad Schindler"], "title": "Solving Inverse Problems with FLAIR", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Flow-based latent generative models such as Stable Diffusion 3 are able to\ngenerate images with remarkable quality, even enabling photorealistic\ntext-to-image generation. Their impressive performance suggests that these\nmodels should also constitute powerful priors for inverse imaging problems, but\nthat approach has not yet led to comparable fidelity. There are several key\nobstacles: (i) the encoding into a lower-dimensional latent space makes the\nunderlying (forward) mapping non-linear; (ii) the data likelihood term is\nusually intractable; and (iii) learned generative models struggle to recover\nrare, atypical data modes during inference. We present FLAIR, a novel training\nfree variational framework that leverages flow-based generative models as a\nprior for inverse problems. To that end, we introduce a variational objective\nfor flow matching that is agnostic to the type of degradation, and combine it\nwith deterministic trajectory adjustments to recover atypical modes. To enforce\nexact consistency with the observed data, we decouple the optimization of the\ndata fidelity and regularization terms. Moreover, we introduce a time-dependent\ncalibration scheme in which the strength of the regularization is modulated\naccording to off-line accuracy estimates. Results on standard imaging\nbenchmarks demonstrate that FLAIR consistently outperforms existing diffusion-\nand flow-based methods in terms of reconstruction quality and sample diversity."}
{"id": "2506.02690", "pdf": "https://arxiv.org/pdf/2506.02690", "abs": "https://arxiv.org/abs/2506.02690", "authors": ["Yurui Zhao", "Xiang Wang", "Jiahong Liu", "Irwin King", "Zhitao Huang"], "title": "Towards Geometry Problem Solving in the Large Model Era: A Survey", "categories": ["cs.CV", "math.GT"], "comment": "8pages, 4 figures, conference submission", "summary": "Geometry problem solving (GPS) represents a critical frontier in artificial\nintelligence, with profound applications in education, computer-aided design,\nand computational graphics. Despite its significance, automating GPS remains\nchallenging due to the dual demands of spatial understanding and rigorous\nlogical reasoning. Recent advances in large models have enabled notable\nbreakthroughs, particularly for SAT-level problems, yet the field remains\nfragmented across methodologies, benchmarks, and evaluation frameworks. This\nsurvey systematically synthesizes GPS advancements through three core\ndimensions: (1) benchmark construction, (2) textual and diagrammatic parsing,\nand (3) reasoning paradigms. We further propose a unified analytical paradigm,\nassess current limitations, and identify emerging opportunities to guide future\nresearch toward human-level geometric reasoning, including automated benchmark\ngeneration and interpretable neuro-symbolic integration."}
{"id": "2506.02692", "pdf": "https://arxiv.org/pdf/2506.02692", "abs": "https://arxiv.org/abs/2506.02692", "authors": ["Shu Yang", "Fengtao Zhou", "Leon Mayer", "Fuxiang Huang", "Yiliang Chen", "Yihui Wang", "Sunan He", "Yuxiang Nie", "Xi Wang", "√ñmer S√ºmer", "Yueming Jin", "Huihui Sun", "Shuchang Xu", "Alex Qinyang Liu", "Zheng Li", "Jing Qin", "Jeremy YuenChun Teoh", "Lena Maier-Hein", "Hao Chen"], "title": "Large-scale Self-supervised Video Foundation Model for Intelligent Surgery", "categories": ["cs.CV"], "comment": null, "summary": "Computer-Assisted Intervention (CAI) has the potential to revolutionize\nmodern surgery, with surgical scene understanding serving as a critical\ncomponent in supporting decision-making, improving procedural efficacy, and\nensuring intraoperative safety. While existing AI-driven approaches alleviate\nannotation burdens via self-supervised spatial representation learning, their\nlack of explicit temporal modeling during pre-training fundamentally restricts\nthe capture of dynamic surgical contexts, resulting in incomplete\nspatiotemporal understanding. In this work, we introduce the first video-level\nsurgical pre-training framework that enables joint spatiotemporal\nrepresentation learning from large-scale surgical video data. To achieve this,\nwe constructed a large-scale surgical video dataset comprising 3,650 videos and\napproximately 3.55 million frames, spanning more than 20 surgical procedures\nand over 10 anatomical structures. Building upon this dataset, we propose\nSurgVISTA (Surgical Video-level Spatial-Temporal Architecture), a\nreconstruction-based pre-training method that captures intricate spatial\nstructures and temporal dynamics through joint spatiotemporal modeling.\nAdditionally, SurgVISTA incorporates image-level knowledge distillation guided\nby a surgery-specific expert to enhance the learning of fine-grained anatomical\nand semantic features. To validate its effectiveness, we established a\ncomprehensive benchmark comprising 13 video-level datasets spanning six\nsurgical procedures across four tasks. Extensive experiments demonstrate that\nSurgVISTA consistently outperforms both natural- and surgical-domain\npre-trained models, demonstrating strong potential to advance intelligent\nsurgical systems in clinically meaningful scenarios."}
{"id": "2506.02695", "pdf": "https://arxiv.org/pdf/2506.02695", "abs": "https://arxiv.org/abs/2506.02695", "authors": ["Linquan Wu", "Tianxiang Jiang", "Wenhao Duan", "Yini Fang", "Jacky Keung"], "title": "FaceSleuth: Learning-Driven Single-Orientation Attention Verifies Vertical Dominance in Micro-Expression Recognition", "categories": ["cs.CV"], "comment": "12 pages, 2 figures", "summary": "Micro-expression recognition (MER) demands models that can amplify\nmillisecond-level, low-amplitude facial motions while suppressing\nidentity-specific appearance. We introduce FaceSleuth, a dual-stream\narchitecture that (1) enhances motion along the empirically dominant vertical\naxix through a Continuously Vertical Attention (CVA) block, (2) localises the\nresulting signals with a Facial Position Focalizer built on hierarchical\ncross-window attention, and (3) steers feature learning toward physiologically\nmeaningful regions via lightweight Action-Unit embeddings. To examine whether\nthe hand-chosen vertical axis is indeed optimal, we further propose a\nSingle-Orientation Attention (SOA) module that learns its own pooling direction\nend-to-end. SOA is differentiable, adds only 0.16 % parameters, and collapses\nto CVA when the learned angle converges to {\\Pi}/2. In practice, SOA reliably\ndrifts to 88{\\deg}, confirming the effectiveness of the vertical prior while\ndelivering consistent gains. On three standard MER benchmarks, FaceSleuth with\nCVA already surpasses previous state-of-the-art methods; plugging in SOA lifts\naccuracy and F1 score performance to 95.1 % / 0.918 on CASME II, 87.1 % / 0.840\non SAMM, and 92.9 % / 0.917 on MMEW without sacrificing model compactness.\nThese results establish a new state of the art and, for the first time, provide\nempirical evidence that the vertical attention bias is the most discriminative\norientation for MER."}
{"id": "2506.02697", "pdf": "https://arxiv.org/pdf/2506.02697", "abs": "https://arxiv.org/abs/2506.02697", "authors": ["Yuxuan Wu", "Le Wang", "Sanping Zhou", "Mengnan Liu", "Gang Hua", "Haoxiang Li"], "title": "LayoutRAG: Retrieval-Augmented Model for Content-agnostic Conditional Layout Generation", "categories": ["cs.CV"], "comment": "12 pages, 5 figures", "summary": "Controllable layout generation aims to create plausible visual arrangements\nof element bounding boxes within a graphic design according to certain optional\nconstraints, such as the type or position of a specific component. While recent\ndiffusion or flow-matching models have achieved considerable advances in\nmultifarious conditional generation tasks, there remains considerable room for\ngenerating optimal arrangements under given conditions. In this work, we\npropose to carry out layout generation through retrieving by conditions and\nreference-guided generation. Specifically, we retrieve appropriate layout\ntemplates according to given conditions as references. The references are then\nutilized to guide the denoising or flow-based transport process. By retrieving\nlayouts compatible with the given conditions, we can uncover the potential\ninformation not explicitly provided in the given condition. Such an approach\noffers more effective guidance to the model during the generation process, in\ncontrast to previous models that feed the condition to the model and let the\nmodel infer the unprovided layout attributes directly. Meanwhile, we design a\ncondition-modulated attention that selectively absorbs retrieval knowledge,\nadapting to the difference between retrieved templates and given conditions.\nExtensive experiment results show that our method successfully produces\nhigh-quality layouts that meet the given conditions and outperforms existing\nstate-of-the-art models. Code will be released upon acceptance."}
{"id": "2506.02698", "pdf": "https://arxiv.org/pdf/2506.02698", "abs": "https://arxiv.org/abs/2506.02698", "authors": ["Yunhong Lu", "Qichao Wang", "Hengyuan Cao", "Xiaoyin Xu", "Min Zhang"], "title": "Smoothed Preference Optimization via ReNoise Inversion for Aligning Diffusion Models with Varied Human Preferences", "categories": ["cs.CV"], "comment": "Accepted by ICML 2025", "summary": "Direct Preference Optimization (DPO) aligns text-to-image (T2I) generation\nmodels with human preferences using pairwise preference data. Although\nsubstantial resources are expended in collecting and labeling datasets, a\ncritical aspect is often neglected: \\textit{preferences vary across individuals\nand should be represented with more granularity.} To address this, we propose\nSmPO-Diffusion, a novel method for modeling preference distributions to improve\nthe DPO objective, along with a numerical upper bound estimation for the\ndiffusion optimization objective. First, we introduce a smoothed preference\ndistribution to replace the original binary distribution. We employ a reward\nmodel to simulate human preferences and apply preference likelihood averaging\nto improve the DPO loss, such that the loss function approaches zero when\npreferences are similar. Furthermore, we utilize an inversion technique to\nsimulate the trajectory preference distribution of the diffusion model,\nenabling more accurate alignment with the optimization objective. Our approach\neffectively mitigates issues of excessive optimization and objective\nmisalignment present in existing methods through straightforward modifications.\nOur SmPO-Diffusion achieves state-of-the-art performance in preference\nevaluation, outperforming baselines across metrics with lower training costs.\nThe project page is https://jaydenlyh.github.io/SmPO-project-page/."}
{"id": "2506.02702", "pdf": "https://arxiv.org/pdf/2506.02702", "abs": "https://arxiv.org/abs/2506.02702", "authors": ["Tibor Kub√≠k", "Fran√ßois Guibault", "Michal ≈†panƒõl", "Herv√© Lombaert"], "title": "ToothForge: Automatic Dental Shape Generation using Synchronized Spectral Embeddings", "categories": ["cs.CV"], "comment": "Information Processing in Medical Imaging (IPMI2025)", "summary": "We introduce ToothForge, a spectral approach for automatically generating\nnovel 3D teeth, effectively addressing the sparsity of dental shape datasets.\nBy operating in the spectral domain, our method enables compact machine\nlearning modeling, allowing the generation of high-resolution tooth meshes in\nmilliseconds. However, generating shape spectra comes with the instability of\nthe decomposed harmonics. To address this, we propose modeling the latent\nmanifold on synchronized frequential embeddings. Spectra of all data samples\nare aligned to a common basis prior to the training procedure, effectively\neliminating biases introduced by the decomposition instability. Furthermore,\nsynchronized modeling removes the limiting factor imposed by previous methods,\nwhich require all shapes to share a common fixed connectivity. Using a private\ndataset of real dental crowns, we observe a greater reconstruction quality of\nthe synthetized shapes, exceeding those of models trained on unaligned\nembeddings. We also explore additional applications of spectral analysis in\ndigital dentistry, such as shape compression and interpolation. ToothForge\nfacilitates a range of approaches at the intersection of spectral analysis and\nmachine learning, with fewer restrictions on mesh structure. This makes it\napplicable for shape analysis not only in dentistry, but also in broader\nmedical applications, where guaranteeing consistent connectivity across shapes\nfrom various clinics is unrealistic. The code is available at\nhttps://github.com/tiborkubik/toothForge."}
{"id": "2506.02708", "pdf": "https://arxiv.org/pdf/2506.02708", "abs": "https://arxiv.org/abs/2506.02708", "authors": ["Naoto Tanji", "Toshihiko Yamasaki"], "title": "Iterative Self-Improvement of Vision Language Models for Image Scoring and Self-Explanation", "categories": ["cs.CV", "cs.CL"], "comment": "Accepted to ICIP2025", "summary": "Image scoring is a crucial task in numerous real-world applications. To trust\na model's judgment, understanding its rationale is essential. This paper\nproposes a novel training method for Vision Language Models (VLMs) to generate\nnot only image scores but also corresponding justifications in natural\nlanguage. Leveraging only an image scoring dataset and an instruction-tuned\nVLM, our method enables self-training, utilizing the VLM's generated text\nwithout relying on external data or models. In addition, we introduce a simple\nmethod for creating a dataset designed to improve alignment between predicted\nscores and their textual justifications. By iteratively training the model with\nDirect Preference Optimization on two distinct datasets and merging them, we\ncan improve both scoring accuracy and the coherence of generated explanations."}
{"id": "2506.02733", "pdf": "https://arxiv.org/pdf/2506.02733", "abs": "https://arxiv.org/abs/2506.02733", "authors": ["Xiaoyi Feng", "Kaifeng Zou", "Caichun Cen", "Tao Huang", "Hui Guo", "Zizhou Huang", "Yingli Zhao", "Mingqing Zhang", "Diwei Wang", "Yuntao Zou", "Dagang Li"], "title": "LinkTo-Anime: A 2D Animation Optical Flow Dataset from 3D Model Rendering", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Existing optical flow datasets focus primarily on real-world simulation or\nsynthetic human motion, but few are tailored to Celluloid(cel) anime character\nmotion: a domain with unique visual and motion characteristics. To bridge this\ngap and facilitate research in optical flow estimation and downstream tasks\nsuch as anime video generation and line drawing colorization, we introduce\nLinkTo-Anime, the first high-quality dataset specifically designed for cel\nanime character motion generated with 3D model rendering. LinkTo-Anime provides\nrich annotations including forward and backward optical flow, occlusion masks,\nand Mixamo Skeleton. The dataset comprises 395 video sequences, totally 24,230\ntraining frames, 720 validation frames, and 4,320 test frames. Furthermore, a\ncomprehensive benchmark is constructed with various optical flow estimation\nmethods to analyze the shortcomings and limitations across multiple datasets."}
{"id": "2506.02736", "pdf": "https://arxiv.org/pdf/2506.02736", "abs": "https://arxiv.org/abs/2506.02736", "authors": ["Shufan Qing", "Anzhen Li", "Qiandi Wang", "Yuefeng Niu", "Mingchen Feng", "Guoliang Hu", "Jinqiao Wu", "Fengtao Nan", "Yingchun Fan"], "title": "GeneA-SLAM2: Dynamic SLAM with AutoEncoder-Preprocessed Genetic Keypoints Resampling and Depth Variance-Guided Dynamic Region Removal", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Existing semantic SLAM in dynamic environments mainly identify dynamic\nregions through object detection or semantic segmentation methods. However, in\ncertain highly dynamic scenarios, the detection boxes or segmentation masks\ncannot fully cover dynamic regions. Therefore, this paper proposes a robust and\nefficient GeneA-SLAM2 system that leverages depth variance constraints to\nhandle dynamic scenes. Our method extracts dynamic pixels via depth variance\nand creates precise depth masks to guide the removal of dynamic objects.\nSimultaneously, an autoencoder is used to reconstruct keypoints, improving the\ngenetic resampling keypoint algorithm to obtain more uniformly distributed\nkeypoints and enhance the accuracy of pose estimation. Our system was evaluated\non multiple highly dynamic sequences. The results demonstrate that GeneA-SLAM2\nmaintains high accuracy in dynamic scenes compared to current methods. Code is\navailable at: https://github.com/qingshufan/GeneA-SLAM2."}
{"id": "2506.02738", "pdf": "https://arxiv.org/pdf/2506.02738", "abs": "https://arxiv.org/abs/2506.02738", "authors": ["Negin Baghbanzadeh", "Sajad Ashkezari", "Elham Dolatabadi", "Arash Afkanpour"], "title": "Open-PMC-18M: A High-Fidelity Large Scale Medical Dataset for Multimodal Representation Learning", "categories": ["cs.CV"], "comment": "15 pages", "summary": "Compound figures, which are multi-panel composites containing diverse\nsubfigures, are ubiquitous in biomedical literature, yet large-scale subfigure\nextraction remains largely unaddressed. Prior work on subfigure extraction has\nbeen limited in both dataset size and generalizability, leaving a critical open\nquestion: How does high-fidelity image-text alignment via large-scale subfigure\nextraction impact representation learning in vision-language models? We address\nthis gap by introducing a scalable subfigure extraction pipeline based on\ntransformer-based object detection, trained on a synthetic corpus of 500,000\ncompound figures, and achieving state-of-the-art performance on both ImageCLEF\n2016 and synthetic benchmarks. Using this pipeline, we release OPEN-PMC-18M, a\nlarge-scale high quality biomedical vision-language dataset comprising 18\nmillion clinically relevant subfigure-caption pairs spanning radiology,\nmicroscopy, and visible light photography. We train and evaluate\nvision-language models on our curated datasets and show improved performance\nacross retrieval, zero-shot classification, and robustness benchmarks,\noutperforming existing baselines. We release our dataset, models, and code to\nsupport reproducible benchmarks and further study into biomedical\nvision-language modeling and representation learning."}
{"id": "2506.02741", "pdf": "https://arxiv.org/pdf/2506.02741", "abs": "https://arxiv.org/abs/2506.02741", "authors": ["Pengchong Hu", "Zhizhong Han"], "title": "VTGaussian-SLAM: RGBD SLAM for Large Scale Scenes with Splatting View-Tied 3D Gaussians", "categories": ["cs.CV"], "comment": "ICML 2025", "summary": "Jointly estimating camera poses and mapping scenes from RGBD images is a\nfundamental task in simultaneous localization and mapping (SLAM).\nState-of-the-art methods employ 3D Gaussians to represent a scene, and render\nthese Gaussians through splatting for higher efficiency and better rendering.\nHowever, these methods cannot scale up to extremely large scenes, due to the\ninefficient tracking and mapping strategies that need to optimize all 3D\nGaussians in the limited GPU memories throughout the training to maintain the\ngeometry and color consistency to previous RGBD observations. To resolve this\nissue, we propose novel tracking and mapping strategies to work with a novel 3D\nrepresentation, dubbed view-tied 3D Gaussians, for RGBD SLAM systems. View-tied\n3D Gaussians is a kind of simplified Gaussians, which is tied to depth pixels,\nwithout needing to learn locations, rotations, and multi-dimensional variances.\nTying Gaussians to views not only significantly saves storage but also allows\nus to employ many more Gaussians to represent local details in the limited GPU\nmemory. Moreover, our strategies remove the need of maintaining all Gaussians\nlearnable throughout the training, while improving rendering quality, and\ntracking accuracy. We justify the effectiveness of these designs, and report\nbetter performance over the latest methods on the widely used benchmarks in\nterms of rendering and tracking accuracy and scalability. Please see our\nproject page for code and videos at\nhttps://machineperceptionlab.github.io/VTGaussian-SLAM-Project ."}
{"id": "2506.02751", "pdf": "https://arxiv.org/pdf/2506.02751", "abs": "https://arxiv.org/abs/2506.02751", "authors": ["Chuanyu Fu", "Yuqi Zhang", "Kunbin Yao", "Guanying Chen", "Yuan Xiong", "Chuan Huang", "Shuguang Cui", "Xiaochun Cao"], "title": "RobustSplat: Decoupling Densification and Dynamics for Transient-Free 3DGS", "categories": ["cs.CV"], "comment": "Project page: https://fcyycf.github.io/RobustSplat/", "summary": "3D Gaussian Splatting (3DGS) has gained significant attention for its\nreal-time, photo-realistic rendering in novel-view synthesis and 3D modeling.\nHowever, existing methods struggle with accurately modeling scenes affected by\ntransient objects, leading to artifacts in the rendered images. We identify\nthat the Gaussian densification process, while enhancing scene detail capture,\nunintentionally contributes to these artifacts by growing additional Gaussians\nthat model transient disturbances. To address this, we propose RobustSplat, a\nrobust solution based on two critical designs. First, we introduce a delayed\nGaussian growth strategy that prioritizes optimizing static scene structure\nbefore allowing Gaussian splitting/cloning, mitigating overfitting to transient\nobjects in early optimization. Second, we design a scale-cascaded mask\nbootstrapping approach that first leverages lower-resolution feature similarity\nsupervision for reliable initial transient mask estimation, taking advantage of\nits stronger semantic consistency and robustness to noise, and then progresses\nto high-resolution supervision to achieve more precise mask prediction.\nExtensive experiments on multiple challenging datasets show that our method\noutperforms existing methods, clearly demonstrating the robustness and\neffectiveness of our method. Our project page is\nhttps://fcyycf.github.io/RobustSplat/."}
{"id": "2506.02764", "pdf": "https://arxiv.org/pdf/2506.02764", "abs": "https://arxiv.org/abs/2506.02764", "authors": ["Fatma Youssef Mohammed", "Kostas Alexis"], "title": "Unified Attention Modeling for Efficient Free-Viewing and Visual Search via Shared Representations", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to the 2025 IEEE International Conference on Development and\n  Learning (ICDL)", "summary": "Computational human attention modeling in free-viewing and task-specific\nsettings is often studied separately, with limited exploration of whether a\ncommon representation exists between them. This work investigates this question\nand proposes a neural network architecture that builds upon the Human Attention\ntransformer (HAT) to test the hypothesis. Our results demonstrate that\nfree-viewing and visual search can efficiently share a common representation,\nallowing a model trained in free-viewing attention to transfer its knowledge to\ntask-driven visual search with a performance drop of only 3.86% in the\npredicted fixation scanpaths, measured by the semantic sequence score (SemSS)\nmetric which reflects the similarity between predicted and human scanpaths.\nThis transfer reduces computational costs by 92.29% in terms of GFLOPs and\n31.23% in terms of trainable parameters."}
{"id": "2506.02765", "pdf": "https://arxiv.org/pdf/2506.02765", "abs": "https://arxiv.org/abs/2506.02765", "authors": ["Chunwei Tian", "Kai Liu", "Bob Zhang", "Zhixiang Huang", "Chia-Wen Lin", "David Zhang"], "title": "A Dynamic Transformer Network for Vehicle Detection", "categories": ["cs.CV"], "comment": "8 pages, 5 figures. This paper has been accepted for publication in\n  IEEE Transactions on Consumer Electronics", "summary": "Stable consumer electronic systems can assist traffic better. Good traffic\nconsumer electronic systems require collaborative work between traffic\nalgorithms and hardware. However, performance of popular traffic algorithms\ncontaining vehicle detection methods based on deep networks via learning data\nrelation rather than learning differences in different lighting and occlusions\nis limited. In this paper, we present a dynamic Transformer network for vehicle\ndetection (DTNet). DTNet utilizes a dynamic convolution to guide a deep network\nto dynamically generate weights to enhance adaptability of an obtained\ndetector. Taking into relations of different information account, a mixed\nattention mechanism based channel attention and Transformer is exploited to\nstrengthen relations of channels and pixels to extract more salient information\nfor vehicle detection. To overcome the drawback of difference in an image\naccount, a translation-variant convolution relies on spatial location\ninformation to refine obtained structural information for vehicle detection.\nExperimental results illustrate that our DTNet is competitive for vehicle\ndetection. Code of the proposed DTNet can be obtained at\nhttps://github.com/hellloxiaotian/DTNet."}
{"id": "2506.02781", "pdf": "https://arxiv.org/pdf/2506.02781", "abs": "https://arxiv.org/abs/2506.02781", "authors": ["Tongyuan Bai", "Wangyuanfan Bai", "Dong Chen", "Tieru Wu", "Manyi Li", "Rui Ma"], "title": "FreeScene: Mixed Graph Diffusion for 3D Scene Synthesis from Free Prompts", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Controllability plays a crucial role in the practical applications of 3D\nindoor scene synthesis. Existing works either allow rough language-based\ncontrol, that is convenient but lacks fine-grained scene customization, or\nemploy graph based control, which offers better controllability but demands\nconsiderable knowledge for the cumbersome graph design process. To address\nthese challenges, we present FreeScene, a user-friendly framework that enables\nboth convenient and effective control for indoor scene synthesis.Specifically,\nFreeScene supports free-form user inputs including text description and/or\nreference images, allowing users to express versatile design intentions. The\nuser inputs are adequately analyzed and integrated into a graph representation\nby a VLM-based Graph Designer. We then propose MG-DiT, a Mixed Graph Diffusion\nTransformer, which performs graph-aware denoising to enhance scene generation.\nOur MG-DiT not only excels at preserving graph structure but also offers broad\napplicability to various tasks, including, but not limited to, text-to-scene,\ngraph-to-scene, and rearrangement, all within a single model. Extensive\nexperiments demonstrate that FreeScene provides an efficient and user-friendly\nsolution that unifies text-based and graph based scene synthesis, outperforming\nstate-of-the-art methods in terms of both generation quality and\ncontrollability in a range of applications."}
{"id": "2506.02783", "pdf": "https://arxiv.org/pdf/2506.02783", "abs": "https://arxiv.org/abs/2506.02783", "authors": ["Carlos Garcia-Lopez-de-Haro", "Caterina Fuster-Barcelo", "Curtis T. Rueden", "Jonathan Heras", "Vladimir Ulman", "Daniel Franco-Barranco", "Adrian Ines", "Kevin W. Eliceiri", "Jean-Christophe Olivo-Marin", "Jean-Yves Tinevez", "Daniel Sage", "Arrate Munoz-Barrutia"], "title": "SAMJ: Fast Image Annotation on ImageJ/Fiji via Segment Anything Model", "categories": ["cs.CV"], "comment": null, "summary": "Mask annotation remains a significant bottleneck in AI-driven biomedical\nimage analysis due to its labor-intensive nature. To address this challenge, we\nintroduce SAMJ, a user-friendly ImageJ/Fiji plugin leveraging the Segment\nAnything Model (SAM). SAMJ enables seamless, interactive annotations with\none-click installation on standard computers. Designed for real-time object\ndelineation in large scientific images, SAMJ is an easy-to-use solution that\nsimplifies and accelerates the creation of labeled image datasets."}
{"id": "2506.02789", "pdf": "https://arxiv.org/pdf/2506.02789", "abs": "https://arxiv.org/abs/2506.02789", "authors": ["Renxing Li", "Weiyi Tang", "Peiqi Li", "Qiming Huang", "Jiayuan She", "Shengkai Li", "Haoran Xu", "Yeyun Wan", "Jing Liu", "Hailong Fu", "Xiang Li", "Jiangang Chen"], "title": "Automated Measurement of Optic Nerve Sheath Diameter Using Ocular Ultrasound Video", "categories": ["cs.CV"], "comment": "17 pages, 9 figures", "summary": "Objective. Elevated intracranial pressure (ICP) is recognized as a biomarker\nof secondary brain injury, with a significant linear correlation observed\nbetween optic nerve sheath diameter (ONSD) and ICP. Frequent monitoring of ONSD\ncould effectively support dynamic evaluation of ICP. However, ONSD measurement\nis heavily reliant on the operator's experience and skill, particularly in\nmanually selecting the optimal frame from ultrasound sequences and measuring\nONSD. Approach. This paper presents a novel method to automatically identify\nthe optimal frame from video sequences for ONSD measurement by employing the\nKernel Correlation Filter (KCF) tracking algorithm and Simple Linear Iterative\nClustering (SLIC) segmentation algorithm. The optic nerve sheath is mapped and\nmeasured using a Gaussian Mixture Model (GMM) combined with a\nKL-divergence-based method. Results. When compared with the average\nmeasurements of two expert clinicians, the proposed method achieved a mean\nerror, mean squared deviation, and intraclass correlation coefficient (ICC) of\n0.04, 0.054, and 0.782, respectively. Significance. The findings suggest that\nthis method provides highly accurate automated ONSD measurements, showing\npotential for clinical application."}
{"id": "2506.02843", "pdf": "https://arxiv.org/pdf/2506.02843", "abs": "https://arxiv.org/abs/2506.02843", "authors": ["Shuai Yi", "Yixiong Zou", "Yuhua Li", "Ruixuan Li"], "title": "Random Registers for Cross-Domain Few-Shot Learning", "categories": ["cs.CV"], "comment": "Accepted by ICML 2025", "summary": "Cross-domain few-shot learning (CDFSL) aims to transfer knowledge from a\ndata-sufficient source domain to data-scarce target domains. Although Vision\nTransformer (ViT) has shown superior capability in many vision tasks, its\ntransferability against huge domain gaps in CDFSL is still under-explored. In\nthis paper, we find an intriguing phenomenon: during the source-domain\ntraining, prompt tuning, as a common way to train ViT, could be harmful for the\ngeneralization of ViT in target domains, but setting them to random noises\n(i.e., random registers) could consistently improve target-domain performance.\nWe then delve into this phenomenon for an interpretation. We find that\nlearnable prompts capture domain information during the training on the source\ndataset, which views irrelevant visual patterns as vital cues for recognition.\nThis can be viewed as a kind of overfitting and increases the sharpness of the\nloss landscapes. In contrast, random registers are essentially a novel way of\nperturbing attention for the sharpness-aware minimization, which helps the\nmodel find a flattened minimum in loss landscapes, increasing the\ntransferability. Based on this phenomenon and interpretation, we further\npropose a simple but effective approach for CDFSL to enhance the perturbation\non attention maps by adding random registers on the semantic regions of image\ntokens, improving the effectiveness and efficiency of random registers.\nExtensive experiments on four benchmarks validate our rationale and\nstate-of-the-art performance. Codes and models are available at\nhttps://github.com/shuaiyi308/REAP."}
{"id": "2506.02845", "pdf": "https://arxiv.org/pdf/2506.02845", "abs": "https://arxiv.org/abs/2506.02845", "authors": ["Di Wen", "Lei Qi", "Kunyu Peng", "Kailun Yang", "Fei Teng", "Ao Luo", "Jia Fu", "Yufan Chen", "Ruiping Liu", "Yitian Shi", "M. Saquib Sarfraz", "Rainer Stiefelhagen"], "title": "Go Beyond Earth: Understanding Human Actions and Scenes in Microgravity Environments", "categories": ["cs.CV"], "comment": "15 pages, 3 figures, submitted to NeurIPS 2025", "summary": "Despite substantial progress in video understanding, most existing datasets\nare limited to Earth's gravitational conditions. However, microgravity alters\nhuman motion, interactions, and visual semantics, revealing a critical gap for\nreal-world vision systems. This presents a challenge for domain-robust video\nunderstanding in safety-critical space applications. To address this, we\nintroduce MicroG-4M, the first benchmark for spatio-temporal and semantic\nunderstanding of human activities in microgravity. Constructed from real-world\nspace missions and cinematic simulations, the dataset includes 4,759 clips\ncovering 50 actions, 1,238 context-rich captions, and over 7,000\nquestion-answer pairs on astronaut activities and scene understanding.\nMicroG-4M supports three core tasks: fine-grained multi-label action\nrecognition, temporal video captioning, and visual question answering, enabling\na comprehensive evaluation of both spatial localization and semantic reasoning\nin microgravity contexts. We establish baselines using state-of-the-art models.\nAll data, annotations, and code are available at\nhttps://github.com/LEI-QI-233/HAR-in-Space."}
{"id": "2506.02846", "pdf": "https://arxiv.org/pdf/2506.02846", "abs": "https://arxiv.org/abs/2506.02846", "authors": ["Yujin Chen", "Yinyu Nie", "Benjamin Ummenhofer", "Reiner Birkl", "Michael Paulitsch", "Matthias Nie√üner"], "title": "PBR-SR: Mesh PBR Texture Super Resolution from 2D Image Priors", "categories": ["cs.CV"], "comment": "Project page: https://terencecyj.github.io/projects/PBR-SR/, Video:\n  https://youtu.be/eaM5S3Mt1RM", "summary": "We present PBR-SR, a novel method for physically based rendering (PBR)\ntexture super resolution (SR). It outputs high-resolution, high-quality PBR\ntextures from low-resolution (LR) PBR input in a zero-shot manner. PBR-SR\nleverages an off-the-shelf super-resolution model trained on natural images,\nand iteratively minimizes the deviations between super-resolution priors and\ndifferentiable renderings. These enhancements are then back-projected into the\nPBR map space in a differentiable manner to produce refined, high-resolution\ntextures. To mitigate view inconsistencies and lighting sensitivity, which is\ncommon in view-based super-resolution, our method applies 2D prior constraints\nacross multi-view renderings, iteratively refining the shared, upscaled\ntextures. In parallel, we incorporate identity constraints directly in the PBR\ntexture domain to ensure the upscaled textures remain faithful to the LR input.\nPBR-SR operates without any additional training or data requirements, relying\nentirely on pretrained image priors. We demonstrate that our approach produces\nhigh-fidelity PBR textures for both artist-designed and AI-generated meshes,\noutperforming both direct SR models application and prior texture optimization\nmethods. Our results show high-quality outputs in both PBR and rendering\nevaluations, supporting advanced applications such as relighting."}
{"id": "2506.02850", "pdf": "https://arxiv.org/pdf/2506.02850", "abs": "https://arxiv.org/abs/2506.02850", "authors": ["Mengyue Wang", "Shuo Chen", "Kristian Kersting", "Volker Tresp", "Yunpu Ma"], "title": "METok: Multi-Stage Event-based Token Compression for Efficient Long Video Understanding", "categories": ["cs.CV"], "comment": "14 pages, 10 figures", "summary": "Recent advances in Video Large Language Models (VLLMs) have significantly\nenhanced their ability to understand video content. Nonetheless, processing\nlong videos remains challenging due to high computational demands and the\nredundancy present in the visual data. In this work, we propose METok, a\ntraining-free, Multi-stage Event-based Token compression framework designed to\naccelerate VLLMs' inference while preserving accuracy. METok progressively\neliminates redundant visual tokens across three critical stages: (1)\nevent-aware compression during vision encoding, (2) hierarchical token pruning\nin the prefilling stage based on semantic alignment and event importance, and\n(3) a decoding-stage KV Cache optimization that further reduces memory\nconsumption. Our experiments on diverse video benchmarks demonstrate that METok\nachieves an optimal trade-off between efficiency and accuracy by dynamically\nselecting informative visual tokens. For instance, equipping LongVA-7B with\nMETok realizes an 80.6% FLOPs reduction and 93.5% KV Cache memory savings, all\nwhile maintaining comparable or even superior accuracy."}
{"id": "2506.02853", "pdf": "https://arxiv.org/pdf/2506.02853", "abs": "https://arxiv.org/abs/2506.02853", "authors": ["Mingjie Wei", "Xuemei Xie", "Yutong Zhong", "Guangming Shi"], "title": "Learning Pyramid-structured Long-range Dependencies for 3D Human Pose Estimation", "categories": ["cs.CV"], "comment": "Accepted by IEEE Transactions on Multimedia (TMM)", "summary": "Action coordination in human structure is indispensable for the spatial\nconstraints of 2D joints to recover 3D pose. Usually, action coordination is\nrepresented as a long-range dependence among body parts. However, there are two\nmain challenges in modeling long-range dependencies. First, joints should not\nonly be constrained by other individual joints but also be modulated by the\nbody parts. Second, existing methods make networks deeper to learn dependencies\nbetween non-linked parts. They introduce uncorrelated noise and increase the\nmodel size. In this paper, we utilize a pyramid structure to better learn\npotential long-range dependencies. It can capture the correlation across joints\nand groups, which complements the context of the human sub-structure. In an\neffective cross-scale way, it captures the pyramid-structured long-range\ndependence. Specifically, we propose a novel Pyramid Graph Attention (PGA)\nmodule to capture long-range cross-scale dependencies. It concatenates\ninformation from various scales into a compact sequence, and then computes the\ncorrelation between scales in parallel. Combining PGA with graph convolution\nmodules, we develop a Pyramid Graph Transformer (PGFormer) for 3D human pose\nestimation, which is a lightweight multi-scale transformer architecture. It\nencapsulates human sub-structures into self-attention by pooling. Extensive\nexperiments show that our approach achieves lower error and smaller model size\nthan state-of-the-art methods on Human3.6M and MPI-INF-3DHP datasets. The code\nis available at https://github.com/MingjieWe/PGFormer."}
{"id": "2506.02854", "pdf": "https://arxiv.org/pdf/2506.02854", "abs": "https://arxiv.org/abs/2506.02854", "authors": ["Mengmeng Zhang", "Xingyuan Dai", "Yicheng Sun", "Jing Wang", "Yueyang Yao", "Xiaoyan Gong", "Fuze Cong", "Feiyue Wang", "Yisheng Lv"], "title": "Hierarchical Self-Prompting SAM: A Prompt-Free Medical Image Segmentation Framework", "categories": ["cs.CV"], "comment": null, "summary": "Although the Segment Anything Model (SAM) is highly effective in natural\nimage segmentation, it requires dependencies on prompts, which limits its\napplicability to medical imaging where manual prompts are often unavailable.\nExisting efforts to fine-tune SAM for medical segmentation typically struggle\nto remove this dependency. We propose Hierarchical Self-Prompting SAM\n(HSP-SAM), a novel self-prompting framework that enables SAM to achieve strong\nperformance in prompt-free medical image segmentation. Unlike previous\nself-prompting methods that remain limited to positional prompts similar to\nvanilla SAM, we are the first to introduce learning abstract prompts during the\nself-prompting process. This simple and intuitive self-prompting framework\nachieves superior performance on classic segmentation tasks such as polyp and\nskin lesion segmentation, while maintaining robustness across diverse medical\nimaging modalities. Furthermore, it exhibits strong generalization to unseen\ndatasets, achieving improvements of up to 14.04% over previous state-of-the-art\nmethods on some challenging benchmarks. These results suggest that abstract\nprompts encapsulate richer and higher-dimensional semantic information compared\nto positional prompts, thereby enhancing the model's robustness and\ngeneralization performance. All models and codes will be released upon\nacceptance."}
{"id": "2506.02857", "pdf": "https://arxiv.org/pdf/2506.02857", "abs": "https://arxiv.org/abs/2506.02857", "authors": ["Luca Maiano", "Fabrizio Casadei", "Irene Amerini"], "title": "Enhancing Abnormality Identification: Robust Out-of-Distribution Strategies for Deepfake Detection", "categories": ["cs.CV"], "comment": null, "summary": "Detecting deepfakes has become a critical challenge in Computer Vision and\nArtificial Intelligence. Despite significant progress in detection techniques,\ngeneralizing them to open-set scenarios continues to be a persistent\ndifficulty. Neural networks are often trained on the closed-world assumption,\nbut with new generative models constantly evolving, it is inevitable to\nencounter data generated by models that are not part of the training\ndistribution. To address these challenges, in this paper, we propose two novel\nOut-Of-Distribution (OOD) detection approaches. The first approach is trained\nto reconstruct the input image, while the second incorporates an attention\nmechanism for detecting OODs. Our experiments validate the effectiveness of the\nproposed approaches compared to existing state-of-the-art techniques. Our\nmethod achieves promising results in deepfake detection and ranks among the\ntop-performing configurations on the benchmark, demonstrating their potential\nfor robust, adaptable solutions in dynamic, real-world applications."}
{"id": "2506.02866", "pdf": "https://arxiv.org/pdf/2506.02866", "abs": "https://arxiv.org/abs/2506.02866", "authors": ["Ahsan Baidar Bakht", "Muhayy Ud Din", "Sajid Javed", "Irfan Hussain"], "title": "MVTD: A Benchmark Dataset for Maritime Visual Object Tracking", "categories": ["cs.CV"], "comment": "Submited to Nature Scientific Data", "summary": "Visual Object Tracking (VOT) is a fundamental task with widespread\napplications in autonomous navigation, surveillance, and maritime robotics.\nDespite significant advances in generic object tracking, maritime environments\ncontinue to present unique challenges, including specular water reflections,\nlow-contrast targets, dynamically changing backgrounds, and frequent\nocclusions. These complexities significantly degrade the performance of\nstate-of-the-art tracking algorithms, highlighting the need for domain-specific\ndatasets. To address this gap, we introduce the Maritime Visual Tracking\nDataset (MVTD), a comprehensive and publicly available benchmark specifically\ndesigned for maritime VOT. MVTD comprises 182 high-resolution video sequences,\ntotaling approximately 150,000 frames, and includes four representative object\nclasses: boat, ship, sailboat, and unmanned surface vehicle (USV). The dataset\ncaptures a diverse range of operational conditions and maritime scenarios,\nreflecting the real-world complexities of maritime environments. We evaluated\n14 recent SOTA tracking algorithms on the MVTD benchmark and observed\nsubstantial performance degradation compared to their performance on\ngeneral-purpose datasets. However, when fine-tuned on MVTD, these models\ndemonstrate significant performance gains, underscoring the effectiveness of\ndomain adaptation and the importance of transfer learning in specialized\ntracking contexts. The MVTD dataset fills a critical gap in the visual tracking\ncommunity by providing a realistic and challenging benchmark for maritime\nscenarios. Dataset and Source Code can be accessed here\n\"https://github.com/AhsanBaidar/MVTD\"."}
{"id": "2506.02868", "pdf": "https://arxiv.org/pdf/2506.02868", "abs": "https://arxiv.org/abs/2506.02868", "authors": ["Amal S. Perera", "David Fernandez", "Chandi Witharana", "Elias Manos", "Michael Pimenta", "Anna K. Liljedahl", "Ingmar Nitze", "Yili Yang", "Todd Nicholson", "Chia-Yu Hsu", "Wenwen Li", "Guido Grosse"], "title": "Pan-Arctic Permafrost Landform and Human-built Infrastructure Feature Detection with Vision Transformers and Location Embeddings", "categories": ["cs.CV", "I.4.6; I.5.4; I.5.2; I.2.10"], "comment": "20 pages, 2 column IEEE format, 13 Figures", "summary": "Accurate mapping of permafrost landforms, thaw disturbances, and human-built\ninfrastructure at pan-Arctic scale using sub-meter satellite imagery is\nincreasingly critical. Handling petabyte-scale image data requires\nhigh-performance computing and robust feature detection models. While\nconvolutional neural network (CNN)-based deep learning approaches are widely\nused for remote sensing (RS),similar to the success in transformer based large\nlanguage models, Vision Transformers (ViTs) offer advantages in capturing\nlong-range dependencies and global context via attention mechanisms. ViTs\nsupport pretraining via self-supervised learning-addressing the common\nlimitation of labeled data in Arctic feature detection and outperform CNNs on\nbenchmark datasets. Arctic also poses challenges for model generalization,\nespecially when features with the same semantic class exhibit diverse spectral\ncharacteristics. To address these issues for Arctic feature detection, we\nintegrate geospatial location embeddings into ViTs to improve adaptation across\nregions. This work investigates: (1) the suitability of pre-trained ViTs as\nfeature extractors for high-resolution Arctic remote sensing tasks, and (2) the\nbenefit of combining image and location embeddings. Using previously published\ndatasets for Arctic feature detection, we evaluate our models on three\ntasks-detecting ice-wedge polygons (IWP), retrogressive thaw slumps (RTS), and\nhuman-built infrastructure. We empirically explore multiple configurations to\nfuse image embeddings and location embeddings. Results show that ViTs with\nlocation embeddings outperform prior CNN-based models on two of the three tasks\nincluding F1 score increase from 0.84 to 0.92 for RTS detection, demonstrating\nthe potential of transformer-based models with spatial awareness for Arctic RS\napplications."}
{"id": "2506.02875", "pdf": "https://arxiv.org/pdf/2506.02875", "abs": "https://arxiv.org/abs/2506.02875", "authors": ["Xiaohong Liu", "Xiongkuo Min", "Qiang Hu", "Xiaoyun Zhang", "Jie Guo", "Guangtao Zhai", "Shushi Wang", "Yingjie Zhou", "Lu Liu", "Jingxin Li", "Liu Yang", "Farong Wen", "Li Xu", "Yanwei Jiang", "Xilei Zhu", "Chunyi Li", "Zicheng Zhang", "Huiyu Duan", "Xiele Wu", "Yixuan Gao", "Yuqin Cao", "Jun Jia", "Wei Sun", "Jiezhang Cao", "Radu Timofte", "Baojun Li", "Jiamian Huang", "Dan Luo", "Tao Liu", "Weixia Zhang", "Bingkun Zheng", "Junlin Chen", "Ruikai Zhou", "Meiya Chen", "Yu Wang", "Hao Jiang", "Xiantao Li", "Yuxiang Jiang", "Jun Tang", "Yimeng Zhao", "Bo Hu", "Zelu Qi", "Chaoyang Zhang", "Fei Zhao", "Ping Shi", "Lingzhi Fu", "Heng Cong", "Shuai He", "Rongyu Zhang", "Jiarong He", "Zongyao Hu", "Wei Luo", "Zihao Yu", "Fengbin Guan", "Yiting Lu", "Xin Li", "Zhibo Chen", "Mengjing Su", "Yi Wang", "Tuo Chen", "Chunxiao Li", "Shuaiyu Zhao", "Jiaxin Wen", "Chuyi Lin", "Sitong Liu", "Ningxin Chu", "Jing Wan", "Yu Zhou", "Baoying Chen", "Jishen Zeng", "Jiarui Liu", "Xianjin Liu", "Xin Chen", "Lanzhi Zhou", "Hangyu Li", "You Han", "Bibo Xiang", "Zhenjie Liu", "Jianzhang Lu", "Jialin Gui", "Renjie Lu", "Shangfei Wang", "Donghao Zhou", "Jingyu Lin", "Quanjian Song", "Jiancheng Huang", "Yufeng Yang", "Changwei Wang", "Shupeng Zhong", "Yang Yang", "Lihuo He", "Jia Liu", "Yuting Xing", "Tida Fang", "Yuchun Jin"], "title": "NTIRE 2025 XGC Quality Assessment Challenge: Methods and Results", "categories": ["cs.CV"], "comment": "NTIRE 2025 XGC Quality Assessment Challenge Report. arXiv admin note:\n  text overlap with arXiv:2404.16687", "summary": "This paper reports on the NTIRE 2025 XGC Quality Assessment Challenge, which\nwill be held in conjunction with the New Trends in Image Restoration and\nEnhancement Workshop (NTIRE) at CVPR 2025. This challenge is to address a major\nchallenge in the field of video and talking head processing. The challenge is\ndivided into three tracks, including user generated video, AI generated video\nand talking head. The user-generated video track uses the FineVD-GC, which\ncontains 6,284 user generated videos. The user-generated video track has a\ntotal of 125 registered participants. A total of 242 submissions are received\nin the development phase, and 136 submissions are received in the test phase.\nFinally, 5 participating teams submitted their models and fact sheets. The AI\ngenerated video track uses the Q-Eval-Video, which contains 34,029 AI-Generated\nVideos (AIGVs) generated by 11 popular Text-to-Video (T2V) models. A total of\n133 participants have registered in this track. A total of 396 submissions are\nreceived in the development phase, and 226 submissions are received in the test\nphase. Finally, 6 participating teams submitted their models and fact sheets.\nThe talking head track uses the THQA-NTIRE, which contains 12,247 2D and 3D\ntalking heads. A total of 89 participants have registered in this track. A\ntotal of 225 submissions are received in the development phase, and 118\nsubmissions are received in the test phase. Finally, 8 participating teams\nsubmitted their models and fact sheets. Each participating team in every track\nhas proposed a method that outperforms the baseline, which has contributed to\nthe development of fields in three tracks."}
{"id": "2506.02882", "pdf": "https://arxiv.org/pdf/2506.02882", "abs": "https://arxiv.org/abs/2506.02882", "authors": ["Sohyun Lee", "Yeho Kwon", "Lukas Hoyer", "Suha Kwak"], "title": "GaRA-SAM: Robustifying Segment Anything Model with Gated-Rank Adaptation", "categories": ["cs.CV"], "comment": null, "summary": "Improving robustness of the Segment Anything Model (SAM) to input\ndegradations is critical for its deployment in high-stakes applications such as\nautonomous driving and robotics. Our approach to this challenge prioritizes\nthree key aspects: first, parameter efficiency to maintain the inherent\ngeneralization capability of SAM; second, fine-grained and input-aware\nrobustification to precisely address the input corruption; and third, adherence\nto standard training protocols for ease of training. To this end, we propose\ngated-rank adaptation (GaRA). GaRA introduces lightweight adapters into\nintermediate layers of the frozen SAM, where each adapter dynamically adjusts\nthe effective rank of its weight matrix based on the input by selectively\nactivating (rank-1) components of the matrix using a learned gating module.\nThis adjustment enables fine-grained and input-aware robustification without\ncompromising the generalization capability of SAM. Our model, GaRA-SAM,\nsignificantly outperforms prior work on all robust segmentation benchmarks. In\nparticular, it surpasses the previous best IoU score by up to 21.3\\%p on ACDC,\na challenging real corrupted image dataset."}
{"id": "2506.02891", "pdf": "https://arxiv.org/pdf/2506.02891", "abs": "https://arxiv.org/abs/2506.02891", "authors": ["Jiewen Hu", "Leena Mathur", "Paul Pu Liang", "Louis-Philippe Morency"], "title": "OpenFace 3.0: A Lightweight Multitask System for Comprehensive Facial Behavior Analysis", "categories": ["cs.CV"], "comment": "IEEE FG 2025, \\c{opyright} 2025 IEEE. Personal use of this material\n  is permitted. Permission from IEEE must be obtained for all other uses, in\n  any current or future media, including reprinting/republishing this material\n  for advertising or promotional purposes, creating new collective works, for\n  resale or redistribution to servers or lists, or reuse of any copyrighted\n  component of this work", "summary": "In recent years, there has been increasing interest in automatic facial\nbehavior analysis systems from computing communities such as vision, multimodal\ninteraction, robotics, and affective computing. Building upon the widespread\nutility of prior open-source facial analysis systems, we introduce OpenFace\n3.0, an open-source toolkit capable of facial landmark detection, facial action\nunit detection, eye-gaze estimation, and facial emotion recognition. OpenFace\n3.0 contributes a lightweight unified model for facial analysis, trained with a\nmulti-task architecture across diverse populations, head poses, lighting\nconditions, video resolutions, and facial analysis tasks. By leveraging the\nbenefits of parameter sharing through a unified model and training paradigm,\nOpenFace 3.0 exhibits improvements in prediction performance, inference speed,\nand memory efficiency over similar toolkits and rivals state-of-the-art models.\nOpenFace 3.0 can be installed and run with a single line of code and operate in\nreal-time without specialized hardware. OpenFace 3.0 code for training models\nand running the system is freely available for research purposes and supports\ncontributions from the community."}
{"id": "2506.02893", "pdf": "https://arxiv.org/pdf/2506.02893", "abs": "https://arxiv.org/abs/2506.02893", "authors": ["Jonathan Astermark", "Anders Heyden", "Viktor Larsson"], "title": "Dense Match Summarization for Faster Two-view Estimation", "categories": ["cs.CV"], "comment": "Accepted to Computer Vision and Pattern Recognition (CVPR) 2025", "summary": "In this paper, we speed up robust two-view relative pose from dense\ncorrespondences. Previous work has shown that dense matchers can significantly\nimprove both accuracy and robustness in the resulting pose. However, the large\nnumber of matches comes with a significantly increased runtime during robust\nestimation in RANSAC. To avoid this, we propose an efficient match\nsummarization scheme which provides comparable accuracy to using the full set\nof dense matches, while having 10-100x faster runtime. We validate our approach\non standard benchmark datasets together with multiple state-of-the-art dense\nmatchers."}
{"id": "2506.02896", "pdf": "https://arxiv.org/pdf/2506.02896", "abs": "https://arxiv.org/abs/2506.02896", "authors": ["Adam Pardyl", "Dominik Matuszek", "Mateusz Przebieracz", "Marek Cygan", "Bartosz Zieli≈Ñski", "Maciej Wo≈Çczyk"], "title": "FlySearch: Exploring how vision-language models explore", "categories": ["cs.CV", "cs.LG", "cs.RO"], "comment": null, "summary": "The real world is messy and unstructured. Uncovering critical information\noften requires active, goal-driven exploration. It remains to be seen whether\nVision-Language Models (VLMs), which recently emerged as a popular zero-shot\ntool in many difficult tasks, can operate effectively in such conditions. In\nthis paper, we answer this question by introducing FlySearch, a 3D, outdoor,\nphotorealistic environment for searching and navigating to objects in complex\nscenes. We define three sets of scenarios with varying difficulty and observe\nthat state-of-the-art VLMs cannot reliably solve even the simplest exploration\ntasks, with the gap to human performance increasing as the tasks get harder. We\nidentify a set of central causes, ranging from vision hallucination, through\ncontext misunderstanding, to task planning failures, and we show that some of\nthem can be addressed by finetuning. We publicly release the benchmark,\nscenarios, and the underlying codebase."}
{"id": "2506.02914", "pdf": "https://arxiv.org/pdf/2506.02914", "abs": "https://arxiv.org/abs/2506.02914", "authors": ["Yechi Ma", "Wei Hua", "Shu Kong"], "title": "Towards Auto-Annotation from Annotation Guidelines: A Benchmark through 3D LiDAR Detection", "categories": ["cs.CV"], "comment": null, "summary": "A crucial yet under-appreciated prerequisite in machine learning solutions\nfor real-applications is data annotation: human annotators are hired to\nmanually label data according to detailed, expert-crafted guidelines. This is\noften a laborious, tedious, and costly process. To study methods for\nfacilitating data annotation, we introduce a new benchmark AnnoGuide:\nAuto-Annotation from Annotation Guidelines. It aims to evaluate automated\nmethods for data annotation directly from expert-defined annotation guidelines,\neliminating the need for manual labeling. As a case study, we repurpose the\nwell-established nuScenes dataset, commonly used in autonomous driving\nresearch, which provides comprehensive annotation guidelines for labeling LiDAR\npoint clouds with 3D cuboids across 18 object classes. These guidelines include\na few visual examples and textual descriptions, but no labeled 3D cuboids in\nLiDAR data, making this a novel task of multi-modal few-shot 3D detection\nwithout 3D annotations. The advances of powerful foundation models (FMs) make\nAnnoGuide especially timely, as FMs offer promising tools to tackle its\nchallenges. We employ a conceptually straightforward pipeline that (1) utilizes\nopen-source FMs for object detection and segmentation in RGB images, (2)\nprojects 2D detections into 3D using known camera poses, and (3) clusters LiDAR\npoints within the frustum of each 2D detection to generate a 3D cuboid.\nStarting with a non-learned solution that leverages off-the-shelf FMs, we\nprogressively refine key components and achieve significant performance\nimprovements, boosting 3D detection mAP from 12.1 to 21.9! Nevertheless, our\nresults highlight that AnnoGuide remains an open and challenging problem,\nunderscoring the urgent need for developing LiDAR-based FMs. We release our\ncode and models at GitHub: https://annoguide.github.io/annoguide3Dbenchmark"}
{"id": "2506.02938", "pdf": "https://arxiv.org/pdf/2506.02938", "abs": "https://arxiv.org/abs/2506.02938", "authors": ["Xuhui Chen", "Fei Hou", "Wencheng Wang", "Hong Qin", "Ying He"], "title": "MIND: Material Interface Generation from UDFs for Non-Manifold Surface Reconstruction", "categories": ["cs.CV"], "comment": null, "summary": "Unsigned distance fields (UDFs) are widely used in 3D deep learning due to\ntheir ability to represent shapes with arbitrary topology. While prior work has\nlargely focused on learning UDFs from point clouds or multi-view images,\nextracting meshes from UDFs remains challenging, as the learned fields rarely\nattain exact zero distances. A common workaround is to reconstruct signed\ndistance fields (SDFs) locally from UDFs to enable surface extraction via\nMarching Cubes. However, this often introduces topological artifacts such as\nholes or spurious components. Moreover, local SDFs are inherently incapable of\nrepresenting non-manifold geometry, leading to complete failure in such cases.\nTo address this gap, we propose MIND (Material Interface from Non-manifold\nDistance fields), a novel algorithm for generating material interfaces directly\nfrom UDFs, enabling non-manifold mesh extraction from a global perspective. The\ncore of our method lies in deriving a meaningful spatial partitioning from the\nUDF, where the target surface emerges as the interface between distinct\nregions. We begin by computing a two-signed local field to distinguish the two\nsides of manifold patches, and then extend this to a multi-labeled global field\ncapable of separating all sides of a non-manifold structure. By combining this\nmulti-labeled field with the input UDF, we construct material interfaces that\nsupport non-manifold mesh extraction via a multi-labeled Marching Cubes\nalgorithm. Extensive experiments on UDFs generated from diverse data sources,\nincluding point cloud reconstruction, multi-view reconstruction, and medial\naxis transforms, demonstrate that our approach robustly handles complex\nnon-manifold surfaces and significantly outperforms existing methods."}
{"id": "2506.02964", "pdf": "https://arxiv.org/pdf/2506.02964", "abs": "https://arxiv.org/abs/2506.02964", "authors": ["Guiqiu Liao", "Matjaz Jogan", "Eric Eaton", "Daniel A. Hashimoto"], "title": "FORLA:Federated Object-centric Representation Learning with Slot Attention", "categories": ["cs.CV", "cs.LG"], "comment": "24 pages, 6 figures", "summary": "Learning efficient visual representations across heterogeneous unlabeled\ndatasets remains a central challenge in federated learning. Effective federated\nrepresentations require features that are jointly informative across clients\nwhile disentangling domain-specific factors without supervision. We introduce\nFORLA, a novel framework for federated object-centric representation learning\nand feature adaptation across clients using unsupervised slot attention. At the\ncore of our method is a shared feature adapter, trained collaboratively across\nclients to adapt features from foundation models, and a shared slot attention\nmodule that learns to reconstruct the adapted features. To optimize this\nadapter, we design a two-branch student-teacher architecture. In each client, a\nstudent decoder learns to reconstruct full features from foundation models,\nwhile a teacher decoder reconstructs their adapted, low-dimensional\ncounterpart. The shared slot attention module bridges cross-domain learning by\naligning object-level representations across clients. Experiments in multiple\nreal-world datasets show that our framework not only outperforms centralized\nbaselines on object discovery but also learns a compact, universal\nrepresentation that generalizes well across domains. This work highlights\nfederated slot attention as an effective tool for scalable, unsupervised visual\nrepresentation learning from cross-domain data with distributed concepts."}
{"id": "2506.02975", "pdf": "https://arxiv.org/pdf/2506.02975", "abs": "https://arxiv.org/abs/2506.02975", "authors": ["Yicheng Xiao", "Lin Song", "Rui Yang", "Cheng Cheng", "Zunnan Xu", "Zhaoyang Zhang", "Yixiao Ge", "Xiu Li", "Ying Shan"], "title": "HaploOmni: Unified Single Transformer for Multimodal Video Understanding and Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "With the advancement of language models, unified multimodal understanding and\ngeneration have made significant strides, with model architectures evolving\nfrom separated components to unified single-model frameworks. This paper\nexplores an efficient training paradigm to build a single transformer for\nunified multimodal understanding and generation. Specifically, we propose a\nmultimodal warmup strategy utilizing prior knowledge to extend capabilities. To\naddress cross-modal compatibility challenges, we introduce feature pre-scaling\nand multimodal AdaLN techniques. Integrating the proposed technologies, we\npresent the HaploOmni, a new single multimodal transformer. With limited\ntraining costs, HaploOmni achieves competitive performance across multiple\nimage and video understanding and generation benchmarks over advanced unified\nmodels. All codes will be made public at https://github.com/Tencent/HaploVLM."}
{"id": "2506.02976", "pdf": "https://arxiv.org/pdf/2506.02976", "abs": "https://arxiv.org/abs/2506.02976", "authors": ["Rachid Zeghlache", "Ikram Brahim", "Pierre-Henri Conze", "Mathieu Lamard", "Mohammed El Amine Lazouni", "Zineb Aziza Elaouaber", "Leila Ryma Lazouni", "Christopher Nielsen", "Ahmad O. Ahsan", "Matthias Wilms", "Nils D. Forkert", "Lovre Antonio Budimir", "Ivana Matovinoviƒá", "Donik Vr≈°nak", "Sven Lonƒçariƒá", "Philippe Zhang", "Weili Jiang", "Yihao Li", "Yiding Hao", "Markus Frohmann", "Patrick Binder", "Marcel Huber", "Taha Emre", "Teresa Finisterra Ara√∫jo", "Marzieh Oghbaie", "Hrvoje Bogunoviƒá", "Amerens A. Bekkers", "Nina M. van Liebergen", "Hugo J. Kuijf", "Abdul Qayyum", "Moona Mazher", "Steven A. Niederer", "Alberto J. Beltr√°n-Carrero", "Juan J. G√≥mez-Valverde", "Javier Torresano-Rodr√≠quez", "√Ålvaro Caballero-Sastre", "Mar√≠a J. Ledesma Carbayo", "Yosuke Yamagishi", "Yi Ding", "Robin Peretzke", "Alexandra Ertl", "Maximilian Fischer", "Jessica K√§chele", "Sofiane Zehar", "Karim Boukli Hacene", "Thomas Monfort", "B√©atrice Cochener", "Mostafa El Habib Daho", "Anas-Alexis Benyoussef", "Gwenol√© Quellec"], "title": "Deep Learning for Retinal Degeneration Assessment: A Comprehensive Analysis of the MARIO AMD Progression Challenge", "categories": ["cs.CV", "cs.AI"], "comment": "MARIO-MICCAI-CHALLENGE 2024", "summary": "The MARIO challenge, held at MICCAI 2024, focused on advancing the automated\ndetection and monitoring of age-related macular degeneration (AMD) through the\nanalysis of optical coherence tomography (OCT) images. Designed to evaluate\nalgorithmic performance in detecting neovascular activity changes within AMD,\nthe challenge incorporated unique multi-modal datasets. The primary dataset,\nsourced from Brest, France, was used by participating teams to train and test\ntheir models. The final ranking was determined based on performance on this\ndataset. An auxiliary dataset from Algeria was used post-challenge to evaluate\npopulation and device shifts from submitted solutions. Two tasks were involved\nin the MARIO challenge. The first one was the classification of evolution\nbetween two consecutive 2D OCT B-scans. The second one was the prediction of\nfuture AMD evolution over three months for patients undergoing anti-vascular\nendothelial growth factor (VEGF) therapy. Thirty-five teams participated, with\nthe top 12 finalists presenting their methods. This paper outlines the\nchallenge's structure, tasks, data characteristics, and winning methodologies,\nsetting a benchmark for AMD monitoring using OCT, infrared imaging, and\nclinical data (such as the number of visits, age, gender, etc.). The results of\nthis challenge indicate that artificial intelligence (AI) performs as well as a\nphysician in measuring AMD progression (Task 1) but is not yet able of\npredicting future evolution (Task 2)."}
{"id": "2506.02981", "pdf": "https://arxiv.org/pdf/2506.02981", "abs": "https://arxiv.org/abs/2506.02981", "authors": ["Joonyeoup Kim", "Yu Yuan", "Xingguang Zhang", "Xijun Wang", "Stanley Chan"], "title": "Astrophotography turbulence mitigation via generative models", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Photography is the cornerstone of modern astronomical and space research.\nHowever, most astronomical images captured by ground-based telescopes suffer\nfrom atmospheric turbulence, resulting in degraded imaging quality. While\nmulti-frame strategies like lucky imaging can mitigate some effects, they\ninvolve intensive data acquisition and complex manual processing. In this\npaper, we propose AstroDiff, a generative restoration method that leverages\nboth the high-quality generative priors and restoration capabilities of\ndiffusion models to mitigate atmospheric turbulence. Extensive experiments\ndemonstrate that AstroDiff outperforms existing state-of-the-art learning-based\nmethods in astronomical image turbulence mitigation, providing higher\nperceptual quality and better structural fidelity under severe turbulence\nconditions. Our code and additional results are available at\nhttps://web-six-kappa-66.vercel.app/"}
{"id": "2506.03007", "pdf": "https://arxiv.org/pdf/2506.03007", "abs": "https://arxiv.org/abs/2506.03007", "authors": ["Jiarui Wang", "Huiyu Duan", "Juntong Wang", "Ziheng Jia", "Woo Yi Yang", "Xiaorong Zhu", "Yu Zhao", "Jiaying Qian", "Yuke Xing", "Guangtao Zhai", "Xiongkuo Min"], "title": "DFBench: Benchmarking Deepfake Image Detection Capability of Large Multimodal Models", "categories": ["cs.CV"], "comment": null, "summary": "With the rapid advancement of generative models, the realism of AI-generated\nimages has significantly improved, posing critical challenges for verifying\ndigital content authenticity. Current deepfake detection methods often depend\non datasets with limited generation models and content diversity that fail to\nkeep pace with the evolving complexity and increasing realism of the\nAI-generated content. Large multimodal models (LMMs), widely adopted in various\nvision tasks, have demonstrated strong zero-shot capabilities, yet their\npotential in deepfake detection remains largely unexplored. To bridge this gap,\nwe present \\textbf{DFBench}, a large-scale DeepFake Benchmark featuring (i)\nbroad diversity, including 540,000 images across real, AI-edited, and\nAI-generated content, (ii) latest model, the fake images are generated by 12\nstate-of-the-art generation models, and (iii) bidirectional benchmarking and\nevaluating for both the detection accuracy of deepfake detectors and the\nevasion capability of generative models. Based on DFBench, we propose\n\\textbf{MoA-DF}, Mixture of Agents for DeepFake detection, leveraging a\ncombined probability strategy from multiple LMMs. MoA-DF achieves\nstate-of-the-art performance, further proving the effectiveness of leveraging\nLMMs for deepfake detection. Database and codes are publicly available at\nhttps://github.com/IntMeGroup/DFBench."}
{"id": "2506.03022", "pdf": "https://arxiv.org/pdf/2506.03022", "abs": "https://arxiv.org/abs/2506.03022", "authors": ["David McVicar", "Brian Avant", "Adrian Gould", "Diego Torrejon", "Charles Della Porta", "Ryan Mukherjee"], "title": "Smartflow: Enabling Scalable Spatiotemporal Geospatial Research", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "BlackSky introduces Smartflow, a cloud-based framework enabling scalable\nspatiotemporal geospatial research built on open-source tools and technologies.\nUsing STAC-compliant catalogs as a common input, heterogeneous geospatial data\ncan be processed into standardized datacubes for analysis and model training.\nModel experimentation is managed using a combination of tools, including\nClearML, Tensorboard, and Apache Superset. Underpinning Smartflow is\nKubernetes, which orchestrates the provisioning and execution of workflows to\nsupport both horizontal and vertical scalability. This combination of features\nmakes Smartflow well-suited for geospatial model development and analysis over\nlarge geographic areas, time scales, and expansive image archives.\n  We also present a novel neural architecture, built using Smartflow, to\nmonitor large geographic areas for heavy construction. Qualitative results\nbased on data from the IARPA Space-based Machine Automated Recognition\nTechnique (SMART) program are presented that show the model is capable of\ndetecting heavy construction throughout all major phases of development."}
{"id": "2506.03065", "pdf": "https://arxiv.org/pdf/2506.03065", "abs": "https://arxiv.org/abs/2506.03065", "authors": ["Pengtao Chen", "Xianfang Zeng", "Maosen Zhao", "Peng Ye", "Mingzhu Shen", "Wei Cheng", "Gang Yu", "Tao Chen"], "title": "Sparse-vDiT: Unleashing the Power of Sparse Attention to Accelerate Video Diffusion Transformers", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "While Diffusion Transformers (DiTs) have achieved breakthroughs in video\ngeneration, this long sequence generation task remains constrained by the\nquadratic complexity of attention mechanisms, resulting in significant\ninference latency. Through detailed analysis of attention maps in Video\nDiffusion Transformer (vDiT), we identify three recurring sparsity patterns:\ndiagonal, multi-diagonal, and vertical-stripe structures. And even 3-6\\%\nattention heads can be skipped. Crucially, these patterns exhibit strong\nlayer-depth and head-position correlations but show limited dependence on the\ninput content. Leveraging these findings, we propose Sparse-vDiT, a sparsity\nacceleration framework for vDiT comprising: 1) Pattern-optimized sparse kernels\nthat replace dense attention with computationally efficient implementations for\neach identified sparsity pattern. 2) An offline sparse diffusion search\nalgorithm that selects the optimal sparse computation strategy per layer and\nhead via hardware-aware cost modeling. After determining the optimal\nconfiguration, we fuse heads within the same layer that share the same\nattention strategy, enhancing inference efficiency. Integrated into\nstate-of-the-art vDiT models (CogVideoX1.5, HunyuanVideo, and Wan2.1),\nSparse-vDiT achieves 2.09$\\times$, 2.38$\\times$, and 1.67$\\times$ theoretical\nFLOP reduction, and actual inference speedups of 1.76$\\times$, 1.85$\\times$,\nand 1.58$\\times$, respectively, while maintaining high visual fidelity, with\nPSNR values reaching 24.13, 27.09, and 22.59. Our work demonstrates that latent\nstructural sparsity in vDiTs can be systematically exploited for long video\nsynthesis."}
{"id": "2506.03067", "pdf": "https://arxiv.org/pdf/2506.03067", "abs": "https://arxiv.org/abs/2506.03067", "authors": ["Mingzhe Li", "Gehao Zhang", "Zhenting Wang", "Shiqing Ma", "Siqi Pan", "Richard Cartwright", "Juan Zhai"], "title": "EDITOR: Effective and Interpretable Prompt Inversion for Text-to-Image Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-image generation models~(e.g., Stable Diffusion) have achieved\nsignificant advancements, enabling the creation of high-quality and realistic\nimages based on textual descriptions. Prompt inversion, the task of identifying\nthe textual prompt used to generate a specific artifact, holds significant\npotential for applications including data attribution, model provenance, and\nwatermarking validation. Recent studies introduced a delayed projection scheme\nto optimize for prompts representative of the vocabulary space, though\nchallenges in semantic fluency and efficiency remain. Advanced image captioning\nmodels or visual large language models can generate highly interpretable\nprompts, but they often lack in image similarity. In this paper, we propose a\nprompt inversion technique called \\sys for text-to-image diffusion models,\nwhich includes initializing embeddings using a pre-trained image captioning\nmodel, refining them through reverse-engineering in the latent space, and\nconverting them to texts using an embedding-to-text model. Our experiments on\nthe widely-used datasets, such as MS COCO, LAION, and Flickr, show that our\nmethod outperforms existing methods in terms of image similarity, textual\nalignment, prompt interpretability and generalizability. We further illustrate\nthe application of our generated prompts in tasks such as cross-concept image\nsynthesis, concept manipulation, evolutionary multi-concept generation and\nunsupervised segmentation."}
{"id": "2506.03073", "pdf": "https://arxiv.org/pdf/2506.03073", "abs": "https://arxiv.org/abs/2506.03073", "authors": ["Roman Titkov", "Egor Zubkov", "Dmitry Yudin", "Jaafar Mahmoud", "Malik Mohrat", "Gennady Sidorov"], "title": "LEG-SLAM: Real-Time Language-Enhanced Gaussian Splatting for SLAM", "categories": ["cs.CV"], "comment": null, "summary": "Modern Gaussian Splatting methods have proven highly effective for real-time\nphotorealistic rendering of 3D scenes. However, integrating semantic\ninformation into this representation remains a significant challenge,\nespecially in maintaining real-time performance for SLAM (Simultaneous\nLocalization and Mapping) applications. In this work, we introduce LEG-SLAM --\na novel approach that fuses an optimized Gaussian Splatting implementation with\nvisual-language feature extraction using DINOv2 followed by a learnable feature\ncompressor based on Principal Component Analysis, while enabling an online\ndense SLAM. Our method simultaneously generates high-quality photorealistic\nimages and semantically labeled scene maps, achieving real-time scene\nreconstruction with more than 10 fps on the Replica dataset and 18 fps on\nScanNet. Experimental results show that our approach significantly outperforms\nstate-of-the-art methods in reconstruction speed while achieving competitive\nrendering quality. The proposed system eliminates the need for prior data\npreparation such as camera's ego motion or pre-computed static semantic maps.\nWith its potential applications in autonomous robotics, augmented reality, and\nother interactive domains, LEG-SLAM represents a significant step forward in\nreal-time semantic 3D Gaussian-based SLAM. Project page:\nhttps://titrom025.github.io/LEG-SLAM/"}
{"id": "2506.03079", "pdf": "https://arxiv.org/pdf/2506.03079", "abs": "https://arxiv.org/abs/2506.03079", "authors": ["Xiuyu Yang", "Bohan Li", "Shaocong Xu", "Nan Wang", "Chongjie Ye", "Zhaoxi Chen", "Minghan Qin", "Yikang Ding", "Xin Jin", "Hang Zhao", "Hao Zhao"], "title": "ORV: 4D Occupancy-centric Robot Video Generation", "categories": ["cs.CV"], "comment": "Project page: https://orangesodahub.github.io/ORV/ ; Code:\n  https://github.com/OrangeSodahub/ORV", "summary": "Acquiring real-world robotic simulation data through teleoperation is\nnotoriously time-consuming and labor-intensive. Recently, action-driven\ngenerative models have gained widespread adoption in robot learning and\nsimulation, as they eliminate safety concerns and reduce maintenance efforts.\nHowever, the action sequences used in these methods often result in limited\ncontrol precision and poor generalization due to their globally coarse\nalignment. To address these limitations, we propose ORV, an Occupancy-centric\nRobot Video generation framework, which utilizes 4D semantic occupancy\nsequences as a fine-grained representation to provide more accurate semantic\nand geometric guidance for video generation. By leveraging occupancy-based\nrepresentations, ORV enables seamless translation of simulation data into\nphotorealistic robot videos, while ensuring high temporal consistency and\nprecise controllability. Furthermore, our framework supports the simultaneous\ngeneration of multi-view videos of robot gripping operations - an important\ncapability for downstream robotic learning tasks. Extensive experimental\nresults demonstrate that ORV consistently outperforms existing baseline methods\nacross various datasets and sub-tasks. Demo, Code and Model:\nhttps://orangesodahub.github.io/ORV"}
{"id": "2506.03082", "pdf": "https://arxiv.org/pdf/2506.03082", "abs": "https://arxiv.org/abs/2506.03082", "authors": ["Ssharvien Kumar Sivakumar", "Yannik Frisch", "Ghazal Ghazaei", "Anirban Mukhopadhyay"], "title": "SG2VID: Scene Graphs Enable Fine-Grained Control for Video Synthesis", "categories": ["cs.CV"], "comment": null, "summary": "Surgical simulation plays a pivotal role in training novice surgeons,\naccelerating their learning curve and reducing intra-operative errors. However,\nconventional simulation tools fall short in providing the necessary\nphotorealism and the variability of human anatomy. In response, current methods\nare shifting towards generative model-based simulators. Yet, these approaches\nprimarily focus on using increasingly complex conditioning for precise\nsynthesis while neglecting the fine-grained human control aspect. To address\nthis gap, we introduce SG2VID, the first diffusion-based video model that\nleverages Scene Graphs for both precise video synthesis and fine-grained human\ncontrol. We demonstrate SG2VID's capabilities across three public datasets\nfeaturing cataract and cholecystectomy surgery. While SG2VID outperforms\nprevious methods both qualitatively and quantitatively, it also enables precise\nsynthesis, providing accurate control over tool and anatomy's size and\nmovement, entrance of new tools, as well as the overall scene layout. We\nqualitatively motivate how SG2VID can be used for generative augmentation and\npresent an experiment demonstrating its ability to improve a downstream phase\ndetection task when the training set is extended with our synthetic videos.\nFinally, to showcase SG2VID's ability to retain human control, we interact with\nthe Scene Graphs to generate new video samples depicting major yet rare\nintra-operative irregularities."}
{"id": "2506.03084", "pdf": "https://arxiv.org/pdf/2506.03084", "abs": "https://arxiv.org/abs/2506.03084", "authors": ["Zizhao Wu", "Yingying Sun", "Yiming Chen", "Xiaoling Gu", "Ruyu Liu", "Jiazhou Chen"], "title": "InterMamba: Efficient Human-Human Interaction Generation with Adaptive Spatio-Temporal Mamba", "categories": ["cs.CV"], "comment": null, "summary": "Human-human interaction generation has garnered significant attention in\nmotion synthesis due to its vital role in understanding humans as social\nbeings. However, existing methods typically rely on transformer-based\narchitectures, which often face challenges related to scalability and\nefficiency. To address these issues, we propose a novel, efficient human-human\ninteraction generation method based on the Mamba framework, designed to meet\nthe demands of effectively capturing long-sequence dependencies while providing\nreal-time feedback. Specifically, we introduce an adaptive spatio-temporal\nMamba framework that utilizes two parallel SSM branches with an adaptive\nmechanism to integrate the spatial and temporal features of motion sequences.\nTo further enhance the model's ability to capture dependencies within\nindividual motion sequences and the interactions between different individual\nsequences, we develop two key modules: the self-adaptive spatio-temporal Mamba\nmodule and the cross-adaptive spatio-temporal Mamba module, enabling efficient\nfeature learning. Extensive experiments demonstrate that our method achieves\nstate-of-the-art results on two interaction datasets with remarkable quality\nand efficiency. Compared to the baseline method InterGen, our approach not only\nimproves accuracy but also requires a minimal parameter size of just 66M ,only\n36% of InterGen's, while achieving an average inference speed of 0.57 seconds,\nwhich is 46% of InterGen's execution time."}
{"id": "2506.03089", "pdf": "https://arxiv.org/pdf/2506.03089", "abs": "https://arxiv.org/abs/2506.03089", "authors": ["Lucas Piper", "Arlindo L. Oliveira", "Tiago Marques"], "title": "Explicitly Modeling Subcortical Vision with a Neuro-Inspired Front-End Improves CNN Robustness", "categories": ["cs.CV", "q-bio.NC"], "comment": null, "summary": "Convolutional neural networks (CNNs) trained on object recognition achieve\nhigh task performance but continue to exhibit vulnerability under a range of\nvisual perturbations and out-of-domain images, when compared with biological\nvision. Prior work has demonstrated that coupling a standard CNN with a\nfront-end block (VOneBlock) that mimics the primate primary visual cortex (V1)\ncan improve overall model robustness. Expanding on this, we introduce Early\nVision Networks (EVNets), a new class of hybrid CNNs that combine the VOneBlock\nwith a novel SubcorticalBlock, whose architecture draws from computational\nmodels in neuroscience and is parameterized to maximize alignment with\nsubcortical responses reported across multiple experimental studies. Without\nbeing optimized to do so, the assembly of the SubcorticalBlock with the\nVOneBlock improved V1 alignment across most standard V1 benchmarks, and better\nmodeled extra-classical receptive field phenomena. In addition, EVNets exhibit\nstronger emergent shape bias and overperform the base CNN architecture by 8.5%\non an aggregate benchmark of robustness evaluations, including adversarial\nperturbations, common corruptions, and domain shifts. Finally, we show that\nEVNets can be further improved when paired with a state-of-the-art data\naugmentation technique, surpassing the performance of the isolated data\naugmentation approach by 7.3% on our robustness benchmark. This result reveals\ncomplementary benefits between changes in architecture to better mimic biology\nand training-based machine learning approaches."}
{"id": "2506.03096", "pdf": "https://arxiv.org/pdf/2506.03096", "abs": "https://arxiv.org/abs/2506.03096", "authors": ["Christian Schlarmann", "Francesco Croce", "Nicolas Flammarion", "Matthias Hein"], "title": "FuseLIP: Multimodal Embeddings via Early Fusion of Discrete Tokens", "categories": ["cs.CV", "cs.LG"], "comment": "Code and models available at https://github.com/chs20/fuselip", "summary": "Contrastive language-image pre-training aligns the features of text-image\npairs in a common latent space via distinct encoders for each modality. While\nthis approach achieves impressive performance in several zero-shot tasks, it\ncannot natively handle multimodal inputs, i.e., encoding image and text into a\nsingle feature vector. As a remedy, it is common practice to use additional\nmodules to merge the features extracted by the unimodal encoders. In this work,\nwe present FuseLIP, an alternative architecture for multimodal embedding.\nLeveraging recent progress in discrete image tokenizers, we propose to use a\nsingle transformer model which operates on an extended vocabulary of text and\nimage tokens. This early fusion approach allows the different modalities to\ninteract at each depth of encoding and obtain richer representations compared\nto common late fusion. We collect new datasets for multimodal pre-training and\nevaluation, designing challenging tasks for multimodal encoder models. We show\nthat FuseLIP outperforms other approaches in multimodal embedding tasks such as\nVQA and text-guided image transformation retrieval, while being comparable to\nbaselines on unimodal tasks."}
{"id": "2506.03097", "pdf": "https://arxiv.org/pdf/2506.03097", "abs": "https://arxiv.org/abs/2506.03097", "authors": ["Ashwin Vinod", "Shrey Pandit", "Aditya Vavre", "Linshen Liu"], "title": "EgoVLM: Policy Optimization for Egocentric Video Understanding", "categories": ["cs.CV", "cs.AI"], "comment": "Our Code can be found at https://github.com/adityavavre/VidEgoVLM", "summary": "Emerging embodied AI applications, such as wearable cameras and autonomous\nagents, have underscored the need for robust reasoning from first person video\nstreams. We introduce EgoVLM, a vision-language model specifically designed to\nintegrate visual comprehension and spatial-temporal reasoning within egocentric\nvideo contexts. EgoVLM is fine-tuned via Group Relative Policy Optimization\n(GRPO), a reinforcement learning method adapted to align model outputs with\nhuman-like reasoning steps. Following DeepSeek R1-Zero's approach, we directly\ntune using RL without any supervised fine-tuning phase on chain-of-thought\n(CoT) data. We evaluate EgoVLM on egocentric video question answering\nbenchmarks and show that domain-specific training substantially improves\nperformance over general-purpose VLMs. Our EgoVLM-3B, trained exclusively on\nnon-CoT egocentric data, outperforms the base Qwen2.5-VL 3B and 7B models by\n14.33 and 13.87 accuracy points on the EgoSchema benchmark, respectively. By\nexplicitly generating reasoning traces, EgoVLM enhances interpretability,\nmaking it well-suited for downstream applications. Furthermore, we introduce a\nnovel keyframe-based reward that incorporates salient frame selection to guide\nreinforcement learning optimization. This reward formulation opens a promising\navenue for future exploration in temporally grounded egocentric reasoning."}
{"id": "2506.03103", "pdf": "https://arxiv.org/pdf/2506.03103", "abs": "https://arxiv.org/abs/2506.03103", "authors": ["Xiaoyan Cong", "Angela Xing", "Chandradeep Pokhariya", "Rao Fu", "Srinath Sridhar"], "title": "DyTact: Capturing Dynamic Contacts in Hand-Object Manipulation", "categories": ["cs.CV"], "comment": null, "summary": "Reconstructing dynamic hand-object contacts is essential for realistic\nmanipulation in AI character animation, XR, and robotics, yet it remains\nchallenging due to heavy occlusions, complex surface details, and limitations\nin existing capture techniques. In this paper, we introduce DyTact, a\nmarkerless capture method for accurately capturing dynamic contact in\nhand-object manipulations in a non-intrusive manner. Our approach leverages a\ndynamic, articulated representation based on 2D Gaussian surfels to model\ncomplex manipulations. By binding these surfels to MANO meshes, DyTact\nharnesses the inductive bias of template models to stabilize and accelerate\noptimization. A refinement module addresses time-dependent high-frequency\ndeformations, while a contact-guided adaptive sampling strategy selectively\nincreases surfel density in contact regions to handle heavy occlusion.\nExtensive experiments demonstrate that DyTact not only achieves\nstate-of-the-art dynamic contact estimation accuracy but also significantly\nimproves novel view synthesis quality, all while operating with fast\noptimization and efficient memory usage. Project Page:\nhttps://oliver-cong02.github.io/DyTact.github.io/ ."}
{"id": "2506.03107", "pdf": "https://arxiv.org/pdf/2506.03107", "abs": "https://arxiv.org/abs/2506.03107", "authors": ["Di Chang", "Mingdeng Cao", "Yichun Shi", "Bo Liu", "Shengqu Cai", "Shijie Zhou", "Weilin Huang", "Gordon Wetzstein", "Mohammad Soleymani", "Peng Wang"], "title": "ByteMorph: Benchmarking Instruction-Guided Image Editing with Non-Rigid Motions", "categories": ["cs.CV"], "comment": "Website: https://boese0601.github.io/bytemorph Dataset:\n  https://huggingface.co/datasets/ByteDance-Seed/BM-6M Benchmark:\n  https://huggingface.co/datasets/ByteDance-Seed/BM-Bench Code:\n  https://github.com/ByteDance-Seed/BM-code Demo:\n  https://huggingface.co/spaces/Boese0601/ByteMorph-Demo", "summary": "Editing images with instructions to reflect non-rigid motions, camera\nviewpoint shifts, object deformations, human articulations, and complex\ninteractions, poses a challenging yet underexplored problem in computer vision.\nExisting approaches and datasets predominantly focus on static scenes or rigid\ntransformations, limiting their capacity to handle expressive edits involving\ndynamic motion. To address this gap, we introduce ByteMorph, a comprehensive\nframework for instruction-based image editing with an emphasis on non-rigid\nmotions. ByteMorph comprises a large-scale dataset, ByteMorph-6M, and a strong\nbaseline model built upon the Diffusion Transformer (DiT), named ByteMorpher.\nByteMorph-6M includes over 6 million high-resolution image editing pairs for\ntraining, along with a carefully curated evaluation benchmark ByteMorph-Bench.\nBoth capture a wide variety of non-rigid motion types across diverse\nenvironments, human figures, and object categories. The dataset is constructed\nusing motion-guided data generation, layered compositing techniques, and\nautomated captioning to ensure diversity, realism, and semantic coherence. We\nfurther conduct a comprehensive evaluation of recent instruction-based image\nediting methods from both academic and commercial domains."}
{"id": "2506.03110", "pdf": "https://arxiv.org/pdf/2506.03110", "abs": "https://arxiv.org/abs/2506.03110", "authors": ["Shuai Yi", "Yixiong Zou", "Yuhua Li", "Ruixuan Li"], "title": "Revisiting Continuity of Image Tokens for Cross-domain Few-shot Learning", "categories": ["cs.CV"], "comment": "Accepted by ICML 2025(spotlight)", "summary": "Vision Transformer (ViT) has achieved remarkable success due to its\nlarge-scale pretraining on general domains, but it still faces challenges when\napplying it to downstream distant domains that have only scarce training data,\nwhich gives rise to the Cross-Domain Few-Shot Learning (CDFSL) task. Inspired\nby Self-Attention's insensitivity to token orders, we find an interesting\nphenomenon neglected in current works: disrupting the continuity of image\ntokens (i.e., making pixels not smoothly transited across patches) in ViT leads\nto a noticeable performance decline in the general (source) domain but only a\nmarginal decrease in downstream target domains. This questions the role of\nimage tokens' continuity in ViT's generalization under large domain gaps. In\nthis paper, we delve into this phenomenon for an interpretation. We find\ncontinuity aids ViT in learning larger spatial patterns, which are harder to\ntransfer than smaller ones, enlarging domain distances. Meanwhile, it implies\nthat only smaller patterns within each patch could be transferred under extreme\ndomain gaps. Based on this interpretation, we further propose a simple yet\neffective method for CDFSL that better disrupts the continuity of image tokens,\nencouraging the model to rely less on large patterns and more on smaller ones.\nExtensive experiments show the effectiveness of our method in reducing domain\ngaps and outperforming state-of-the-art works. Codes and models are available\nat https://github.com/shuaiyi308/ReCIT."}
{"id": "2506.03114", "pdf": "https://arxiv.org/pdf/2506.03114", "abs": "https://arxiv.org/abs/2506.03114", "authors": ["Michelle Chen", "David Russell", "Amritha Pallavoor", "Derek Young", "Jane Wu"], "title": "Zero-Shot Tree Detection and Segmentation from Aerial Forest Imagery", "categories": ["cs.CV"], "comment": "Code:\n  https://github.com/open-forest-observatory/tree-detection-framework", "summary": "Large-scale delineation of individual trees from remote sensing imagery is\ncrucial to the advancement of ecological research, particularly as climate\nchange and other environmental factors rapidly transform forest landscapes\nacross the world. Current RGB tree segmentation methods rely on training\nspecialized machine learning models with labeled tree datasets. While these\nlearning-based approaches can outperform manual data collection when accurate,\nthe existing models still depend on training data that's hard to scale. In this\npaper, we investigate the efficacy of using a state-of-the-art image\nsegmentation model, Segment Anything Model 2 (SAM2), in a zero-shot manner for\nindividual tree detection and segmentation. We evaluate a pretrained SAM2 model\non two tasks in this domain: (1) zero-shot segmentation and (2) zero-shot\ntransfer by using predictions from an existing tree detection model as prompts.\nOur results suggest that SAM2 not only has impressive generalization\ncapabilities, but also can form a natural synergy with specialized methods\ntrained on in-domain labeled data. We find that applying large pretrained\nmodels to problems in remote sensing is a promising avenue for future progress.\nWe make our code available at:\nhttps://github.com/open-forest-observatory/tree-detection-framework."}
{"id": "2506.03117", "pdf": "https://arxiv.org/pdf/2506.03117", "abs": "https://arxiv.org/abs/2506.03117", "authors": ["Zeliang Zhang", "Gaowen Liu", "Charles Fleming", "Ramana Rao Kompella", "Chenliang Xu"], "title": "Targeted Forgetting of Image Subgroups in CLIP Models", "categories": ["cs.CV"], "comment": "12 Figures,5 Pages. The project page is\n  \\url{https://zhangaipi.github.io/forget_clip/}", "summary": "Foundation models (FMs) such as CLIP have demonstrated impressive zero-shot\nperformance across various tasks by leveraging large-scale, unsupervised\npre-training. However, they often inherit harmful or unwanted knowledge from\nnoisy internet-sourced datasets, compromising their reliability in real-world\napplications. Existing model unlearning methods either rely on access to\npre-trained datasets or focus on coarse-grained unlearning (e.g., entire\nclasses), leaving a critical gap for fine-grained unlearning. In this paper, we\naddress the challenging scenario of selectively forgetting specific portions of\nknowledge within a class, without access to pre-trained data, while preserving\nthe model's overall performance. We propose a novel three-stage approach that\nprogressively unlearns targeted knowledge while mitigating over-forgetting. It\nconsists of (1) a forgetting stage to fine-tune the CLIP on samples to be\nforgotten, (2) a reminding stage to restore performance on retained samples,\nand (3) a restoring stage to recover zero-shot capabilities using model\nsouping. Additionally, we introduce knowledge distillation to handle the\ndistribution disparity between forgetting, retaining samples, and unseen\npre-trained data. Extensive experiments on CIFAR-10, ImageNet-1K, and style\ndatasets demonstrate that our approach effectively unlearns specific subgroups\nwhile maintaining strong zero-shot performance on semantically similar\nsubgroups and other categories, significantly outperforming baseline unlearning\nmethods, which lose effectiveness under the CLIP unlearning setting."}
{"id": "2506.03119", "pdf": "https://arxiv.org/pdf/2506.03119", "abs": "https://arxiv.org/abs/2506.03119", "authors": ["Zujin Guo", "Size Wu", "Zhongang Cai", "Wei Li", "Chen Change Loy"], "title": "Controllable Human-centric Keyframe Interpolation with Generative Prior", "categories": ["cs.CV"], "comment": "Project Page: https://gseancdat.github.io/projects/PoseFuse3D_KI", "summary": "Existing interpolation methods use pre-trained video diffusion priors to\ngenerate intermediate frames between sparsely sampled keyframes. In the absence\nof 3D geometric guidance, these methods struggle to produce plausible results\nfor complex, articulated human motions and offer limited control over the\nsynthesized dynamics. In this paper, we introduce PoseFuse3D Keyframe\nInterpolator (PoseFuse3D-KI), a novel framework that integrates 3D human\nguidance signals into the diffusion process for Controllable Human-centric\nKeyframe Interpolation (CHKI). To provide rich spatial and structural cues for\ninterpolation, our PoseFuse3D, a 3D-informed control model, features a novel\nSMPL-X encoder that transforms 3D geometry and shape into the 2D latent\nconditioning space, alongside a fusion network that integrates these 3D cues\nwith 2D pose embeddings. For evaluation, we build CHKI-Video, a new dataset\nannotated with both 2D poses and 3D SMPL-X parameters. We show that\nPoseFuse3D-KI consistently outperforms state-of-the-art baselines on\nCHKI-Video, achieving a 9% improvement in PSNR and a 38% reduction in LPIPS.\nComprehensive ablations demonstrate that our PoseFuse3D model improves\ninterpolation fidelity."}
{"id": "2506.03123", "pdf": "https://arxiv.org/pdf/2506.03123", "abs": "https://arxiv.org/abs/2506.03123", "authors": ["Zhengyao Lv", "Chenyang Si", "Tianlin Pan", "Zhaoxi Chen", "Kwan-Yee K. Wong", "Yu Qiao", "Ziwei Liu"], "title": "DCM: Dual-Expert Consistency Model for Efficient and High-Quality Video Generation", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion Models have achieved remarkable results in video synthesis but\nrequire iterative denoising steps, leading to substantial computational\noverhead. Consistency Models have made significant progress in accelerating\ndiffusion models. However, directly applying them to video diffusion models\noften results in severe degradation of temporal consistency and appearance\ndetails. In this paper, by analyzing the training dynamics of Consistency\nModels, we identify a key conflicting learning dynamics during the distillation\nprocess: there is a significant discrepancy in the optimization gradients and\nloss contributions across different timesteps. This discrepancy prevents the\ndistilled student model from achieving an optimal state, leading to compromised\ntemporal consistency and degraded appearance details. To address this issue, we\npropose a parameter-efficient \\textbf{Dual-Expert Consistency Model~(DCM)},\nwhere a semantic expert focuses on learning semantic layout and motion, while a\ndetail expert specializes in fine detail refinement. Furthermore, we introduce\nTemporal Coherence Loss to improve motion consistency for the semantic expert\nand apply GAN and Feature Matching Loss to enhance the synthesis quality of the\ndetail expert.Our approach achieves state-of-the-art visual quality with\nsignificantly reduced sampling steps, demonstrating the effectiveness of expert\nspecialization in video diffusion model distillation. Our code and models are\navailable at\n\\href{https://github.com/Vchitect/DCM}{https://github.com/Vchitect/DCM}."}
{"id": "2506.03126", "pdf": "https://arxiv.org/pdf/2506.03126", "abs": "https://arxiv.org/abs/2506.03126", "authors": ["Lu Qiu", "Yizhuo Li", "Yuying Ge", "Yixiao Ge", "Ying Shan", "Xihui Liu"], "title": "AnimeShooter: A Multi-Shot Animation Dataset for Reference-Guided Video Generation", "categories": ["cs.CV"], "comment": "Project released at: https://qiulu66.github.io/animeshooter/", "summary": "Recent advances in AI-generated content (AIGC) have significantly accelerated\nanimation production. To produce engaging animations, it is essential to\ngenerate coherent multi-shot video clips with narrative scripts and character\nreferences. However, existing public datasets primarily focus on real-world\nscenarios with global descriptions, and lack reference images for consistent\ncharacter guidance. To bridge this gap, we present AnimeShooter, a\nreference-guided multi-shot animation dataset. AnimeShooter features\ncomprehensive hierarchical annotations and strong visual consistency across\nshots through an automated pipeline. Story-level annotations provide an\noverview of the narrative, including the storyline, key scenes, and main\ncharacter profiles with reference images, while shot-level annotations\ndecompose the story into consecutive shots, each annotated with scene,\ncharacters, and both narrative and descriptive visual captions. Additionally, a\ndedicated subset, AnimeShooter-audio, offers synchronized audio tracks for each\nshot, along with audio descriptions and sound sources. To demonstrate the\neffectiveness of AnimeShooter and establish a baseline for the reference-guided\nmulti-shot video generation task, we introduce AnimeShooterGen, which leverages\nMultimodal Large Language Models (MLLMs) and video diffusion models. The\nreference image and previously generated shots are first processed by MLLM to\nproduce representations aware of both reference and context, which are then\nused as the condition for the diffusion model to decode the subsequent shot.\nExperimental results show that the model trained on AnimeShooter achieves\nsuperior cross-shot visual consistency and adherence to reference visual\nguidance, which highlight the value of our dataset for coherent animated video\ngeneration."}
{"id": "2506.03131", "pdf": "https://arxiv.org/pdf/2506.03131", "abs": "https://arxiv.org/abs/2506.03131", "authors": ["Zidong Wang", "Lei Bai", "Xiangyu Yue", "Wanli Ouyang", "Yiyuan Zhang"], "title": "Native-Resolution Image Synthesis", "categories": ["cs.CV", "cs.LG"], "comment": "Project Page: https://wzdthu.github.io/NiT/", "summary": "We introduce native-resolution image synthesis, a novel generative modeling\nparadigm that enables the synthesis of images at arbitrary resolutions and\naspect ratios. This approach overcomes the limitations of conventional\nfixed-resolution, square-image methods by natively handling variable-length\nvisual tokens, a core challenge for traditional techniques. To this end, we\nintroduce the Native-resolution diffusion Transformer (NiT), an architecture\ndesigned to explicitly model varying resolutions and aspect ratios within its\ndenoising process. Free from the constraints of fixed formats, NiT learns\nintrinsic visual distributions from images spanning a broad range of\nresolutions and aspect ratios. Notably, a single NiT model simultaneously\nachieves the state-of-the-art performance on both ImageNet-256x256 and 512x512\nbenchmarks. Surprisingly, akin to the robust zero-shot capabilities seen in\nadvanced large language models, NiT, trained solely on ImageNet, demonstrates\nexcellent zero-shot generalization performance. It successfully generates\nhigh-fidelity images at previously unseen high resolutions (e.g., 1536 x 1536)\nand diverse aspect ratios (e.g., 16:9, 3:1, 4:3), as shown in Figure 1. These\nfindings indicate the significant potential of native-resolution modeling as a\nbridge between visual generative modeling and advanced LLM methodologies."}
{"id": "2506.03135", "pdf": "https://arxiv.org/pdf/2506.03135", "abs": "https://arxiv.org/abs/2506.03135", "authors": ["Mengdi Jia", "Zekun Qi", "Shaochen Zhang", "Wenyao Zhang", "Xinqiang Yu", "Jiawei He", "He Wang", "Li Yi"], "title": "OmniSpatial: Towards Comprehensive Spatial Reasoning Benchmark for Vision Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Project Page: https://qizekun.github.io/omnispatial/", "summary": "Spatial reasoning is a key aspect of cognitive psychology and remains a major\nbottleneck for current vision-language models (VLMs). While extensive research\nhas aimed to evaluate or improve VLMs' understanding of basic spatial\nrelations, such as distinguishing left from right, near from far, and object\ncounting, these tasks represent only the most fundamental level of spatial\nreasoning. In this work, we introduce OmniSpatial, a comprehensive and\nchallenging benchmark for spatial reasoning, grounded in cognitive psychology.\nOmniSpatial covers four major categories: dynamic reasoning, complex spatial\nlogic, spatial interaction, and perspective-taking, with 50 fine-grained\nsubcategories. Through Internet data crawling and careful manual annotation, we\nconstruct over 1.5K question-answer pairs. Extensive experiments show that both\nopen- and closed-source VLMs, as well as existing reasoning and spatial\nunderstanding models, exhibit significant limitations in comprehensive spatial\nunderstanding. We further analyze failure cases and propose potential\ndirections for future research."}
{"id": "2506.03139", "pdf": "https://arxiv.org/pdf/2506.03139", "abs": "https://arxiv.org/abs/2506.03139", "authors": ["Siqi Chen", "Xinyu Dong", "Haolei Xu", "Xingyu Wu", "Fei Tang", "Hang Zhang", "Yuchen Yan", "Linjuan Wu", "Wenqi Zhang", "Guiyang Hou", "Yongliang Shen", "Weiming Lu", "Yueting Zhuang"], "title": "SVGenius: Benchmarking LLMs in SVG Understanding, Editing and Generation", "categories": ["cs.CV", "cs.AI"], "comment": "19 pages,4 figures, Project page:\n  https://zju-real.github.io/SVGenius, Code:\n  https://github.com/ZJU-REAL/SVGenius-Bench", "summary": "Large Language Models (LLMs) and Multimodal LLMs have shown promising\ncapabilities for SVG processing, yet existing benchmarks suffer from limited\nreal-world coverage, lack of complexity stratification, and fragmented\nevaluation paradigms. We introduce SVGenius, a comprehensive benchmark\ncomprising 2,377 queries across three progressive dimensions: understanding,\nediting, and generation. Built on real-world data from 24 application domains\nwith systematic complexity stratification, SVGenius evaluates models through 8\ntask categories and 18 metrics. We assess 22 mainstream models spanning\ndifferent scales, architectures, training paradigms, and accessibility levels.\nOur analysis reveals that while proprietary models significantly outperform\nopen-source counterparts, all models exhibit systematic performance degradation\nwith increasing complexity, indicating fundamental limitations in current\napproaches; however, reasoning-enhanced training proves more effective than\npure scaling for overcoming these limitations, though style transfer remains\nthe most challenging capability across all model types. SVGenius establishes\nthe first systematic evaluation framework for SVG processing, providing crucial\ninsights for developing more capable vector graphics models and advancing\nautomated graphic design applications. Appendix and supplementary materials\n(including all data and code) are available at\nhttps://zju-real.github.io/SVGenius."}
{"id": "2506.03140", "pdf": "https://arxiv.org/pdf/2506.03140", "abs": "https://arxiv.org/abs/2506.03140", "authors": ["Yawen Luo", "Jianhong Bai", "Xiaoyu Shi", "Menghan Xia", "Xintao Wang", "Pengfei Wan", "Di Zhang", "Kun Gai", "Tianfan Xue"], "title": "CamCloneMaster: Enabling Reference-based Camera Control for Video Generation", "categories": ["cs.CV"], "comment": "Project Page: https://camclonemaster.github.io/", "summary": "Camera control is crucial for generating expressive and cinematic videos.\nExisting methods rely on explicit sequences of camera parameters as control\nconditions, which can be cumbersome for users to construct, particularly for\nintricate camera movements. To provide a more intuitive camera control method,\nwe propose CamCloneMaster, a framework that enables users to replicate camera\nmovements from reference videos without requiring camera parameters or\ntest-time fine-tuning. CamCloneMaster seamlessly supports reference-based\ncamera control for both Image-to-Video and Video-to-Video tasks within a\nunified framework. Furthermore, we present the Camera Clone Dataset, a\nlarge-scale synthetic dataset designed for camera clone learning, encompassing\ndiverse scenes, subjects, and camera movements. Extensive experiments and user\nstudies demonstrate that CamCloneMaster outperforms existing methods in terms\nof both camera controllability and visual quality."}
{"id": "2506.03141", "pdf": "https://arxiv.org/pdf/2506.03141", "abs": "https://arxiv.org/abs/2506.03141", "authors": ["Jiwen Yu", "Jianhong Bai", "Yiran Qin", "Quande Liu", "Xintao Wang", "Pengfei Wan", "Di Zhang", "Xihui Liu"], "title": "Context as Memory: Scene-Consistent Interactive Long Video Generation with Memory Retrieval", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in interactive video generation have shown promising results,\nyet existing approaches struggle with scene-consistent memory capabilities in\nlong video generation due to limited use of historical context. In this work,\nwe propose Context-as-Memory, which utilizes historical context as memory for\nvideo generation. It includes two simple yet effective designs: (1) storing\ncontext in frame format without additional post-processing; (2) conditioning by\nconcatenating context and frames to be predicted along the frame dimension at\nthe input, requiring no external control modules. Furthermore, considering the\nenormous computational overhead of incorporating all historical context, we\npropose the Memory Retrieval module to select truly relevant context frames by\ndetermining FOV (Field of View) overlap between camera poses, which\nsignificantly reduces the number of candidate frames without substantial\ninformation loss. Experiments demonstrate that Context-as-Memory achieves\nsuperior memory capabilities in interactive long video generation compared to\nSOTAs, even generalizing effectively to open-domain scenarios not seen during\ntraining. The link of our project page is https://context-as-memory.github.io/."}
{"id": "2506.03144", "pdf": "https://arxiv.org/pdf/2506.03144", "abs": "https://arxiv.org/abs/2506.03144", "authors": ["Wei Chow", "Yuan Gao", "Linfeng Li", "Xian Wang", "Qi Xu", "Hang Song", "Lingdong Kong", "Ran Zhou", "Yi Zeng", "Yidong Cai", "Botian Jiang", "Shilin Xu", "Jiajun Zhang", "Minghui Qiu", "Xiangtai Li", "Tianshu Yang", "Siliang Tang", "Juncheng Li"], "title": "MERIT: Multilingual Semantic Retrieval with Interleaved Multi-Condition Query", "categories": ["cs.CV", "cs.CL", "cs.MM"], "comment": "Preprint; Project Page, Code, and Dataset at:\n  https://merit-2025.github.io/", "summary": "Semantic retrieval is crucial for modern applications yet remains\nunderexplored in current research. Existing datasets are limited to single\nlanguages, single images, or singular retrieval conditions, often failing to\nfully exploit the expressive capacity of visual information as evidenced by\nmaintained performance when images are replaced with captions. However,\npractical retrieval scenarios frequently involve interleaved multi-condition\nqueries with multiple images. Hence, this paper introduces MERIT, the first\nmultilingual dataset for interleaved multi-condition semantic retrieval,\ncomprising 320,000 queries with 135,000 products in 5 languages, covering 7\ndistinct product categories. Extensive experiments on MERIT identify existing\nmodels's limitation: focusing solely on global semantic information while\nneglecting specific conditional elements in queries. Consequently, we propose\nCoral, a novel fine-tuning framework that adapts pre-trained MLLMs by\nintegrating embedding reconstruction to preserve fine-grained conditional\nelements and contrastive learning to extract comprehensive global semantics.\nExperiments demonstrate that Coral achieves a 45.9% performance improvement\nover conventional approaches on MERIT, with strong generalization capabilities\nvalidated across 8 established retrieval benchmarks. Collectively, our\ncontributions - a novel dataset, identification of critical limitations in\nexisting approaches, and an innovative fine-tuning framework - establish a\nfoundation for future research in interleaved multi-condition semantic\nretrieval."}
{"id": "2506.03147", "pdf": "https://arxiv.org/pdf/2506.03147", "abs": "https://arxiv.org/abs/2506.03147", "authors": ["Bin Lin", "Zongjian Li", "Xinhua Cheng", "Yuwei Niu", "Yang Ye", "Xianyi He", "Shenghai Yuan", "Wangbo Yu", "Shaodong Wang", "Yunyang Ge", "Yatian Pang", "Li Yuan"], "title": "UniWorld: High-Resolution Semantic Encoders for Unified Visual Understanding and Generation", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Although existing unified models deliver strong performance on\nvision-language understanding and text-to-image generation, their models are\nlimited in exploring image perception and manipulation tasks, which are\nurgently desired by users for wide applications. Recently, OpenAI released\ntheir powerful GPT-4o-Image model for comprehensive image perception and\nmanipulation, achieving expressive capability and attracting community\ninterests. By observing the performance of GPT-4o-Image in our carefully\nconstructed experiments, we infer that GPT-4o-Image leverages features\nextracted by semantic encoders instead of VAE, while VAEs are considered\nessential components in many image manipulation models. Motivated by such\ninspiring observations, we present a unified generative framework named\nUniWorld based on semantic features provided by powerful visual-language models\nand contrastive semantic encoders. As a result, we build a strong unified model\nusing only 1% amount of BAGEL's data, which consistently outperforms BAGEL on\nimage editing benchmarks. UniWorld also maintains competitive image\nunderstanding and generation capabilities, achieving strong performance across\nmultiple image perception tasks. We fully open-source our models, including\nmodel weights, training and evaluation scripts, and datasets."}
{"id": "2506.03148", "pdf": "https://arxiv.org/pdf/2506.03148", "abs": "https://arxiv.org/abs/2506.03148", "authors": ["Ayush Shrivastava", "Andrew Owens"], "title": "Self-Supervised Spatial Correspondence Across Modalities", "categories": ["cs.CV"], "comment": "CVPR 2025. Project link: https://www.ayshrv.com/cmrw . Code:\n  https://github.com/ayshrv/cmrw", "summary": "We present a method for finding cross-modal space-time correspondences. Given\ntwo images from different visual modalities, such as an RGB image and a depth\nmap, our model identifies which pairs of pixels correspond to the same physical\npoints in the scene. To solve this problem, we extend the contrastive random\nwalk framework to simultaneously learn cycle-consistent feature representations\nfor both cross-modal and intra-modal matching. The resulting model is simple\nand has no explicit photo-consistency assumptions. It can be trained entirely\nusing unlabeled data, without the need for any spatially aligned multimodal\nimage pairs. We evaluate our method on both geometric and semantic\ncorrespondence tasks. For geometric matching, we consider challenging tasks\nsuch as RGB-to-depth and RGB-to-thermal matching (and vice versa); for semantic\nmatching, we evaluate on photo-sketch and cross-style image alignment. Our\nmethod achieves strong performance across all benchmarks."}
{"id": "2506.03150", "pdf": "https://arxiv.org/pdf/2506.03150", "abs": "https://arxiv.org/abs/2506.03150", "authors": ["Yuanze Lin", "Yi-Wen Chen", "Yi-Hsuan Tsai", "Ronald Clark", "Ming-Hsuan Yang"], "title": "IllumiCraft: Unified Geometry and Illumination Diffusion for Controllable Video Generation", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM"], "comment": "Tech Report", "summary": "Although diffusion-based models can generate high-quality and high-resolution\nvideo sequences from textual or image inputs, they lack explicit integration of\ngeometric cues when controlling scene lighting and visual appearance across\nframes. To address this limitation, we propose IllumiCraft, an end-to-end\ndiffusion framework accepting three complementary inputs: (1)\nhigh-dynamic-range (HDR) video maps for detailed lighting control; (2)\nsynthetically relit frames with randomized illumination changes (optionally\npaired with a static background reference image) to provide appearance cues;\nand (3) 3D point tracks that capture precise 3D geometry information. By\nintegrating the lighting, appearance, and geometry cues within a unified\ndiffusion architecture, IllumiCraft generates temporally coherent videos\naligned with user-defined prompts. It supports background-conditioned and\ntext-conditioned video relighting and provides better fidelity than existing\ncontrollable video generation methods. Project Page:\nhttps://yuanze-lin.me/IllumiCraft_page"}
{"id": "2505.21777", "pdf": "https://arxiv.org/pdf/2505.21777", "abs": "https://arxiv.org/abs/2505.21777", "authors": ["Bao Pham", "Gabriel Raya", "Matteo Negri", "Mohammed J. Zaki", "Luca Ambrogioni", "Dmitry Krotov"], "title": "Memorization to Generalization: Emergence of Diffusion Models from Associative Memory", "categories": ["cs.LG", "cond-mat.dis-nn", "cs.CV", "q-bio.NC", "stat.ML"], "comment": null, "summary": "Hopfield networks are associative memory (AM) systems, designed for storing\nand retrieving patterns as local minima of an energy landscape. In the\nclassical Hopfield model, an interesting phenomenon occurs when the amount of\ntraining data reaches its critical memory load $- spurious\\,\\,states$, or\nunintended stable points, emerge at the end of the retrieval dynamics, leading\nto incorrect recall. In this work, we examine diffusion models, commonly used\nin generative modeling, from the perspective of AMs. The training phase of\ndiffusion model is conceptualized as memory encoding (training data is stored\nin the memory). The generation phase is viewed as an attempt of memory\nretrieval. In the small data regime the diffusion model exhibits a strong\nmemorization phase, where the network creates distinct basins of attraction\naround each sample in the training set, akin to the Hopfield model below the\ncritical memory load. In the large data regime, a different phase appears where\nan increase in the size of the training set fosters the creation of new\nattractor states that correspond to manifolds of the generated samples.\nSpurious states appear at the boundary of this transition and correspond to\nemergent attractor states, which are absent in the training set, but, at the\nsame time, have distinct basins of attraction around them. Our findings\nprovide: a novel perspective on the memorization-generalization phenomenon in\ndiffusion models via the lens of AMs, theoretical prediction of existence of\nspurious states, empirical validation of this prediction in commonly-used\ndiffusion models."}
{"id": "2506.01970", "pdf": "https://arxiv.org/pdf/2506.01970", "abs": "https://arxiv.org/abs/2506.01970", "authors": ["Ruizhuo Song", "Beiming Yuan"], "title": "Johnny: Structuring Representation Space to Enhance Machine Abstract Reasoning Ability", "categories": ["cs.LG", "cs.CV"], "comment": "15 pages, 15 figures, 5 tables", "summary": "This paper thoroughly investigates the challenges of enhancing AI's abstract\nreasoning capabilities, with a particular focus on Raven's Progressive Matrices\n(RPM) tasks involving complex human-like concepts. Firstly, it dissects the\nempirical reality that traditional end-to-end RPM-solving models heavily rely\non option pool configurations, highlighting that this dependency constrains the\nmodel's reasoning capabilities. To address this limitation, the paper proposes\nthe Johnny architecture - a novel representation space-based framework for\nRPM-solving. Through the synergistic operation of its Representation Extraction\nModule and Reasoning Module, Johnny significantly enhances reasoning\nperformance by supplementing primitive negative option configurations with a\nlearned representation space. Furthermore, to strengthen the model's capacity\nfor capturing positional relationships among local features, the paper\nintroduces the Spin-Transformer network architecture, accompanied by a\nlightweight Straw Spin-Transformer variant that reduces computational overhead\nthrough parameter sharing and attention mechanism optimization. Experimental\nevaluations demonstrate that both Johnny and Spin-Transformer achieve superior\nperformance on RPM tasks, offering innovative methodologies for advancing AI's\nabstract reasoning capabilities."}
{"id": "2506.01980", "pdf": "https://arxiv.org/pdf/2506.01980", "abs": "https://arxiv.org/abs/2506.01980", "authors": ["Lianhao Yin", "Ozanan Meireles", "Guy Rosman", "Daniela Rus"], "title": "Surgical Foundation Model Leveraging Compression and Entropy Maximization for Image-Guided Surgical Assistance", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Real-time video understanding is critical to guide procedures in minimally\ninvasive surgery (MIS). However, supervised learning approaches require large,\nannotated datasets that are scarce due to annotation efforts that are\nprohibitive, e.g., in medical fields. Although self-supervision methods can\naddress such limitations, current self-supervised methods often fail to capture\nstructural and physical information in a form that generalizes across tasks. We\npropose Compress-to-Explore (C2E), a novel self-supervised framework that\nleverages Kolmogorov complexity to learn compact, informative representations\nfrom surgical videos. C2E uses entropy-maximizing decoders to compress images\nwhile preserving clinically relevant details, improving encoder performance\nwithout labeled data. Trained on large-scale unlabeled surgical datasets, C2E\ndemonstrates strong generalization across a variety of surgical ML tasks, such\nas workflow classification, tool-tissue interaction classification,\nsegmentation, and diagnosis tasks, providing improved performance as a surgical\nvisual foundation model. As we further show in the paper, the model's internal\ncompact representation better disentangles features from different structural\nparts of images. The resulting performance improvements highlight the yet\nuntapped potential of self-supervised learning to enhance surgical AI and\nimprove outcomes in MIS."}
{"id": "2506.02060", "pdf": "https://arxiv.org/pdf/2506.02060", "abs": "https://arxiv.org/abs/2506.02060", "authors": ["Javier Salazar Cavazos", "Scott Peltier"], "title": "Alzheimers Disease Classification in Functional MRI With 4D Joint Temporal-Spatial Kernels in Novel 4D CNN Model", "categories": ["eess.IV", "cs.CV"], "comment": "Published in International Society for Magnetic Resonance in Medicine\n  (ISMRM) 2025 under submission number 3398", "summary": "Previous works in the literature apply 3D spatial-only models on 4D\nfunctional MRI data leading to possible sub-par feature extraction to be used\nfor downstream tasks like classification. In this work, we aim to develop a\nnovel 4D convolution network to extract 4D joint temporal-spatial kernels that\nnot only learn spatial information but in addition also capture temporal\ndynamics. Experimental results show promising performance in capturing\nspatial-temporal data in functional MRI compared to 3D models. The 4D CNN model\nimproves Alzheimers disease diagnosis for rs-fMRI data, enabling earlier\ndetection and better interventions. Future research could explore task-based\nfMRI applications and regression tasks, enhancing understanding of cognitive\nperformance and disease progression."}
{"id": "2506.02065", "pdf": "https://arxiv.org/pdf/2506.02065", "abs": "https://arxiv.org/abs/2506.02065", "authors": ["Shriraj P. Sawant", "Krishna P. Miyapuram"], "title": "EWGN: Elastic Weight Generation and Context Switching in Deep Learning", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "The ability to learn and retain a wide variety of tasks is a hallmark of\nhuman intelligence that has inspired research in artificial general\nintelligence. Continual learning approaches provide a significant step towards\nachieving this goal. It has been known that task variability and context\nswitching are challenging for learning in neural networks. Catastrophic\nforgetting refers to the poor performance on retention of a previously learned\ntask when a new task is being learned. Switching between different task\ncontexts can be a useful approach to mitigate the same by preventing the\ninterference between the varying task weights of the network. This paper\nintroduces Elastic Weight Generative Networks (EWGN) as an idea for context\nswitching between two different tasks. The proposed EWGN architecture uses an\nadditional network that generates the weights of the primary network\ndynamically while consolidating the weights learned. The weight generation is\ninput-dependent and thus enables context switching. Using standard computer\nvision datasets, namely MNIST and fashion-MNIST, we analyse the retention of\npreviously learned task representations in Fully Connected Networks,\nConvolutional Neural Networks, and EWGN architectures with Stochastic Gradient\nDescent and Elastic Weight Consolidation learning algorithms. Understanding\ndynamic weight generation and context-switching ability can be useful in\nenabling continual learning for improved performance."}
{"id": "2506.02079", "pdf": "https://arxiv.org/pdf/2506.02079", "abs": "https://arxiv.org/abs/2506.02079", "authors": ["Xuefeng Jiang", "Tian Wen", "Zhiqin Yang", "Lvhua Wu", "Yufeng Chen", "Sheng Sun", "Yuwei Wang", "Min Liu"], "title": "Robust Federated Learning against Noisy Clients via Masked Optimization", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "comment": "Under review", "summary": "In recent years, federated learning (FL) has made significant advance in\nprivacy-sensitive applications. However, it can be hard to ensure that FL\nparticipants provide well-annotated data for training. The corresponding\nannotations from different clients often contain complex label noise at varying\nlevels. This label noise issue has a substantial impact on the performance of\nthe trained models, and clients with greater noise levels can be largely\nattributed for this degradation. To this end, it is necessary to develop an\neffective optimization strategy to alleviate the adverse effects of these noisy\nclients.In this study, we present a two-stage optimization framework,\nMaskedOptim, to address this intricate label noise problem. The first stage is\ndesigned to facilitate the detection of noisy clients with higher label noise\nrates. The second stage focuses on rectifying the labels of the noisy clients'\ndata through an end-to-end label correction mechanism, aiming to mitigate the\nnegative impacts caused by misinformation within datasets. This is achieved by\nlearning the potential ground-truth labels of the noisy clients' datasets via\nbackpropagation. To further enhance the training robustness, we apply the\ngeometric median based model aggregation instead of the commonly-used vanilla\naveraged model aggregation. We implement sixteen related methods and conduct\nevaluations on three image datasets and one text dataset with diverse label\nnoise patterns for a comprehensive comparison. Extensive experimental results\nindicate that our proposed framework shows its robustness in different\nscenarios. Additionally, our label correction framework effectively enhances\nthe data quality of the detected noisy clients' local datasets. % Our codes\nwill be open-sourced to facilitate related research communities. Our codes are\navailable via https://github.com/Sprinter1999/MaskedOptim ."}
{"id": "2506.02093", "pdf": "https://arxiv.org/pdf/2506.02093", "abs": "https://arxiv.org/abs/2506.02093", "authors": ["Tianyu Lin", "Xinran Li", "Chuntung Zhuang", "Qi Chen", "Yuanhao Cai", "Kai Ding", "Alan L. Yuille", "Zongwei Zhou"], "title": "Are Pixel-Wise Metrics Reliable for Sparse-View Computed Tomography Reconstruction?", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Widely adopted evaluation metrics for sparse-view CT reconstruction--such as\nStructural Similarity Index Measure and Peak Signal-to-Noise Ratio--prioritize\npixel-wise fidelity but often fail to capture the completeness of critical\nanatomical structures, particularly small or thin regions that are easily\nmissed. To address this limitation, we propose a suite of novel anatomy-aware\nevaluation metrics designed to assess structural completeness across anatomical\nstructures, including large organs, small organs, intestines, and vessels.\nBuilding on these metrics, we introduce CARE, a Completeness-Aware\nReconstruction Enhancement framework that incorporates structural penalties\nduring training to encourage anatomical preservation of significant structures.\nCARE is model-agnostic and can be seamlessly integrated into analytical,\nimplicit, and generative methods. When applied to these methods, CARE\nsubstantially improves structural completeness in CT reconstructions, achieving\nup to +32% improvement for large organs, +22% for small organs, +40% for\nintestines, and +36% for vessels."}
{"id": "2506.02096", "pdf": "https://arxiv.org/pdf/2506.02096", "abs": "https://arxiv.org/abs/2506.02096", "authors": ["Zijian Wu", "Jinjie Ni", "Xiangyan Liu", "Zichen Liu", "Hang Yan", "Michael Qizhe Shieh"], "title": "SynthRL: Scaling Visual Reasoning with Verifiable Data Synthesis", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": null, "summary": "Vision-language models (VLMs) trained via reinforcement learning with\nverifiable reward (RLVR) have shown notable progress in scaling test-time\ncompute effectively. In this work, we investigate how synthesized RL data can\nfurther improve RLVR. To this end, we propose \\textbf{SynthRL}-a scalable and\nguaranteed pipeline for automatic data scaling in reasoning-oriented RL\ntraining. SynthRL comprises three key stages: (1) selecting seed questions with\nappropriate distribution, (2) augmenting them into more challenging variants\nwhile preserving the original answers, and (3) a guaranteed verification stage\nthat ensures near-perfect correctness and difficulty enhancement. Our empirical\nexperiments demonstrate SynthRL's scalability and effectiveness. When applied\nto the MMK12 dataset, SynthRL synthesizes over 3.3K additional verifiable,\nchallenging questions from approximately 8K seed samples. Models trained with\nour synthesized data achieve consistent gains across five out-of-domain visual\nmath reasoning benchmarks, with a significant improvement over baseline models\ntrained on seed data alone. Notably, detailed analysis reveals that the gains\nare more pronounced on the most challenging evaluation samples, highlighting\nSynthRL's effectiveness in eliciting deeper and more complex reasoning\npatterns."}
{"id": "2506.02197", "pdf": "https://arxiv.org/pdf/2506.02197", "abs": "https://arxiv.org/abs/2506.02197", "authors": ["Marcos V. Conde", "Radu Timofte", "Zihao Lu", "Xiangyu Kongand Xiaoxia Xingand Fan Wangand Suejin Hanand MinKyu Parkand Tianyu Zhangand Xin Luoand Yeda Chenand Dong Liuand Li Pangand Yuhang Yangand Hongzhong Wangand Xiangyong Caoand Ruixuan Jiangand Senyan Xuand Siyuan Jiangand Xueyang Fuand Zheng-Jun Zhaand Tianyu Haoand Yuhong Heand Ruoqi Liand Yueqi Yangand Xiang Yuand Guanlan Hongand Minmin Yiand Yuanjia Chenand Liwen Zhangand Zijie Jinand Cheng Liand Lian Liuand Wei Songand Heng Sunand Yubo Wangand Jinghua Wangand Jiajie Luand Watchara Ruangsangand"], "title": "NTIRE 2025 Challenge on RAW Image Restoration and Super-Resolution", "categories": ["eess.IV", "cs.CV"], "comment": "CVPR 2025 - New Trends in Image Restoration and Enhancement (NTIRE)", "summary": "This paper reviews the NTIRE 2025 RAW Image Restoration and Super-Resolution\nChallenge, highlighting the proposed solutions and results. New methods for RAW\nRestoration and Super-Resolution could be essential in modern Image Signal\nProcessing (ISP) pipelines, however, this problem is not as explored as in the\nRGB domain. The goal of this challenge is two fold, (i) restore RAW images with\nblur and noise degradations, (ii) upscale RAW Bayer images by 2x, considering\nunknown noise and blur. In the challenge, a total of 230 participants\nregistered, and 45 submitted results during thee challenge period. This report\npresents the current state-of-the-art in RAW Restoration."}
{"id": "2506.02214", "pdf": "https://arxiv.org/pdf/2506.02214", "abs": "https://arxiv.org/abs/2506.02214", "authors": ["Alexey Burdakov", "Max Jaihyun Ahn"], "title": "Is PMBOK Guide the Right Fit for AI? Re-evaluating Project Management in the Face of Artificial Intelligence Projects", "categories": ["cs.SE", "cs.CV", "D.2.9; I.4"], "comment": "9 pages, 1 figure", "summary": "This paper critically evaluates the applicability of the Project Management\nBody of Knowledge (PMBOK) Guide framework to Artificial Intelligence (AI)\nsoftware projects, highlighting key limitations and proposing tailored\nadaptations. Unlike traditional projects, AI initiatives rely heavily on\ncomplex data, iterative experimentation, and specialized expertise while\nnavigating significant ethical considerations. Our analysis identifies gaps in\nthe PMBOK Guide, including its limited focus on data management, insufficient\nsupport for iterative development, and lack of guidance on ethical and\nmultidisciplinary challenges. To address these deficiencies, we recommend\nintegrating data lifecycle management, adopting iterative and AI project\nmanagement frameworks, and embedding ethical considerations within project\nplanning and execution. Additionally, we explore alternative approaches that\nbetter align with AI's dynamic and exploratory nature. We aim to enhance\nproject management practices for AI software projects by bridging these gaps."}
{"id": "2506.02312", "pdf": "https://arxiv.org/pdf/2506.02312", "abs": "https://arxiv.org/abs/2506.02312", "authors": ["Md Tauhidul Islam", "Wu Da-Wen", "Tang Qing-Qing", "Zhao Kai-Yang", "Yin Teng", "Li Yan-Fei", "Shang Wen-Yi", "Liu Jing-Yu", "Zhang Hai-Xian"], "title": "Dual encoding feature filtering generalized attention UNET for retinal vessel segmentation", "categories": ["eess.IV", "cs.CV", "I.4; I.5"], "comment": null, "summary": "Retinal blood vessel segmentation is crucial for diagnosing ocular and\ncardiovascular diseases. Although the introduction of U-Net in 2015 by Olaf\nRonneberger significantly advanced this field, yet issues like limited training\ndata, imbalance data distribution, and inadequate feature extraction persist,\nhindering both the segmentation performance and optimal model generalization.\nAddressing these critical issues, the DEFFA-Unet is proposed featuring an\nadditional encoder to process domain-invariant pre-processed inputs, thereby\nimproving both richer feature encoding and enhanced model generalization. A\nfeature filtering fusion module is developed to ensure the precise feature\nfiltering and robust hybrid feature fusion. In response to the task-specific\nneed for higher precision where false positives are very costly, traditional\nskip connections are replaced with the attention-guided feature reconstructing\nfusion module. Additionally, innovative data augmentation and balancing methods\nare proposed to counter data scarcity and distribution imbalance, further\nboosting the robustness and generalization of the model. With a comprehensive\nsuite of evaluation metrics, extensive validations on four benchmark datasets\n(DRIVE, CHASEDB1, STARE, and HRF) and an SLO dataset (IOSTAR), demonstrate the\nproposed method's superiority over both baseline and state-of-the-art models.\nParticularly the proposed method significantly outperforms the compared methods\nin cross-validation model generalization."}
{"id": "2506.02351", "pdf": "https://arxiv.org/pdf/2506.02351", "abs": "https://arxiv.org/abs/2506.02351", "authors": ["Jeonghun Kang", "Soonmok Kwon", "Joonseok Lee", "Byung-Hak Kim"], "title": "DIAMOND: An LLM-Driven Agent for Context-Aware Baseball Highlight Summarization", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "To appear in the First REALM (Research on Agent Language Models)\n  workshop at ACL 2025", "summary": "Traditional approaches -- such as Win Probability Added (WPA)-based ranking\nor computer vision-driven event detection -- can identify scoring plays but\noften miss strategic depth, momentum shifts, and storyline progression. Manual\ncuration remains the gold standard but is resource-intensive and not scalable.\nWe introduce DIAMOND, an LLM-driven agent for context-aware baseball highlight\nsummarization that integrates structured sports analytics with natural language\nreasoning. DIAMOND leverages sabermetric features -- Win Expectancy, WPA, and\nLeverage Index -- to quantify play importance, while an LLM module enhances\nselection based on contextual narrative value. This hybrid approach ensures\nboth quantitative rigor and qualitative richness, surpassing the limitations of\npurely statistical or vision-based systems. Evaluated on five diverse Korean\nBaseball Organization League games, DIAMOND improves F1-score from 42.9%\n(WPA-only) to 84.8%, outperforming both commercial and statistical baselines.\nThough limited in scale, our results highlight the potential of modular,\ninterpretable agent-based frameworks for event-level summarization in sports\nand beyond."}
{"id": "2506.02380", "pdf": "https://arxiv.org/pdf/2506.02380", "abs": "https://arxiv.org/abs/2506.02380", "authors": ["Zihao Ding", "Cheng-Tse Lee", "Mufeng Zhu", "Tao Guan", "Yuan-Chun Sun", "Cheng-Hsin Hsu", "Yao Liu"], "title": "EyeNavGS: A 6-DoF Navigation Dataset and Record-n-Replay Software for Real-World 3DGS Scenes in VR", "categories": ["cs.MM", "cs.CV", "cs.GR", "cs.HC"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) is an emerging media representation that\nreconstructs real-world 3D scenes in high fidelity, enabling\n6-degrees-of-freedom (6-DoF) navigation in virtual reality (VR). However,\ndeveloping and evaluating 3DGS-enabled applications and optimizing their\nrendering performance, require realistic user navigation data. Such data is\ncurrently unavailable for photorealistic 3DGS reconstructions of real-world\nscenes. This paper introduces EyeNavGS (EyeNavGS), the first publicly available\n6-DoF navigation dataset featuring traces from 46 participants exploring twelve\ndiverse, real-world 3DGS scenes. The dataset was collected at two sites, using\nthe Meta Quest Pro headsets, recording the head pose and eye gaze data for each\nrendered frame during free world standing 6-DoF navigation. For each of the\ntwelve scenes, we performed careful scene initialization to correct for scene\ntilt and scale, ensuring a perceptually-comfortable VR experience. We also\nrelease our open-source SIBR viewer software fork with record-and-replay\nfunctionalities and a suite of utility tools for data processing, conversion,\nand visualization. The EyeNavGS dataset and its accompanying software tools\nprovide valuable resources for advancing research in 6-DoF viewport prediction,\nadaptive streaming, 3D saliency, and foveated rendering for 3DGS scenes. The\nEyeNavGS dataset is available at: https://symmru.github.io/EyeNavGS/."}
{"id": "2506.02381", "pdf": "https://arxiv.org/pdf/2506.02381", "abs": "https://arxiv.org/abs/2506.02381", "authors": ["Songlin Wei", "Gene Cheung", "Fei Chen", "Ivan Selesnick"], "title": "Unrolling Nonconvex Graph Total Variation for Image Denoising", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Conventional model-based image denoising optimizations employ convex\nregularization terms, such as total variation (TV) that convexifies the\n$\\ell_0$-norm to promote sparse signal representation. Instead, we propose a\nnew non-convex total variation term in a graph setting (NC-GTV), such that when\ncombined with an $\\ell_2$-norm fidelity term for denoising, leads to a convex\nobjective with no extraneous local minima. We define NC-GTV using a new graph\nvariant of the Huber function, interpretable as a Moreau envelope. The crux is\nthe selection of a parameter $a$ characterizing the graph Huber function that\nensures overall objective convexity; we efficiently compute $a$ via an\nadaptation of Gershgorin Circle Theorem (GCT). To minimize the convex\nobjective, we design a linear-time algorithm based on Alternating Direction\nMethod of Multipliers (ADMM) and unroll it into a lightweight feed-forward\nnetwork for data-driven parameter learning. Experiments show that our method\noutperforms unrolled GTV and other representative image denoising schemes,\nwhile employing far fewer network parameters."}
{"id": "2506.02467", "pdf": "https://arxiv.org/pdf/2506.02467", "abs": "https://arxiv.org/abs/2506.02467", "authors": ["Haowen Pang", "Weiyan Guo", "Chuyang Ye"], "title": "Multi-modal brain MRI synthesis based on SwinUNETR", "categories": ["eess.IV", "cs.CV"], "comment": "9 pages, 5 figures", "summary": "Multi-modal brain magnetic resonance imaging (MRI) plays a crucial role in\nclinical diagnostics by providing complementary information across different\nimaging modalities. However, a common challenge in clinical practice is missing\nMRI modalities. In this paper, we apply SwinUNETR to the synthesize of missing\nmodalities in brain MRI. SwinUNETR is a novel neural network architecture\ndesigned for medical image analysis, integrating the strengths of Swin\nTransformer and convolutional neural networks (CNNs). The Swin Transformer, a\nvariant of the Vision Transformer (ViT), incorporates hierarchical feature\nextraction and window-based self-attention mechanisms, enabling it to capture\nboth local and global contextual information effectively. By combining the Swin\nTransformer with CNNs, SwinUNETR merges global context awareness with detailed\nspatial resolution. This hybrid approach addresses the challenges posed by the\nvarying modality characteristics and complex brain structures, facilitating the\ngeneration of accurate and realistic synthetic images. We evaluate the\nperformance of SwinUNETR on brain MRI datasets and demonstrate its superior\ncapability in generating clinically valuable images. Our results show\nsignificant improvements in image quality, anatomical consistency, and\ndiagnostic value."}
{"id": "2506.02489", "pdf": "https://arxiv.org/pdf/2506.02489", "abs": "https://arxiv.org/abs/2506.02489", "authors": ["Tao Zhong", "Jonah Buchanan", "Christine Allen-Blanchette"], "title": "Grasp2Grasp: Vision-Based Dexterous Grasp Translation via Schr√∂dinger Bridges", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": "19 pages, 4 figures", "summary": "We propose a new approach to vision-based dexterous grasp translation, which\naims to transfer grasp intent across robotic hands with differing morphologies.\nGiven a visual observation of a source hand grasping an object, our goal is to\nsynthesize a functionally equivalent grasp for a target hand without requiring\npaired demonstrations or hand-specific simulations. We frame this problem as a\nstochastic transport between grasp distributions using the Schr\\\"odinger Bridge\nformalism. Our method learns to map between source and target latent grasp\nspaces via score and flow matching, conditioned on visual observations. To\nguide this translation, we introduce physics-informed cost functions that\nencode alignment in base pose, contact maps, wrench space, and manipulability.\nExperiments across diverse hand-object pairs demonstrate our approach generates\nstable, physically grounded grasps with strong generalization. This work\nenables semantic grasp transfer for heterogeneous manipulators and bridges\nvision-based grasping with probabilistic generative modeling."}
{"id": "2506.02494", "pdf": "https://arxiv.org/pdf/2506.02494", "abs": "https://arxiv.org/abs/2506.02494", "authors": ["Junzhe Zhang", "Huixuan Zhang", "Xinyu Hu", "Li Lin", "Mingqi Gao", "Shi Qiu", "Xiaojun Wan"], "title": "Minos: A Multimodal Evaluation Model for Bidirectional Generation Between Image and Text", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Evaluation is important for multimodal generation tasks. With the rapid\nprogress of MLLMs, there is growing interest in applying MLLMs to build general\nevaluation systems. However, existing work overlooks two aspects: (1) the\ndevelopment of evaluation capabilities for text-to-image (T2I) generation task,\nand (2) the incorporation of large-scale human evaluation data. In this paper,\nwe introduce Minos-Corpus, a large-scale multimodal evaluation dataset that\ncombines evaluation data from both human and GPT. The corpus contains\nevaluation data across both image-to-text(I2T) and T2I generation tasks. Based\non this corpus, we propose Data Selection and Balance, Mix-SFT training\nmethods, and apply DPO to develop Minos, a multimodal evaluation model built\nupon a 7B backbone. Minos achieves state-of-the-art (SoTA) performance among\nall open-source evaluation models of similar scale on the average of evaluation\nperformance on all tasks, and outperforms all open-source and closed-source\nmodels on evaluation of T2I generation task. Extensive experiments demonstrate\nthe importance of leveraging high-quality human evaluation data and jointly\ntraining on evaluation data from both I2T and T2I generation tasks."}
{"id": "2506.02541", "pdf": "https://arxiv.org/pdf/2506.02541", "abs": "https://arxiv.org/abs/2506.02541", "authors": ["Minsung Kim", "Nakyeong Yang", "Kyomin Jung"], "title": "Rethinking Post-Unlearning Behavior of Large Vision-Language Models", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "10 pages, 5 figures", "summary": "Machine unlearning is used to mitigate the privacy risks of Large\nVision-Language Models (LVLMs) arising from training on large-scale web data.\nHowever, existing unlearning methods often fail to carefully select substitute\noutputs for forget targets, resulting in Unlearning Aftermaths-undesirable\nbehaviors such as degenerate, hallucinated, or excessively refused responses.\nWe highlight that, especially for generative LVLMs, it is crucial to consider\nthe quality and informativeness of post-unlearning responses rather than\nrelying solely on naive suppression. To address this, we introduce a new\nunlearning task for LVLMs that requires models to provide privacy-preserving\nyet informative and visually grounded responses. We also propose PUBG, a novel\nunlearning method that explicitly guides post-unlearning behavior toward a\ndesirable output distribution. Experiments show that, while existing methods\nsuffer from Unlearning Aftermaths despite successfully preventing privacy\nviolations, PUBG effectively mitigates these issues, generating visually\ngrounded and informative responses without privacy leakage for forgotten\ntargets."}
{"id": "2506.02542", "pdf": "https://arxiv.org/pdf/2506.02542", "abs": "https://arxiv.org/abs/2506.02542", "authors": ["Niklas Kormann", "Masoud Ramuz", "Zeeshan Nisar", "Nadine S. Schaadt", "Hendrik Annuth", "Benjamin Doerr", "Friedrich Feuerhake", "Thomas Lampert", "Johannes F. Lutzeyer"], "title": "HIEGNet: A Heterogenous Graph Neural Network Including the Immune Environment in Glomeruli Classification", "categories": ["cs.LG", "cs.AI", "cs.CV", "q-bio.QM"], "comment": "Accepted for poster presentation at MIDL 2025", "summary": "Graph Neural Networks (GNNs) have recently been found to excel in\nhistopathology. However, an important histopathological task, where GNNs have\nnot been extensively explored, is the classification of glomeruli health as an\nimportant indicator in nephropathology. This task presents unique difficulties,\nparticularly for the graph construction, i.e., the identification of nodes,\nedges, and informative features. In this work, we propose a pipeline composed\nof different traditional and machine learning-based computer vision techniques\nto identify nodes, edges, and their corresponding features to form a\nheterogeneous graph. We then proceed to propose a novel heterogeneous GNN\narchitecture for glomeruli classification, called HIEGNet, that integrates both\nglomeruli and their surrounding immune cells. Hence, HIEGNet is able to\nconsider the immune environment of each glomerulus in its classification. Our\nHIEGNet was trained and tested on a dataset of Whole Slide Images from kidney\ntransplant patients. Experimental results demonstrate that HIEGNet outperforms\nseveral baseline models and generalises best between patients among all\nbaseline models. Our implementation is publicly available at\nhttps://github.com/nklsKrmnn/HIEGNet.git."}
{"id": "2506.02554", "pdf": "https://arxiv.org/pdf/2506.02554", "abs": "https://arxiv.org/abs/2506.02554", "authors": ["Timo Osterburg", "Franz Albers", "Christopher Diehl", "Rajesh Pushparaj", "Torsten Bertram"], "title": "HiLO: High-Level Object Fusion for Autonomous Driving using Transformers", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "comment": "6 pages, accepted at IEEE Intelligent Vehicles Symposium (IV) 2025", "summary": "The fusion of sensor data is essential for a robust perception of the\nenvironment in autonomous driving. Learning-based fusion approaches mainly use\nfeature-level fusion to achieve high performance, but their complexity and\nhardware requirements limit their applicability in near-production vehicles.\nHigh-level fusion methods offer robustness with lower computational\nrequirements. Traditional methods, such as the Kalman filter, dominate this\narea. This paper modifies the Adapted Kalman Filter (AKF) and proposes a novel\ntransformer-based high-level object fusion method called HiLO. Experimental\nresults demonstrate improvements of $25.9$ percentage points in $\\textrm{F}_1$\nscore and $6.1$ percentage points in mean IoU. Evaluation on a new large-scale\nreal-world dataset demonstrates the effectiveness of the proposed approaches.\nTheir generalizability is further validated by cross-domain evaluation between\nurban and highway scenarios. Code, data, and models are available at\nhttps://github.com/rst-tu-dortmund/HiLO ."}
{"id": "2506.02574", "pdf": "https://arxiv.org/pdf/2506.02574", "abs": "https://arxiv.org/abs/2506.02574", "authors": ["Shuai Yuan", "Shuang Chen", "Tianwu Lin", "Jie Wang", "Peng Gong"], "title": "Dynamic mapping from static labels: remote sensing dynamic sample generation with temporal-spectral embedding", "categories": ["eess.IV", "cs.CV", "cs.MM"], "comment": null, "summary": "Accurate remote sensing geographic mapping depends heavily on representative\nand timely sample data. However, rapid changes in land surface dynamics\nnecessitate frequent updates, quickly rendering previously collected samples\nobsolete and imposing significant labor demands for continuous manual updates.\nIn this study, we aim to address this problem by dynamic sample generation\nusing existing single-date static labeled samples. We introduce TasGen, a\ntwo-stage automated framework to automatically generate dynamic samples,\ndesigned to simultaneously model spectral and temporal dependencies in\ntime-series remote sensing imagery via temporal-spectral embedding, capturing\nland surface changes without additional manual annotations."}
{"id": "2506.02585", "pdf": "https://arxiv.org/pdf/2506.02585", "abs": "https://arxiv.org/abs/2506.02585", "authors": ["Chunwei Tian", "Mingjian Song", "Xiaopeng Fan", "Xiangtao Zheng", "Bob Zhang", "David Zhang"], "title": "A Tree-guided CNN for image super-resolution", "categories": ["eess.IV", "cs.CV"], "comment": "This paper has been accepted for publication in IEEE Transactions on\n  Consumer Electronics. 10 pages, 6 figures. Its code can be obtained at\n  https://github.com/hellloxiaotian/TSRNet", "summary": "Deep convolutional neural networks can extract more accurate structural\ninformation via deep architectures to obtain good performance in image\nsuper-resolution. However, it is not easy to find effect of important layers in\na single network architecture to decrease performance of super-resolution. In\nthis paper, we design a tree-guided CNN for image super-resolution (TSRNet). It\nuses a tree architecture to guide a deep network to enhance effect of key nodes\nto amplify the relation of hierarchical information for improving the ability\nof recovering images. To prevent insufficiency of the obtained structural\ninformation, cosine transform techniques in the TSRNet are used to extract\ncross-domain information to improve the performance of image super-resolution.\nAdaptive Nesterov momentum optimizer (Adan) is applied to optimize parameters\nto boost effectiveness of training a super-resolution model. Extended\nexperiments can verify superiority of the proposed TSRNet for restoring\nhigh-quality images. Its code can be obtained at\nhttps://github.com/hellloxiaotian/TSRNet."}
{"id": "2506.02618", "pdf": "https://arxiv.org/pdf/2506.02618", "abs": "https://arxiv.org/abs/2506.02618", "authors": ["Jialiang Zhang", "Haoran Geng", "Yang You", "Congyue Deng", "Pieter Abbeel", "Jitendra Malik", "Leonidas Guibas"], "title": "Rodrigues Network for Learning Robot Actions", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Understanding and predicting articulated actions is important in robot\nlearning. However, common architectures such as MLPs and Transformers lack\ninductive biases that reflect the underlying kinematic structure of articulated\nsystems. To this end, we propose the Neural Rodrigues Operator, a learnable\ngeneralization of the classical forward kinematics operation, designed to\ninject kinematics-aware inductive bias into neural computation. Building on\nthis operator, we design the Rodrigues Network (RodriNet), a novel neural\narchitecture specialized for processing actions. We evaluate the expressivity\nof our network on two synthetic tasks on kinematic and motion prediction,\nshowing significant improvements compared to standard backbones. We further\ndemonstrate its effectiveness in two realistic applications: (i) imitation\nlearning on robotic benchmarks with the Diffusion Policy, and (ii) single-image\n3D hand reconstruction. Our results suggest that integrating structured\nkinematic priors into the network architecture improves action learning in\nvarious domains."}
{"id": "2506.02620", "pdf": "https://arxiv.org/pdf/2506.02620", "abs": "https://arxiv.org/abs/2506.02620", "authors": ["Dongyu Yan", "Leyi Wu", "Jiantao Lin", "Luozhou Wang", "Tianshuo Xu", "Zhifei Chen", "Zhen Yang", "Lie Xu", "Shunsi Zhang", "Yingcong Chen"], "title": "FlexPainter: Flexible and Multi-View Consistent Texture Generation", "categories": ["cs.GR", "cs.CV"], "comment": "11 pages, 10 figures in main paper, 10 pages, 12 figures in\n  supplementary", "summary": "Texture map production is an important part of 3D modeling and determines the\nrendering quality. Recently, diffusion-based methods have opened a new way for\ntexture generation. However, restricted control flexibility and limited prompt\nmodalities may prevent creators from producing desired results. Furthermore,\ninconsistencies between generated multi-view images often lead to poor texture\ngeneration quality. To address these issues, we introduce \\textbf{FlexPainter},\na novel texture generation pipeline that enables flexible multi-modal\nconditional guidance and achieves highly consistent texture generation. A\nshared conditional embedding space is constructed to perform flexible\naggregation between different input modalities. Utilizing such embedding space,\nwe present an image-based CFG method to decompose structural and style\ninformation, achieving reference image-based stylization. Leveraging the 3D\nknowledge within the image diffusion prior, we first generate multi-view images\nsimultaneously using a grid representation to enhance global understanding.\nMeanwhile, we propose a view synchronization and adaptive weighting module\nduring diffusion sampling to further ensure local consistency. Finally, a\n3D-aware texture completion model combined with a texture enhancement model is\nused to generate seamless, high-resolution texture maps. Comprehensive\nexperiments demonstrate that our framework significantly outperforms\nstate-of-the-art methods in both flexibility and generation quality."}
{"id": "2506.02623", "pdf": "https://arxiv.org/pdf/2506.02623", "abs": "https://arxiv.org/abs/2506.02623", "authors": ["Yuyang Zhou", "Ferrante Neri", "Yew-Soon Ong", "Ruibin Bai"], "title": "SiamNAS: Siamese Surrogate Model for Dominance Relation Prediction in Multi-objective Neural Architecture Search", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "Genetic and Evolutionary Computation Conference (GECCO' 25)", "summary": "Modern neural architecture search (NAS) is inherently multi-objective,\nbalancing trade-offs such as accuracy, parameter count, and computational cost.\nThis complexity makes NAS computationally expensive and nearly impossible to\nsolve without efficient approximations. To address this, we propose a novel\nsurrogate modelling approach that leverages an ensemble of Siamese network\nblocks to predict dominance relationships between candidate architectures.\nLightweight and easy to train, the surrogate achieves 92% accuracy and replaces\nthe crowding distance calculation in the survivor selection strategy with a\nheuristic rule based on model size. Integrated into a framework termed SiamNAS,\nthis design eliminates costly evaluations during the search process.\nExperiments on NAS-Bench-201 demonstrate the framework's ability to identify\nPareto-optimal solutions with significantly reduced computational costs. The\nproposed SiamNAS identified a final non-dominated set containing the best\narchitecture in NAS-Bench-201 for CIFAR-10 and the second-best for ImageNet, in\nterms of test error rate, within 0.01 GPU days. This proof-of-concept study\nhighlights the potential of the proposed Siamese network surrogate model to\ngeneralise to multi-tasking optimisation, enabling simultaneous optimisation\nacross tasks. Additionally, it offers opportunities to extend the approach for\ngenerating Sets of Pareto Sets (SOS), providing diverse Pareto-optimal\nsolutions for heterogeneous task settings."}
{"id": "2506.02661", "pdf": "https://arxiv.org/pdf/2506.02661", "abs": "https://arxiv.org/abs/2506.02661", "authors": ["Mingyang Huang", "Peng Zhang", "Bang Zhang"], "title": "MotionRAG-Diff: A Retrieval-Augmented Diffusion Framework for Long-Term Music-to-Dance Generation", "categories": ["cs.SD", "cs.CV", "cs.GR", "eess.AS"], "comment": "12 pages, 5 figures", "summary": "Generating long-term, coherent, and realistic music-conditioned dance\nsequences remains a challenging task in human motion synthesis. Existing\napproaches exhibit critical limitations: motion graph methods rely on fixed\ntemplate libraries, restricting creative generation; diffusion models, while\ncapable of producing novel motions, often lack temporal coherence and musical\nalignment. To address these challenges, we propose $\\textbf{MotionRAG-Diff}$, a\nhybrid framework that integrates Retrieval-Augmented Generation (RAG) with\ndiffusion-based refinement to enable high-quality, musically coherent dance\ngeneration for arbitrary long-term music inputs. Our method introduces three\ncore innovations: (1) A cross-modal contrastive learning architecture that\naligns heterogeneous music and dance representations in a shared latent space,\nestablishing unsupervised semantic correspondence without paired data; (2) An\noptimized motion graph system for efficient retrieval and seamless\nconcatenation of motion segments, ensuring realism and temporal coherence\nacross long sequences; (3) A multi-condition diffusion model that jointly\nconditions on raw music signals and contrastive features to enhance motion\nquality and global synchronization. Extensive experiments demonstrate that\nMotionRAG-Diff achieves state-of-the-art performance in motion quality,\ndiversity, and music-motion synchronization accuracy. This work establishes a\nnew paradigm for music-driven dance generation by synergizing retrieval-based\ntemplate fidelity with diffusion-based creative enhancement."}
{"id": "2506.02761", "pdf": "https://arxiv.org/pdf/2506.02761", "abs": "https://arxiv.org/abs/2506.02761", "authors": ["Renyang Liu", "Wenjie Feng", "Tianwei Zhang", "Wei Zhou", "Xueqi Cheng", "See-Kiong Ng"], "title": "Rethinking Machine Unlearning in Image Generation Models", "categories": ["cs.AI", "cs.CL", "cs.CR", "cs.CV"], "comment": "Accepted by ACM CCS 2025", "summary": "With the surge and widespread application of image generation models, data\nprivacy and content safety have become major concerns and attracted great\nattention from users, service providers, and policymakers. Machine unlearning\n(MU) is recognized as a cost-effective and promising means to address these\nchallenges. Despite some advancements, image generation model unlearning (IGMU)\nstill faces remarkable gaps in practice, e.g., unclear task discrimination and\nunlearning guidelines, lack of an effective evaluation framework, and\nunreliable evaluation metrics. These can hinder the understanding of unlearning\nmechanisms and the design of practical unlearning algorithms. We perform\nexhaustive assessments over existing state-of-the-art unlearning algorithms and\nevaluation standards, and discover several critical flaws and challenges in\nIGMU tasks. Driven by these limitations, we make several core contributions, to\nfacilitate the comprehensive understanding, standardized categorization, and\nreliable evaluation of IGMU. Specifically, (1) We design CatIGMU, a novel\nhierarchical task categorization framework. It provides detailed implementation\nguidance for IGMU, assisting in the design of unlearning algorithms and the\nconstruction of testbeds. (2) We introduce EvalIGMU, a comprehensive evaluation\nframework. It includes reliable quantitative metrics across five critical\naspects. (3) We construct DataIGM, a high-quality unlearning dataset, which can\nbe used for extensive evaluations of IGMU, training content detectors for\njudgment, and benchmarking the state-of-the-art unlearning algorithms. With\nEvalIGMU and DataIGM, we discover that most existing IGMU algorithms cannot\nhandle the unlearning well across different evaluation dimensions, especially\nfor preservation and robustness. Code and models are available at\nhttps://github.com/ryliu68/IGMU."}
{"id": "2506.02794", "pdf": "https://arxiv.org/pdf/2506.02794", "abs": "https://arxiv.org/abs/2506.02794", "authors": ["Mijeong Kim", "Gunhee Kim", "Jungyoon Choi", "Wonjae Roh", "Bohyung Han"], "title": "PhysGaia: A Physics-Aware Dataset of Multi-Body Interactions for Dynamic Novel View Synthesis", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": "Project page: http://cvlab.snu.ac.kr/research/PhysGaia, Data:\n  https://huggingface.co/datasets/mijeongkim/PhysGaia/tree/main", "summary": "We introduce PhysGaia, a novel physics-aware dataset specifically designed\nfor Dynamic Novel View Synthesis (DyNVS), encompassing both structured objects\nand unstructured physical phenomena. Unlike existing datasets that primarily\nfocus on photorealistic reconstruction, PhysGaia is created to actively support\nphysics-aware dynamic scene modeling. Our dataset provides complex dynamic\nscenarios with rich interactions among multiple objects, where they\nrealistically collide with each other and exchange forces. Furthermore, it\ncontains a diverse range of physical materials, such as liquid, gas,\nviscoelastic substance, and textile, which moves beyond the rigid bodies\nprevalent in existing datasets. All scenes in PhysGaia are faithfully generated\nto strictly adhere to physical laws, leveraging carefully selected\nmaterial-specific physics solvers. To enable quantitative evaluation of\nphysical modeling, our dataset provides essential ground-truth information,\nincluding 3D particle trajectories and physics parameters, e.g., viscosity. To\nfacilitate research adoption, we also provide essential integration pipelines\nfor using state-of-the-art DyNVS models with our dataset and report their\nresults. By addressing the critical lack of datasets for physics-aware\nmodeling, PhysGaia will significantly advance research in dynamic view\nsynthesis, physics-based scene understanding, and deep learning models\nintegrated with physical simulation -- ultimately enabling more faithful\nreconstruction and interpretation of complex dynamic scenes. Our datasets and\ncodes are available in the project website,\nhttp://cvlab.snu.ac.kr/research/PhysGaia."}
{"id": "2506.02803", "pdf": "https://arxiv.org/pdf/2506.02803", "abs": "https://arxiv.org/abs/2506.02803", "authors": ["Sifan Li", "Yujun Cai", "Yiwei Wang"], "title": "SemVink: Advancing VLMs' Semantic Understanding of Optical Illusions via Visual Global Thinking", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Vision-language models (VLMs) excel in semantic tasks but falter at a core\nhuman capability: detecting hidden content in optical illusions or AI-generated\nimages through perceptual adjustments like zooming. We introduce HC-Bench, a\nbenchmark of 112 images with hidden text, objects, and illusions, revealing\nthat leading VLMs achieve near-zero accuracy (0-5.36%)-even with explicit\nprompting. Humans resolve such ambiguities instinctively, yet VLMs fail due to\nan overreliance on high-level semantics. Strikingly, we propose SemVink\n(Semantic Visual Thinking) by simply scaling images to low resolutions (32-128\npixels), which unlocks >99% accuracy by eliminating redundant visual noise.\nThis exposes a critical architectural flaw: VLMs prioritize abstract reasoning\nover low-level visual operations crucial for real-world robustness. Our work\nurges a shift toward hybrid models integrating multi-scale processing, bridging\nthe gap between computational vision and human cognition for applications in\nmedical imaging, security, and beyond."}
{"id": "2506.02895", "pdf": "https://arxiv.org/pdf/2506.02895", "abs": "https://arxiv.org/abs/2506.02895", "authors": ["Ahmad AlMughrabi", "Umair Haroon", "Ricardo Marques", "Petia Radeva"], "title": "VolTex: Food Volume Estimation using Text-Guided Segmentation and Neural Surface Reconstruction", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Accurate food volume estimation is crucial for dietary monitoring, medical\nnutrition management, and food intake analysis. Existing 3D Food Volume\nestimation methods accurately compute the food volume but lack for food\nportions selection. We present VolTex, a framework that improves \\change{the\nfood object selection} in food volume estimation. Allowing users to specify a\ntarget food item via text input to be segmented, our method enables the precise\nselection of specific food objects in real-world scenes. The segmented object\nis then reconstructed using the Neural Surface Reconstruction method to\ngenerate high-fidelity 3D meshes for volume computation. Extensive evaluations\non the MetaFood3D dataset demonstrate the effectiveness of our approach in\nisolating and reconstructing food items for accurate volume estimation. The\nsource code is accessible at https://github.com/GCVCG/VolTex."}
{"id": "2506.02950", "pdf": "https://arxiv.org/pdf/2506.02950", "abs": "https://arxiv.org/abs/2506.02950", "authors": ["Stepan I. Manukhov", "Alexander Kolesov", "Vladimir V. Palyulin", "Alexander Korotin"], "title": "Interaction Field Matching: Overcoming Limitations of Electrostatic Models", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Electrostatic field matching (EFM) has recently appeared as a novel\nphysics-inspired paradigm for data generation and transfer using the idea of an\nelectric capacitor. However, it requires modeling electrostatic fields using\nneural networks, which is non-trivial because of the necessity to take into\naccount the complex field outside the capacitor plates. In this paper, we\npropose Interaction Field Matching (IFM), a generalization of EFM which allows\nusing general interaction fields beyond the electrostatic one. Furthermore,\ninspired by strong interactions between quarks and antiquarks in physics, we\ndesign a particular interaction field realization which solves the problems\nwhich arise when modeling electrostatic fields in EFM. We show the performance\non a series of toy and image data transfer problems."}
{"id": "2506.03004", "pdf": "https://arxiv.org/pdf/2506.03004", "abs": "https://arxiv.org/abs/2506.03004", "authors": ["Junyu Liu", "R. Kenny Jones", "Daniel Ritchie"], "title": "PartComposer: Learning and Composing Part-Level Concepts from Single-Image Examples", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "We present PartComposer: a framework for part-level concept learning from\nsingle-image examples that enables text-to-image diffusion models to compose\nnovel objects from meaningful components. Existing methods either struggle with\neffectively learning fine-grained concepts or require a large dataset as input.\nWe propose a dynamic data synthesis pipeline generating diverse part\ncompositions to address one-shot data scarcity. Most importantly, we propose to\nmaximize the mutual information between denoised latents and structured concept\ncodes via a concept predictor, enabling direct regulation on concept\ndisentanglement and re-composition supervision. Our method achieves strong\ndisentanglement and controllable composition, outperforming subject and\npart-level baselines when mixing concepts from the same, or different, object\ncategories."}
{"id": "2506.03095", "pdf": "https://arxiv.org/pdf/2506.03095", "abs": "https://arxiv.org/abs/2506.03095", "authors": ["Man Luo", "David Cobbley", "Xin Su", "Shachar Rosenman", "Vasudev Lal", "Shao-Yen Tseng", "Phillip Howard"], "title": "DPO Learning with LLMs-Judge Signal for Computer Use Agents", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "Computer use agents (CUA) are systems that automatically interact with\ngraphical user interfaces (GUIs) to complete tasks. CUA have made significant\nprogress with the advent of large vision-language models (VLMs). However, these\nagents typically rely on cloud-based inference with substantial compute\ndemands, raising critical privacy and scalability concerns, especially when\noperating on personal devices. In this work, we take a step toward\nprivacy-preserving and resource-efficient agents by developing a lightweight\nvision-language model that runs entirely on local machines. To train this\ncompact agent, we introduce an LLM-as-Judge framework that automatically\nevaluates and filters synthetic interaction trajectories, producing\nhigh-quality data for reinforcement learning without human annotation.\nExperiments on the OS-World benchmark demonstrate that our fine-tuned local\nmodel outperforms existing baselines, highlighting a promising path toward\nprivate, efficient, and generalizable GUI agents."}
{"id": "2506.03118", "pdf": "https://arxiv.org/pdf/2506.03118", "abs": "https://arxiv.org/abs/2506.03118", "authors": ["Zhiyuan Yu", "Zhe Li", "Hujun Bao", "Can Yang", "Xiaowei Zhou"], "title": "HumanRAM: Feed-forward Human Reconstruction and Animation Model using Transformers", "categories": ["cs.GR", "cs.CV"], "comment": "Accepted by SIGGRAPH 2025 (Conference Track). Project page:\n  https://zju3dv.github.io/humanram/", "summary": "3D human reconstruction and animation are long-standing topics in computer\ngraphics and vision. However, existing methods typically rely on sophisticated\ndense-view capture and/or time-consuming per-subject optimization procedures.\nTo address these limitations, we propose HumanRAM, a novel feed-forward\napproach for generalizable human reconstruction and animation from monocular or\nsparse human images. Our approach integrates human reconstruction and animation\ninto a unified framework by introducing explicit pose conditions, parameterized\nby a shared SMPL-X neural texture, into transformer-based large reconstruction\nmodels (LRM). Given monocular or sparse input images with associated camera\nparameters and SMPL-X poses, our model employs scalable transformers and a\nDPT-based decoder to synthesize realistic human renderings under novel\nviewpoints and novel poses. By leveraging the explicit pose conditions, our\nmodel simultaneously enables high-quality human reconstruction and\nhigh-fidelity pose-controlled animation. Experiments show that HumanRAM\nsignificantly surpasses previous methods in terms of reconstruction accuracy,\nanimation fidelity, and generalization performance on real-world datasets.\nVideo results are available at https://zju3dv.github.io/humanram/."}
{"id": "2506.03134", "pdf": "https://arxiv.org/pdf/2506.03134", "abs": "https://arxiv.org/abs/2506.03134", "authors": ["Weiqing Xiao", "Hao Huang", "Chonghao Zhong", "Yujie Lin", "Nan Wang", "Xiaoxue Chen", "Zhaoxi Chen", "Saining Zhang", "Shuocheng Yang", "Pierre Merriaux", "Lei Lei", "Hao Zhao"], "title": "Simulate Any Radar: Attribute-Controllable Radar Simulation via Waveform Parameter Embedding", "categories": ["eess.SP", "cs.CV"], "comment": "Code: https://github.com/zhuxing0/SA-Radar Project page:\n  https://zhuxing0.github.io/projects/SA-Radar", "summary": "We present SA-Radar (Simulate Any Radar), a radar simulation approach that\nenables controllable and efficient generation of radar cubes conditioned on\ncustomizable radar attributes. Unlike prior generative or physics-based\nsimulators, SA-Radar integrates both paradigms through a waveform-parameterized\nattribute embedding. We design ICFAR-Net, a 3D U-Net conditioned on radar\nattributes encoded via waveform parameters, which captures signal variations\ninduced by different radar configurations. This formulation bypasses the need\nfor detailed radar hardware specifications and allows efficient simulation of\nrange-azimuth-Doppler (RAD) tensors across diverse sensor settings. We further\nconstruct a mixed real-simulated dataset with attribute annotations to robustly\ntrain the network. Extensive evaluations on multiple downstream tasks-including\n2D/3D object detection and radar semantic segmentation-demonstrate that\nSA-Radar's simulated data is both realistic and effective, consistently\nimproving model performance when used standalone or in combination with real\ndata. Our framework also supports simulation in novel sensor viewpoints and\nedited scenes, showcasing its potential as a general-purpose radar data engine\nfor autonomous driving applications. Code and additional materials are\navailable at https://zhuxing0.github.io/projects/SA-Radar."}
{"id": "2506.03143", "pdf": "https://arxiv.org/pdf/2506.03143", "abs": "https://arxiv.org/abs/2506.03143", "authors": ["Qianhui Wu", "Kanzhi Cheng", "Rui Yang", "Chaoyun Zhang", "Jianwei Yang", "Huiqiang Jiang", "Jian Mu", "Baolin Peng", "Bo Qiao", "Reuben Tan", "Si Qin", "Lars Liden", "Qingwei Lin", "Huan Zhang", "Tong Zhang", "Jianbing Zhang", "Dongmei Zhang", "Jianfeng Gao"], "title": "GUI-Actor: Coordinate-Free Visual Grounding for GUI Agents", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "One of the principal challenges in building VLM-powered GUI agents is visual\ngrounding, i.e., localizing the appropriate screen region for action execution\nbased on both the visual content and the textual plans. Most existing work\nformulates this as a text-based coordinate generation task. However, these\napproaches suffer from several limitations: weak spatial-semantic alignment,\ninability to handle ambiguous supervision targets, and a mismatch between the\ndense nature of screen coordinates and the coarse, patch-level granularity of\nvisual features extracted by models like Vision Transformers. In this paper, we\npropose GUI-Actor, a VLM-based method for coordinate-free GUI grounding. At its\ncore, GUI-Actor introduces an attention-based action head that learns to align\na dedicated <ACTOR> token with all relevant visual patch tokens, enabling the\nmodel to propose one or more action regions in a single forward pass. In line\nwith this, we further design a grounding verifier to evaluate and select the\nmost plausible action region from the candidates proposed for action execution.\nExtensive experiments show that GUI-Actor outperforms prior state-of-the-art\nmethods on multiple GUI action grounding benchmarks, with improved\ngeneralization to unseen screen resolutions and layouts. Notably, GUI-Actor-7B\neven surpasses UI-TARS-72B (38.1) on ScreenSpot-Pro, achieving scores of 40.7\nwith Qwen2-VL and 44.6 with Qwen2.5-VL as backbones. Furthermore, by\nincorporating the verifier, we find that fine-tuning only the newly introduced\naction head (~100M parameters for 7B model) while keeping the VLM backbone\nfrozen is sufficient to achieve performance comparable to previous\nstate-of-the-art models, highlighting that GUI-Actor can endow the underlying\nVLM with effective grounding capabilities without compromising its\ngeneral-purpose strengths."}
