{"id": "2505.18291", "pdf": "https://arxiv.org/pdf/2505.18291", "abs": "https://arxiv.org/abs/2505.18291", "authors": ["Zifu Wan", "Yaqi Xie", "Ce Zhang", "Zhiqiu Lin", "Zihan Wang", "Simon Stepputtis", "Deva Ramanan", "Katia Sycara"], "title": "InstructPart: Task-Oriented Part Segmentation with Instruction Reasoning", "categories": ["cs.CV", "cs.RO"], "comment": "Accepted by ACL 2025 Main. Project page:\n  https://zifuwan.github.io/InstructPart/", "summary": "Large multimodal foundation models, particularly in the domains of language\nand vision, have significantly advanced various tasks, including robotics,\nautonomous driving, information retrieval, and grounding. However, many of\nthese models perceive objects as indivisible, overlooking the components that\nconstitute them. Understanding these components and their associated\naffordances provides valuable insights into an object's functionality, which is\nfundamental for performing a wide range of tasks. In this work, we introduce a\nnovel real-world benchmark, InstructPart, comprising hand-labeled part\nsegmentation annotations and task-oriented instructions to evaluate the\nperformance of current models in understanding and executing part-level tasks\nwithin everyday contexts. Through our experiments, we demonstrate that\ntask-oriented part segmentation remains a challenging problem, even for\nstate-of-the-art Vision-Language Models (VLMs). In addition to our benchmark,\nwe introduce a simple baseline that achieves a twofold performance improvement\nthrough fine-tuning with our dataset. With our dataset and benchmark, we aim to\nfacilitate research on task-oriented part segmentation and enhance the\napplicability of VLMs across various domains, including robotics, virtual\nreality, information retrieval, and other related fields. Project website:\nhttps://zifuwan.github.io/InstructPart/."}
{"id": "2505.18302", "pdf": "https://arxiv.org/pdf/2505.18302", "abs": "https://arxiv.org/abs/2505.18302", "authors": ["Gefei Shen", "Yung-Hong Sun", "Yu Hen Hu", "Hongrui Jiang"], "title": "Sampling Strategies for Efficient Training of Deep Learning Object Detection Algorithms", "categories": ["cs.CV", "cs.IT", "math.IT"], "comment": null, "summary": "Two sampling strategies are investigated to enhance efficiency in training a\ndeep learning object detection model. These sampling strategies are employed\nunder the assumption of Lipschitz continuity of deep learning models. The first\nstrategy is uniform sampling which seeks to obtain samples evenly yet randomly\nthrough the state space of the object dynamics. The second strategy of frame\ndifference sampling is developed to explore the temporal redundancy among\nsuccessive frames in a video. Experiment result indicates that these proposed\nsampling strategies provide a dataset that yields good training performance\nwhile requiring relatively few manually labelled samples."}
{"id": "2505.18306", "pdf": "https://arxiv.org/pdf/2505.18306", "abs": "https://arxiv.org/abs/2505.18306", "authors": ["Karly Hou", "Wanhua Li", "Hanspeter Pfister"], "title": "CTRL-GS: Cascaded Temporal Residue Learning for 4D Gaussian Splatting", "categories": ["cs.CV"], "comment": "Accepted to 4D Vision Workshop @ CVPR 2025", "summary": "Recently, Gaussian Splatting methods have emerged as a desirable substitute\nfor prior Radiance Field methods for novel-view synthesis of scenes captured\nwith multi-view images or videos. In this work, we propose a novel extension to\n4D Gaussian Splatting for dynamic scenes. Drawing on ideas from residual\nlearning, we hierarchically decompose the dynamic scene into a\n\"video-segment-frame\" structure, with segments dynamically adjusted by optical\nflow. Then, instead of directly predicting the time-dependent signals, we model\nthe signal as the sum of video-constant values, segment-constant values, and\nframe-specific residuals, as inspired by the success of residual learning. This\napproach allows more flexible models that adapt to highly variable scenes. We\ndemonstrate state-of-the-art visual quality and real-time rendering on several\nestablished datasets, with the greatest improvements on complex scenes with\nlarge movements, occlusions, and fine details, where current methods degrade\nmost."}
{"id": "2505.18315", "pdf": "https://arxiv.org/pdf/2505.18315", "abs": "https://arxiv.org/abs/2505.18315", "authors": ["Mariano Rivera", "Angello Hoyos"], "title": "COLORA: Efficient Fine-Tuning for Convolutional Models with a Study Case on Optical Coherence Tomography Image Classification", "categories": ["cs.CV", "cs.AI", "68T07", "I.1.2; I.4.0; I.4.10; I.4.0"], "comment": "15 pages, 12 figures. Submitted to Jou. Pattern Recognition", "summary": "We introduce the Convolutional Low-Rank Adaptation (CoLoRA) method, designed\nexplicitly to overcome the inefficiencies found in current CNN fine-tuning\nmethods. CoLoRA can be seen as a natural extension of the convolutional\narchitectures of the Low-Rank Adaptation (LoRA) technique. We demonstrate the\ncapabilities of our method by developing and evaluating models using the widely\nadopted CNN backbone pre-trained on ImageNet. We observed that this strategy\nresults in a stable and accurate coarse-tuning procedure. Moreover, this\nstrategy is computationally efficient and significantly reduces the number of\nparameters required for fine-tuning compared to traditional methods.\nFurthermore, our method substantially improves the speed and stability of\ntraining. Our case study focuses on classifying retinal diseases from optical\ncoherence tomography (OCT) images, specifically using the OCTMNIST dataset.\nExperimental results demonstrate that a CNN backbone fine-tuned with CoLoRA\nsurpasses nearly 1\\% in accuracy. Such a performance is comparable to the\nVision Transformer, State-space discrete, and Kolmogorov-Arnold network models."}
{"id": "2505.18337", "pdf": "https://arxiv.org/pdf/2505.18337", "abs": "https://arxiv.org/abs/2505.18337", "authors": ["Rajarshi Bhattacharya", "Shakeeb Murtaza", "Christian Desrosiers", "Jose Dolz", "Maguelonne Heritier", "Eric Granger"], "title": "DART$^3$: Leveraging Distance for Test Time Adaptation in Person Re-Identification", "categories": ["cs.CV"], "comment": null, "summary": "Person re-identification (ReID) models are known to suffer from camera bias,\nwhere learned representations cluster according to camera viewpoints rather\nthan identity, leading to significant performance degradation under\n(inter-camera) domain shifts in real-world surveillance systems when new\ncameras are added to camera networks. State-of-the-art test-time adaptation\n(TTA) methods, largely designed for classification tasks, rely on\nclassification entropy-based objectives that fail to generalize well to ReID,\nthus making them unsuitable for tackling camera bias. In this paper, we\nintroduce DART$^3$, a TTA framework specifically designed to mitigate\ncamera-induced domain shifts in person ReID. DART$^3$ (Distance-Aware Retrieval\nTuning at Test Time) leverages a distance-based objective that aligns better\nwith image retrieval tasks like ReID by exploiting the correlation between\nnearest-neighbor distance and prediction error. Unlike prior ReID-specific\ndomain adaptation methods, DART$^3$ requires no source data, architectural\nmodifications, or retraining, and can be deployed in both fully black-box and\nhybrid settings. Empirical evaluations on multiple ReID benchmarks indicate\nthat DART$^3$ and DART$^3$ LITE, a lightweight alternative to the approach,\nconsistently outperforms state-of-the-art TTA baselines, making for a viable\noption to online learning to mitigate the adverse effects of camera bias."}
{"id": "2505.18342", "pdf": "https://arxiv.org/pdf/2505.18342", "abs": "https://arxiv.org/abs/2505.18342", "authors": ["Jack Goffinet", "Youngjo Min", "Carlo Tomasi", "David E. Carlson"], "title": "Pose Splatter: A 3D Gaussian Splatting Model for Quantifying Animal Pose and Appearance", "categories": ["cs.CV", "cs.LG"], "comment": "19 pages, 13 figures", "summary": "Accurate and scalable quantification of animal pose and appearance is crucial\nfor studying behavior. Current 3D pose estimation techniques, such as keypoint-\nand mesh-based techniques, often face challenges including limited\nrepresentational detail, labor-intensive annotation requirements, and expensive\nper-frame optimization. These limitations hinder the study of subtle movements\nand can make large-scale analyses impractical. We propose Pose Splatter, a\nnovel framework leveraging shape carving and 3D Gaussian splatting to model the\ncomplete pose and appearance of laboratory animals without prior knowledge of\nanimal geometry, per-frame optimization, or manual annotations. We also propose\na novel rotation-invariant visual embedding technique for encoding pose and\nappearance, designed to be a plug-in replacement for 3D keypoint data in\ndownstream behavioral analyses. Experiments on datasets of mice, rats, and\nzebra finches show Pose Splatter learns accurate 3D animal geometries. Notably,\nPose Splatter represents subtle variations in pose, provides better\nlow-dimensional pose embeddings over state-of-the-art as evaluated by humans,\nand generalizes to unseen data. By eliminating annotation and per-frame\noptimization bottlenecks, Pose Splatter enables analysis of large-scale,\nlongitudinal behavior needed to map genotype, neural activity, and\nmicro-behavior at unprecedented resolution."}
{"id": "2505.18358", "pdf": "https://arxiv.org/pdf/2505.18358", "abs": "https://arxiv.org/abs/2505.18358", "authors": ["Jianyang Gu", "Haonan Wang", "Ruoxi Jia", "Saeed Vahidian", "Vyacheslav Kungurtsev", "Wei Jiang", "Yiran Chen"], "title": "CONCORD: Concept-Informed Diffusion for Dataset Distillation", "categories": ["cs.CV"], "comment": null, "summary": "Dataset distillation (DD) has witnessed significant progress in creating\nsmall datasets that encapsulate rich information from large original ones.\nParticularly, methods based on generative priors show promising performance,\nwhile maintaining computational efficiency and cross-architecture\ngeneralization. However, the generation process lacks explicit controllability\nfor each sample. Previous distillation methods primarily match the real\ndistribution from the perspective of the entire dataset, whereas overlooking\nconcept completeness at the instance level. The missing or incorrectly\nrepresented object details cannot be efficiently compensated due to the\nconstrained sample amount typical in DD settings. To this end, we propose\nincorporating the concept understanding of large language models (LLMs) to\nperform Concept-Informed Diffusion (CONCORD) for dataset distillation.\nSpecifically, distinguishable and fine-grained concepts are retrieved based on\ncategory labels to inform the denoising process and refine essential object\ndetails. By integrating these concepts, the proposed method significantly\nenhances both the controllability and interpretability of the distilled image\ngeneration, without relying on pre-trained classifiers. We demonstrate the\nefficacy of CONCORD by achieving state-of-the-art performance on ImageNet-1K\nand its subsets. The code implementation is released in\nhttps://github.com/vimar-gu/CONCORD."}
{"id": "2505.18368", "pdf": "https://arxiv.org/pdf/2505.18368", "abs": "https://arxiv.org/abs/2505.18368", "authors": ["Yike Zhang", "Jack H. Noble"], "title": "Weakly-supervised Mamba-Based Mastoidectomy Shape Prediction for Cochlear Implant Surgery Using 3D T-Distribution Loss", "categories": ["cs.CV"], "comment": null, "summary": "Cochlear implant surgery is a treatment for individuals with severe hearing\nloss. It involves inserting an array of electrodes inside the cochlea to\nelectrically stimulate the auditory nerve and restore hearing sensation. A\ncrucial step in this procedure is mastoidectomy, a surgical intervention that\nremoves part of the mastoid region of the temporal bone, providing a critical\npathway to the cochlea for electrode placement. Accurate prediction of the\nmastoidectomy region from preoperative imaging assists presurgical planning,\nreduces surgical risks, and improves surgical outcomes. In previous work, a\nself-supervised network was introduced to predict the mastoidectomy region\nusing only preoperative CT scans. While promising, the method suffered from\nsuboptimal robustness, limiting its practical application. To address this\nlimitation, we propose a novel weakly-supervised Mamba-based framework to\npredict accurate mastoidectomy regions directly from preoperative CT scans. Our\napproach utilizes a 3D T-Distribution loss function inspired by the Student-t\ndistribution, which effectively handles the complex geometric variability\ninherent in mastoidectomy shapes. Weak supervision is achieved using the\nsegmentation results from the prior self-supervised network to eliminate the\nneed for manual data cleaning or labeling throughout the training process. The\nproposed method is extensively evaluated against state-of-the-art approaches,\ndemonstrating superior performance in predicting accurate and clinically\nrelevant mastoidectomy regions. Our findings highlight the robustness and\nefficiency of the weakly-supervised learning framework with the proposed novel\n3D T-Distribution loss."}
{"id": "2505.18381", "pdf": "https://arxiv.org/pdf/2505.18381", "abs": "https://arxiv.org/abs/2505.18381", "authors": ["Yike Zhang", "Eduardo Davalos Anaya", "Jack H. Noble"], "title": "Monocular Marker-free Patient-to-Image Intraoperative Registration for Cochlear Implant Surgery", "categories": ["cs.CV"], "comment": null, "summary": "This paper presents a novel method for monocular patient-to-image\nintraoperative registration, specifically designed to operate without any\nexternal hardware tracking equipment or fiducial point markers. Leveraging a\nsynthetic microscopy surgical scene dataset with a wide range of\ntransformations, our approach directly maps preoperative CT scans to 2D\nintraoperative surgical frames through a lightweight neural network for\nreal-time cochlear implant surgery guidance via a zero-shot learning approach.\nUnlike traditional methods, our framework seamlessly integrates with monocular\nsurgical microscopes, making it highly practical for clinical use without\nadditional hardware dependencies and requirements. Our method estimates camera\nposes, which include a rotation matrix and a translation vector, by learning\nfrom the synthetic dataset, enabling accurate and efficient intraoperative\nregistration. The proposed framework was evaluated on nine clinical cases using\na patient-specific and cross-patient validation strategy. Our results suggest\nthat our approach achieves clinically relevant accuracy in predicting 6D camera\nposes for registering 3D preoperative CT scans to 2D surgical scenes with an\nangular error within 10 degrees in most cases, while also addressing\nlimitations of traditional methods, such as reliance on external tracking\nsystems or fiducial markers."}
{"id": "2505.18399", "pdf": "https://arxiv.org/pdf/2505.18399", "abs": "https://arxiv.org/abs/2505.18399", "authors": ["Lin Zhao", "Yushu Wu", "Xinru Jiang", "Jianyang Gu", "Yanzhi Wang", "Xiaolin Xu", "Pu Zhao", "Xue Lin"], "title": "Taming Diffusion for Dataset Distillation with High Representativeness", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "The paper is accepted by ICML 2025", "summary": "Recent deep learning models demand larger datasets, driving the need for\ndataset distillation to create compact, cost-efficient datasets while\nmaintaining performance. Due to the powerful image generation capability of\ndiffusion, it has been introduced to this field for generating distilled\nimages. In this paper, we systematically investigate issues present in current\ndiffusion-based dataset distillation methods, including inaccurate distribution\nmatching, distribution deviation with random noise, and separate sampling.\nBuilding on this, we propose D^3HR, a novel diffusion-based framework to\ngenerate distilled datasets with high representativeness. Specifically, we\nadopt DDIM inversion to map the latents of the full dataset from a\nlow-normality latent domain to a high-normality Gaussian domain, preserving\ninformation and ensuring structural consistency to generate representative\nlatents for the distilled dataset. Furthermore, we propose an efficient\nsampling scheme to better align the representative latents with the\nhigh-normality Gaussian distribution. Our comprehensive experiments demonstrate\nthat D^3HR can achieve higher accuracy across different model architectures\ncompared with state-of-the-art baselines in dataset distillation. Source code:\nhttps://github.com/lin-zhao-resoLve/D3HR."}
{"id": "2505.18401", "pdf": "https://arxiv.org/pdf/2505.18401", "abs": "https://arxiv.org/abs/2505.18401", "authors": ["Jiangbei Yue", "He Wang"], "title": "Recent Deep Learning in Crowd Behaviour Analysis: A Brief Review", "categories": ["cs.CV"], "comment": "51 pages, 7 figures, Book Chapter", "summary": "Crowd behaviour analysis is essential to numerous real-world applications,\nsuch as public safety and urban planning, and therefore has been studied for\ndecades. In the last decade or so, the development of deep learning has\nsignificantly propelled the research on crowd behaviours. This chapter reviews\nrecent advances in crowd behaviour analysis using deep learning. We mainly\nreview the research in two core tasks in this field, crowd behaviour prediction\nand recognition. We broadly cover how different deep neural networks, after\nfirst being proposed in machine learning, are applied to analysing crowd\nbehaviours. This includes pure deep neural network models as well as recent\ndevelopment of methodologies combining physics with deep learning. In addition,\nrepresentative studies are discussed and compared in detail. Finally, we\ndiscuss the effectiveness of existing methods and future research directions in\nthis rapidly evolving field. This chapter aims to provide a high-level summary\nof the ongoing deep learning research in crowd behaviour analysis. It intends\nto help new researchers who just entered this field to obtain an overall\nunderstanding of the ongoing research, as well as to provide a retrospective\nanalysis for existing researchers to identify possible future directions"}
{"id": "2505.18412", "pdf": "https://arxiv.org/pdf/2505.18412", "abs": "https://arxiv.org/abs/2505.18412", "authors": ["Jessica Tang", "Ali Abedi", "Tracey J. F. Colella", "Shehroz S. Khan"], "title": "Rehabilitation Exercise Quality Assessment and Feedback Generation Using Large Language Models with Prompt Engineering", "categories": ["cs.CV", "cs.HC"], "comment": "16 pages, 3 figures, 5 tables", "summary": "Exercise-based rehabilitation improves quality of life and reduces morbidity,\nmortality, and rehospitalization, though transportation constraints and staff\nshortages lead to high dropout rates from rehabilitation programs. Virtual\nplatforms enable patients to complete prescribed exercises at home, while AI\nalgorithms analyze performance, deliver feedback, and update clinicians.\nAlthough many studies have developed machine learning and deep learning models\nfor exercise quality assessment, few have explored the use of large language\nmodels (LLMs) for feedback and are limited by the lack of rehabilitation\ndatasets containing textual feedback. In this paper, we propose a new method in\nwhich exercise-specific features are extracted from the skeletal joints of\npatients performing rehabilitation exercises and fed into pre-trained LLMs.\nUsing a range of prompting techniques, such as zero-shot, few-shot,\nchain-of-thought, and role-play prompting, LLMs are leveraged to evaluate\nexercise quality and provide feedback in natural language to help patients\nimprove their movements. The method was evaluated through extensive experiments\non two publicly available rehabilitation exercise assessment datasets (UI-PRMD\nand REHAB24-6) and showed promising results in exercise assessment, reasoning,\nand feedback generation. This approach can be integrated into virtual\nrehabilitation platforms to help patients perform exercises correctly, support\nrecovery, and improve health outcomes."}
{"id": "2505.18416", "pdf": "https://arxiv.org/pdf/2505.18416", "abs": "https://arxiv.org/abs/2505.18416", "authors": ["Gelareh Hajian", "Ali Abedi", "Bing Ye", "Jennifer Campos", "Alex Mihailidis"], "title": "Dynamics of Affective States During Takeover Requests in Conditionally Automated Driving Among Older Adults with and without Cognitive Impairment", "categories": ["cs.CV", "cs.HC"], "comment": "16 pages, 3 figures, 2 tables", "summary": "Driving is a key component of independence and quality of life for older\nadults. However, cognitive decline associated with conditions such as mild\ncognitive impairment and dementia can compromise driving safety and often lead\nto premature driving cessation. Conditionally automated vehicles, which require\ndrivers to take over control when automation reaches its operational limits,\noffer a potential assistive solution. However, their effectiveness depends on\nthe driver's ability to respond to takeover requests (TORs) in a timely and\nappropriate manner. Understanding emotional responses during TORs can provide\ninsight into drivers' engagement, stress levels, and readiness to resume\ncontrol, particularly in cognitively vulnerable populations. This study\ninvestigated affective responses, measured via facial expression analysis of\nvalence and arousal, during TORs among cognitively healthy older adults and\nthose with cognitive impairment. Facial affect data were analyzed across\ndifferent road geometries and speeds to evaluate within- and between-group\ndifferences in affective states. Within-group comparisons using the Wilcoxon\nsigned-rank test revealed significant changes in valence and arousal during\nTORs for both groups. Cognitively healthy individuals showed adaptive increases\nin arousal under higher-demand conditions, while those with cognitive\nimpairment exhibited reduced arousal and more positive valence in several\nscenarios. Between-group comparisons using the Mann-Whitney U test indicated\nthat cognitively impaired individuals displayed lower arousal and higher\nvalence than controls across different TOR conditions. These findings suggest\nreduced emotional response and awareness in cognitively impaired drivers,\nhighlighting the need for adaptive vehicle systems that detect affective states\nand support safe handovers for vulnerable users."}
{"id": "2505.18423", "pdf": "https://arxiv.org/pdf/2505.18423", "abs": "https://arxiv.org/abs/2505.18423", "authors": ["Afshin Bozorgpour", "Sina Ghorbani Kolahi", "Reza Azad", "Ilker Hacihaliloglu", "Dorit Merhof"], "title": "CENet: Context Enhancement Network for Medical Image Segmentation", "categories": ["cs.CV"], "comment": "Provisionally accepted at MICCAI-2025", "summary": "Medical image segmentation, particularly in multi-domain scenarios, requires\nprecise preservation of anatomical structures across diverse representations.\nWhile deep learning has advanced this field, existing models often struggle\nwith accurate boundary representation, variability in organ morphology, and\ninformation loss during downsampling, limiting their accuracy and robustness.\nTo address these challenges, we propose the Context Enhancement Network\n(CENet), a novel segmentation framework featuring two key innovations. First,\nthe Dual Selective Enhancement Block (DSEB) integrated into skip connections\nenhances boundary details and improves the detection of smaller organs in a\ncontext-aware manner. Second, the Context Feature Attention Module (CFAM) in\nthe decoder employs a multi-scale design to maintain spatial integrity, reduce\nfeature redundancy, and mitigate overly enhanced representations. Extensive\nevaluations on both radiology and dermoscopic datasets demonstrate that CENet\noutperforms state-of-the-art (SOTA) methods in multi-organ segmentation and\nboundary detail preservation, offering a robust and accurate solution for\ncomplex medical image analysis tasks. The code is publicly available at\nhttps://github.com/xmindflow/cenet."}
{"id": "2505.18434", "pdf": "https://arxiv.org/pdf/2505.18434", "abs": "https://arxiv.org/abs/2505.18434", "authors": ["Yuliang Cai", "Jesse Thomason", "Mohammad Rostami"], "title": "TNG-CLIP:Training-Time Negation Data Generation for Negation Awareness of CLIP", "categories": ["cs.CV", "cs.AI"], "comment": "15 pages, 3 figures", "summary": "Vision-language models (VLMs), such as CLIP, have demonstrated strong\nperformance across a range of downstream tasks. However, CLIP is still limited\nin negation understanding: the ability to recognize the absence or exclusion of\na concept. Existing methods address the problem by using a large language model\n(LLM) to generate large-scale data of image captions containing negation for\nfurther fine-tuning CLIP. However, these methods are both time- and\ncompute-intensive, and their evaluations are typically restricted to image-text\nmatching tasks. To expand the horizon, we (1) introduce a training-time\nnegation data generation pipeline such that negation captions are generated\nduring the training stage, which only increases 2.5% extra training time, and\n(2) we propose the first benchmark, Neg-TtoI, for evaluating text-to-image\ngeneration models on prompts containing negation, assessing model's ability to\nproduce semantically accurate images. We show that our proposed method,\nTNG-CLIP, achieves SOTA performance on diverse negation benchmarks of\nimage-to-text matching, text-to-image retrieval, and image generation."}
{"id": "2505.18445", "pdf": "https://arxiv.org/pdf/2505.18445", "abs": "https://arxiv.org/abs/2505.18445", "authors": ["Yiren Song", "Cheng Liu", "Mike Zheng Shou"], "title": "OmniConsistency: Learning Style-Agnostic Consistency from Paired Stylization Data", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion models have advanced image stylization significantly, yet two core\nchallenges persist: (1) maintaining consistent stylization in complex scenes,\nparticularly identity, composition, and fine details, and (2) preventing style\ndegradation in image-to-image pipelines with style LoRAs. GPT-4o's exceptional\nstylization consistency highlights the performance gap between open-source\nmethods and proprietary models. To bridge this gap, we propose\n\\textbf{OmniConsistency}, a universal consistency plugin leveraging large-scale\nDiffusion Transformers (DiTs). OmniConsistency contributes: (1) an in-context\nconsistency learning framework trained on aligned image pairs for robust\ngeneralization; (2) a two-stage progressive learning strategy decoupling style\nlearning from consistency preservation to mitigate style degradation; and (3) a\nfully plug-and-play design compatible with arbitrary style LoRAs under the Flux\nframework. Extensive experiments show that OmniConsistency significantly\nenhances visual coherence and aesthetic quality, achieving performance\ncomparable to commercial state-of-the-art model GPT-4o."}
{"id": "2505.18446", "pdf": "https://arxiv.org/pdf/2505.18446", "abs": "https://arxiv.org/abs/2505.18446", "authors": ["Hojun Son", "Asma Almutairi", "Arpan Kusari"], "title": "Mitigating Context Bias in Domain Adaptation for Object Detection using Mask Pooling", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Context bias refers to the association between the foreground objects and\nbackground during the object detection training process. Various methods have\nbeen proposed to minimize the context bias when applying the trained model to\nan unseen domain, known as domain adaptation for object detection (DAOD). But a\nprincipled approach to understand why the context bias occurs and how to remove\nit has been missing.\n  In this work, we provide a causal view of the context bias, pointing towards\nthe pooling operation in the convolution network architecture as the possible\nsource of this bias. We present an alternative, Mask Pooling, which uses an\nadditional input of foreground masks, to separate the pooling process in the\nrespective foreground and background regions and show that this process leads\nthe trained model to detect objects in a more robust manner under different\ndomains. We also provide a benchmark designed to create an ultimate test for\nDAOD, using foregrounds in the presence of absolute random backgrounds, to\nanalyze the robustness of the intended trained models. Through these\nexperiments, we hope to provide a principled approach for minimizing context\nbias under domain shift."}
{"id": "2505.18465", "pdf": "https://arxiv.org/pdf/2505.18465", "abs": "https://arxiv.org/abs/2505.18465", "authors": ["Ruize Yang", "Ann Kennedy", "R. James Cotton"], "title": "BiomechGPT: Towards a Biomechanically Fluent Multimodal Foundation Model for Clinically Relevant Motion Tasks", "categories": ["cs.CV"], "comment": null, "summary": "Advances in markerless motion capture are expanding access to biomechanical\nmovement analysis, making it feasible to obtain high-quality movement data from\noutpatient clinics, inpatient hospitals, therapy, and even home. Expanding\naccess to movement data in these diverse contexts makes the challenge of\nperforming downstream analytics all the more acute. Creating separate bespoke\nanalysis code for all the tasks end users might want is both intractable and\ndoes not take advantage of the common features of human movement underlying\nthem all. Recent studies have shown that fine-tuning language models to accept\ntokenized movement as an additional modality enables successful descriptive\ncaptioning of movement. Here, we explore whether such a multimodal\nmotion-language model can answer detailed, clinically meaningful questions\nabout movement. We collected over 30 hours of biomechanics from nearly 500\nparticipants, many with movement impairments from a variety of etiologies,\nperforming a range of movements used in clinical outcomes assessments. After\ntokenizing these movement trajectories, we created a multimodal dataset of\nmotion-related questions and answers spanning a range of tasks. We developed\nBiomechGPT, a multimodal biomechanics-language model, on this dataset. Our\nresults show that BiomechGPT demonstrates high performance across a range of\ntasks such as activity recognition, identifying movement impairments,\ndiagnosis, scoring clinical outcomes, and measuring walking. BiomechGPT\nprovides an important step towards a foundation model for rehabilitation\nmovement data."}
{"id": "2505.18469", "pdf": "https://arxiv.org/pdf/2505.18469", "abs": "https://arxiv.org/abs/2505.18469", "authors": ["Jingkai Wang", "Wu Miao", "Jue Gong", "Zheng Chen", "Xing Liu", "Hong Gu", "Yutong Liu", "Yulun Zhang"], "title": "HonestFace: Towards Honest Face Restoration with One-Step Diffusion Model", "categories": ["cs.CV"], "comment": null, "summary": "Face restoration has achieved remarkable advancements through the years of\ndevelopment. However, ensuring that restored facial images exhibit high\nfidelity, preserve authentic features, and avoid introducing artifacts or\nbiases remains a significant challenge. This highlights the need for models\nthat are more \"honest\" in their reconstruction from low-quality inputs,\naccurately reflecting original characteristics. In this work, we propose\nHonestFace, a novel approach designed to restore faces with a strong emphasis\non such honesty, particularly concerning identity consistency and texture\nrealism. To achieve this, HonestFace incorporates several key components.\nFirst, we propose an identity embedder to effectively capture and preserve\ncrucial identity features from both the low-quality input and multiple\nreference faces. Second, a masked face alignment method is presented to enhance\nfine-grained details and textural authenticity, thereby preventing the\ngeneration of patterned or overly synthetic textures and improving overall\nclarity. Furthermore, we present a new landmark-based evaluation metric. Based\non affine transformation principles, this metric improves the accuracy compared\nto conventional L2 distance calculations for facial feature alignment.\nLeveraging these contributions within a one-step diffusion model framework,\nHonestFace delivers exceptional restoration results in terms of facial fidelity\nand realism. Extensive experiments demonstrate that our approach surpasses\nexisting state-of-the-art methods, achieving superior performance in both\nvisual quality and quantitative assessments. The code and pre-trained models\nwill be made publicly available at https://github.com/jkwang28/HonestFace ."}
{"id": "2505.18477", "pdf": "https://arxiv.org/pdf/2505.18477", "abs": "https://arxiv.org/abs/2505.18477", "authors": ["Fukun Liu", "Adam T. Greer", "Gengchen Mai", "Jin Sun"], "title": "ZooplanktonBench: A Geo-Aware Zooplankton Recognition and Classification Dataset from Marine Observations", "categories": ["cs.CV"], "comment": null, "summary": "Plankton are small drifting organisms found throughout the world's oceans.\nOne component of this plankton community is the zooplankton, which includes\ngelatinous animals and crustaceans (e.g. shrimp), as well as the early life\nstages (i.e., eggs and larvae) of many commercially important fishes. Being\nable to monitor zooplankton abundances accurately and understand how\npopulations change in relation to ocean conditions is invaluable to marine\nscience research, with important implications for future marine seafood\nproductivity. While new imaging technologies generate massive amounts of video\ndata of zooplankton, analyzing them using general-purpose computer vision tools\ndeveloped for general objects turns out to be highly challenging due to the\nhigh similarity in appearance between the zooplankton and its background (e.g.,\nmarine snow). In this work, we present the ZooplanktonBench, a benchmark\ndataset containing images and videos of zooplankton associated with rich\ngeospatial metadata (e.g., geographic coordinates, depth, etc.) in various\nwater ecosystems. ZooplanktonBench defines a collection of tasks to detect,\nclassify, and track zooplankton in challenging settings, including highly\ncluttered environments, living vs non-living classification, objects with\nsimilar shapes, and relatively small objects. Our dataset presents unique\nchallenges and opportunities for state-of-the-art computer vision systems to\nevolve and improve visual understanding in a dynamic environment with huge\nvariations and be geo-aware."}
{"id": "2505.18479", "pdf": "https://arxiv.org/pdf/2505.18479", "abs": "https://arxiv.org/abs/2505.18479", "authors": ["Li-Syun Hsiung", "Jun-Kai Tu", "Kuan-Wu Chu", "Yu-Hsuan Chiu", "Yan-Tsung Peng", "Sheng-Luen Chung", "Gee-Sern Jison Hsu"], "title": "Syn3DTxt: Embedding 3D Cues for Scene Text Generation", "categories": ["cs.CV"], "comment": "CVPR workshop 2025: SyntaGen", "summary": "This study aims to investigate the challenge of insufficient\nthree-dimensional context in synthetic datasets for scene text rendering.\nAlthough recent advances in diffusion models and related techniques have\nimproved certain aspects of scene text generation, most existing approaches\ncontinue to rely on 2D data, sourcing authentic training examples from movie\nposters and book covers, which limits their ability to capture the complex\ninteractions among spatial layout and visual effects in real-world scenes. In\nparticular, traditional 2D datasets do not provide the necessary geometric cues\nfor accurately embedding text into diverse backgrounds. To address this\nlimitation, we propose a novel standard for constructing synthetic datasets\nthat incorporates surface normals to enrich three-dimensional scene\ncharacteristic. By adding surface normals to conventional 2D data, our approach\naims to enhance the representation of spatial relationships and provide a more\nrobust foundation for future scene text rendering methods. Extensive\nexperiments demonstrate that datasets built under this new standard offer\nimproved geometric context, facilitating further advancements in text rendering\nunder complex 3D-spatial conditions."}
{"id": "2505.18503", "pdf": "https://arxiv.org/pdf/2505.18503", "abs": "https://arxiv.org/abs/2505.18503", "authors": ["Aofei Chang", "Le Huang", "Alex James Boyd", "Parminder Bhatia", "Taha Kass-Hout", "Cao Xiao", "Fenglong Ma"], "title": "Focus on What Matters: Enhancing Medical Vision-Language Models with Automatic Attention Alignment Tuning", "categories": ["cs.CV"], "comment": "Accepted to ACL2025 (main)", "summary": "Medical Large Vision-Language Models (Med-LVLMs) often exhibit suboptimal\nattention distribution on visual inputs, leading to hallucinated or inaccurate\noutputs. Existing mitigation methods primarily rely on inference-time\ninterventions, which are limited in attention adaptation or require additional\nsupervision. To address this, we propose A$^3$Tune, a novel fine-tuning\nframework for Automatic Attention Alignment Tuning. A$^3$Tune leverages\nzero-shot weak labels from SAM, refines them into prompt-aware labels using\nBioMedCLIP, and then selectively modifies visually-critical attention heads to\nimprove alignment while minimizing interference. Additionally, we introduce a\nA$^3$MoE module, enabling adaptive parameter selection for attention tuning\nacross diverse prompts and images. Extensive experiments on medical VQA and\nreport generation benchmarks show that A$^3$Tune outperforms state-of-the-art\nbaselines, achieving enhanced attention distributions and performance in\nMed-LVLMs."}
{"id": "2505.18521", "pdf": "https://arxiv.org/pdf/2505.18521", "abs": "https://arxiv.org/abs/2505.18521", "authors": ["Yiheng Li", "Feng Liang", "Dan Kondratyuk", "Masayoshi Tomizuka", "Kurt Keutzer", "Chenfeng Xu"], "title": "Improved Immiscible Diffusion: Accelerate Diffusion Training by Reducing Its Miscibility", "categories": ["cs.CV"], "comment": null, "summary": "The substantial training cost of diffusion models hinders their deployment.\nImmiscible Diffusion recently showed that reducing diffusion trajectory mixing\nin the noise space via linear assignment accelerates training by simplifying\ndenoising. To extend immiscible diffusion beyond the inefficient linear\nassignment under high batch sizes and high dimensions, we refine this concept\nto a broader miscibility reduction at any layer and by any implementation.\nSpecifically, we empirically demonstrate the bijective nature of the denoising\nprocess with respect to immiscible diffusion, ensuring its preservation of\ngenerative diversity. Moreover, we provide thorough analysis and show\nstep-by-step how immiscibility eases denoising and improves efficiency.\nExtending beyond linear assignment, we propose a family of implementations\nincluding K-nearest neighbor (KNN) noise selection and image scaling to reduce\nmiscibility, achieving up to >4x faster training across diverse models and\ntasks including unconditional/conditional generation, image editing, and\nrobotics planning. Furthermore, our analysis of immiscibility offers a novel\nperspective on how optimal transport (OT) enhances diffusion training. By\nidentifying trajectory miscibility as a fundamental bottleneck, we believe this\nwork establishes a potentially new direction for future research into\nhigh-efficiency diffusion training. The code is available at\nhttps://github.com/yhli123/Immiscible-Diffusion."}
{"id": "2505.18525", "pdf": "https://arxiv.org/pdf/2505.18525", "abs": "https://arxiv.org/abs/2505.18525", "authors": ["Haoyu Yang", "Yuxiang Cai", "Jintao Chen", "Xuhong Zhang", "Wenhui Lei", "Xiaoming Shi", "Jianwei Yin", "Yankai Jiang"], "title": "TK-Mamba: Marrying KAN with Mamba for Text-Driven 3D Medical Image Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "3D medical image segmentation is vital for clinical diagnosis and treatment\nbut is challenged by high-dimensional data and complex spatial dependencies.\nTraditional single-modality networks, such as CNNs and Transformers, are often\nlimited by computational inefficiency and constrained contextual modeling in 3D\nsettings. We introduce a novel multimodal framework that leverages Mamba and\nKolmogorov-Arnold Networks (KAN) as an efficient backbone for long-sequence\nmodeling. Our approach features three key innovations: First, an EGSC (Enhanced\nGated Spatial Convolution) module captures spatial information when unfolding\n3D images into 1D sequences. Second, we extend Group-Rational KAN (GR-KAN), a\nKolmogorov-Arnold Networks variant with rational basis functions, into\n3D-Group-Rational KAN (3D-GR-KAN) for 3D medical imaging - its first\napplication in this domain - enabling superior feature representation tailored\nto volumetric data. Third, a dual-branch text-driven strategy leverages CLIP's\ntext embeddings: one branch swaps one-hot labels for semantic vectors to\npreserve inter-organ semantic relationships, while the other aligns images with\ndetailed organ descriptions to enhance semantic alignment. Experiments on the\nMedical Segmentation Decathlon (MSD) and KiTS23 datasets show our method\nachieving state-of-the-art performance, surpassing existing approaches in\naccuracy and efficiency. This work highlights the power of combining advanced\nsequence modeling, extended network architectures, and vision-language synergy\nto push forward 3D medical image segmentation, delivering a scalable solution\nfor clinical use. The source code is openly available at\nhttps://github.com/yhy-whu/TK-Mamba."}
{"id": "2505.18561", "pdf": "https://arxiv.org/pdf/2505.18561", "abs": "https://arxiv.org/abs/2505.18561", "authors": ["Shiu-hong Kao", "Yu-Wing Tai", "Chi-Keung Tang"], "title": "ThinkVideo: High-Quality Reasoning Video Segmentation with Chain of Thoughts", "categories": ["cs.CV"], "comment": "Project page: https://cse.hkust.edu.hk/~skao/thinkvideo.html", "summary": "Reasoning Video Object Segmentation is a challenging task, which generates a\nmask sequence from an input video and an implicit, complex text query. Existing\nworks probe into the problem by finetuning Multimodal Large Language Models\n(MLLM) for segmentation-based output, while still falling short in difficult\ncases on videos given temporally-sensitive queries, primarily due to the\nfailure to integrate temporal and spatial information. In this paper, we\npropose ThinkVideo, a novel framework which leverages the zero-shot\nChain-of-Thought (CoT) capability of MLLM to address these challenges.\nSpecifically, ThinkVideo utilizes the CoT prompts to extract object\nselectivities associated with particular keyframes, then bridging the reasoning\nimage segmentation model and SAM2 video processor to output mask sequences. The\nThinkVideo framework is training-free and compatible with closed-source MLLMs,\nwhich can be applied to Reasoning Video Instance Segmentation. We further\nextend the framework for online video streams, where the CoT is used to update\nthe object of interest when a better target starts to emerge and becomes\nvisible. We conduct extensive experiments on video object segmentation with\nexplicit and implicit queries. The results show that ThinkVideo significantly\noutperforms previous works in both cases, qualitatively and quantitatively."}
{"id": "2505.18582", "pdf": "https://arxiv.org/pdf/2505.18582", "abs": "https://arxiv.org/abs/2505.18582", "authors": ["Dongyang Jin", "Chao Fan", "Jingzhe Ma", "Jingkai Zhou", "Weihua Chen", "Shiqi Yu"], "title": "On Denoising Walking Videos for Gait Recognition", "categories": ["cs.CV", "cs.AI"], "comment": "8pages, 4 figures", "summary": "To capture individual gait patterns, excluding identity-irrelevant cues in\nwalking videos, such as clothing texture and color, remains a persistent\nchallenge for vision-based gait recognition. Traditional silhouette- and\npose-based methods, though theoretically effective at removing such\ndistractions, often fall short of high accuracy due to their sparse and less\ninformative inputs. Emerging end-to-end methods address this by directly\ndenoising RGB videos using human priors. Building on this trend, we propose\nDenoisingGait, a novel gait denoising method. Inspired by the philosophy that\n\"what I cannot create, I do not understand\", we turn to generative diffusion\nmodels, uncovering how they partially filter out irrelevant factors for gait\nunderstanding. Additionally, we introduce a geometry-driven Feature Matching\nmodule, which, combined with background removal via human silhouettes,\ncondenses the multi-channel diffusion features at each foreground pixel into a\ntwo-channel direction vector. Specifically, the proposed within- and\ncross-frame matching respectively capture the local vectorized structures of\ngait appearance and motion, producing a novel flow-like gait representation\ntermed Gait Feature Field, which further reduces residual noise in diffusion\nfeatures. Experiments on the CCPG, CASIA-B*, and SUSTech1K datasets demonstrate\nthat DenoisingGait achieves a new SoTA performance in most cases for both\nwithin- and cross-domain evaluations. Code is available at\nhttps://github.com/ShiqiYu/OpenGait."}
{"id": "2505.18584", "pdf": "https://arxiv.org/pdf/2505.18584", "abs": "https://arxiv.org/abs/2505.18584", "authors": ["Chaofan Gan", "Yuanpeng Tu", "Xi Chen", "Tieyuan Chen", "Yuxi Li", "Mehrtash Harandi", "Weiyao Lin"], "title": "Unleashing Diffusion Transformers for Visual Correspondence by Modulating Massive Activations", "categories": ["cs.CV"], "comment": "Under Review", "summary": "Pre-trained stable diffusion models (SD) have shown great advances in visual\ncorrespondence. In this paper, we investigate the capabilities of Diffusion\nTransformers (DiTs) for accurate dense correspondence. Distinct from SD, DiTs\nexhibit a critical phenomenon in which very few feature activations exhibit\nsignificantly larger values than others, known as \\textit{massive activations},\nleading to uninformative representations and significant performance\ndegradation for DiTs. The massive activations consistently concentrate at very\nfew fixed dimensions across all image patch tokens, holding little local\ninformation. We trace these dimension-concentrated massive activations and find\nthat such concentration can be effectively localized by the zero-initialized\nAdaptive Layer Norm (AdaLN-zero). Building on these findings, we propose\nDiffusion Transformer Feature (DiTF), a training-free framework designed to\nextract semantic-discriminative features from DiTs. Specifically, DiTF employs\nAdaLN to adaptively localize and normalize massive activations with\nchannel-wise modulation. In addition, we develop a channel discard strategy to\nfurther eliminate the negative impacts from massive activations. Experimental\nresults demonstrate that our DiTF outperforms both DINO and SD-based models and\nestablishes a new state-of-the-art performance for DiTs in different visual\ncorrespondence tasks (\\eg, with +9.4\\% on Spair-71k and +4.4\\% on AP-10K-C.S.)."}
{"id": "2505.18586", "pdf": "https://arxiv.org/pdf/2505.18586", "abs": "https://arxiv.org/abs/2505.18586", "authors": ["Chengxi Min", "Wei Wang", "Yahui Liu", "Weixin Ye", "Enver Sangineto", "Qi Wang", "Yao Zhao"], "title": "Guiding the Experts: Semantic Priors for Efficient and Focused MoE Routing", "categories": ["cs.CV"], "comment": null, "summary": "Mixture-of-Experts (MoE) models have emerged as a promising direction for\nscaling vision architectures efficiently. Among them, Soft MoE improves\ntraining stability by assigning each token to all experts via continuous\ndispatch weights. However, current designs overlook the semantic structure\nwhich is implicitly encoded in these weights, resulting in suboptimal expert\nrouting. In this paper, we discover that dispatch weights in Soft MoE\ninherently exhibit segmentation-like patterns but are not explicitly aligned\nwith semantic regions. Motivated by this observation, we propose a\nforeground-guided enhancement strategy. Specifically, we introduce a spatially\naware auxiliary loss that encourages expert activation to align with semantic\nforeground regions. To further reinforce this supervision, we integrate a\nlightweight LayerScale mechanism that improves information flow and stabilizes\noptimization in skip connections. Our method necessitates only minor\narchitectural adjustments and can be seamlessly integrated into prevailing Soft\nMoE frameworks. Comprehensive experiments on ImageNet-1K and multiple\nsmaller-scale classification benchmarks not only showcase consistent\nperformance enhancements but also reveal more interpretable expert routing\nmechanisms."}
{"id": "2505.18587", "pdf": "https://arxiv.org/pdf/2505.18587", "abs": "https://arxiv.org/abs/2505.18587", "authors": ["Pavan C Shekar", "Pawan Soni", "Vivek Kanhangad"], "title": "HyperFake: Hyperspectral Reconstruction and Attention-Guided Analysis for Advanced Deepfake Detection", "categories": ["cs.CV", "cs.AI"], "comment": "6 pages, 3 figures, 1 table. Preliminary results on FaceForensics++\n  dataset. First approach to use hyperspectral reconstruction for deepfake\n  detection", "summary": "Deepfakes pose a significant threat to digital media security, with current\ndetection methods struggling to generalize across different manipulation\ntechniques and datasets. While recent approaches combine CNN-based\narchitectures with Vision Transformers or leverage multi-modal learning, they\nremain limited by the inherent constraints of RGB data. We introduce HyperFake,\na novel deepfake detection pipeline that reconstructs 31-channel hyperspectral\ndata from standard RGB videos, revealing hidden manipulation traces invisible\nto conventional methods. Using an improved MST++ architecture, HyperFake\nenhances hyperspectral reconstruction, while a spectral attention mechanism\nselects the most critical spectral features for deepfake detection. The refined\nspectral data is then processed by an EfficientNet-based classifier optimized\nfor spectral analysis, enabling more accurate and generalizable detection\nacross different deepfake styles and datasets, all without the need for\nexpensive hyperspectral cameras. To the best of our knowledge, this is the\nfirst approach to leverage hyperspectral imaging reconstruction for deepfake\ndetection, opening new possibilities for detecting increasingly sophisticated\nmanipulations."}
{"id": "2505.18594", "pdf": "https://arxiv.org/pdf/2505.18594", "abs": "https://arxiv.org/abs/2505.18594", "authors": ["GuangHao Meng", "Sunan He", "Jinpeng Wang", "Tao Dai", "Letian Zhang", "Jieming Zhu", "Qing Li", "Gang Wang", "Rui Zhang", "Yong Jiang"], "title": "EvdCLIP: Improving Vision-Language Retrieval with Entity Visual Descriptions from Large Language Models", "categories": ["cs.CV", "cs.IR"], "comment": "9 pages, 6 figures", "summary": "Vision-language retrieval (VLR) has attracted significant attention in both\nacademia and industry, which involves using text (or images) as queries to\nretrieve corresponding images (or text). However, existing methods often\nneglect the rich visual semantics knowledge of entities, thus leading to\nincorrect retrieval results. To address this problem, we propose the Entity\nVisual Description enhanced CLIP (EvdCLIP), designed to leverage the visual\nknowledge of entities to enrich queries. Specifically, since humans recognize\nentities through visual cues, we employ a large language model (LLM) to\ngenerate Entity Visual Descriptions (EVDs) as alignment cues to complement\ntextual data. These EVDs are then integrated into raw queries to create\nvisually-rich, EVD-enhanced queries. Furthermore, recognizing that EVD-enhanced\nqueries may introduce noise or low-quality expansions, we develop a novel,\ntrainable EVD-aware Rewriter (EaRW) for vision-language retrieval tasks. EaRW\nutilizes EVD knowledge and the generative capabilities of the language model to\neffectively rewrite queries. With our specialized training strategy, EaRW can\ngenerate high-quality and low-noise EVD-enhanced queries. Extensive\nquantitative and qualitative experiments on image-text retrieval benchmarks\nvalidate the superiority of EvdCLIP on vision-language retrieval tasks."}
{"id": "2505.18600", "pdf": "https://arxiv.org/pdf/2505.18600", "abs": "https://arxiv.org/abs/2505.18600", "authors": ["Bryan Sangwoo Kim", "Jeongsol Kim", "Jong Chul Ye"], "title": "Chain-of-Zoom: Extreme Super-Resolution via Scale Autoregression and Preference Alignment", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Modern single-image super-resolution (SISR) models deliver photo-realistic\nresults at the scale factors on which they are trained, but collapse when asked\nto magnify far beyond that regime. We address this scalability bottleneck with\nChain-of-Zoom (CoZ), a model-agnostic framework that factorizes SISR into an\nautoregressive chain of intermediate scale-states with multi-scale-aware\nprompts. CoZ repeatedly re-uses a backbone SR model, decomposing the\nconditional probability into tractable sub-problems to achieve extreme\nresolutions without additional training. Because visual cues diminish at high\nmagnifications, we augment each zoom step with multi-scale-aware text prompts\ngenerated by a vision-language model (VLM). The prompt extractor itself is\nfine-tuned using Generalized Reward Policy Optimization (GRPO) with a critic\nVLM, aligning text guidance towards human preference. Experiments show that a\nstandard 4x diffusion SR model wrapped in CoZ attains beyond 256x enlargement\nwith high perceptual quality and fidelity."}
{"id": "2505.18605", "pdf": "https://arxiv.org/pdf/2505.18605", "abs": "https://arxiv.org/abs/2505.18605", "authors": ["Xiaohuan Pei", "Tao Huang", "YanXiang Ma", "Chang Xu"], "title": "Rethinking Causal Mask Attention for Vision-Language Inference", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Causal attention has become a foundational mechanism in autoregressive\nvision-language models (VLMs), unifying textual and visual inputs under a\nsingle generative framework. However, existing causal mask-based strategies are\ninherited from large language models (LLMs) where they are tailored for\ntext-only decoding, and their adaptation to vision tokens is insufficiently\naddressed in the prefill stage. Strictly masking future positions for vision\nqueries introduces overly rigid constraints, which hinder the model's ability\nto leverage future context that often contains essential semantic cues for\naccurate inference. In this work, we empirically investigate how different\ncausal masking strategies affect vision-language inference and then propose a\nfamily of future-aware attentions tailored for this setting. We first\nempirically analyze the effect of previewing future tokens for vision queries\nand demonstrate that rigid masking undermines the model's capacity to capture\nuseful contextual semantic representations. Based on these findings, we propose\na lightweight attention family that aggregates future visual context into past\nrepresentations via pooling, effectively preserving the autoregressive\nstructure while enhancing cross-token dependencies. We evaluate a range of\ncausal masks across diverse vision-language inference settings and show that\nselectively compressing future semantic context into past representations\nbenefits the inference."}
{"id": "2505.18608", "pdf": "https://arxiv.org/pdf/2505.18608", "abs": "https://arxiv.org/abs/2505.18608", "authors": ["Yuetong Fang", "Deming Zhou", "Ziqing Wang", "Hongwei Ren", "ZeCui Zeng", "Lusong Li", "Shibo Zhou", "Renjing Xu"], "title": "Spiking Transformers Need High Frequency Information", "categories": ["cs.CV"], "comment": null, "summary": "Spiking Transformers offer an energy-efficient alternative to conventional\ndeep learning by transmitting information solely through binary (0/1) spikes.\nHowever, there remains a substantial performance gap compared to artificial\nneural networks. A common belief is that their binary and sparse activation\ntransmission leads to information loss, thus degrading feature representation\nand accuracy. In this work, however, we reveal for the first time that spiking\nneurons preferentially propagate low-frequency information. We hypothesize that\nthe rapid dissipation of high-frequency components is the primary cause of\nperformance degradation. For example, on Cifar-100, adopting Avg-Pooling\n(low-pass) for token mixing lowers performance to 76.73%; interestingly,\nreplacing it with Max-Pooling (high-pass) pushes the top-1 accuracy to 79.12%,\nsurpassing the well-tuned Spikformer baseline by 0.97%. Accordingly, we\nintroduce Max-Former that restores high-frequency signals through two\nfrequency-enhancing operators: extra Max-Pooling in patch embedding and\nDepth-Wise Convolution in place of self-attention. Notably, our Max-Former\n(63.99 M) hits the top-1 accuracy of 82.39% on ImageNet, showing a +7.58%\nimprovement over Spikformer with comparable model size (74.81%, 66.34 M). We\nhope this simple yet effective solution inspires future research to explore the\ndistinctive nature of spiking neural networks, beyond the established practice\nin standard deep learning."}
{"id": "2505.18612", "pdf": "https://arxiv.org/pdf/2505.18612", "abs": "https://arxiv.org/abs/2505.18612", "authors": ["Weizhi Zhong", "Huan Yang", "Zheng Liu", "Huiguo He", "Zijian He", "Xuesong Niu", "Di Zhang", "Guanbin Li"], "title": "Mod-Adapter: Tuning-Free and Versatile Multi-concept Personalization via Modulation Adapter", "categories": ["cs.CV"], "comment": "Project page: https://weizhi-zhong.github.io/Mod-Adapter", "summary": "Personalized text-to-image generation aims to synthesize images of\nuser-provided concepts in diverse contexts. Despite recent progress in\nmulti-concept personalization, most are limited to object concepts and struggle\nto customize abstract concepts (e.g., pose, lighting). Some methods have begun\nexploring multi-concept personalization supporting abstract concepts, but they\nrequire test-time fine-tuning for each new concept, which is time-consuming and\nprone to overfitting on limited training images. In this work, we propose a\nnovel tuning-free method for multi-concept personalization that can effectively\ncustomize both object and abstract concepts without test-time fine-tuning. Our\nmethod builds upon the modulation mechanism in pretrained Diffusion\nTransformers (DiTs) model, leveraging the localized and semantically meaningful\nproperties of the modulation space. Specifically, we propose a novel module,\nMod-Adapter, to predict concept-specific modulation direction for the\nmodulation process of concept-related text tokens. It incorporates\nvision-language cross-attention for extracting concept visual features, and\nMixture-of-Experts (MoE) layers that adaptively map the concept features into\nthe modulation space. Furthermore, to mitigate the training difficulty caused\nby the large gap between the concept image space and the modulation space, we\nintroduce a VLM-guided pretraining strategy that leverages the strong image\nunderstanding capabilities of vision-language models to provide semantic\nsupervision signals. For a comprehensive comparison, we extend a standard\nbenchmark by incorporating abstract concepts. Our method achieves\nstate-of-the-art performance in multi-concept personalization, supported by\nquantitative, qualitative, and human evaluations."}
{"id": "2505.18634", "pdf": "https://arxiv.org/pdf/2505.18634", "abs": "https://arxiv.org/abs/2505.18634", "authors": ["NH Wanigasingha", "ES Sithpahan", "MKA Ariyaratne", "PRS De Silva"], "title": "SerendibCoins: Exploring The Sri Lankan Coins Dataset", "categories": ["cs.CV"], "comment": "20 pages", "summary": "The recognition and classification of coins are essential in numerous\nfinancial and automated systems. This study introduces a comprehensive Sri\nLankan coin image dataset and evaluates its impact on machine learning model\naccuracy for coin classification. We experiment with traditional machine\nlearning classifiers K-Nearest Neighbors (KNN), Support Vector Machines (SVM),\nand Random Forest as well as a custom Convolutional Neural Network (CNN) to\nbenchmark performance at different levels of classification. Our results show\nthat SVM outperforms KNN and Random Forest in traditional classification\napproaches, while the CNN model achieves near-perfect classification accuracy\nwith minimal misclassifications. The dataset demonstrates significant potential\nin enhancing automated coin recognition systems, offering a robust foundation\nfor future research in regional currency classification and deep learning\napplications."}
{"id": "2505.18649", "pdf": "https://arxiv.org/pdf/2505.18649", "abs": "https://arxiv.org/abs/2505.18649", "authors": ["Shiyun Xie", "Zhiru Wang", "Yinghao Zhu", "Xu Wang", "Chengwei Pan", "Xiwang Dong"], "title": "SuperGS: Consistent and Detailed 3D Super-Resolution Scene Reconstruction via Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "Recently, 3D Gaussian Splatting (3DGS) has excelled in novel view synthesis\n(NVS) with its real-time rendering capabilities and superior quality. However,\nit encounters challenges for high-resolution novel view synthesis (HRNVS) due\nto the coarse nature of primitives derived from low-resolution input views. To\naddress this issue, we propose SuperGS, an expansion of Scaffold-GS designed\nwith a two-stage coarse-to-fine training framework. In the low-resolution\nstage, we introduce a latent feature field to represent the low-resolution\nscene, which serves as both the initialization and foundational information for\nsuper-resolution optimization. In the high-resolution stage, we propose a\nmulti-view consistent densification strategy that backprojects high-resolution\ndepth maps based on error maps and employs a multi-view voting mechanism,\nmitigating ambiguities caused by multi-view inconsistencies in the pseudo\nlabels provided by 2D prior models while avoiding Gaussian redundancy.\nFurthermore, we model uncertainty through variational feature learning and use\nit to guide further scene representation refinement and adjust the supervisory\neffect of pseudo-labels, ensuring consistent and detailed scene reconstruction.\nExtensive experiments demonstrate that SuperGS outperforms state-of-the-art\nHRNVS methods on both forward-facing and 360-degree datasets."}
{"id": "2505.18650", "pdf": "https://arxiv.org/pdf/2505.18650", "abs": "https://arxiv.org/abs/2505.18650", "authors": ["Xiaodong Wang", "Peixi Peng"], "title": "ProphetDWM: A Driving World Model for Rolling Out Future Actions and Videos", "categories": ["cs.CV"], "comment": "9 pages, 7 figures", "summary": "Real-world driving requires people to observe the current environment,\nanticipate the future, and make appropriate driving decisions. This requirement\nis aligned well with the capabilities of world models, which understand the\nenvironment and predict the future. However, recent world models in autonomous\ndriving are built explicitly, where they could predict the future by\ncontrollable driving video generation. We argue that driving world models\nshould have two additional abilities: action control and action prediction.\nFollowing this line, previous methods are limited because they predict the\nvideo requires given actions of the same length as the video and ignore the\ndynamical action laws. To address these issues, we propose ProphetDWM, a novel\nend-to-end driving world model that jointly predicts future videos and actions.\nOur world model has an action module to learn latent action from the present to\nthe future period by giving the action sequence and observations. And a\ndiffusion-model-based transition module to learn the state distribution. The\nmodel is jointly trained by learning latent actions given finite states and\npredicting action and video. The joint learning connects the action dynamics\nand states and enables long-term future prediction. We evaluate our method in\nvideo generation and action prediction tasks on the Nuscenes dataset. Compared\nto the state-of-the-art methods, our method achieves the best video consistency\nand best action prediction accuracy, while also enabling high-quality long-term\nvideo and action generation."}
{"id": "2505.18652", "pdf": "https://arxiv.org/pdf/2505.18652", "abs": "https://arxiv.org/abs/2505.18652", "authors": ["Yicheng Lin", "Yunlong Jiang", "Xujia Jiao", "Bin Han"], "title": "Why Not Replace? Sustaining Long-Term Visual Localization via Handcrafted-Learned Feature Collaboration on CPU", "categories": ["cs.CV"], "comment": "8 pages, 6 gifures", "summary": "Robust long-term visual localization in complex industrial environments is\ncritical for mobile robotic systems. Existing approaches face limitations:\nhandcrafted features are illumination-sensitive, learned features are\ncomputationally intensive, and semantic- or marker-based methods are\nenvironmentally constrained. Handcrafted and learned features share similar\nrepresentations but differ functionally. Handcrafted features are optimized for\ncontinuous tracking, while learned features excel in wide-baseline matching.\nTheir complementarity calls for integration rather than replacement. Building\non this, we propose a hierarchical localization framework. It leverages\nreal-time handcrafted feature extraction for relative pose estimation. In\nparallel, it employs selective learned keypoint detection on optimized\nkeyframes for absolute positioning. This design enables CPU-efficient,\nlong-term visual localization. Experiments systematically progress through\nthree validation phases: Initially establishing feature complementarity through\ncomparative analysis, followed by computational latency profiling across\nalgorithm stages on CPU platforms. Final evaluation under photometric\nvariations (including seasonal transitions and diurnal cycles) demonstrates 47%\naverage error reduction with significantly improved localization consistency.\nThe code implementation is publicly available at\nhttps://github.com/linyicheng1/ORB_SLAM3_localization."}
{"id": "2505.18660", "pdf": "https://arxiv.org/pdf/2505.18660", "abs": "https://arxiv.org/abs/2505.18660", "authors": ["Zhenglin Huang", "Tianxiao Li", "Xiangtai Li", "Haiquan Wen", "Yiwei He", "Jiangning Zhang", "Hao Fei", "Xi Yang", "Xiaowei Huang", "Bei Peng", "Guangliang Cheng"], "title": "So-Fake: Benchmarking and Explaining Social Media Image Forgery Detection", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in AI-powered generative models have enabled the creation of\nincreasingly realistic synthetic images, posing significant risks to\ninformation integrity and public trust on social media platforms. While robust\ndetection frameworks and diverse, large-scale datasets are essential to\nmitigate these risks, existing academic efforts remain limited in scope:\ncurrent datasets lack the diversity, scale, and realism required for social\nmedia contexts, while detection methods struggle with generalization to unseen\ngenerative technologies. To bridge this gap, we introduce So-Fake-Set, a\ncomprehensive social media-oriented dataset with over 2 million high-quality\nimages, diverse generative sources, and photorealistic imagery synthesized\nusing 35 state-of-the-art generative models. To rigorously evaluate\ncross-domain robustness, we establish a novel and large-scale (100K)\nout-of-domain benchmark (So-Fake-OOD) featuring synthetic imagery from\ncommercial models explicitly excluded from the training distribution, creating\na realistic testbed for evaluating real-world performance. Leveraging these\nresources, we present So-Fake-R1, an advanced vision-language framework that\nemploys reinforcement learning for highly accurate forgery detection, precise\nlocalization, and explainable inference through interpretable visual\nrationales. Extensive experiments show that So-Fake-R1 outperforms the\nsecond-best method, with a 1.3% gain in detection accuracy and a 4.5% increase\nin localization IoU. By integrating a scalable dataset, a challenging OOD\nbenchmark, and an advanced detection framework, this work establishes a new\nfoundation for social media-centric forgery detection research. The code,\nmodels, and datasets will be released publicly."}
{"id": "2505.18663", "pdf": "https://arxiv.org/pdf/2505.18663", "abs": "https://arxiv.org/abs/2505.18663", "authors": ["Zhiteng Li", "Hanxuan Li", "Junyi Wu", "Kai Liu", "Linghe Kong", "Guihai Chen", "Yulun Zhang", "Xiaokang Yang"], "title": "DVD-Quant: Data-free Video Diffusion Transformers Quantization", "categories": ["cs.CV"], "comment": "Code and models will be available at\n  \\url{https://github.com/lhxcs/DVD-Quant}", "summary": "Diffusion Transformers (DiTs) have emerged as the state-of-the-art\narchitecture for video generation, yet their computational and memory demands\nhinder practical deployment. While post-training quantization (PTQ) presents a\npromising approach to accelerate Video DiT models, existing methods suffer from\ntwo critical limitations: (1) dependence on lengthy, computation-heavy\ncalibration procedures, and (2) considerable performance deterioration after\nquantization. To address these challenges, we propose DVD-Quant, a novel\nData-free quantization framework for Video DiTs. Our approach integrates three\nkey innovations: (1) Progressive Bounded Quantization (PBQ) and (2)\nAuto-scaling Rotated Quantization (ARQ) for calibration data-free quantization\nerror reduction, as well as (3) $\\delta$-Guided Bit Switching ($\\delta$-GBS)\nfor adaptive bit-width allocation. Extensive experiments across multiple video\ngeneration benchmarks demonstrate that DVD-Quant achieves an approximately\n2$\\times$ speedup over full-precision baselines on HunyuanVideo while\nmaintaining visual fidelity. Notably, DVD-Quant is the first to enable W4A4 PTQ\nfor Video DiTs without compromising video quality. Code and models will be\navailable at https://github.com/lhxcs/DVD-Quant."}
{"id": "2505.18668", "pdf": "https://arxiv.org/pdf/2505.18668", "abs": "https://arxiv.org/abs/2505.18668", "authors": ["Zhen Li", "Yukai Guo", "Duan Li", "Xinyuan Guo", "Bowen Li", "Lanxi Xiao", "Shenyu Qiao", "Jiashu Chen", "Zijian Wu", "Hui Zhang", "Xinhuan Shu", "Shixia Liu"], "title": "ChartGalaxy: A Dataset for Infographic Chart Understanding and Generation", "categories": ["cs.CV", "cs.CL"], "comment": "63 pages, submitted to NeurIPS 2025 Datasets and Benchmarks Track", "summary": "Infographic charts are a powerful medium for communicating abstract data by\ncombining visual elements (e.g., charts, images) with textual information.\nHowever, their visual and structural richness poses challenges for large\nvision-language models (LVLMs), which are typically trained on plain charts. To\nbridge this gap, we introduce ChartGalaxy, a million-scale dataset designed to\nadvance the understanding and generation of infographic charts. The dataset is\nconstructed through an inductive process that identifies 75 chart types, 330\nchart variations, and 68 layout templates from real infographic charts and uses\nthem to create synthetic ones programmatically. We showcase the utility of this\ndataset through: 1) improving infographic chart understanding via fine-tuning,\n2) benchmarking code generation for infographic charts, and 3) enabling\nexample-based infographic chart generation. By capturing the visual and\nstructural complexity of real design, ChartGalaxy provides a useful resource\nfor enhancing multimodal reasoning and generation in LVLMs."}
{"id": "2505.18674", "pdf": "https://arxiv.org/pdf/2505.18674", "abs": "https://arxiv.org/abs/2505.18674", "authors": ["Peng Xiao", "Hongbo Zhao", "Yijun Wang", "Jianxin Lin"], "title": "Restoring Real-World Images with an Internal Detail Enhancement Diffusion Model", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Restoring real-world degraded images, such as old photographs or\nlow-resolution images, presents a significant challenge due to the complex,\nmixed degradations they exhibit, such as scratches, color fading, and noise.\nRecent data-driven approaches have struggled with two main challenges:\nachieving high-fidelity restoration and providing object-level control over\ncolorization. While diffusion models have shown promise in generating\nhigh-quality images with specific controls, they often fail to fully preserve\nimage details during restoration. In this work, we propose an internal\ndetail-preserving diffusion model for high-fidelity restoration of real-world\ndegraded images. Our method utilizes a pre-trained Stable Diffusion model as a\ngenerative prior, eliminating the need to train a model from scratch. Central\nto our approach is the Internal Image Detail Enhancement (IIDE) technique,\nwhich directs the diffusion model to preserve essential structural and textural\ninformation while mitigating degradation effects. The process starts by mapping\nthe input image into a latent space, where we inject the diffusion denoising\nprocess with degradation operations that simulate the effects of various\ndegradation factors. Extensive experiments demonstrate that our method\nsignificantly outperforms state-of-the-art models in both qualitative\nassessments and perceptual quantitative evaluations. Additionally, our approach\nsupports text-guided restoration, enabling object-level colorization control\nthat mimics the expertise of professional photo editing."}
{"id": "2505.18675", "pdf": "https://arxiv.org/pdf/2505.18675", "abs": "https://arxiv.org/abs/2505.18675", "authors": ["Sicheng Feng", "Song Wang", "Shuyi Ouyang", "Lingdong Kong", "Zikai Song", "Jianke Zhu", "Huan Wang", "Xinchao Wang"], "title": "Can MLLMs Guide Me Home? A Benchmark Study on Fine-Grained Visual Reasoning from Transit Maps", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Multimodal large language models (MLLMs) have recently achieved significant\nprogress in visual tasks, including semantic scene understanding and text-image\nalignment, with reasoning variants enhancing performance on complex tasks\ninvolving mathematics and logic. However, their capacity for reasoning tasks\ninvolving fine-grained visual understanding remains insufficiently evaluated.\nTo address this gap, we introduce ReasonMap, a benchmark designed to assess the\nfine-grained visual understanding and spatial reasoning abilities of MLLMs.\nReasonMap encompasses high-resolution transit maps from 30 cities across 13\ncountries and includes 1,008 question-answer pairs spanning two question types\nand three templates. Furthermore, we design a two-level evaluation pipeline\nthat properly assesses answer correctness and quality. Comprehensive\nevaluations of 15 popular MLLMs, including both base and reasoning variants,\nreveal a counterintuitive pattern: among open-source models, base models\noutperform reasoning ones, while the opposite trend is observed in\nclosed-source models. Additionally, performance generally degrades when visual\ninputs are masked, indicating that while MLLMs can leverage prior knowledge to\nanswer some questions, fine-grained visual reasoning tasks still require\ngenuine visual perception for strong performance. Our benchmark study offers\nnew insights into visual reasoning and contributes to investigating the gap\nbetween open-source and closed-source models."}
{"id": "2505.18679", "pdf": "https://arxiv.org/pdf/2505.18679", "abs": "https://arxiv.org/abs/2505.18679", "authors": ["Bin Ren", "Yawei Li", "Xu Zheng", "Yuqian Fu", "Danda Pani Paudel", "Ming-Hsuan Yang", "Luc Van Gool", "Nicu Sebe"], "title": "Manifold-aware Representation Learning for Degradation-agnostic Image Restoration", "categories": ["cs.CV"], "comment": "ALl-in-One Image Restoration, low-level vision", "summary": "Image Restoration (IR) aims to recover high quality images from degraded\ninputs affected by various corruptions such as noise, blur, haze, rain, and low\nlight conditions. Despite recent advances, most existing approaches treat IR as\na direct mapping problem, relying on shared representations across degradation\ntypes without modeling their structural diversity. In this work, we present\nMIRAGE, a unified and lightweight framework for all in one IR that explicitly\ndecomposes the input feature space into three semantically aligned parallel\nbranches, each processed by a specialized module attention for global context,\nconvolution for local textures, and MLP for channel-wise statistics. This\nmodular decomposition significantly improves generalization and efficiency\nacross diverse degradations. Furthermore, we introduce a cross layer\ncontrastive learning scheme that aligns shallow and latent features to enhance\nthe discriminability of shared representations. To better capture the\nunderlying geometry of feature representations, we perform contrastive learning\nin a Symmetric Positive Definite (SPD) manifold space rather than the\nconventional Euclidean space. Extensive experiments show that MIRAGE not only\nachieves new state of the art performance across a variety of degradation types\nbut also offers a scalable solution for challenging all-in-one IR scenarios.\nOur code and models will be publicly available at\nhttps://amazingren.github.io/MIRAGE/."}
{"id": "2505.18686", "pdf": "https://arxiv.org/pdf/2505.18686", "abs": "https://arxiv.org/abs/2505.18686", "authors": ["Yang Liu", "Silin Cheng", "Xinwei He", "Sebastien Ourselin", "Lei Tan", "Gen Luo"], "title": "WeakMCN: Multi-task Collaborative Network for Weakly Supervised Referring Expression Comprehension and Segmentation", "categories": ["cs.CV"], "comment": "Accepted by CVPR2025", "summary": "Weakly supervised referring expression comprehension(WREC) and\nsegmentation(WRES) aim to learn object grounding based on a given expression\nusing weak supervision signals like image-text pairs. While these tasks have\ntraditionally been modeled separately, we argue that they can benefit from\njoint learning in a multi-task framework. To this end, we propose WeakMCN, a\nnovel multi-task collaborative network that effectively combines WREC and WRES\nwith a dual-branch architecture. Specifically, the WREC branch is formulated as\nanchor-based contrastive learning, which also acts as a teacher to supervise\nthe WRES branch. In WeakMCN, we propose two innovative designs to facilitate\nmulti-task collaboration, namely Dynamic Visual Feature Enhancement(DVFE) and\nCollaborative Consistency Module(CCM). DVFE dynamically combines various\npre-trained visual knowledge to meet different task requirements, while CCM\npromotes cross-task consistency from the perspective of optimization. Extensive\nexperimental results on three popular REC and RES benchmarks, i.e., RefCOCO,\nRefCOCO+, and RefCOCOg, consistently demonstrate performance gains of WeakMCN\nover state-of-the-art single-task alternatives, e.g., up to 3.91% and 13.11% on\nRefCOCO for WREC and WRES tasks, respectively. Furthermore, experiments also\nvalidate the strong generalization ability of WeakMCN in both semi-supervised\nREC and RES settings against existing methods, e.g., +8.94% for semi-REC and\n+7.71% for semi-RES on 1% RefCOCO. The code is publicly available at\nhttps://github.com/MRUIL/WeakMCN."}
{"id": "2505.18699", "pdf": "https://arxiv.org/pdf/2505.18699", "abs": "https://arxiv.org/abs/2505.18699", "authors": ["Peixuan Zhang", "Shuchen Weng", "Chengxuan Zhu", "Binghao Tang", "Zijian Jia", "Si Li", "Boxin Shi"], "title": "Affective Image Editing: Shaping Emotional Factors via Text Descriptions", "categories": ["cs.CV"], "comment": null, "summary": "In daily life, images as common affective stimuli have widespread\napplications. Despite significant progress in text-driven image editing, there\nis limited work focusing on understanding users' emotional requests. In this\npaper, we introduce AIEdiT for Affective Image Editing using Text descriptions,\nwhich evokes specific emotions by adaptively shaping multiple emotional factors\nacross the entire images. To represent universal emotional priors, we build the\ncontinuous emotional spectrum and extract nuanced emotional requests. To\nmanipulate emotional factors, we design the emotional mapper to translate\nvisually-abstract emotional requests to visually-concrete semantic\nrepresentations. To ensure that editing results evoke specific emotions, we\nintroduce an MLLM to supervise the model training. During inference, we\nstrategically distort visual elements and subsequently shape corresponding\nemotional factors to edit images according to users' instructions.\nAdditionally, we introduce a large-scale dataset that includes the\nemotion-aligned text and image pair set for training and evaluation. Extensive\nexperiments demonstrate that AIEdiT achieves superior performance, effectively\nreflecting users' emotional requests."}
{"id": "2505.18700", "pdf": "https://arxiv.org/pdf/2505.18700", "abs": "https://arxiv.org/abs/2505.18700", "authors": ["Chun Wang", "Xiaoran Pan", "Zihao Pan", "Haofan Wang", "Yiren Song"], "title": "GRE Suite: Geo-localization Inference via Fine-Tuned Vision-Language Models and Enhanced Reasoning Chains", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advances in Visual Language Models (VLMs) have demonstrated\nexceptional performance in visual reasoning tasks. However, geo-localization\npresents unique challenges, requiring the extraction of multigranular visual\ncues from images and their integration with external world knowledge for\nsystematic reasoning. Current approaches to geo-localization tasks often lack\nrobust reasoning mechanisms and explainability, limiting their effectiveness.\nTo address these limitations, we propose the Geo Reason Enhancement (GRE)\nSuite, a novel framework that augments VLMs with structured reasoning chains\nfor accurate and interpretable location inference. The GRE Suite is\nsystematically developed across three key dimensions: dataset, model, and\nbenchmark. First, we introduce GRE30K, a high-quality geo-localization\nreasoning dataset designed to facilitate fine-grained visual and contextual\nanalysis. Next, we present the GRE model, which employs a multi-stage reasoning\nstrategy to progressively infer scene attributes, local details, and semantic\nfeatures, thereby narrowing down potential geographic regions with enhanced\nprecision. Finally, we construct the Geo Reason Evaluation Benchmark\n(GREval-Bench), a comprehensive evaluation framework that assesses VLMs across\ndiverse urban, natural, and landmark scenes to measure both coarse-grained\n(e.g., country, continent) and fine-grained (e.g., city, street) localization\nperformance. Experimental results demonstrate that GRE significantly\noutperforms existing methods across all granularities of geo-localization\ntasks, underscoring the efficacy of reasoning-augmented VLMs in complex\ngeographic inference. Code and data will be released at\nhttps://github.com/Thorin215/GRE."}
{"id": "2505.18725", "pdf": "https://arxiv.org/pdf/2505.18725", "abs": "https://arxiv.org/abs/2505.18725", "authors": ["Mahmudul Hasan"], "title": "Deep Learning for Breast Cancer Detection: Comparative Analysis of ConvNeXT and EfficientNet", "categories": ["cs.CV"], "comment": null, "summary": "Breast cancer is the most commonly occurring cancer worldwide. This cancer\ncaused 670,000 deaths globally in 2022, as reported by the WHO. Yet since\nhealth officials began routine mammography screening in age groups deemed at\nrisk in the 1980s, breast cancer mortality has decreased by 40% in high-income\nnations. Every day, a greater and greater number of people are receiving a\nbreast cancer diagnosis. Reducing cancer-related deaths requires early\ndetection and treatment. This paper compares two convolutional neural networks\ncalled ConvNeXT and EfficientNet to predict the likelihood of cancer in\nmammograms from screening exams. Preprocessing of the images, classification,\nand performance evaluation are main parts of the whole procedure. Several\nevaluation metrics were used to compare and evaluate the performance of the\nmodels. The result shows that ConvNeXT generates better results with a 94.33%\nAUC score, 93.36% accuracy, and 95.13% F-score compared to EfficientNet with a\n92.34% AUC score, 91.47% accuracy, and 93.06% F-score on RSNA screening\nmammography breast cancer dataset."}
{"id": "2505.18727", "pdf": "https://arxiv.org/pdf/2505.18727", "abs": "https://arxiv.org/abs/2505.18727", "authors": ["Xiaohe Li", "Pengfei Li", "Zide Fan", "Ying Geng", "Fangli Mou", "Haohua Wu", "Yunping Ge"], "title": "FusionTrack: End-to-End Multi-Object Tracking in Arbitrary Multi-View Environment", "categories": ["cs.CV"], "comment": null, "summary": "Multi-view multi-object tracking (MVMOT) has found widespread applications in\nintelligent transportation, surveillance systems, and urban management.\nHowever, existing studies rarely address genuinely free-viewpoint MVMOT\nsystems, which could significantly enhance the flexibility and scalability of\ncooperative tracking systems. To bridge this gap, we first construct the\nMulti-Drone Multi-Object Tracking (MDMOT) dataset, captured by mobile drone\nswarms across diverse real-world scenarios, initially establishing the first\nbenchmark for multi-object tracking in arbitrary multi-view environment.\nBuilding upon this foundation, we propose \\textbf{FusionTrack}, an end-to-end\nframework that reasonably integrates tracking and re-identification to leverage\nmulti-view information for robust trajectory association. Extensive experiments\non our MDMOT and other benchmark datasets demonstrate that FusionTrack achieves\nstate-of-the-art performance in both single-view and multi-view tracking."}
{"id": "2505.18730", "pdf": "https://arxiv.org/pdf/2505.18730", "abs": "https://arxiv.org/abs/2505.18730", "authors": ["Wenchao Zhang", "Jiahe Tian", "Runze He", "Jizhong Han", "Jiao Dai", "Miaomiao Feng", "Wei Mi", "Xiaodan Zhang"], "title": "Align Beyond Prompts: Evaluating World Knowledge Alignment in Text-to-Image Generation", "categories": ["cs.CV"], "comment": "Code: https://github.com/smile365317/ABP", "summary": "Recent text-to-image (T2I) generation models have advanced significantly,\nenabling the creation of high-fidelity images from textual prompts. However,\nexisting evaluation benchmarks primarily focus on the explicit alignment\nbetween generated images and prompts, neglecting the alignment with real-world\nknowledge beyond prompts. To address this gap, we introduce Align Beyond\nPrompts (ABP), a comprehensive benchmark designed to measure the alignment of\ngenerated images with real-world knowledge that extends beyond the explicit\nuser prompts. ABP comprises over 2,000 meticulously crafted prompts, covering\nreal-world knowledge across six distinct scenarios. We further introduce\nABPScore, a metric that utilizes existing Multimodal Large Language Models\n(MLLMs) to assess the alignment between generated images and world knowledge\nbeyond prompts, which demonstrates strong correlations with human judgments.\nThrough a comprehensive evaluation of 8 popular T2I models using ABP, we find\nthat even state-of-the-art models, such as GPT-4o, face limitations in\nintegrating simple real-world knowledge into generated images. To mitigate this\nissue, we introduce a training-free strategy within ABP, named Inference-Time\nKnowledge Injection (ITKI). By applying this strategy to optimize 200\nchallenging samples, we achieved an improvement of approximately 43% in\nABPScore. The dataset and code are available in\nhttps://github.com/smile365317/ABP."}
{"id": "2505.18736", "pdf": "https://arxiv.org/pdf/2505.18736", "abs": "https://arxiv.org/abs/2505.18736", "authors": ["Junyong Kang", "Seohyun Lim", "Kyungjune Baek", "Hyunjung Shim"], "title": "Rethinking Direct Preference Optimization in Diffusion Models", "categories": ["cs.CV"], "comment": "21 pages, 12 figures, preprint", "summary": "Aligning text-to-image (T2I) diffusion models with human preferences has\nemerged as a critical research challenge. While recent advances in this area\nhave extended preference optimization techniques from large language models\n(LLMs) to the diffusion setting, they often struggle with limited exploration.\nIn this work, we propose a novel and orthogonal approach to enhancing\ndiffusion-based preference optimization. First, we introduce a stable reference\nmodel update strategy that relaxes the frozen reference model, encouraging\nexploration while maintaining a stable optimization anchor through reference\nmodel regularization. Second, we present a timestep-aware training strategy\nthat mitigates the reward scale imbalance problem across timesteps. Our method\ncan be integrated into various preference optimization algorithms. Experimental\nresults show that our approach improves the performance of state-of-the-art\nmethods on human preference evaluation benchmarks."}
{"id": "2505.18741", "pdf": "https://arxiv.org/pdf/2505.18741", "abs": "https://arxiv.org/abs/2505.18741", "authors": ["Han Li", "Hu Han", "S. Kevin Zhou"], "title": "MoMBS: Mixed-order minibatch sampling enhances model training from diverse-quality images", "categories": ["cs.CV", "cs.AI"], "comment": "16 pages,8 figures", "summary": "Natural images exhibit label diversity (clean vs. noisy) in noisy-labeled\nimage classification and prevalence diversity (abundant vs. sparse) in\nlong-tailed image classification. Similarly, medical images in universal lesion\ndetection (ULD) exhibit substantial variations in image quality, encompassing\nattributes such as clarity and label correctness. How to effectively leverage\ntraining images with diverse qualities becomes a problem in learning deep\nmodels. Conventional training mechanisms, such as self-paced curriculum\nlearning (SCL) and online hard example mining (OHEM), relieve this problem by\nreweighting images with high loss values. Despite their success, these methods\nstill confront two challenges: (i) the loss-based measure of sample hardness is\nimprecise, preventing optimum handling of different cases, and (ii) there\nexists under-utilization in SCL or over-utilization OHEM with the identified\nhard samples. To address these issues, this paper revisits the minibatch\nsampling (MBS), a technique widely used in deep network training but largely\nunexplored concerning the handling of diverse-quality training samples. We\ndiscover that the samples within a minibatch influence each other during\ntraining; thus, we propose a novel Mixed-order Minibatch Sampling (MoMBS)\nmethod to optimize the use of training samples with diverse qualities. MoMBS\nintroduces a measure that takes both loss and uncertainty into account to\nsurpass a sole reliance on loss and allows for a more refined categorization of\nhigh-loss samples by distinguishing them as either poorly labeled and under\nrepresented or well represented and overfitted. We prioritize under represented\nsamples as the main gradient contributors in a minibatch and keep them from the\nnegative influences of poorly labeled or overfitted samples with a mixed-order\nminibatch sampling design."}
{"id": "2505.18745", "pdf": "https://arxiv.org/pdf/2505.18745", "abs": "https://arxiv.org/abs/2505.18745", "authors": ["Umar Marikkar", "Syed Sameed Husain", "Muhammad Awais", "Sara Atito"], "title": "C3R: Channel Conditioned Cell Representations for unified evaluation in microscopy imaging", "categories": ["cs.CV", "cs.LG", "q-bio.QM"], "comment": null, "summary": "Immunohistochemical (IHC) images reveal detailed information about structures\nand functions at the subcellular level. However, unlike natural images, IHC\ndatasets pose challenges for deep learning models due to their inconsistencies\nin channel count and configuration, stemming from varying staining protocols\nacross laboratories and studies. Existing approaches build channel-adaptive\nmodels, which unfortunately fail to support out-of-distribution (OOD)\nevaluation across IHC datasets and cannot be applied in a true zero-shot\nsetting with mismatched channel counts. To address this, we introduce a\nstructured view of cellular image channels by grouping them into either context\nor concept, where we treat the context channels as a reference to the concept\nchannels in the image. We leverage this context-concept principle to develop\nChannel Conditioned Cell Representations (C3R), a framework designed for\nunified evaluation on in-distribution (ID) and OOD datasets. C3R is a two-fold\nframework comprising a channel-adaptive encoder architecture and a masked\nknowledge distillation training strategy, both built around the context-concept\nprinciple. We find that C3R outperforms existing benchmarks on both ID and OOD\ntasks, while a trivial implementation of our core idea also outperforms the\nchannel-adaptive methods reported on the CHAMMI benchmark. Our method opens a\nnew pathway for cross-dataset generalization between IHC datasets, without\nrequiring dataset-specific adaptation or retraining."}
{"id": "2505.18757", "pdf": "https://arxiv.org/pdf/2505.18757", "abs": "https://arxiv.org/abs/2505.18757", "authors": ["Duo Li", "Zuhao Yang", "Shijian Lu"], "title": "ToDRE: Visual Token Pruning via Diversity and Task Awareness for Efficient Large Vision-Language Models", "categories": ["cs.CV"], "comment": "21 pages, 7 figures", "summary": "The representation of visual inputs of large vision-language models (LVLMs)\nusually involves substantially more tokens than that of textual inputs, leading\nto significant computational overhead. Several recent studies strive to\nmitigate this issue by either conducting token compression to prune redundant\nvisual tokens or guiding them to bypass certain computational stages. While\nmost existing work exploits token importance as the redundancy indicator, our\nstudy reveals that two largely neglected factors, namely, the diversity of\nretained visual tokens and their task relevance, often offer more robust\ncriteria in token pruning. To this end, we design ToDRE, a two-stage and\ntraining-free token compression framework that achieves superior performance by\npruning Tokens based on token Diversity and token-task RElevance. Instead of\npruning redundant tokens, ToDRE introduces a greedy k-center algorithm to\nselect and retain a small subset of diverse visual tokens after the vision\nencoder. Additionally, ToDRE addresses the \"information migration\" by further\neliminating task-irrelevant visual tokens within the decoder of large language\nmodel (LLM). Extensive experiments show that ToDRE effectively reduces 90% of\nvisual tokens after vision encoder and adaptively prunes all visual tokens\nwithin certain LLM's decoder layers, leading to a 2.6x speed-up in total\ninference time while maintaining 95.1% of model performance and excellent\ncompatibility with efficient attention operators."}
{"id": "2505.18766", "pdf": "https://arxiv.org/pdf/2505.18766", "abs": "https://arxiv.org/abs/2505.18766", "authors": ["Yanjie Li", "Wenxuan Zhang", "Xinqi Lyu", "Yihao Liu", "Bin Xiao"], "title": "StyleGuard: Preventing Text-to-Image-Model-based Style Mimicry Attacks by Style Perturbations", "categories": ["cs.CV", "cs.AI"], "comment": "submitted to NIPS2025", "summary": "Recently, text-to-image diffusion models have been widely used for style\nmimicry and personalized customization through methods such as DreamBooth and\nTextual Inversion. This has raised concerns about intellectual property\nprotection and the generation of deceptive content. Recent studies, such as\nGlaze and Anti-DreamBooth, have proposed using adversarial noise to protect\nimages from these attacks. However, recent purification-based methods, such as\nDiffPure and Noise Upscaling, have successfully attacked these latest defenses,\nshowing the vulnerabilities of these methods. Moreover, present methods show\nlimited transferability across models, making them less effective against\nunknown text-to-image models. To address these issues, we propose a novel\nanti-mimicry method, StyleGuard. We propose a novel style loss that optimizes\nthe style-related features in the latent space to make it deviate from the\noriginal image, which improves model-agnostic transferability. Additionally, to\nenhance the perturbation's ability to bypass diffusion-based purification, we\ndesigned a novel upscale loss that involves ensemble purifiers and upscalers\nduring training. Extensive experiments on the WikiArt and CelebA datasets\ndemonstrate that StyleGuard outperforms existing methods in robustness against\nvarious transformations and purifications, effectively countering style mimicry\nin various models. Moreover, StyleGuard is effective on different style mimicry\nmethods, including DreamBooth and Textual Inversion."}
{"id": "2505.18770", "pdf": "https://arxiv.org/pdf/2505.18770", "abs": "https://arxiv.org/abs/2505.18770", "authors": ["Yuedi Zhang", "Shuanghao Bai", "Wanqi Zhou", "Zhirong Luan", "Badong Chen"], "title": "Dual-Path Stable Soft Prompt Generation for Domain Generalization", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Domain generalization (DG) aims to learn a model using data from one or\nmultiple related but distinct source domains that can generalize well to unseen\nout-of-distribution target domains. Inspired by the success of large\npre-trained vision-language models (VLMs), prompt tuning has emerged as an\neffective generalization strategy. However, it often struggles to capture\ndomain-specific features due to its reliance on manually or fixed prompt\ninputs. Recently, some prompt generation methods have addressed this limitation\nby dynamically generating instance-specific and domain-specific prompts for\neach input, enriching domain information and demonstrating potential for\nenhanced generalization. Through further investigation, we identify a notable\nissue in existing prompt generation methods: the same input often yields\nsignificantly different and suboptimal prompts across different random seeds, a\nphenomenon we term Prompt Variability. To address this, we introduce negative\nlearning into the prompt generation process and propose Dual-Path Stable Soft\nPrompt Generation (DPSPG), a transformer-based framework designed to improve\nboth the stability and generalization of prompts. Specifically, DPSPG\nincorporates a complementary prompt generator to produce negative prompts,\nthereby reducing the risk of introducing misleading information. Both\ntheoretical and empirical analyses demonstrate that negative learning leads to\nmore robust and effective prompts by increasing the effective margin and\nreducing the upper bound of the gradient norm. Extensive experiments on five DG\nbenchmark datasets show that DPSPG consistently outperforms state-of-the-art\nmethods while maintaining prompt stability."}
{"id": "2505.18775", "pdf": "https://arxiv.org/pdf/2505.18775", "abs": "https://arxiv.org/abs/2505.18775", "authors": ["Jiayu Wang", "Yang Jiao", "Yue Yu", "Tianwen Qian", "Shaoxiang Chen", "Jingjing Chen", "Yu-Gang Jiang"], "title": "OmniGenBench: A Benchmark for Omnipotent Multimodal Generation across 50+ Tasks", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent breakthroughs in large multimodal models (LMMs), such as the\nimpressive GPT-4o-Native, have demonstrated remarkable proficiency in following\ngeneral-purpose instructions for image generation. However, current benchmarks\noften lack the necessary breadth and depth to fully evaluate the diverse\ncapabilities of these models. To overcome this limitation, we introduce\nOmniGenBench, a novel and comprehensive benchmark meticulously designed to\nassess the instruction-following abilities of state-of-the-art LMMs across both\nperception-centric and cognition-centric dimensions. Our OmniGenBench includes\n57 diverse sub-tasks grounded in real-world scenarios, systematically\ncategorized according to the specific model capabilities they demand. For\nrigorous evaluation, we further employ a dual-mode protocol. This protocol\nutilizes off-the-shelf visual parsing tools for perception-centric tasks and a\npowerful LLM-based judger for cognition-centric tasks to assess the alignment\nbetween generated images and user instructions. Using OmniGenBench, we evaluate\nmainstream generative models, including prevalent models like GPT-4o,\nGemini-2.0-Flash, and Seedream, and provide in-depth comparisons and analyses\nof their performance.Code and data are available at\nhttps://github.com/emilia113/OmniGenBench."}
{"id": "2505.18787", "pdf": "https://arxiv.org/pdf/2505.18787", "abs": "https://arxiv.org/abs/2505.18787", "authors": ["Hong-Hanh Nguyen-Le", "Van-Tuan Tran", "Dinh-Thuc Nguyen", "Nhien-An Le-Khac"], "title": "Think Twice before Adaptation: Improving Adaptability of DeepFake Detection via Online Test-Time Adaptation", "categories": ["cs.CV", "cs.AI", "cs.CR"], "comment": "Accepted at 34th International Joint Conference on Artificial\n  Intelligence (IJCAI-25)", "summary": "Deepfake (DF) detectors face significant challenges when deployed in\nreal-world environments, particularly when encountering test samples deviated\nfrom training data through either postprocessing manipulations or distribution\nshifts. We demonstrate postprocessing techniques can completely obscure\ngeneration artifacts presented in DF samples, leading to performance\ndegradation of DF detectors. To address these challenges, we propose Think\nTwice before Adaptation (\\texttt{T$^2$A}), a novel online test-time adaptation\nmethod that enhances the adaptability of detectors during inference without\nrequiring access to source training data or labels. Our key idea is to enable\nthe model to explore alternative options through an Uncertainty-aware Negative\nLearning objective rather than solely relying on its initial predictions as\ncommonly seen in entropy minimization (EM)-based approaches. We also introduce\nan Uncertain Sample Prioritization strategy and Gradients Masking technique to\nimprove the adaptation by focusing on important samples and model parameters.\nOur theoretical analysis demonstrates that the proposed negative learning\nobjective exhibits complementary behavior to EM, facilitating better adaptation\ncapability. Empirically, our method achieves state-of-the-art results compared\nto existing test-time adaptation (TTA) approaches and significantly enhances\nthe resilience and generalization of DF detectors during inference. Code is\navailable\n\\href{https://github.com/HongHanh2104/T2A-Think-Twice-Before-Adaptation}{here}."}
{"id": "2505.18809", "pdf": "https://arxiv.org/pdf/2505.18809", "abs": "https://arxiv.org/abs/2505.18809", "authors": ["Wenhao Sun", "Rong-Cheng Tu", "Yifu Ding", "Zhao Jin", "Jingyi Liao", "Shunyu Liu", "Dacheng Tao"], "title": "VORTA: Efficient Video Diffusion via Routing Sparse Attention", "categories": ["cs.CV"], "comment": "19 pages, 15 figures. The code is available at\n  https://github.com/wenhao728/VORTA", "summary": "Video Diffusion Transformers (VDiTs) have achieved remarkable progress in\nhigh-quality video generation, but remain computationally expensive due to the\nquadratic complexity of attention over high-dimensional video sequences. Recent\nattention acceleration methods leverage the sparsity of attention patterns to\nimprove efficiency; however, they often overlook inefficiencies of redundant\nlong-range interactions. To address this problem, we propose \\textbf{VORTA}, an\nacceleration framework with two novel components: 1) a sparse attention\nmechanism that efficiently captures long-range dependencies, and 2) a routing\nstrategy that adaptively replaces full 3D attention with specialized sparse\nattention variants throughout the sampling process. It achieves a $1.76\\times$\nend-to-end speedup without quality loss on VBench. Furthermore, VORTA can\nseamlessly integrate with various other acceleration methods, such as caching\nand step distillation, reaching up to $14.41\\times$ speedup with negligible\nperformance degradation. VORTA demonstrates its efficiency and enhances the\npracticality of VDiTs in real-world settings."}
{"id": "2505.18812", "pdf": "https://arxiv.org/pdf/2505.18812", "abs": "https://arxiv.org/abs/2505.18812", "authors": ["Ye Sun", "Hao Zhang", "Henghui Ding", "Tiehua Zhang", "Xingjun Ma", "Yu-Gang Jiang"], "title": "SAMA: Towards Multi-Turn Referential Grounded Video Chat with Large Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Achieving fine-grained spatio-temporal understanding in videos remains a\nmajor challenge for current Video Large Multimodal Models (Video LMMs).\nAddressing this challenge requires mastering two core capabilities: video\nreferring understanding, which captures the semantics of video regions, and\nvideo grounding, which segments object regions based on natural language\ndescriptions. However, most existing approaches tackle these tasks in\nisolation, limiting progress toward unified, referentially grounded video\ninteraction. We identify a key bottleneck in the lack of high-quality, unified\nvideo instruction data and a comprehensive benchmark for evaluating\nreferentially grounded video chat. To address these challenges, we contribute\nin three core aspects: dataset, model, and benchmark. First, we introduce\nSAMA-239K, a large-scale dataset comprising 15K videos specifically curated to\nenable joint learning of video referring understanding, grounding, and\nmulti-turn video chat. Second, we propose the SAMA model, which incorporates a\nversatile spatio-temporal context aggregator and a Segment Anything Model to\njointly enhance fine-grained video comprehension and precise grounding\ncapabilities. Finally, we establish SAMA-Bench, a meticulously designed\nbenchmark consisting of 5,067 questions from 522 videos, to comprehensively\nevaluate the integrated capabilities of Video LMMs in multi-turn,\nspatio-temporal referring understanding and grounded dialogue. Extensive\nexperiments and benchmarking results show that SAMA not only achieves strong\nperformance on SAMA-Bench but also sets a new state-of-the-art on general\ngrounding benchmarks, while maintaining highly competitive performance on\nstandard visual understanding benchmarks."}
{"id": "2505.18816", "pdf": "https://arxiv.org/pdf/2505.18816", "abs": "https://arxiv.org/abs/2505.18816", "authors": ["Yiqing Shen", "Chenjia Li", "Fei Xiong", "Jeong-O Jeong", "Tianpeng Wang", "Michael Latman", "Mathias Unberath"], "title": "Reasoning Segmentation for Images and Videos: A Survey", "categories": ["cs.CV"], "comment": null, "summary": "Reasoning Segmentation (RS) aims to delineate objects based on implicit text\nqueries, the interpretation of which requires reasoning and knowledge\nintegration. Unlike the traditional formulation of segmentation problems that\nrelies on fixed semantic categories or explicit prompting, RS bridges the gap\nbetween visual perception and human-like reasoning capabilities, facilitating\nmore intuitive human-AI interaction through natural language. Our work presents\nthe first comprehensive survey of RS for image and video processing, examining\n26 state-of-the-art methods together with a review of the corresponding\nevaluation metrics, as well as 29 datasets and benchmarks. We also explore\nexisting applications of RS across diverse domains and identify their potential\nextensions. Finally, we identify current research gaps and highlight promising\nfuture directions."}
{"id": "2505.18819", "pdf": "https://arxiv.org/pdf/2505.18819", "abs": "https://arxiv.org/abs/2505.18819", "authors": ["Guofeng Mei", "Bin Ren", "Juan Liu", "Luigi Riz", "Xiaoshui Huang", "Xu Zheng", "Yongshun Gong", "Ming-Hsuan Yang", "Nicu Sebe", "Fabio Poiesi"], "title": "Self-Supervised and Generalizable Tokenization for CLIP-Based 3D Understanding", "categories": ["cs.CV"], "comment": "10 pages, tokenizer", "summary": "Vision-language models like CLIP can offer a promising foundation for 3D\nscene understanding when extended with 3D tokenizers. However, standard\napproaches, such as k-nearest neighbor or radius-based tokenization, struggle\nwith cross-domain generalization due to sensitivity to dataset-specific spatial\nscales. We present a universal 3D tokenizer designed for scale-invariant\nrepresentation learning with a frozen CLIP backbone. We show that combining\nsuperpoint-based grouping with coordinate scale normalization consistently\noutperforms conventional methods through extensive experimental analysis.\nSpecifically, we introduce S4Token, a tokenization pipeline that produces\nsemantically-informed tokens regardless of scene scale. Our tokenizer is\ntrained without annotations using masked point modeling and clustering-based\nobjectives, along with cross-modal distillation to align 3D tokens with 2D\nmulti-view image features. For dense prediction tasks, we propose a\nsuperpoint-level feature propagation module to recover point-level detail from\nsparse tokens."}
{"id": "2505.18823", "pdf": "https://arxiv.org/pdf/2505.18823", "abs": "https://arxiv.org/abs/2505.18823", "authors": ["Libin Lan", "Yanxin Li", "Xiaojuan Liu", "Juan Zhou", "Jianxun Zhang", "Nannan Huang", "Yudong Zhang"], "title": "MSLAU-Net: A Hybird CNN-Transformer Network for Medical Image Segmentation", "categories": ["cs.CV"], "comment": "13 pages, 7 figures, 7 tables", "summary": "Both CNN-based and Transformer-based methods have achieved remarkable success\nin medical image segmentation tasks. However, CNN-based methods struggle to\neffectively capture global contextual information due to the inherent\nlimitations of convolution operations. Meanwhile, Transformer-based methods\nsuffer from insufficient local feature modeling and face challenges related to\nthe high computational complexity caused by the self-attention mechanism. To\naddress these limitations, we propose a novel hybrid CNN-Transformer\narchitecture, named MSLAU-Net, which integrates the strengths of both\nparadigms. The proposed MSLAU-Net incorporates two key ideas. First, it\nintroduces Multi-Scale Linear Attention, designed to efficiently extract\nmulti-scale features from medical images while modeling long-range dependencies\nwith low computational complexity. Second, it adopts a top-down feature\naggregation mechanism, which performs multi-level feature aggregation and\nrestores spatial resolution using a lightweight structure. Extensive\nexperiments conducted on benchmark datasets covering three imaging modalities\ndemonstrate that the proposed MSLAU-Net outperforms other state-of-the-art\nmethods on nearly all evaluation metrics, validating the superiority,\neffectiveness, and robustness of our approach. Our code is available at\nhttps://github.com/Monsoon49/MSLAU-Net."}
{"id": "2505.18832", "pdf": "https://arxiv.org/pdf/2505.18832", "abs": "https://arxiv.org/abs/2505.18832", "authors": ["Arman Zarei", "Samyadeep Basu", "Keivan Rezaei", "Zihao Lin", "Sayan Nag", "Soheil Feizi"], "title": "Localizing Knowledge in Diffusion Transformers", "categories": ["cs.CV"], "comment": null, "summary": "Understanding how knowledge is distributed across the layers of generative\nmodels is crucial for improving interpretability, controllability, and\nadaptation. While prior work has explored knowledge localization in UNet-based\narchitectures, Diffusion Transformer (DiT)-based models remain underexplored in\nthis context. In this paper, we propose a model- and knowledge-agnostic method\nto localize where specific types of knowledge are encoded within the DiT\nblocks. We evaluate our method on state-of-the-art DiT-based models, including\nPixArt-alpha, FLUX, and SANA, across six diverse knowledge categories. We show\nthat the identified blocks are both interpretable and causally linked to the\nexpression of knowledge in generated outputs. Building on these insights, we\napply our localization framework to two key applications: model personalization\nand knowledge unlearning. In both settings, our localized fine-tuning approach\nenables efficient and targeted updates, reducing computational cost, improving\ntask-specific performance, and better preserving general model behavior with\nminimal interference to unrelated or surrounding content. Overall, our findings\noffer new insights into the internal structure of DiTs and introduce a\npractical pathway for more interpretable, efficient, and controllable model\nediting."}
{"id": "2505.18855", "pdf": "https://arxiv.org/pdf/2505.18855", "abs": "https://arxiv.org/abs/2505.18855", "authors": ["Peiqi Wang", "ShengYun Peng", "Xuewen Zhang", "Hanchao Yu", "Yibo Yang", "Lifu Huang", "Fujun Liu", "Qifan Wang"], "title": "Inference Compute-Optimal Video Vision Language Models", "categories": ["cs.CV", "cs.CL"], "comment": "Annual Meeting of the Association for Computational Linguistics\n  (ACL), 2025", "summary": "This work investigates the optimal allocation of inference compute across\nthree key scaling factors in video vision language models: language model size,\nframe count, and the number of visual tokens per frame. While prior works\ntypically focuses on optimizing model efficiency or improving performance\nwithout considering resource constraints, we instead identify optimal model\nconfiguration under fixed inference compute budgets. We conduct large-scale\ntraining sweeps and careful parametric modeling of task performance to identify\nthe inference compute-optimal frontier. Our experiments reveal how task\nperformance depends on scaling factors and finetuning data size, as well as how\nchanges in data size shift the compute-optimal frontier. These findings\ntranslate to practical tips for selecting these scaling factors."}
{"id": "2505.18869", "pdf": "https://arxiv.org/pdf/2505.18869", "abs": "https://arxiv.org/abs/2505.18869", "authors": ["Ankan Dash", "Jingyi Gu", "Guiling Wang", "Chen Chen"], "title": "Eye-See-You: Reverse Pass-Through VR and Head Avatars", "categories": ["cs.CV"], "comment": "34th International Joint Conference on Artificial Intelligence, IJCAI\n  2025", "summary": "Virtual Reality (VR) headsets, while integral to the evolving digital\necosystem, present a critical challenge: the occlusion of users' eyes and\nportions of their faces, which hinders visual communication and may contribute\nto social isolation. To address this, we introduce RevAvatar, an innovative\nframework that leverages AI methodologies to enable reverse pass-through\ntechnology, fundamentally transforming VR headset design and interaction\nparadigms. RevAvatar integrates state-of-the-art generative models and\nmultimodal AI techniques to reconstruct high-fidelity 2D facial images and\ngenerate accurate 3D head avatars from partially observed eye and lower-face\nregions. This framework represents a significant advancement in AI4Tech by\nenabling seamless interaction between virtual and physical environments,\nfostering immersive experiences such as VR meetings and social engagements.\nAdditionally, we present VR-Face, a novel dataset comprising 200,000 samples\ndesigned to emulate diverse VR-specific conditions, including occlusions,\nlighting variations, and distortions. By addressing fundamental limitations in\ncurrent VR systems, RevAvatar exemplifies the transformative synergy between AI\nand next-generation technologies, offering a robust platform for enhancing\nhuman connection and interaction in virtual environments."}
{"id": "2505.18875", "pdf": "https://arxiv.org/pdf/2505.18875", "abs": "https://arxiv.org/abs/2505.18875", "authors": ["Shuo Yang", "Haocheng Xi", "Yilong Zhao", "Muyang Li", "Jintao Zhang", "Han Cai", "Yujun Lin", "Xiuyu Li", "Chenfeng Xu", "Kelly Peng", "Jianfei Chen", "Song Han", "Kurt Keutzer", "Ion Stoica"], "title": "Sparse VideoGen2: Accelerate Video Generation with Sparse Attention via Semantic-Aware Permutation", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion Transformers (DiTs) are essential for video generation but suffer\nfrom significant latency due to the quadratic complexity of attention. By\ncomputing only critical tokens, sparse attention reduces computational costs\nand offers a promising acceleration approach. However, we identify that\nexisting methods fail to approach optimal generation quality under the same\ncomputation budget for two reasons: (1) Inaccurate critical token\nidentification: current methods cluster tokens based on position rather than\nsemantics, leading to imprecise aggregated representations. (2) Excessive\ncomputation waste: critical tokens are scattered among non-critical ones,\nleading to wasted computation on GPUs, which are optimized for processing\ncontiguous tokens. In this paper, we propose SVG2, a training-free framework\nthat maximizes identification accuracy and minimizes computation waste,\nachieving a Pareto frontier trade-off between generation quality and\nefficiency. The core of SVG2 is semantic-aware permutation, which clusters and\nreorders tokens based on semantic similarity using k-means. This approach\nensures both a precise cluster representation, improving identification\naccuracy, and a densified layout of critical tokens, enabling efficient\ncomputation without padding. Additionally, SVG2 integrates top-p dynamic budget\ncontrol and customized kernel implementations, achieving up to 2.30x and 1.89x\nspeedup while maintaining a PSNR of up to 30 and 26 on HunyuanVideo and Wan\n2.1, respectively."}
{"id": "2505.18880", "pdf": "https://arxiv.org/pdf/2505.18880", "abs": "https://arxiv.org/abs/2505.18880", "authors": ["Weihan Xu", "Yimeng Ma", "Jingyue Huang", "Yang Li", "Wenye Ma", "Taylor Berg-Kirkpatrick", "Julian McAuley", "Paul Pu Liang", "Hao-Wen Dong"], "title": "REGen: Multimodal Retrieval-Embedded Generation for Long-to-Short Video Editing", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Short videos are an effective tool for promoting contents and improving\nknowledge accessibility. While existing extractive video summarization methods\nstruggle to produce a coherent narrative, existing abstractive methods cannot\n`quote' from the input videos, i.e., inserting short video clips in their\noutputs. In this work, we explore novel video editing models for generating\nshorts that feature a coherent narrative with embedded video insertions\nextracted from a long input video. We propose a novel retrieval-embedded\ngeneration framework that allows a large language model to quote multimodal\nresources while maintaining a coherent narrative. Our proposed REGen system\nfirst generates the output story script with quote placeholders using a\nfinetuned large language model, and then uses a novel retrieval model to\nreplace the quote placeholders by selecting a video clip that best supports the\nnarrative from a pool of candidate quotable video clips. We examine the\nproposed method on the task of documentary teaser generation, where short\ninterview insertions are commonly used to support the narrative of a\ndocumentary. Our objective evaluations show that the proposed method can\neffectively insert short video clips while maintaining a coherent narrative. In\na subjective survey, we show that our proposed method outperforms existing\nabstractive and extractive approaches in terms of coherence, alignment, and\nrealism in teaser generation."}
{"id": "2505.18881", "pdf": "https://arxiv.org/pdf/2505.18881", "abs": "https://arxiv.org/abs/2505.18881", "authors": ["Dicong Qiu", "Jiadi You", "Zeying Gong", "Ronghe Qiu", "Hui Xiong", "Junwei Liang"], "title": "SD-OVON: A Semantics-aware Dataset and Benchmark Generation Pipeline for Open-Vocabulary Object Navigation in Dynamic Scenes", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "Preprint. 21 pages", "summary": "We present the Semantics-aware Dataset and Benchmark Generation Pipeline for\nOpen-vocabulary Object Navigation in Dynamic Scenes (SD-OVON). It utilizes\npretraining multimodal foundation models to generate infinite unique\nphoto-realistic scene variants that adhere to real-world semantics and daily\ncommonsense for the training and the evaluation of navigation agents,\naccompanied with a plugin for generating object navigation task episodes\ncompatible to the Habitat simulator. In addition, we offer two pre-generated\nobject navigation task datasets, SD-OVON-3k and SD-OVON-10k, comprising\nrespectively about 3k and 10k episodes of the open-vocabulary object navigation\ntask, derived from the SD-OVON-Scenes dataset with 2.5k photo-realistic scans\nof real-world environments and the SD-OVON-Objects dataset with 0.9k manually\ninspected scanned and artist-created manipulatable object models. Unlike prior\ndatasets limited to static environments, SD-OVON covers dynamic scenes and\nmanipulatable objects, facilitating both real-to-sim and sim-to-real robotic\napplications. This approach enhances the realism of navigation tasks, the\ntraining and the evaluation of open-vocabulary object navigation agents in\ncomplex settings. To demonstrate the effectiveness of our pipeline and\ndatasets, we propose two baselines and evaluate them along with\nstate-of-the-art baselines on SD-OVON-3k. The datasets, benchmark and source\ncode are publicly available."}
{"id": "2505.18899", "pdf": "https://arxiv.org/pdf/2505.18899", "abs": "https://arxiv.org/abs/2505.18899", "authors": ["Andrea Ramazzina", "Vittorio Giammarino", "Matteo El-Hariry", "Mario Bijelic"], "title": "Beyond Domain Randomization: Event-Inspired Perception for Visually Robust Adversarial Imitation from Videos", "categories": ["cs.CV", "cs.LG", "cs.RO"], "comment": null, "summary": "Imitation from videos often fails when expert demonstrations and learner\nenvironments exhibit domain shifts, such as discrepancies in lighting, color,\nor texture. While visual randomization partially addresses this problem by\naugmenting training data, it remains computationally intensive and inherently\nreactive, struggling with unseen scenarios. We propose a different approach:\ninstead of randomizing appearances, we eliminate their influence entirely by\nrethinking the sensory representation itself. Inspired by biological vision\nsystems that prioritize temporal transients (e.g., retinal ganglion cells) and\nby recent sensor advancements, we introduce event-inspired perception for\nvisually robust imitation. Our method converts standard RGB videos into a\nsparse, event-based representation that encodes temporal intensity gradients,\ndiscarding static appearance features. This biologically grounded approach\ndisentangles motion dynamics from visual style, enabling robust visual\nimitation from observations even in the presence of visual mismatches between\nexpert and agent environments. By training policies on event streams, we\nachieve invariance to appearance-based distractors without requiring\ncomputationally expensive and environment-specific data augmentation\ntechniques. Experiments across the DeepMind Control Suite and the Adroit\nplatform for dynamic dexterous manipulation show the efficacy of our method.\nOur code is publicly available at Eb-LAIfO."}
{"id": "2505.18915", "pdf": "https://arxiv.org/pdf/2505.18915", "abs": "https://arxiv.org/abs/2505.18915", "authors": ["Yixiong Chen", "Wenjie Xiao", "Pedro R. A. S. Bassi", "Xinze Zhou", "Sezgin Er", "Ibrahim Ethem Hamamci", "Zongwei Zhou", "Alan Yuille"], "title": "Are Vision Language Models Ready for Clinical Diagnosis? A 3D Medical Benchmark for Tumor-centric Visual Question Answering", "categories": ["cs.CV"], "comment": "NeurIPS 2025 datasets&benchmarks track submission", "summary": "Vision-Language Models (VLMs) have shown promise in various 2D visual tasks,\nyet their readiness for 3D clinical diagnosis remains unclear due to stringent\ndemands for recognition precision, reasoning ability, and domain knowledge. To\nsystematically evaluate these dimensions, we present DeepTumorVQA, a diagnostic\nvisual question answering (VQA) benchmark targeting abdominal tumors in CT\nscans. It comprises 9,262 CT volumes (3.7M slices) from 17 public datasets,\nwith 395K expert-level questions spanning four categories: Recognition,\nMeasurement, Visual Reasoning, and Medical Reasoning. DeepTumorVQA introduces\nunique challenges, including small tumor detection and clinical reasoning\nacross 3D anatomy. Benchmarking four advanced VLMs (RadFM, M3D, Merlin,\nCT-CHAT), we find current models perform adequately on measurement tasks but\nstruggle with lesion recognition and reasoning, and are still not meeting\nclinical needs. Two key insights emerge: (1) large-scale multimodal pretraining\nplays a crucial role in DeepTumorVQA testing performance, making RadFM stand\nout among all VLMs. (2) Our dataset exposes critical differences in VLM\ncomponents, where proper image preprocessing and design of vision modules\nsignificantly affect 3D perception. To facilitate medical multimodal research,\nwe have released DeepTumorVQA as a rigorous benchmark:\nhttps://github.com/Schuture/DeepTumorVQA."}
{"id": "2505.18924", "pdf": "https://arxiv.org/pdf/2505.18924", "abs": "https://arxiv.org/abs/2505.18924", "authors": ["Chenxi Li", "Nuo Chen", "Fengyun Tan", "Yantong Chen", "Bochun Yuan", "Tianrui Li", "Chongshou Li"], "title": "LLM-Guided Taxonomy and Hierarchical Uncertainty for 3D Point CLoud Active Learning", "categories": ["cs.CV"], "comment": null, "summary": "We present a novel active learning framework for 3D point cloud semantic\nsegmentation that, for the first time, integrates large language models (LLMs)\nto construct hierarchical label structures and guide uncertainty-based sample\nselection. Unlike prior methods that treat labels as flat and independent, our\napproach leverages LLM prompting to automatically generate multi-level semantic\ntaxonomies and introduces a recursive uncertainty projection mechanism that\npropagates uncertainty across hierarchy levels. This enables spatially diverse,\nlabel-aware point selection that respects the inherent semantic structure of 3D\nscenes. Experiments on S3DIS and ScanNet v2 show that our method achieves up to\n4% mIoU improvement under extremely low annotation budgets (e.g., 0.02%),\nsubstantially outperforming existing baselines. Our results highlight the\nuntapped potential of LLMs as knowledge priors in 3D vision and establish\nhierarchical uncertainty modeling as a powerful paradigm for efficient point\ncloud annotation."}
{"id": "2505.18925", "pdf": "https://arxiv.org/pdf/2505.18925", "abs": "https://arxiv.org/abs/2505.18925", "authors": ["Ross Greer", "Alisha Ukani", "Katherine Izhikevich", "Earlence Fernandes", "Stefan Savage", "Alex C. Snoeren"], "title": "Words as Geometric Features: Estimating Homography using Optical Character Recognition as Compressed Image Representation", "categories": ["cs.CV"], "comment": null, "summary": "Document alignment and registration play a crucial role in numerous\nreal-world applications, such as automated form processing, anomaly detection,\nand workflow automation. Traditional methods for document alignment rely on\nimage-based features like keypoints, edges, and textures to estimate geometric\ntransformations, such as homographies. However, these approaches often require\naccess to the original document images, which may not always be available due\nto privacy, storage, or transmission constraints. This paper introduces a novel\napproach that leverages Optical Character Recognition (OCR) outputs as features\nfor homography estimation. By utilizing the spatial positions and textual\ncontent of OCR-detected words, our method enables document alignment without\nrelying on pixel-level image data. This technique is particularly valuable in\nscenarios where only OCR outputs are accessible. Furthermore, the method is\nrobust to OCR noise, incorporating RANSAC to handle outliers and inaccuracies\nin the OCR data. On a set of test documents, we demonstrate that our OCR-based\napproach even performs more accurately than traditional image-based methods,\noffering a more efficient and scalable solution for document registration\ntasks. The proposed method facilitates applications in document processing, all\nwhile reducing reliance on high-dimensional image data."}
{"id": "2505.18930", "pdf": "https://arxiv.org/pdf/2505.18930", "abs": "https://arxiv.org/abs/2505.18930", "authors": ["Yanben Shen", "Timilehin T. Ayanlade", "Venkata Naresh Boddepalli", "Mojdeh Saadati", "Ashlyn Rairdin", "Zi K. Deng", "Muhammad Arbab Arshad", "Aditya Balu", "Daren Mueller", "Asheesh K Singh", "Wesley Everman", "Nirav Merchant", "Baskar Ganapathysubramanian", "Meaghan Anderson", "Soumik Sarkar", "Arti Singh"], "title": "WeedNet: A Foundation Model-Based Global-to-Local AI Approach for Real-Time Weed Species Identification and Classification", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Early identification of weeds is essential for effective management and\ncontrol, and there is growing interest in automating the process using computer\nvision techniques coupled with AI methods. However, challenges associated with\ntraining AI-based weed identification models, such as limited expert-verified\ndata and complexity and variability in morphological features, have hindered\nprogress. To address these issues, we present WeedNet, the first global-scale\nweed identification model capable of recognizing an extensive set of weed\nspecies, including noxious and invasive plant species. WeedNet is an end-to-end\nreal-time weed identification pipeline and uses self-supervised learning,\nfine-tuning, and enhanced trustworthiness strategies. WeedNet achieved 91.02%\naccuracy across 1,593 weed species, with 41% species achieving 100% accuracy.\nUsing a fine-tuning strategy and a Global-to-Local approach, the local Iowa\nWeedNet model achieved an overall accuracy of 97.38% for 85 Iowa weeds, most\nclasses exceeded a 90% mean accuracy per class. Testing across intra-species\ndissimilarity (developmental stages) and inter-species similarity (look-alike\nspecies) suggests that diversity in the images collected, spanning all the\ngrowth stages and distinguishable plant characteristics, is crucial in driving\nmodel performance. The generalizability and adaptability of the Global WeedNet\nmodel enable it to function as a foundational model, with the Global-to-Local\nstrategy allowing fine-tuning for region-specific weed communities. Additional\nvalidation of drone- and ground-rover-based images highlights the potential of\nWeedNet for integration into robotic platforms. Furthermore, integration with\nAI for conversational use provides intelligent agricultural and ecological\nconservation consulting tools for farmers, agronomists, researchers, land\nmanagers, and government agencies across diverse landscapes."}
{"id": "2505.18932", "pdf": "https://arxiv.org/pdf/2505.18932", "abs": "https://arxiv.org/abs/2505.18932", "authors": ["Hyunho Ha", "Lei Xiao", "Christian Richardt", "Thu Nguyen-Phuoc", "Changil Kim", "Min H. Kim", "Douglas Lanman", "Numair Khan"], "title": "Geometry-guided Online 3D Video Synthesis with Multi-View Temporal Consistency", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025. Project website:\n  https://nkhan2.github.io/projects/geometry-guided-2025/index.html", "summary": "We introduce a novel geometry-guided online video view synthesis method with\nenhanced view and temporal consistency. Traditional approaches achieve\nhigh-quality synthesis from dense multi-view camera setups but require\nsignificant computational resources. In contrast, selective-input methods\nreduce this cost but often compromise quality, leading to multi-view and\ntemporal inconsistencies such as flickering artifacts. Our method addresses\nthis challenge to deliver efficient, high-quality novel-view synthesis with\nview and temporal consistency. The key innovation of our approach lies in using\nglobal geometry to guide an image-based rendering pipeline. To accomplish this,\nwe progressively refine depth maps using color difference masks across time.\nThese depth maps are then accumulated through truncated signed distance fields\nin the synthesized view's image space. This depth representation is view and\ntemporally consistent, and is used to guide a pre-trained blending network that\nfuses multiple forward-rendered input-view images. Thus, the network is\nencouraged to output geometrically consistent synthesis results across multiple\nviews and time. Our approach achieves consistent, high-quality video synthesis,\nwhile running efficiently in an online manner."}
{"id": "2505.18945", "pdf": "https://arxiv.org/pdf/2505.18945", "abs": "https://arxiv.org/abs/2505.18945", "authors": ["Jintao Sun", "Hu Zhang", "Gangyi Ding", "Zhedong Zheng"], "title": "Echo Planning for Autonomous Driving: From Current Observations to Future Trajectories and Back", "categories": ["cs.CV", "cs.RO"], "comment": "13 pages, 4 figures", "summary": "Modern end-to-end autonomous driving systems suffer from a critical\nlimitation: their planners lack mechanisms to enforce temporal consistency\nbetween predicted trajectories and evolving scene dynamics. This absence of\nself-supervision allows early prediction errors to compound catastrophically\nover time. We introduce Echo Planning, a novel self-correcting framework that\nestablishes a closed-loop Current - Future - Current (CFC) cycle to harmonize\ntrajectory prediction with scene coherence. Our key insight is that plausible\nfuture trajectories must be bi-directionally consistent, ie, not only generated\nfrom current observations but also capable of reconstructing them. The CFC\nmechanism first predicts future trajectories from the Bird's-Eye-View (BEV)\nscene representation, then inversely maps these trajectories back to estimate\nthe current BEV state. By enforcing consistency between the original and\nreconstructed BEV representations through a cycle loss, the framework\nintrinsically penalizes physically implausible or misaligned trajectories.\nExperiments on nuScenes demonstrate state-of-the-art performance, reducing L2\nerror by 0.04 m and collision rate by 0.12% compared to one-shot planners.\nCrucially, our method requires no additional supervision, leveraging the CFC\ncycle as an inductive bias for robust planning. This work offers a deployable\nsolution for safety-critical autonomous systems."}
{"id": "2505.18947", "pdf": "https://arxiv.org/pdf/2505.18947", "abs": "https://arxiv.org/abs/2505.18947", "authors": ["Zhenhao Zhang", "Ye Shi", "Lingxiao Yang", "Suting Ni", "Qi Ye", "Jingya Wang"], "title": "OpenHOI: Open-World Hand-Object Interaction Synthesis with Multimodal Large Language Model", "categories": ["cs.CV"], "comment": null, "summary": "Understanding and synthesizing realistic 3D hand-object interactions (HOI) is\ncritical for applications ranging from immersive AR/VR to dexterous robotics.\nExisting methods struggle with generalization, performing well on closed-set\nobjects and predefined tasks but failing to handle unseen objects or\nopen-vocabulary instructions. We introduce OpenHOI, the first framework for\nopen-world HOI synthesis, capable of generating long-horizon manipulation\nsequences for novel objects guided by free-form language commands. Our approach\nintegrates a 3D Multimodal Large Language Model (MLLM) fine-tuned for joint\naffordance grounding and semantic task decomposition, enabling precise\nlocalization of interaction regions (e.g., handles, buttons) and breakdown of\ncomplex instructions (e.g., \"Find a water bottle and take a sip\") into\nexecutable sub-tasks. To synthesize physically plausible interactions, we\npropose an affordance-driven diffusion model paired with a training-free\nphysics refinement stage that minimizes penetration and optimizes affordance\nalignment. Evaluations across diverse scenarios demonstrate OpenHOI's\nsuperiority over state-of-the-art methods in generalizing to novel object\ncategories, multi-stage tasks, and complex language instructions. Our project\npage at \\href{https://openhoi.github.io}"}
{"id": "2505.18956", "pdf": "https://arxiv.org/pdf/2505.18956", "abs": "https://arxiv.org/abs/2505.18956", "authors": ["Yining Pan", "Qiongjie Cui", "Xulei Yang", "Na Zhao"], "title": "How Do Images Align and Complement LiDAR? Towards a Harmonized Multi-modal 3D Panoptic Segmentation", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM"], "comment": "Accepted at the 2025 International Conference on Machine Learning\n  (ICML)", "summary": "LiDAR-based 3D panoptic segmentation often struggles with the inherent\nsparsity of data from LiDAR sensors, which makes it challenging to accurately\nrecognize distant or small objects. Recently, a few studies have sought to\novercome this challenge by integrating LiDAR inputs with camera images,\nleveraging the rich and dense texture information provided by the latter. While\nthese approaches have shown promising results, they still face challenges, such\nas misalignment during data augmentation and the reliance on post-processing\nsteps. To address these issues, we propose Image-Assists-LiDAR (IAL), a novel\nmulti-modal 3D panoptic segmentation framework. In IAL, we first introduce a\nmodality-synchronized data augmentation strategy, PieAug, to ensure alignment\nbetween LiDAR and image inputs from the start. Next, we adopt a transformer\ndecoder to directly predict panoptic segmentation results. To effectively fuse\nLiDAR and image features into tokens for the decoder, we design a\nGeometric-guided Token Fusion (GTF) module. Additionally, we leverage the\ncomplementary strengths of each modality as priors for query initialization\nthrough a Prior-based Query Generation (PQG) module, enhancing the decoder's\nability to generate accurate instance masks. Our IAL framework achieves\nstate-of-the-art performance compared to previous multi-modal 3D panoptic\nsegmentation methods on two widely used benchmarks. Code and models are\npublicly available at <https://github.com/IMPL-Lab/IAL.git>."}
{"id": "2505.18958", "pdf": "https://arxiv.org/pdf/2505.18958", "abs": "https://arxiv.org/abs/2505.18958", "authors": ["Jiong Wu", "Yang Xing", "Boxiao Yu", "Wei Shao", "Kuang Gong"], "title": "CDPDNet: Integrating Text Guidance with Hybrid Vision Encoders for Medical Image Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Most publicly available medical segmentation datasets are only partially\nlabeled, with annotations provided for a subset of anatomical structures. When\nmultiple datasets are combined for training, this incomplete annotation poses\nchallenges, as it limits the model's ability to learn shared anatomical\nrepresentations among datasets. Furthermore, vision-only frameworks often fail\nto capture complex anatomical relationships and task-specific distinctions,\nleading to reduced segmentation accuracy and poor generalizability to unseen\ndatasets. In this study, we proposed a novel CLIP-DINO Prompt-Driven\nSegmentation Network (CDPDNet), which combined a self-supervised vision\ntransformer with CLIP-based text embedding and introduced task-specific text\nprompts to tackle these challenges. Specifically, the framework was constructed\nupon a convolutional neural network (CNN) and incorporated DINOv2 to extract\nboth fine-grained and global visual features, which were then fused using a\nmulti-head cross-attention module to overcome the limited long-range modeling\ncapability of CNNs. In addition, CLIP-derived text embeddings were projected\ninto the visual space to help model complex relationships among organs and\ntumors. To further address the partial label challenge and enhance inter-task\ndiscriminative capability, a Text-based Task Prompt Generation (TTPG) module\nthat generated task-specific prompts was designed to guide the segmentation.\nExtensive experiments on multiple medical imaging datasets demonstrated that\nCDPDNet consistently outperformed existing state-of-the-art segmentation\nmethods. Code and pretrained model are available at:\nhttps://github.com/wujiong-hub/CDPDNet.git."}
{"id": "2505.18963", "pdf": "https://arxiv.org/pdf/2505.18963", "abs": "https://arxiv.org/abs/2505.18963", "authors": ["Jeffrey A. Chan-Santiago", "Praveen Tirupattur", "Gaurav Kumar Nayak", "Gaowen Liu", "Mubarak Shah"], "title": "MGD$^3$: Mode-Guided Dataset Distillation using Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "Dataset distillation has emerged as an effective strategy, significantly\nreducing training costs and facilitating more efficient model deployment.\nRecent advances have leveraged generative models to distill datasets by\ncapturing the underlying data distribution. Unfortunately, existing methods\nrequire model fine-tuning with distillation losses to encourage diversity and\nrepresentativeness. However, these methods do not guarantee sample diversity,\nlimiting their performance. We propose a mode-guided diffusion model leveraging\na pre-trained diffusion model without the need to fine-tune with distillation\nlosses. Our approach addresses dataset diversity in three stages: Mode\nDiscovery to identify distinct data modes, Mode Guidance to enhance intra-class\ndiversity, and Stop Guidance to mitigate artifacts in synthetic samples that\naffect performance. Our approach outperforms state-of-the-art methods,\nachieving accuracy gains of 4.4%, 2.9%, 1.6%, and 1.6% on ImageNette, ImageIDC,\nImageNet-100, and ImageNet-1K, respectively. Our method eliminates the need for\nfine-tuning diffusion models with distillation losses, significantly reducing\ncomputational costs. Our code is available on the project webpage:\nhttps://jachansantiago.github.io/mode-guided-distillation/"}
{"id": "2505.18986", "pdf": "https://arxiv.org/pdf/2505.18986", "abs": "https://arxiv.org/abs/2505.18986", "authors": ["Zhiwei Lin", "Yongtao Wang"], "title": "VL-SAM-V2: Open-World Object Detection with General and Specific Query Fusion", "categories": ["cs.CV"], "comment": null, "summary": "Current perception models have achieved remarkable success by leveraging\nlarge-scale labeled datasets, but still face challenges in open-world\nenvironments with novel objects. To address this limitation, researchers\nintroduce open-set perception models to detect or segment arbitrary test-time\nuser-input categories. However, open-set models rely on human involvement to\nprovide predefined object categories as input during inference. More recently,\nresearchers have framed a more realistic and challenging task known as\nopen-ended perception that aims to discover unseen objects without requiring\nany category-level input from humans at inference time. Nevertheless,\nopen-ended models suffer from low performance compared to open-set models. In\nthis paper, we present VL-SAM-V2, an open-world object detection framework that\nis capable of discovering unseen objects while achieving favorable performance.\nTo achieve this, we combine queries from open-set and open-ended models and\npropose a general and specific query fusion module to allow different queries\nto interact. By adjusting queries from open-set models, we enable VL-SAM-V2 to\nbe evaluated in the open-set or open-ended mode. In addition, to learn more\ndiverse queries, we introduce ranked learnable queries to match queries with\nproposals from open-ended models by sorting. Moreover, we design a denoising\npoint training strategy to facilitate the training process. Experimental\nresults on LVIS show that our method surpasses the previous open-set and\nopen-ended methods, especially on rare objects."}
{"id": "2505.18988", "pdf": "https://arxiv.org/pdf/2505.18988", "abs": "https://arxiv.org/abs/2505.18988", "authors": ["Varun Jain", "Zongwei Wu", "Quan Zou", "Louis Florentin", "Henrik Turbell", "Sandeep Siddhartha", "Radu Timofte", "others"], "title": "NTIRE 2025 Challenge on Video Quality Enhancement for Video Conferencing: Datasets, Methods and Results", "categories": ["cs.CV"], "comment": null, "summary": "This paper presents a comprehensive review of the 1st Challenge on Video\nQuality Enhancement for Video Conferencing held at the NTIRE workshop at CVPR\n2025, and highlights the problem statement, datasets, proposed solutions, and\nresults. The aim of this challenge was to design a Video Quality Enhancement\n(VQE) model to enhance video quality in video conferencing scenarios by (a)\nimproving lighting, (b) enhancing colors, (c) reducing noise, and (d) enhancing\nsharpness - giving a professional studio-like effect. Participants were given a\ndifferentiable Video Quality Assessment (VQA) model, training, and test videos.\nA total of 91 participants registered for the challenge. We received 10 valid\nsubmissions that were evaluated in a crowdsourced framework."}
{"id": "2505.18989", "pdf": "https://arxiv.org/pdf/2505.18989", "abs": "https://arxiv.org/abs/2505.18989", "authors": ["Catalina Tan", "Yipeng Hu", "Shaheer U. Saeed"], "title": "SPARS: Self-Play Adversarial Reinforcement Learning for Segmentation of Liver Tumours", "categories": ["cs.CV"], "comment": "Accepted at Medical Image Understanding and Analysis (MIUA) 2025", "summary": "Accurate tumour segmentation is vital for various targeted diagnostic and\ntherapeutic procedures for cancer, e.g., planning biopsies or tumour ablations.\nManual delineation is extremely labour-intensive, requiring substantial expert\ntime. Fully-supervised machine learning models aim to automate such\nlocalisation tasks, but require a large number of costly and often subjective\n3D voxel-level labels for training. The high-variance and subjectivity in such\nlabels impacts model generalisability, even when large datasets are available.\nHistopathology labels may offer more objective labels but the infeasibility of\nacquiring pixel-level annotations to develop tumour localisation methods based\non histology remains challenging in-vivo. In this work, we propose a novel\nweakly-supervised semantic segmentation framework called SPARS (Self-Play\nAdversarial Reinforcement Learning for Segmentation), which utilises an object\npresence classifier, trained on a small number of image-level binary cancer\npresence labels, to localise cancerous regions on CT scans. Such binary labels\nof patient-level cancer presence can be sourced more feasibly from biopsies and\nhistopathology reports, enabling a more objective cancer localisation on\nmedical images. Evaluating with real patient data, we observed that SPARS\nyielded a mean dice score of $77.3 \\pm 9.4$, which outperformed other\nweakly-supervised methods by large margins. This performance was comparable\nwith recent fully-supervised methods that require voxel-level annotations. Our\nresults demonstrate the potential of using SPARS to reduce the need for\nextensive human-annotated labels to detect cancer in real-world healthcare\nsettings."}
{"id": "2505.18991", "pdf": "https://arxiv.org/pdf/2505.18991", "abs": "https://arxiv.org/abs/2505.18991", "authors": ["Hancong Jin", "Zihan Cao", "Liangjian Deng"], "title": "Kernel Space Diffusion Model for Efficient Remote Sensing Pansharpening", "categories": ["cs.CV"], "comment": null, "summary": "Pansharpening is a fundamental task in remote sensing that integrates\nhigh-resolution panchromatic imagery (PAN) with low-resolution multispectral\nimagery (LRMS) to produce an enhanced image with both high spatial and spectral\nresolution. Despite significant progress in deep learning-based approaches,\nexisting methods often fail to capture the global priors inherent in remote\nsensing data distributions. Diffusion-based models have recently emerged as\npromising solutions due to their powerful distribution mapping capabilities;\nhowever, they suffer from significant inference latency, which limits their\npractical applicability. In this work, we propose the Kernel Space Diffusion\nModel (KSDiff), a novel approach that leverages diffusion processes in a latent\nspace to generate convolutional kernels enriched with global contextual\ninformation, thereby improving pansharpening quality while enabling faster\ninference. Specifically, KSDiff constructs these kernels through the\nintegration of a low-rank core tensor generator and a unified factor generator,\norchestrated by a structure-aware multi-head attention mechanism. We further\nintroduce a two-stage training strategy tailored for pansharpening, enabling\nKSDiff to serve as a framework for enhancing existing pansharpening\narchitectures. Experiments on three widely used datasets, including\nWorldView-3, GaoFen-2, and QuickBird, demonstrate the superior performance of\nKSDiff both qualitatively and quantitatively. Code will be released upon\npossible acceptance."}
{"id": "2505.18992", "pdf": "https://arxiv.org/pdf/2505.18992", "abs": "https://arxiv.org/abs/2505.18992", "authors": ["Tianchen Deng", "Wenhua Wu", "Junjie He", "Yue Pan", "Xirui Jiang", "Shenghai Yuan", "Danwei Wang", "Hesheng Wang", "Weidong Chen"], "title": "VPGS-SLAM: Voxel-based Progressive 3D Gaussian SLAM in Large-Scale Scenes", "categories": ["cs.CV"], "comment": null, "summary": "3D Gaussian Splatting has recently shown promising results in dense visual\nSLAM. However, existing 3DGS-based SLAM methods are all constrained to\nsmall-room scenarios and struggle with memory explosion in large-scale scenes\nand long sequences. To this end, we propose VPGS-SLAM, the first 3DGS-based\nlarge-scale RGBD SLAM framework for both indoor and outdoor scenarios. We\ndesign a novel voxel-based progressive 3D Gaussian mapping method with multiple\nsubmaps for compact and accurate scene representation in large-scale and\nlong-sequence scenes. This allows us to scale up to arbitrary scenes and\nimproves robustness (even under pose drifts). In addition, we propose a 2D-3D\nfusion camera tracking method to achieve robust and accurate camera tracking in\nboth indoor and outdoor large-scale scenes. Furthermore, we design a 2D-3D\nGaussian loop closure method to eliminate pose drift. We further propose a\nsubmap fusion method with online distillation to achieve global consistency in\nlarge-scale scenes when detecting a loop. Experiments on various indoor and\noutdoor datasets demonstrate the superiority and generalizability of the\nproposed framework. The code will be open source on\nhttps://github.com/dtc111111/vpgs-slam."}
{"id": "2505.19010", "pdf": "https://arxiv.org/pdf/2505.19010", "abs": "https://arxiv.org/abs/2505.19010", "authors": ["Md. Mithun Hossain", "Md. Shakil Hossain", "Sudipto Chaki", "M. F. Mridha"], "title": "Co-AttenDWG: Co-Attentive Dimension-Wise Gating and Expert Fusion for Multi-Modal Offensive Content Detection", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Multi-modal learning has become a critical research area because integrating\ntext and image data can significantly improve performance in tasks such as\nclassification, retrieval, and scene understanding. However, despite progress\nwith pre-trained models, current approaches are limited by inadequate\ncross-modal interactions and static fusion strategies that do not fully exploit\nthe complementary nature of different modalities. To address these\nshortcomings, we introduce a novel multi-modal Co-AttenDWG architecture that\nleverages dual-path encoding, co-attention with dimension-wise gating, and\nadvanced expert fusion. Our approach begins by projecting text and image\nfeatures into a common embedding space, where a dedicated co-attention\nmechanism enables simultaneous, fine-grained interactions between modalities.\nThis mechanism is further enhanced by a dimension-wise gating network that\nadaptively regulates the feature contributions at the channel level, ensuring\nthat only the most relevant information is emphasized. In parallel, dual-path\nencoders refine the representations by processing cross-modal information\nseparately before an additional cross-attention layer further aligns\nmodalities. The refined features are then aggregated via an expert fusion\nmodule that combines learned gating and self-attention to produce a robust,\nunified representation. We validate our approach on the MIMIC and SemEval\nMemotion 1.0, where experimental results demonstrate significant improvements\nin cross-modal alignment and state-of-the-art performance, underscoring the\npotential of our model for a wide range of multi-modal applications."}
{"id": "2505.19015", "pdf": "https://arxiv.org/pdf/2505.19015", "abs": "https://arxiv.org/abs/2505.19015", "authors": ["Jingping Liu", "Ziyan Liu", "Zhedong Cen", "Yan Zhou", "Yinan Zou", "Weiyan Zhang", "Haiyun Jiang", "Tong Ruan"], "title": "Can Multimodal Large Language Models Understand Spatial Relations?", "categories": ["cs.CV", "cs.MM"], "comment": "13 pages, 19 figures", "summary": "Spatial relation reasoning is a crucial task for multimodal large language\nmodels (MLLMs) to understand the objective world. However, current benchmarks\nhave issues like relying on bounding boxes, ignoring perspective substitutions,\nor allowing questions to be answered using only the model's prior knowledge\nwithout image understanding. To address these issues, we introduce SpatialMQA,\na human-annotated spatial relation reasoning benchmark based on COCO2017, which\nenables MLLMs to focus more on understanding images in the objective world. To\nensure data quality, we design a well-tailored annotation procedure, resulting\nin SpatialMQA consisting of 5,392 samples. Based on this benchmark, a series of\nclosed- and open-source MLLMs are implemented and the results indicate that the\ncurrent state-of-the-art MLLM achieves only 48.14% accuracy, far below the\nhuman-level accuracy of 98.40%. Extensive experimental analyses are also\nconducted, suggesting the future research directions. The benchmark and codes\nare available at https://github.com/ziyan-xiaoyu/SpatialMQA.git."}
{"id": "2505.19022", "pdf": "https://arxiv.org/pdf/2505.19022", "abs": "https://arxiv.org/abs/2505.19022", "authors": ["Zihao Liu", "Xiaoyu Wu", "Wenna Li", "Linlin Yang"], "title": "Rethinking Metrics and Benchmarks of Video Anomaly Detection", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Video Anomaly Detection (VAD), which aims to detect anomalies that deviate\nfrom expectation, has attracted increasing attention in recent years. Existing\nadvancements in VAD primarily focus on model architectures and training\nstrategies, while devoting insufficient attention to evaluation metrics and\nbenchmarks. In this paper, we rethink VAD evaluation protocols through\ncomprehensive experimental analyses, revealing three critical limitations in\ncurrent practices: 1) existing metrics are significantly influenced by single\nannotation bias; 2) current metrics fail to reward early detection of\nanomalies; 3) available benchmarks lack the capability to evaluate scene\noverfitting. To address these limitations, we propose three novel evaluation\nmethods: first, we establish averaged AUC/AP metrics over multi-round\nannotations to mitigate single annotation bias; second, we develop a\nLatency-aware Average Precision (LaAP) metric that rewards early and accurate\nanomaly detection; and finally, we introduce two hard normal benchmarks\n(UCF-HN, MSAD-HN) with videos specifically designed to evaluate scene\noverfitting. We report performance comparisons of ten state-of-the-art VAD\napproaches using our proposed evaluation methods, providing novel perspectives\nfor future VAD model development."}
{"id": "2505.19023", "pdf": "https://arxiv.org/pdf/2505.19023", "abs": "https://arxiv.org/abs/2505.19023", "authors": ["Huda Alghoraibi", "Nuha Alqurashi", "Sarah Alotaibi", "Renad Alkhudaydi", "Bdoor Aldajani", "Lubna Alqurashi", "Jood Batweel", "Maha A. Thafar"], "title": "A Smart Healthcare System for Monkeypox Skin Lesion Detection and Tracking", "categories": ["cs.CV", "cs.AI", "cs.ET", "cs.LG"], "comment": "23 pages, 5 figures", "summary": "Monkeypox is a viral disease characterized by distinctive skin lesions and\nhas been reported in many countries. The recent global outbreak has emphasized\nthe urgent need for scalable, accessible, and accurate diagnostic solutions to\nsupport public health responses.\n  In this study, we developed ITMAINN, an intelligent, AI-driven healthcare\nsystem specifically designed to detect Monkeypox from skin lesion images using\nadvanced deep learning techniques. Our system consists of three main\ncomponents. First, we trained and evaluated several pretrained models using\ntransfer learning on publicly available skin lesion datasets to identify the\nmost effective models. For binary classification (Monkeypox vs. non-Monkeypox),\nthe Vision Transformer, MobileViT, Transformer-in-Transformer, and VGG16\nachieved the highest performance, each with an accuracy and F1-score of 97.8%.\nFor multiclass classification, which contains images of patients with Monkeypox\nand five other classes (chickenpox, measles, hand-foot-mouth disease, cowpox,\nand healthy), ResNetViT and ViT Hybrid models achieved 92% accuracy, with F1\nscores of 92.24% and 92.19%, respectively. The best-performing and most\nlightweight model, MobileViT, was deployed within the mobile application. The\nsecond component is a cross-platform smartphone application that enables users\nto detect Monkeypox through image analysis, track symptoms, and receive\nrecommendations for nearby healthcare centers based on their location. The\nthird component is a real-time monitoring dashboard designed for health\nauthorities to support them in tracking cases, analyzing symptom trends,\nguiding public health interventions, and taking proactive measures.\n  This system is fundamental in developing responsive healthcare infrastructure\nwithin smart cities. Our solution, ITMAINN, is part of revolutionizing public\nhealth management."}
{"id": "2505.19028", "pdf": "https://arxiv.org/pdf/2505.19028", "abs": "https://arxiv.org/abs/2505.19028", "authors": ["Minzhi Lin", "Tianchi Xie", "Mengchen Liu", "Yilin Ye", "Changjian Chen", "Shixia Liu"], "title": "InfoChartQA: A Benchmark for Multimodal Question Answering on Infographic Charts", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Understanding infographic charts with design-driven visual elements (e.g.,\npictograms, icons) requires both visual recognition and reasoning, posing\nchallenges for multimodal large language models (MLLMs). However, existing\nvisual-question answering benchmarks fall short in evaluating these\ncapabilities of MLLMs due to the lack of paired plain charts and\nvisual-element-based questions. To bridge this gap, we introduce InfoChartQA, a\nbenchmark for evaluating MLLMs on infographic chart understanding. It includes\n5,642 pairs of infographic and plain charts, each sharing the same underlying\ndata but differing in visual presentations. We further design\nvisual-element-based questions to capture their unique visual designs and\ncommunicative intent. Evaluation of 20 MLLMs reveals a substantial performance\ndecline on infographic charts, particularly for visual-element-based questions\nrelated to metaphors. The paired infographic and plain charts enable\nfine-grained error analysis and ablation studies, which highlight new\nopportunities for advancing MLLMs in infographic chart understanding. We\nrelease InfoChartQA at https://github.com/CoolDawnAnt/InfoChartQA."}
{"id": "2505.19031", "pdf": "https://arxiv.org/pdf/2505.19031", "abs": "https://arxiv.org/abs/2505.19031", "authors": ["Xikai Yang", "Juzheng Miao", "Yuchen Yuan", "Jiaze Wang", "Qi Dou", "Jinpeng Li", "Pheng-Ann Heng"], "title": "Medical Large Vision Language Models with Multi-Image Visual Ability", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages, 4 figures", "summary": "Medical large vision-language models (LVLMs) have demonstrated promising\nperformance across various single-image question answering (QA) benchmarks, yet\ntheir capability in processing multi-image clinical scenarios remains\nunderexplored. Unlike single image based tasks, medical tasks involving\nmultiple images often demand sophisticated visual understanding capabilities,\nsuch as temporal reasoning and cross-modal analysis, which are poorly supported\nby current medical LVLMs. To bridge this critical gap, we present the Med-MIM\ninstruction dataset, comprising 83.2K medical multi-image QA pairs that span\nfour types of multi-image visual abilities (temporal understanding, reasoning,\ncomparison, co-reference). Using this dataset, we fine-tune Mantis and\nLLaVA-Med, resulting in two specialized medical VLMs: MIM-LLaVA-Med and\nMed-Mantis, both optimized for multi-image analysis. Additionally, we develop\nthe Med-MIM benchmark to comprehensively evaluate the medical multi-image\nunderstanding capabilities of LVLMs. We assess eight popular LVLMs, including\nour two models, on the Med-MIM benchmark. Experimental results show that both\nMed-Mantis and MIM-LLaVA-Med achieve superior performance on the held-in and\nheld-out subsets of the Med-MIM benchmark, demonstrating that the Med-MIM\ninstruction dataset effectively enhances LVLMs' multi-image understanding\ncapabilities in the medical domain."}
{"id": "2505.19049", "pdf": "https://arxiv.org/pdf/2505.19049", "abs": "https://arxiv.org/abs/2505.19049", "authors": ["Lu Wang", "Xishuai Peng", "S. Kevin Zhou"], "title": "Disentangled Human Body Representation Based on Unsupervised Semantic-Aware Learning", "categories": ["cs.CV"], "comment": "8 pages", "summary": "In recent years, more and more attention has been paid to the learning of 3D\nhuman representation. However, the complexity of lots of hand-defined human\nbody constraints and the absence of supervision data limit that the existing\nworks controllably and accurately represent the human body in views of\nsemantics and representation ability. In this paper, we propose a human body\nrepresentation with controllable fine-grained semantics and high precison of\nreconstruction in an unsupervised learning framework. In particularly, we\ndesign a whole-aware skeleton-grouped disentangle strategy to learn a\ncorrespondence between geometric semantical measurement of body and latent\ncodes, which facilitates the control of shape and posture of human body by\nmodifying latent coding paramerers. With the help of skeleton-grouped\nwhole-aware encoder and unsupervised disentanglement losses, our representation\nmodel is learned by an unsupervised manner. Besides, a based-template residual\nlearning scheme is injected into the encoder to ease of learning human body\nlatent parameter in complicated body shape and pose spaces. Because of the\ngeometrically meaningful latent codes, it can be used in a wide range of\napplications, from human body pose transfer to bilinear latent code\ninterpolation. Further more, a part-aware decoder is utlized to promote the\nlearning of controllable fine-grained semantics. The experimental results on\npublic 3D human datasets show that the method has the ability of precise\nreconstruction."}
{"id": "2505.19057", "pdf": "https://arxiv.org/pdf/2505.19057", "abs": "https://arxiv.org/abs/2505.19057", "authors": ["Pedro Alonso", "Tianrui Li", "Chongshou Li"], "title": "Less is More: Efficient Point Cloud Reconstruction via Multi-Head Decoders", "categories": ["cs.CV"], "comment": null, "summary": "We challenge the common assumption that deeper decoder architectures always\nyield better performance in point cloud reconstruction. Our analysis reveals\nthat, beyond a certain depth, increasing decoder complexity leads to\noverfitting and degraded generalization. Additionally, we propose a novel\nmulti-head decoder architecture that exploits the inherent redundancy in point\nclouds by reconstructing complete shapes from multiple independent heads, each\noperating on a distinct subset of points. The final output is obtained by\nconcatenating the predictions from all heads, enhancing both diversity and\nfidelity. Extensive experiments on ModelNet40 and ShapeNetPart demonstrate that\nour approach achieves consistent improvements across key metrics--including\nChamfer Distance (CD), Hausdorff Distance (HD), Earth Mover's Distance (EMD),\nand F1-score--outperforming standard single-head baselines. Our findings\nhighlight that output diversity and architectural design can be more critical\nthan depth alone for effective and efficient point cloud reconstruction."}
{"id": "2505.19063", "pdf": "https://arxiv.org/pdf/2505.19063", "abs": "https://arxiv.org/abs/2505.19063", "authors": ["Xin Ma", "Yaohui Wang", "Xinyuan Chen", "Tien-Tsin Wong", "Cunjian Chen"], "title": "Training-free Stylized Text-to-Image Generation with Fast Inference", "categories": ["cs.CV"], "comment": "Project Page: https://maxin-cn.github.io/omnipainter_project", "summary": "Although diffusion models exhibit impressive generative capabilities,\nexisting methods for stylized image generation based on these models often\nrequire textual inversion or fine-tuning with style images, which is\ntime-consuming and limits the practical applicability of large-scale diffusion\nmodels. To address these challenges, we propose a novel stylized image\ngeneration method leveraging a pre-trained large-scale diffusion model without\nrequiring fine-tuning or any additional optimization, termed as OmniPainter.\nSpecifically, we exploit the self-consistency property of latent consistency\nmodels to extract the representative style statistics from reference style\nimages to guide the stylization process. Additionally, we then introduce the\nnorm mixture of self-attention, which enables the model to query the most\nrelevant style patterns from these statistics for the intermediate output\ncontent features. This mechanism also ensures that the stylized results align\nclosely with the distribution of the reference style images. Our qualitative\nand quantitative experimental results demonstrate that the proposed method\noutperforms state-of-the-art approaches."}
{"id": "2505.19065", "pdf": "https://arxiv.org/pdf/2505.19065", "abs": "https://arxiv.org/abs/2505.19065", "authors": ["Jiashuo Chang", "Zhengyi Li", "Jianxun Lou", "Zhen Qiu", "Hanhe Lin"], "title": "MMP-2K: A Benchmark Multi-Labeled Macro Photography Image Quality Assessment Database", "categories": ["cs.CV"], "comment": "Accepted to the IEEE International Conference on Image Processing,\n  IEEE ICIP 2025", "summary": "Macro photography (MP) is a specialized field of photography that captures\nobjects at an extremely close range, revealing tiny details. Although an\naccurate macro photography image quality assessment (MPIQA) metric can benefit\nmacro photograph capturing, which is vital in some domains such as scientific\nresearch and medical applications, the lack of MPIQA data limits the\ndevelopment of MPIQA metrics. To address this limitation, we conducted a\nlarge-scale MPIQA study. Specifically, to ensure diversity both in content and\nquality, we sampled 2,000 MP images from 15,700 MP images, collected from three\npublic image websites. For each MP image, 17 (out of 21 after outlier removal)\nquality ratings and a detailed quality report of distortion magnitudes, types,\nand positions are gathered by a lab study. The images, quality ratings, and\nquality reports form our novel multi-labeled MPIQA database, MMP-2k.\nExperimental results showed that the state-of-the-art generic IQA metrics\nunderperform on MP images. The database and supplementary materials are\navailable at https://github.com/Future-IQA/MMP-2k."}
{"id": "2505.19076", "pdf": "https://arxiv.org/pdf/2505.19076", "abs": "https://arxiv.org/abs/2505.19076", "authors": ["Muye Huang", "Lingling Zhang", "Jie Ma", "Han Lai", "Fangzhi Xu", "Yifei Li", "Wenjun Wu", "Yaqiang Wu", "Jun Liu"], "title": "ChartSketcher: Reasoning with Multimodal Feedback and Reflection for Chart Understanding", "categories": ["cs.CV"], "comment": "23 pages, 9 figures", "summary": "Charts are high-density visualization carriers for complex data, serving as a\ncrucial medium for information extraction and analysis. Automated chart\nunderstanding poses significant challenges to existing multimodal large\nlanguage models (MLLMs) due to the need for precise and complex visual\nreasoning. Current step-by-step reasoning models primarily focus on text-based\nlogical reasoning for chart understanding. However, they struggle to refine or\ncorrect their reasoning when errors stem from flawed visual understanding, as\nthey lack the ability to leverage multimodal interaction for deeper\ncomprehension. Inspired by human cognitive behavior, we propose ChartSketcher,\na multimodal feedback-driven step-by-step reasoning method designed to address\nthese limitations. ChartSketcher is a chart understanding model that employs\nSketch-CoT, enabling MLLMs to annotate intermediate reasoning steps directly\nonto charts using a programmatic sketching library, iteratively feeding these\nvisual annotations back into the reasoning process. This mechanism enables the\nmodel to visually ground its reasoning and refine its understanding over\nmultiple steps. We employ a two-stage training strategy: a cold start phase to\nlearn sketch-based reasoning patterns, followed by off-policy reinforcement\nlearning to enhance reflection and generalization. Experiments demonstrate that\nChartSketcher achieves promising performance on chart understanding benchmarks\nand general vision tasks, providing an interactive and interpretable approach\nto chart comprehension."}
{"id": "2505.19081", "pdf": "https://arxiv.org/pdf/2505.19081", "abs": "https://arxiv.org/abs/2505.19081", "authors": ["Ruiyang Xia", "Dawei Zhou", "Decheng Liu", "Lin Yuan", "Jie Li", "Nannan Wang", "Xinbo Gao"], "title": "Towards Generalized Proactive Defense against Face Swappingwith Contour-Hybrid Watermark", "categories": ["cs.CV"], "comment": "16 pages, 11 figures, under review", "summary": "Face swapping, recognized as a privacy and security concern, has prompted\nconsiderable defensive research. With the advancements in AI-generated content,\nthe discrepancies between the real and swapped faces have become nuanced.\nConsidering the difficulty of forged traces detection, we shift the focus to\nthe face swapping purpose and proactively embed elaborate watermarks against\nunknown face swapping techniques. Given that the constant purpose is to swap\nthe original face identity while preserving the background, we concentrate on\nthe regions surrounding the face to ensure robust watermark generation, while\nembedding the contour texture and face identity information to achieve\nprogressive image determination. The watermark is located in the facial contour\nand contains hybrid messages, dubbed the contour-hybrid watermark (CMark). Our\napproach generalizes face swapping detection without requiring any swapping\ntechniques during training and the storage of large-scale messages in advance.\nExperiments conducted across 8 face swapping techniques demonstrate the\nsuperiority of our approach compared with state-of-the-art passive and\nproactive detectors while achieving a favorable balance between the image\nquality and watermark robustness."}
{"id": "2505.19084", "pdf": "https://arxiv.org/pdf/2505.19084", "abs": "https://arxiv.org/abs/2505.19084", "authors": ["Yifeng Xu", "Zhenliang He", "Meina Kan", "Shiguang Shan", "Xilin Chen"], "title": "Jodi: Unification of Visual Generation and Understanding via Joint Modeling", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Code: https://github.com/VIPL-GENUN/Jodi", "summary": "Visual generation and understanding are two deeply interconnected aspects of\nhuman intelligence, yet they have been traditionally treated as separate tasks\nin machine learning. In this paper, we propose Jodi, a diffusion framework that\nunifies visual generation and understanding by jointly modeling the image\ndomain and multiple label domains. Specifically, Jodi is built upon a linear\ndiffusion transformer along with a role switch mechanism, which enables it to\nperform three particular types of tasks: (1) joint generation, where the model\nsimultaneously generates images and multiple labels; (2) controllable\ngeneration, where images are generated conditioned on any combination of\nlabels; and (3) image perception, where multiple labels can be predicted at\nonce from a given image. Furthermore, we present the Joint-1.6M dataset, which\ncontains 200,000 high-quality images collected from public sources, automatic\nlabels for 7 visual domains, and LLM-generated captions. Extensive experiments\ndemonstrate that Jodi excels in both generation and understanding tasks and\nexhibits strong extensibility to a wider range of visual domains. Code is\navailable at https://github.com/VIPL-GENUN/Jodi."}
{"id": "2505.19089", "pdf": "https://arxiv.org/pdf/2505.19089", "abs": "https://arxiv.org/abs/2505.19089", "authors": ["Xuejie Liu", "Anji Liu", "Guy Van den Broeck", "Yitao Liang"], "title": "Plug-and-Play Context Feature Reuse for Efficient Masked Generation", "categories": ["cs.CV"], "comment": null, "summary": "Masked generative models (MGMs) have emerged as a powerful framework for\nimage synthesis, combining parallel decoding with strong bidirectional context\nmodeling. However, generating high-quality samples typically requires many\niterative decoding steps, resulting in high inference costs. A straightforward\nway to speed up generation is by decoding more tokens in each step, thereby\nreducing the total number of steps. However, when many tokens are decoded\nsimultaneously, the model can only estimate the univariate marginal\ndistributions independently, failing to capture the dependency among them. As a\nresult, reducing the number of steps significantly compromises generation\nfidelity. In this work, we introduce ReCAP (Reused Context-Aware Prediction), a\nplug-and-play module that accelerates inference in MGMs by constructing\nlow-cost steps via reusing feature embeddings from previously decoded context\ntokens. ReCAP interleaves standard full evaluations with lightweight steps that\ncache and reuse context features, substantially reducing computation while\npreserving the benefits of fine-grained, iterative generation. We demonstrate\nits effectiveness on top of three representative MGMs (MaskGIT, MAGE, and MAR),\nincluding both discrete and continuous token spaces and covering diverse\narchitectural designs. In particular, on ImageNet256 class-conditional\ngeneration, ReCAP achieves up to 2.4x faster inference than the base model with\nminimal performance drop, and consistently delivers better efficiency-fidelity\ntrade-offs under various generation settings."}
{"id": "2505.19094", "pdf": "https://arxiv.org/pdf/2505.19094", "abs": "https://arxiv.org/abs/2505.19094", "authors": ["Chuming Shen", "Wei Wei", "Xiaoye Qu", "Yu Cheng"], "title": "SATORI-R1: Incentivizing Multimodal Reasoning with Spatial Grounding and Verifiable Rewards", "categories": ["cs.CV", "cs.AI"], "comment": "Under review", "summary": "DeepSeek-R1 has demonstrated powerful reasoning capabilities in the text\ndomain through stable reinforcement learning (RL). Recently, in the multimodal\ndomain, works have begun to directly apply RL to generate R1-like free-form\nreasoning for Visual Question Answering (VQA) tasks. However, multimodal tasks\nshare an intrinsically different nature from textual tasks, which heavily rely\non the understanding of the input image to solve the problem. Therefore, such\nfree-form reasoning faces two critical limitations in the VQA task: (1)\nExtended reasoning chains diffuse visual focus away from task-critical regions,\ndegrading answer accuracy. (2) Unverifiable intermediate steps amplify\npolicy-gradient variance and computational costs overhead. To address these\nissues, in this paper, we introduce SATORI ($\\textbf{S}patially$\n$\\textbf{A}nchored$ $\\textbf{T}ask$ $\\textbf{O}ptimization$ with\n$\\textbf{R}e\\textbf{I}nforcement$ Learning), which decomposes VQA into three\nverifiable stages, including global image captioning, region localization, and\nanswer prediction, each supplying explicit reward signals. Furthermore, we also\nintroduce VQA-Verify, a 12k dataset annotated with answer-aligned captions and\nbounding-boxes to facilitate training. Experiments demonstrate consistent\nperformance improvements across seven VQA benchmarks, achieving up to $15.7\\%$\nimprovement in accuracy in accuracy compared to the R1-like baseline. Our\nanalysis of the attention map confirms enhanced focus on critical regions,\nwhich brings improvements in accuracy. Our code is available at\nhttps://github.com/justairr/SATORI-R1."}
{"id": "2505.19110", "pdf": "https://arxiv.org/pdf/2505.19110", "abs": "https://arxiv.org/abs/2505.19110", "authors": ["Vishwa Mohan Singh", "Alberto Gaston Villagran Asiares", "Luisa Sophie Schuhmacher", "Kate Rendall", "Simon Weißbrod", "David Rügamer", "Inga Körte"], "title": "An Interpretable Representation Learning Approach for Diffusion Tensor Imaging", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Accepted for publication at MIDL 2025", "summary": "Diffusion Tensor Imaging (DTI) tractography offers detailed insights into the\nstructural connectivity of the brain, but presents challenges in effective\nrepresentation and interpretation in deep learning models. In this work, we\npropose a novel 2D representation of DTI tractography that encodes tract-level\nfractional anisotropy (FA) values into a 9x9 grayscale image. This\nrepresentation is processed through a Beta-Total Correlation Variational\nAutoencoder with a Spatial Broadcast Decoder to learn a disentangled and\ninterpretable latent embedding. We evaluate the quality of this embedding using\nsupervised and unsupervised representation learning strategies, including\nauxiliary classification, triplet loss, and SimCLR-based contrastive learning.\nCompared to the 1D Group deep neural network (DNN) baselines, our approach\nimproves the F1 score in a downstream sex classification task by 15.74% and\nshows a better disentanglement than the 3D representation."}
{"id": "2505.19111", "pdf": "https://arxiv.org/pdf/2505.19111", "abs": "https://arxiv.org/abs/2505.19111", "authors": ["Yaping He", "Jianfeng Cai", "Qicong Hu", "Peiqing Wang"], "title": "Remote Sensing Image Classification with Decoupled Knowledge Distillation", "categories": ["cs.CV"], "comment": "7", "summary": "To address the challenges posed by the large number of parameters in existing\nremote sensing image classification models, which hinder deployment on\nresource-constrained devices, this paper proposes a lightweight classification\nmethod based on knowledge distillation. Specifically, G-GhostNet is adopted as\nthe backbone network, leveraging feature reuse to reduce redundant parameters\nand significantly improve inference efficiency. In addition, a decoupled\nknowledge distillation strategy is employed, which separates target and\nnon-target classes to effectively enhance classification accuracy. Experimental\nresults on the RSOD and AID datasets demonstrate that, compared with the\nhigh-parameter VGG-16 model, the proposed method achieves nearly equivalent\nTop-1 accuracy while reducing the number of parameters by 6.24 times. This\napproach strikes an excellent balance between model size and classification\nperformance, offering an efficient solution for deployment on resource-limited\ndevices."}
{"id": "2505.19114", "pdf": "https://arxiv.org/pdf/2505.19114", "abs": "https://arxiv.org/abs/2505.19114", "authors": ["Hui Zhang", "Dexiang Hong", "Maoke Yang", "Yutao Chen", "Zhao Zhang", "Jie Shao", "Xinglong Wu", "Zuxuan Wu", "Yu-Gang Jiang"], "title": "CreatiDesign: A Unified Multi-Conditional Diffusion Transformer for Creative Graphic Design", "categories": ["cs.CV"], "comment": null, "summary": "Graphic design plays a vital role in visual communication across advertising,\nmarketing, and multimedia entertainment. Prior work has explored automated\ngraphic design generation using diffusion models, aiming to streamline creative\nworkflows and democratize design capabilities. However, complex graphic design\nscenarios require accurately adhering to design intent specified by multiple\nheterogeneous user-provided elements (\\eg images, layouts, and texts), which\npose multi-condition control challenges for existing methods. Specifically,\nprevious single-condition control models demonstrate effectiveness only within\ntheir specialized domains but fail to generalize to other conditions, while\nexisting multi-condition methods often lack fine-grained control over each\nsub-condition and compromise overall compositional harmony. To address these\nlimitations, we introduce CreatiDesign, a systematic solution for automated\ngraphic design covering both model architecture and dataset construction.\nFirst, we design a unified multi-condition driven architecture that enables\nflexible and precise integration of heterogeneous design elements with minimal\narchitectural modifications to the base diffusion model. Furthermore, to ensure\nthat each condition precisely controls its designated image region and to avoid\ninterference between conditions, we propose a multimodal attention mask\nmechanism. Additionally, we develop a fully automated pipeline for constructing\ngraphic design datasets, and introduce a new dataset with 400K samples\nfeaturing multi-condition annotations, along with a comprehensive benchmark.\nExperimental results show that CreatiDesign outperforms existing models by a\nclear margin in faithfully adhering to user intent."}
{"id": "2505.19120", "pdf": "https://arxiv.org/pdf/2505.19120", "abs": "https://arxiv.org/abs/2505.19120", "authors": ["Xiaoyang Liu", "Bolin Qiu", "Jiezhang Cao", "Zheng Chen", "Yulun Zhang", "Xiaokang Yang"], "title": "Freqformer: Image-Demoiréing Transformer via Efficient Frequency Decomposition", "categories": ["cs.CV"], "comment": null, "summary": "Image demoir\\'eing remains a challenging task due to the complex interplay\nbetween texture corruption and color distortions caused by moir\\'e patterns.\nExisting methods, especially those relying on direct image-to-image\nrestoration, often fail to disentangle these intertwined artifacts effectively.\nWhile wavelet-based frequency-aware approaches offer a promising direction,\ntheir potential remains underexplored. In this paper, we present Freqformer, a\nTransformer-based framework specifically designed for image demoir\\'eing\nthrough targeted frequency separation. Our method performs an effective\nfrequency decomposition that explicitly splits moir\\'e patterns into\nhigh-frequency spatially-localized textures and low-frequency scale-robust\ncolor distortions, which are then handled by a dual-branch architecture\ntailored to their distinct characteristics. We further propose a learnable\nFrequency Composition Transform (FCT) module to adaptively fuse the\nfrequency-specific outputs, enabling consistent and high-fidelity\nreconstruction. To better aggregate the spatial dependencies and the\ninter-channel complementary information, we introduce a Spatial-Aware Channel\nAttention (SA-CA) module that refines moir\\'e-sensitive regions without\nincurring high computational cost. Extensive experiments on various\ndemoir\\'eing benchmarks demonstrate that Freqformer achieves state-of-the-art\nperformance with a compact model size. The code is publicly available at\nhttps://github.com/xyLiu339/Freqformer."}
{"id": "2505.19122", "pdf": "https://arxiv.org/pdf/2505.19122", "abs": "https://arxiv.org/abs/2505.19122", "authors": ["Eric Tillman Bill", "Cristian Perez Jensen", "Sotiris Anagnostidis", "Dimitri von Rütte"], "title": "Exploring Magnitude Preservation and Rotation Modulation in Diffusion Transformers", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Denoising diffusion models exhibit remarkable generative capabilities, but\nremain challenging to train due to their inherent stochasticity, where\nhigh-variance gradient estimates lead to slow convergence. Previous works have\nshown that magnitude preservation helps with stabilizing training in the U-net\narchitecture. This work explores whether this effect extends to the Diffusion\nTransformer (DiT) architecture. As such, we propose a magnitude-preserving\ndesign that stabilizes training without normalization layers. Motivated by the\ngoal of maintaining activation magnitudes, we additionally introduce rotation\nmodulation, which is a novel conditioning method using learned rotations\ninstead of traditional scaling or shifting. Through empirical evaluations and\nablation studies on small-scale models, we show that magnitude-preserving\nstrategies significantly improve performance, notably reducing FID scores by\n$\\sim$12.8%. Further, we show that rotation modulation combined with scaling is\ncompetitive with AdaLN, while requiring $\\sim$5.4% fewer parameters. This work\nprovides insights into conditioning strategies and magnitude control. We will\npublicly release the implementation of our method."}
{"id": "2505.19125", "pdf": "https://arxiv.org/pdf/2505.19125", "abs": "https://arxiv.org/abs/2505.19125", "authors": ["Yuqi Liu", "Qin Jin", "Tianyuan Qu", "Xuan Liu", "Yang Du", "Bei Yu", "Jiaya Jia"], "title": "RTime-QA: A Benchmark for Atomic Temporal Event Understanding in Large Multi-modal Models", "categories": ["cs.CV"], "comment": null, "summary": "Understanding accurate atomic temporal event is essential for video\ncomprehension. However, current video-language benchmarks often fall short to\nevaluate Large Multi-modal Models' (LMMs) temporal event understanding\ncapabilities, as they can be effectively addressed using image-language models.\nIn this paper, we introduce RTime-QA, a novel benchmark specifically designed\nto assess the atomic temporal event understanding ability of LMMs. RTime-QA\ncomprises 822 high-quality, carefully-curated video-text questions, each\nmeticulously annotated by human experts. Each question features a video\ndepicting an atomic temporal event, paired with both correct answers and\ntemporal negative descriptions, specifically designed to evaluate temporal\nunderstanding. To advance LMMs' temporal event understanding ability, we\nfurther introduce RTime-IT, a 14k instruction-tuning dataset that employs a\nsimilar annotation process as RTime-QA. Extensive experimental analysis\ndemonstrates that RTime-QA presents a significant challenge for LMMs: the\nstate-of-the-art model Qwen2-VL achieves only 34.6 on strict-ACC metric,\nsubstantially lagging behind human performance. Furthermore, our experiments\nreveal that RTime-IT effectively enhance LMMs' capacity in temporal\nunderstanding. By fine-tuning on RTime-IT, our Qwen2-VL achieves 65.9 on\nRTime-QA."}
{"id": "2505.19138", "pdf": "https://arxiv.org/pdf/2505.19138", "abs": "https://arxiv.org/abs/2505.19138", "authors": ["Myeongseok Nam", "Wongi Park", "Minsol Kim", "Hyejin Hur", "Soomok Lee"], "title": "Veta-GS: View-dependent deformable 3D Gaussian Splatting for thermal infrared Novel-view Synthesis", "categories": ["cs.CV"], "comment": null, "summary": "Recently, 3D Gaussian Splatting (3D-GS) based on Thermal Infrared (TIR)\nimaging has gained attention in novel-view synthesis, showing real-time\nrendering. However, novel-view synthesis with thermal infrared images suffers\nfrom transmission effects, emissivity, and low resolution, leading to floaters\nand blur effects in rendered images. To address these problems, we introduce\nVeta-GS, which leverages a view-dependent deformation field and a Thermal\nFeature Extractor (TFE) to precisely capture subtle thermal variations and\nmaintain robustness. Specifically, we design view-dependent deformation field\nthat leverages camera position and viewing direction, which capture thermal\nvariations. Furthermore, we introduce the Thermal Feature Extractor (TFE) and\nMonoSSIM loss, which consider appearance, edge, and frequency to maintain\nrobustness. Extensive experiments on the TI-NSD benchmark show that our method\nachieves better performance over existing methods."}
{"id": "2505.19139", "pdf": "https://arxiv.org/pdf/2505.19139", "abs": "https://arxiv.org/abs/2505.19139", "authors": ["Feiran Liu", "Yuzhe Zhang", "Xinyi Huang", "Yinan Peng", "Xinfeng Li", "Lixu Wang", "Yutong Shen", "Ranjie Duan", "Simeng Qin", "Xiaojun Jia", "Qingsong Wen", "Wei Dong"], "title": "The Eye of Sherlock Holmes: Uncovering User Private Attribute Profiling via Vision-Language Model Agentic Framework", "categories": ["cs.CV"], "comment": null, "summary": "Our research reveals a new privacy risk associated with the vision-language\nmodel (VLM) agentic framework: the ability to infer sensitive attributes (e.g.,\nage and health information) and even abstract ones (e.g., personality and\nsocial traits) from a set of personal images, which we term \"image private\nattribute profiling.\" This threat is particularly severe given that modern apps\ncan easily access users' photo albums, and inference from image sets enables\nmodels to exploit inter-image relations for more sophisticated profiling.\nHowever, two main challenges hinder our understanding of how well VLMs can\nprofile an individual from a few personal photos: (1) the lack of benchmark\ndatasets with multi-image annotations for private attributes, and (2) the\nlimited ability of current multimodal large language models (MLLMs) to infer\nabstract attributes from large image collections. In this work, we construct\nPAPI, the largest dataset for studying private attribute profiling in personal\nimages, comprising 2,510 images from 251 individuals with 3,012 annotated\nprivacy attributes. We also propose HolmesEye, a hybrid agentic framework that\ncombines VLMs and LLMs to enhance privacy inference. HolmesEye uses VLMs to\nextract both intra-image and inter-image information and LLMs to guide the\ninference process as well as consolidate the results through forensic analysis,\novercoming existing limitations in long-context visual reasoning. Experiments\nreveal that HolmesEye achieves a 10.8% improvement in average accuracy over\nstate-of-the-art baselines and surpasses human-level performance by 15.0% in\npredicting abstract attributes. This work highlights the urgency of addressing\nprivacy risks in image-based profiling and offers both a new dataset and an\nadvanced framework to guide future research in this area."}
{"id": "2505.19148", "pdf": "https://arxiv.org/pdf/2505.19148", "abs": "https://arxiv.org/abs/2505.19148", "authors": ["Shengdong Han", "Shangdong Yang", "Xin Zhang", "Yuxuan Li", "Xiang Li", "Jian Yang", "Ming-Ming Cheng", "Yimian Dai"], "title": "DISTA-Net: Dynamic Closely-Spaced Infrared Small Target Unmixing", "categories": ["cs.CV"], "comment": null, "summary": "Resolving closely-spaced small targets in dense clusters presents a\nsignificant challenge in infrared imaging, as the overlapping signals hinder\nprecise determination of their quantity, sub-pixel positions, and radiation\nintensities. While deep learning has advanced the field of infrared small\ntarget detection, its application to closely-spaced infrared small targets has\nnot yet been explored. This gap exists primarily due to the complexity of\nseparating superimposed characteristics and the lack of an open-source\ninfrastructure. In this work, we propose the Dynamic Iterative Shrinkage\nThresholding Network (DISTA-Net), which reconceptualizes traditional sparse\nreconstruction within a dynamic framework. DISTA-Net adaptively generates\nconvolution weights and thresholding parameters to tailor the reconstruction\nprocess in real time. To the best of our knowledge, DISTA-Net is the first deep\nlearning model designed specifically for the unmixing of closely-spaced\ninfrared small targets, achieving superior sub-pixel detection accuracy.\nMoreover, we have established the first open-source ecosystem to foster further\nresearch in this field. This ecosystem comprises three key components: (1)\nCSIST-100K, a publicly available benchmark dataset; (2) CSO-mAP, a custom\nevaluation metric for sub-pixel detection; and (3) GrokCSO, an open-source\ntoolkit featuring DISTA-Net and other models. Our code and dataset are\navailable at https://github.com/GrokCV/GrokCSO."}
{"id": "2505.19149", "pdf": "https://arxiv.org/pdf/2505.19149", "abs": "https://arxiv.org/abs/2505.19149", "authors": ["Shuyu Wang", "Weiqi Li", "Qian Wang", "Shijie Zhao", "Jian Zhang"], "title": "MIND-Edit: MLLM Insight-Driven Editing via Language-Vision Projection", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in AI-generated content (AIGC) have significantly accelerated\nimage editing techniques, driving increasing demand for diverse and\nfine-grained edits. Despite these advances, existing image editing methods\nstill face challenges in achieving high precision and semantic accuracy in\ncomplex scenarios. Recent studies address this issue by incorporating\nmultimodal large language models (MLLMs) into image editing pipelines. However,\ncurrent MLLM-based methods mainly rely on interpreting textual instructions,\nleaving the intrinsic visual understanding of large models largely unexplored,\nthus resulting in insufficient alignment between textual semantics and visual\noutcomes. To overcome these limitations, we propose MIND-Edit, an end-to-end\nimage-editing framework integrating pretrained diffusion model with MLLM.\nMIND-Edit introduces two complementary strategies: (1) a text instruction\noptimization strategy that clarifies ambiguous user instructions based on\nsemantic reasoning from the MLLM, and (2) an MLLM insight-driven editing\nstrategy that explicitly leverages the intrinsic visual understanding\ncapability of the MLLM to infer editing intent and guide the diffusion process\nvia generated visual embeddings. Furthermore, we propose a joint training\napproach to effectively integrate both strategies, allowing them to reinforce\neach other for more accurate instruction interpretation and visually coherent\nedits aligned with user intent. Extensive experiments demonstrate that\nMIND-Edit outperforms state-of-the-art image editing methods in both\nquantitative metrics and visual quality, particularly under complex and\nchallenging scenarios."}
{"id": "2505.19154", "pdf": "https://arxiv.org/pdf/2505.19154", "abs": "https://arxiv.org/abs/2505.19154", "authors": ["Q. G. Duan", "Benyun Zhao", "Mingqiao Han Yijun Huang", "Ben M. Chen"], "title": "FHGS: Feature-Homogenized Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "Scene understanding based on 3D Gaussian Splatting (3DGS) has recently\nachieved notable advances. Although 3DGS related methods have efficient\nrendering capabilities, they fail to address the inherent contradiction between\nthe anisotropic color representation of gaussian primitives and the isotropic\nrequirements of semantic features, leading to insufficient cross-view feature\nconsistency. To overcome the limitation, we proposes $\\textit{FHGS}$\n(Feature-Homogenized Gaussian Splatting), a novel 3D feature fusion framework\ninspired by physical models, which can achieve high-precision mapping of\narbitrary 2D features from pre-trained models to 3D scenes while preserving the\nreal-time rendering efficiency of 3DGS. Specifically, our $\\textit{FHGS}$\nintroduces the following innovations: Firstly, a universal feature fusion\narchitecture is proposed, enabling robust embedding of large-scale pre-trained\nmodels' semantic features (e.g., SAM, CLIP) into sparse 3D structures.\nSecondly, a non-differentiable feature fusion mechanism is introduced, which\nenables semantic features to exhibit viewpoint independent isotropic\ndistributions. This fundamentally balances the anisotropic rendering of\ngaussian primitives and the isotropic expression of features; Thirdly, a\ndual-driven optimization strategy inspired by electric potential fields is\nproposed, which combines external supervision from semantic feature fields with\ninternal primitive clustering guidance. This mechanism enables synergistic\noptimization of global semantic alignment and local structural consistency.\nMore interactive results can be accessed on: https://fhgs.cuastro.org/."}
{"id": "2505.19155", "pdf": "https://arxiv.org/pdf/2505.19155", "abs": "https://arxiv.org/abs/2505.19155", "authors": ["Xuan Zhang", "Cunxiao Du", "Sicheng Yu", "Jiawei Wu", "Fengzhuo Zhang", "Wei Gao", "Qian Liu"], "title": "Sparse-to-Dense: A Free Lunch for Lossless Acceleration of Video Understanding in LLMs", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Due to the auto-regressive nature of current video large language models\n(Video-LLMs), the inference latency increases as the input sequence length\ngrows, posing challenges for the efficient processing of video sequences that\nare usually very long. We observe that during decoding, the attention scores of\nmost tokens in Video-LLMs tend to be sparse and concentrated, with only certain\ntokens requiring comprehensive full attention. Based on this insight, we\nintroduce Sparse-to-Dense (StD), a novel decoding strategy that integrates two\ndistinct modules: one leveraging sparse top-K attention and the other employing\ndense full attention. These modules collaborate to accelerate Video-LLMs\nwithout loss. The fast (sparse) model speculatively decodes multiple tokens,\nwhile the slow (dense) model verifies them in parallel. StD is a tuning-free,\nplug-and-play solution that achieves up to a 1.94$\\times$ walltime speedup in\nvideo processing. It maintains model performance while enabling a seamless\ntransition from a standard Video-LLM to a sparse Video-LLM with minimal code\nmodifications."}
{"id": "2505.19159", "pdf": "https://arxiv.org/pdf/2505.19159", "abs": "https://arxiv.org/abs/2505.19159", "authors": ["Yuze Wang", "Mariana Belgiu", "Haiyang Wu", "Dandan Zhong", "Yangyang Cao", "Chao Tao"], "title": "A Joint Learning Framework with Feature Reconstruction and Prediction for Incomplete Satellite Image Time Series in Agricultural Semantic Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Satellite Image Time Series (SITS) is crucial for agricultural semantic\nsegmentation. However, Cloud contamination introduces time gaps in SITS,\ndisrupting temporal dependencies and causing feature shifts, leading to\ndegraded performance of models trained on complete SITS. Existing methods\ntypically address this by reconstructing the entire SITS before prediction or\nusing data augmentation to simulate missing data. Yet, full reconstruction may\nintroduce noise and redundancy, while the data-augmented model can only handle\nlimited missing patterns, leading to poor generalization. We propose a joint\nlearning framework with feature reconstruction and prediction to address\nincomplete SITS more effectively. During training, we simulate data-missing\nscenarios using temporal masks. The two tasks are guided by both ground-truth\nlabels and the teacher model trained on complete SITS. The prediction task\nconstrains the model from selectively reconstructing critical features from\nmasked inputs that align with the teacher's temporal feature representations.\nIt reduces unnecessary reconstruction and limits noise propagation. By\nintegrating reconstructed features into the prediction task, the model avoids\nlearning shortcuts and maintains its ability to handle varied missing patterns\nand complete SITS. Experiments on SITS from Hunan Province, Western France, and\nCatalonia show that our method improves mean F1-scores by 6.93% in cropland\nextraction and 7.09% in crop classification over baselines. It also generalizes\nwell across satellite sensors, including Sentinel-2 and PlanetScope, under\nvarying temporal missing rates and model backbones."}
{"id": "2505.19161", "pdf": "https://arxiv.org/pdf/2505.19161", "abs": "https://arxiv.org/abs/2505.19161", "authors": ["Jialun Pei", "Diandian Guo", "Donghui Yang", "Zhixi Li", "Yuxin Feng", "Long Ma", "Bo Du", "Pheng-Ann Heng"], "title": "Benchmarking Laparoscopic Surgical Image Restoration and Beyond", "categories": ["cs.CV"], "comment": null, "summary": "In laparoscopic surgery, a clear and high-quality visual field is critical\nfor surgeons to make accurate intraoperative decisions. However, persistent\nvisual degradation, including smoke generated by energy devices, lens fogging\nfrom thermal gradients, and lens contamination due to blood or tissue fluid\nsplashes during surgical procedures, severely impair visual clarity. These\ndegenerations can seriously hinder surgical workflow and pose risks to patient\nsafety. To systematically investigate and address various forms of surgical\nscene degradation, we introduce a real-world open-source surgical image\nrestoration dataset covering laparoscopic environments, called SurgClean, which\ninvolves multi-type image restoration tasks, e.g., desmoking, defogging, and\ndesplashing. SurgClean comprises 1,020 images with diverse degradation types\nand corresponding paired reference labels. Based on SurgClean, we establish a\nstandardized evaluation benchmark and provide performance for 22 representative\ngeneric task-specific image restoration approaches, including 12 generic and 10\ntask-specific image restoration approaches. Experimental results reveal\nsubstantial performance gaps relative to clinical requirements, highlighting a\ncritical opportunity for algorithm advancements in intelligent surgical\nrestoration. Furthermore, we explore the degradation discrepancies between\nsurgical and natural scenes from structural perception and semantic\nunderstanding perspectives, providing fundamental insights for domain-specific\nimage restoration research. Our work aims to empower the capabilities of\nrestoration algorithms to increase surgical environments and improve the\nefficiency of clinical procedures."}
{"id": "2505.19166", "pdf": "https://arxiv.org/pdf/2505.19166", "abs": "https://arxiv.org/abs/2505.19166", "authors": ["Eric Tillmann Bill", "Enis Simsar", "Thomas Hofmann"], "title": "JEDI: The Force of Jensen-Shannon Divergence in Disentangling Diffusion Models", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "We introduce JEDI, a test-time adaptation method that enhances subject\nseparation and compositional alignment in diffusion models without requiring\nretraining or external supervision. JEDI operates by minimizing semantic\nentanglement in attention maps using a novel Jensen-Shannon divergence based\nobjective. To improve efficiency, we leverage adversarial optimization,\nreducing the number of updating steps required.\n  JEDI is model-agnostic and applicable to architectures such as Stable\nDiffusion 1.5 and 3.5, consistently improving prompt alignment and\ndisentanglement in complex scenes. Additionally, JEDI provides a lightweight,\nCLIP-free disentanglement score derived from internal attention distributions,\noffering a principled benchmark for compositional alignment under test-time\nconditions. We will publicly release the implementation of our method."}
{"id": "2505.19169", "pdf": "https://arxiv.org/pdf/2505.19169", "abs": "https://arxiv.org/abs/2505.19169", "authors": ["Ryosei Hara", "Wataru Ikeda", "Masashi Hatano", "Mariko Isogawa"], "title": "EventEgoHands: Event-based Egocentric 3D Hand Mesh Reconstruction", "categories": ["cs.CV"], "comment": "IEEE International Conference on Image Processing 2025", "summary": "Reconstructing 3D hand mesh is challenging but an important task for\nhuman-computer interaction and AR/VR applications. In particular, RGB and/or\ndepth cameras have been widely used in this task. However, methods using these\nconventional cameras face challenges in low-light environments and during\nmotion blur. Thus, to address these limitations, event cameras have been\nattracting attention in recent years for their high dynamic range and high\ntemporal resolution. Despite their advantages, event cameras are sensitive to\nbackground noise or camera motion, which has limited existing studies to static\nbackgrounds and fixed cameras. In this study, we propose EventEgoHands, a novel\nmethod for event-based 3D hand mesh reconstruction in an egocentric view. Our\napproach introduces a Hand Segmentation Module that extracts hand regions,\neffectively mitigating the influence of dynamic background events. We evaluated\nour approach and demonstrated its effectiveness on the N-HOT3D dataset,\nimproving MPJPE by approximately more than 4.5 cm (43%)."}
{"id": "2505.19175", "pdf": "https://arxiv.org/pdf/2505.19175", "abs": "https://arxiv.org/abs/2505.19175", "authors": ["Jan Held", "Renaud Vandeghen", "Adrien Deliege", "Abdullah Hamdi", "Silvio Giancola", "Anthony Cioppa", "Andrea Vedaldi", "Bernard Ghanem", "Andrea Tagliasacchi", "Marc Van Droogenbroeck"], "title": "Triangle Splatting for Real-Time Radiance Field Rendering", "categories": ["cs.CV"], "comment": "18 pages, 13 figures, 10 tables", "summary": "The field of computer graphics was revolutionized by models such as Neural\nRadiance Fields and 3D Gaussian Splatting, displacing triangles as the dominant\nrepresentation for photogrammetry. In this paper, we argue for a triangle\ncomeback. We develop a differentiable renderer that directly optimizes\ntriangles via end-to-end gradients. We achieve this by rendering each triangle\nas differentiable splats, combining the efficiency of triangles with the\nadaptive density of representations based on independent primitives. Compared\nto popular 2D and 3D Gaussian Splatting methods, our approach achieves higher\nvisual fidelity, faster convergence, and increased rendering throughput. On the\nMip-NeRF360 dataset, our method outperforms concurrent non-volumetric\nprimitives in visual fidelity and achieves higher perceptual quality than the\nstate-of-the-art Zip-NeRF on indoor scenes. Triangles are simple, compatible\nwith standard graphics stacks and GPU hardware, and highly efficient: for the\n\\textit{Garden} scene, we achieve over 2,400 FPS at 1280x720 resolution using\nan off-the-shelf mesh renderer. These results highlight the efficiency and\neffectiveness of triangle-based representations for high-quality novel view\nsynthesis. Triangles bring us closer to mesh-based optimization by combining\nclassical computer graphics with modern differentiable rendering frameworks.\nThe project page is https://trianglesplatting.github.io/"}
{"id": "2505.19178", "pdf": "https://arxiv.org/pdf/2505.19178", "abs": "https://arxiv.org/abs/2505.19178", "authors": ["Akhila Yaragoppa", "Siddharth"], "title": "Saliency-guided Emotion Modeling: Predicting Viewer Reactions from Video Stimuli", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Accepted for publication at IBPRIA 2025 Conference in Coimbra,\n  Portugal", "summary": "Understanding the emotional impact of videos is crucial for applications in\ncontent creation, advertising, and Human-Computer Interaction (HCI).\nTraditional affective computing methods rely on self-reported emotions, facial\nexpression analysis, and biosensing data, yet they often overlook the role of\nvisual saliency -- the naturally attention-grabbing regions within a video. In\nthis study, we utilize deep learning to introduce a novel saliency-based\napproach to emotion prediction by extracting two key features: saliency area\nand number of salient regions. Using the HD2S saliency model and OpenFace\nfacial action unit analysis, we examine the relationship between video saliency\nand viewer emotions. Our findings reveal three key insights: (1) Videos with\nmultiple salient regions tend to elicit high-valence, low-arousal emotions, (2)\nVideos with a single dominant salient region are more likely to induce\nlow-valence, high-arousal responses, and (3) Self-reported emotions often\nmisalign with facial expression-based emotion detection, suggesting limitations\nin subjective reporting. By leveraging saliency-driven insights, this work\nprovides a computationally efficient and interpretable alternative for emotion\nmodeling, with implications for content creation, personalized media\nexperiences, and affective computing research."}
{"id": "2505.19186", "pdf": "https://arxiv.org/pdf/2505.19186", "abs": "https://arxiv.org/abs/2505.19186", "authors": ["Rushiraj Gadhvi", "Priyansh Desai", "Siddharth"], "title": "PosePilot: An Edge-AI Solution for Posture Correction in Physical Exercises", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted for publication at IBPRIA 2025 Conference in Coimbra,\n  Portugal", "summary": "Automated pose correction remains a significant challenge in AI-driven\nfitness systems, despite extensive research in activity recognition. This work\npresents PosePilot, a novel system that integrates pose recognition with\nreal-time personalized corrective feedback, overcoming the limitations of\ntraditional fitness solutions. Using Yoga, a discipline requiring precise\nspatio-temporal alignment as a case study, we demonstrate PosePilot's ability\nto analyze complex physical movements. Designed for deployment on edge devices,\nPosePilot can be extended to various at-home and outdoor exercises. We employ a\nVanilla LSTM, allowing the system to capture temporal dependencies for pose\nrecognition. Additionally, a BiLSTM with multi-head Attention enhances the\nmodel's ability to process motion contexts, selectively focusing on key limb\nangles for accurate error detection while maintaining computational efficiency.\nAs part of this work, we introduce a high-quality video dataset used for\nevaluating our models. Most importantly, PosePilot provides instant corrective\nfeedback at every stage of a movement, ensuring precise posture adjustments\nthroughout the exercise routine. The proposed approach 1) performs automatic\nhuman posture recognition, 2) provides personalized posture correction feedback\nat each instant which is crucial in Yoga, and 3) offers a lightweight and\nrobust posture correction model feasible for deploying on edge devices in\nreal-world environments."}
{"id": "2505.19196", "pdf": "https://arxiv.org/pdf/2505.19196", "abs": "https://arxiv.org/abs/2505.19196", "authors": ["Xinyao Liao", "Wei Wei", "Xiaoye Qu", "Yu Cheng"], "title": "Step-level Reward for Free in RL-based T2I Diffusion Model Fine-tuning", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in text-to-image (T2I) diffusion model fine-tuning leverage\nreinforcement learning (RL) to align generated images with learnable reward\nfunctions. The existing approaches reformulate denoising as a Markov decision\nprocess for RL-driven optimization. However, they suffer from reward sparsity,\nreceiving only a single delayed reward per generated trajectory. This flaw\nhinders precise step-level attribution of denoising actions, undermines\ntraining efficiency. To address this, we propose a simple yet effective credit\nassignment framework that dynamically distributes dense rewards across\ndenoising steps. Specifically, we track changes in cosine similarity between\nintermediate and final images to quantify each step's contribution on\nprogressively reducing the distance to the final image. Our approach avoids\nadditional auxiliary neural networks for step-level preference modeling and\ninstead uses reward shaping to highlight denoising phases that have a greater\nimpact on image quality. Our method achieves 1.25 to 2 times higher sample\nefficiency and better generalization across four human preference reward\nfunctions, without compromising the original optimal policy."}
{"id": "2505.19208", "pdf": "https://arxiv.org/pdf/2505.19208", "abs": "https://arxiv.org/abs/2505.19208", "authors": ["Tyler Ward", "Aaron Moseley", "Abdullah-Al-Zubaer Imran"], "title": "Domain and Task-Focused Example Selection for Data-Efficient Contrastive Medical Image Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Segmentation is one of the most important tasks in the medical imaging\npipeline as it influences a number of image-based decisions. To be effective,\nfully supervised segmentation approaches require large amounts of manually\nannotated training data. However, the pixel-level annotation process is\nexpensive, time-consuming, and error-prone, hindering progress and making it\nchallenging to perform effective segmentations. Therefore, models must learn\nefficiently from limited labeled data. Self-supervised learning (SSL),\nparticularly contrastive learning via pre-training on unlabeled data and\nfine-tuning on limited annotations, can facilitate such limited labeled image\nsegmentation. To this end, we propose a novel self-supervised contrastive\nlearning framework for medical image segmentation, leveraging inherent\nrelationships of different images, dubbed PolyCL. Without requiring any\npixel-level annotations or unreasonable data augmentations, our PolyCL learns\nand transfers context-aware discriminant features useful for segmentation from\nan innovative surrogate, in a task-related manner. Additionally, we integrate\nthe Segment Anything Model (SAM) into our framework in two novel ways: as a\npost-processing refinement module that improves the accuracy of predicted masks\nusing bounding box prompts derived from coarse outputs, and as a propagation\nmechanism via SAM 2 that generates volumetric segmentations from a single\nannotated 2D slice. Experimental evaluations on three public computed\ntomography (CT) datasets demonstrate that PolyCL outperforms fully-supervised\nand self-supervised baselines in both low-data and cross-domain scenarios. Our\ncode is available at https://github.com/tbwa233/PolyCL."}
{"id": "2505.19210", "pdf": "https://arxiv.org/pdf/2505.19210", "abs": "https://arxiv.org/abs/2505.19210", "authors": ["Xiang Li", "Rongrong Wang", "Qing Qu"], "title": "Towards Understanding the Mechanisms of Classifier-Free Guidance", "categories": ["cs.CV"], "comment": null, "summary": "Classifier-free guidance (CFG) is a core technique powering state-of-the-art\nimage generation systems, yet its underlying mechanisms remain poorly\nunderstood. In this work, we begin by analyzing CFG in a simplified linear\ndiffusion model, where we show its behavior closely resembles that observed in\nthe nonlinear case. Our analysis reveals that linear CFG improves generation\nquality via three distinct components: (i) a mean-shift term that approximately\nsteers samples in the direction of class means, (ii) a positive Contrastive\nPrincipal Components (CPC) term that amplifies class-specific features, and\n(iii) a negative CPC term that suppresses generic features prevalent in\nunconditional data. We then verify that these insights in real-world, nonlinear\ndiffusion models: over a broad range of noise levels, linear CFG resembles the\nbehavior of its nonlinear counterpart. Although the two eventually diverge at\nlow noise levels, we discuss how the insights from the linear analysis still\nshed light on the CFG's mechanism in the nonlinear regime."}
{"id": "2505.19218", "pdf": "https://arxiv.org/pdf/2505.19218", "abs": "https://arxiv.org/abs/2505.19218", "authors": ["Jingwei Wu", "Zhewei Huang", "Chang Liu"], "title": "Advancing Video Self-Supervised Learning via Image Foundation Models", "categories": ["cs.CV"], "comment": null, "summary": "In the past decade, image foundation models (IFMs) have achieved\nunprecedented progress. However, the potential of directly using IFMs for video\nself-supervised representation learning has largely been overlooked. In this\nstudy, we propose an advancing video self-supervised learning (AdViSe)\napproach, aimed at significantly reducing the training overhead of video\nrepresentation models using pre-trained IFMs. Specifically, we first introduce\ntemporal modeling modules (ResNet3D) to IFMs, constructing a video\nrepresentation model. We then employ a video self-supervised learning approach,\nplayback rate perception, to train temporal modules while freezing the IFM\ncomponents. Experiments on UCF101 demonstrate that AdViSe achieves performance\ncomparable to state-of-the-art methods while reducing training time by\n$3.4\\times$ and GPU memory usage by $8.2\\times$. This study offers fresh\ninsights into low-cost video self-supervised learning based on pre-trained\nIFMs. Code is available at https://github.com/JingwWu/advise-video-ssl."}
{"id": "2505.19233", "pdf": "https://arxiv.org/pdf/2505.19233", "abs": "https://arxiv.org/abs/2505.19233", "authors": ["Aniruddha Mukherjee", "Spriha Dubey", "Somdyuti Paul"], "title": "RAISE: Realness Assessment for Image Synthesis and Evaluation", "categories": ["cs.CV", "cs.AI", "cs.MM", "eess.IV"], "comment": null, "summary": "The rapid advancement of generative AI has enabled the creation of highly\nphotorealistic visual content, offering practical substitutes for real images\nand videos in scenarios where acquiring real data is difficult or expensive.\nHowever, reliably substituting real visual content with AI-generated\ncounterparts requires robust assessment of the perceived realness of\nAI-generated visual content, a challenging task due to its inherent subjective\nnature. To address this, we conducted a comprehensive human study evaluating\nthe perceptual realness of both real and AI-generated images, resulting in a\nnew dataset, containing images paired with subjective realness scores,\nintroduced as RAISE in this paper. Further, we develop and train multiple\nmodels on RAISE to establish baselines for realness prediction. Our\nexperimental results demonstrate that features derived from deep foundation\nvision models can effectively capture the subjective realness. RAISE thus\nprovides a valuable resource for developing robust, objective models of\nperceptual realness assessment."}
{"id": "2505.19239", "pdf": "https://arxiv.org/pdf/2505.19239", "abs": "https://arxiv.org/abs/2505.19239", "authors": ["Chen Shi", "Shaoshuai Shi", "Kehua Sheng", "Bo Zhang", "Li Jiang"], "title": "DriveX: Omni Scene Modeling for Learning Generalizable World Knowledge in Autonomous Driving", "categories": ["cs.CV"], "comment": null, "summary": "Data-driven learning has advanced autonomous driving, yet task-specific\nmodels struggle with out-of-distribution scenarios due to their narrow\noptimization objectives and reliance on costly annotated data. We present\nDriveX, a self-supervised world model that learns generalizable scene dynamics\nand holistic representations (geometric, semantic, and motion) from large-scale\ndriving videos. DriveX introduces Omni Scene Modeling (OSM), a module that\nunifies multimodal supervision-3D point cloud forecasting, 2D semantic\nrepresentation, and image generation-to capture comprehensive scene evolution.\nTo simplify learning complex dynamics, we propose a decoupled latent world\nmodeling strategy that separates world representation learning from future\nstate decoding, augmented by dynamic-aware ray sampling to enhance motion\nmodeling. For downstream adaptation, we design Future Spatial Attention (FSA),\na unified paradigm that dynamically aggregates spatiotemporal features from\nDriveX's predictions to enhance task-specific inference. Extensive experiments\ndemonstrate DriveX's effectiveness: it achieves significant improvements in 3D\nfuture point cloud prediction over prior work, while attaining state-of-the-art\nresults on diverse tasks including occupancy prediction, flow estimation, and\nend-to-end driving. These results validate DriveX's capability as a\ngeneral-purpose world model, paving the way for robust and unified autonomous\ndriving frameworks."}
{"id": "2505.19242", "pdf": "https://arxiv.org/pdf/2505.19242", "abs": "https://arxiv.org/abs/2505.19242", "authors": ["Alaa Dalaq", "Muzammil Behzad"], "title": "Deformable Attentive Visual Enhancement for Referring Segmentation Using Vision-Language Model", "categories": ["cs.CV"], "comment": null, "summary": "Image segmentation is a fundamental task in computer vision, aimed at\npartitioning an image into semantically meaningful regions. Referring image\nsegmentation extends this task by using natural language expressions to\nlocalize specific objects, requiring effective integration of visual and\nlinguistic information. In this work, we propose SegVLM, a vision-language\nmodel that incorporates architectural improvements to enhance segmentation\naccuracy and cross-modal alignment. The model integrates squeeze-and-excitation\n(SE) blocks for dynamic feature recalibration, deformable convolutions for\ngeometric adaptability, and residual connections for deep feature learning. We\nalso introduce a novel referring-aware fusion (RAF) loss that balances\nregion-level alignment, boundary precision, and class imbalance. Extensive\nexperiments and ablation studies demonstrate that each component contributes to\nconsistent performance improvements. SegVLM also shows strong generalization\nacross diverse datasets and referring expression scenarios."}
{"id": "2505.19256", "pdf": "https://arxiv.org/pdf/2505.19256", "abs": "https://arxiv.org/abs/2505.19256", "authors": ["Vivek Gopalakrishnan", "Neel Dey", "Polina Golland"], "title": "PolyPose: Localizing Deformable Anatomy in 3D from Sparse 2D X-ray Images using Polyrigid Transforms", "categories": ["cs.CV", "physics.med-ph"], "comment": null, "summary": "Determining the 3D pose of a patient from a limited set of 2D X-ray images is\na critical task in interventional settings. While preoperative volumetric\nimaging (e.g., CT and MRI) provides precise 3D localization and visualization\nof anatomical targets, these modalities cannot be acquired during procedures,\nwhere fast 2D imaging (X-ray) is used instead. To integrate volumetric guidance\ninto intraoperative procedures, we present PolyPose, a simple and robust method\nfor deformable 2D/3D registration. PolyPose parameterizes complex 3D\ndeformation fields as a composition of rigid transforms, leveraging the\nbiological constraint that individual bones do not bend in typical motion.\nUnlike existing methods that either assume no inter-joint movement or fail\noutright in this under-determined setting, our polyrigid formulation enforces\nanatomically plausible priors that respect the piecewise rigid nature of human\nmovement. This approach eliminates the need for expensive deformation\nregularizers that require patient- and procedure-specific hyperparameter\noptimization. Across extensive experiments on diverse datasets from orthopedic\nsurgery and radiotherapy, we show that this strong inductive bias enables\nPolyPose to successfully align the patient's preoperative volume to as few as\ntwo X-ray images, thereby providing crucial 3D guidance in challenging\nsparse-view and limited-angle settings where current registration methods fail."}
{"id": "2505.19261", "pdf": "https://arxiv.org/pdf/2505.19261", "abs": "https://arxiv.org/abs/2505.19261", "authors": ["Yu Zhang", "Jialei Zhou", "Xinchen Li", "Qi Zhang", "Zhongwei Wan", "Tianyu Wang", "Duoqian Miao", "Changwei Wang", "Longbing Cao"], "title": "Enhancing Text-to-Image Diffusion Transformer via Split-Text Conditioning", "categories": ["cs.CV", "cs.AI"], "comment": "21 pages", "summary": "Current text-to-image diffusion generation typically employs complete-text\nconditioning. Due to the intricate syntax, diffusion transformers (DiTs)\ninherently suffer from a comprehension defect of complete-text captions.\nOne-fly complete-text input either overlooks critical semantic details or\ncauses semantic confusion by simultaneously modeling diverse semantic primitive\ntypes. To mitigate this defect of DiTs, we propose a novel split-text\nconditioning framework named DiT-ST. This framework converts a complete-text\ncaption into a split-text caption, a collection of simplified sentences, to\nexplicitly express various semantic primitives and their interconnections. The\nsplit-text caption is then injected into different denoising stages of DiT-ST\nin a hierarchical and incremental manner. Specifically, DiT-ST leverages Large\nLanguage Models to parse captions, extracting diverse primitives and\nhierarchically sorting out and constructing these primitives into a split-text\ninput. Moreover, we partition the diffusion denoising process according to its\ndifferential sensitivities to diverse semantic primitive types and determine\nthe appropriate timesteps to incrementally inject tokens of diverse semantic\nprimitive types into input tokens via cross-attention. In this way, DiT-ST\nenhances the representation learning of specific semantic primitive types\nacross different stages. Extensive experiments validate the effectiveness of\nour proposed DiT-ST in mitigating the complete-text comprehension defect."}
{"id": "2505.19264", "pdf": "https://arxiv.org/pdf/2505.19264", "abs": "https://arxiv.org/abs/2505.19264", "authors": ["Guangan Chen", "Anh Minh Truong", "Hanhe Lin", "Michiel Vlaminck", "Wilfried Philips", "Hiep Luong"], "title": "Improving Novel view synthesis of 360$^\\circ$ Scenes in Extremely Sparse Views by Jointly Training Hemisphere Sampled Synthetic Images", "categories": ["cs.CV"], "comment": null, "summary": "Novel view synthesis in 360$^\\circ$ scenes from extremely sparse input views\nis essential for applications like virtual reality and augmented reality. This\npaper presents a novel framework for novel view synthesis in extremely\nsparse-view cases. As typical structure-from-motion methods are unable to\nestimate camera poses in extremely sparse-view cases, we apply DUSt3R to\nestimate camera poses and generate a dense point cloud. Using the poses of\nestimated cameras, we densely sample additional views from the upper hemisphere\nspace of the scenes, from which we render synthetic images together with the\npoint cloud. Training 3D Gaussian Splatting model on a combination of reference\nimages from sparse views and densely sampled synthetic images allows a larger\nscene coverage in 3D space, addressing the overfitting challenge due to the\nlimited input in sparse-view cases. Retraining a diffusion-based image\nenhancement model on our created dataset, we further improve the quality of the\npoint-cloud-rendered images by removing artifacts. We compare our framework\nwith benchmark methods in cases of only four input views, demonstrating\nsignificant improvement in novel view synthesis under extremely sparse-view\nconditions for 360$^\\circ$ scenes."}
{"id": "2505.19291", "pdf": "https://arxiv.org/pdf/2505.19291", "abs": "https://arxiv.org/abs/2505.19291", "authors": ["Kazi Mahathir Rahman", "Showrin Rahman", "Sharmin Sultana Srishty"], "title": "TextDiffuser-RL: Efficient and Robust Text Layout Optimization for High-Fidelity Text-to-Image Synthesis", "categories": ["cs.CV", "cs.AI", "68T05, 68T07, 68U10 68T05, 68T07, 68U10 68T05, 68T07, 68U10", "I.2.6; I.2.7; I.2.10; I.5.1; I.4.9"], "comment": "14 pages, 26 figures. Submitted to arXiv for dissemination. Intended\n  for future submission to a Generative AI conference", "summary": "Text-embedded image generation plays a critical role in industries such as\ngraphic design, advertising, and digital content creation. Text-to-Image\ngeneration methods leveraging diffusion models, such as TextDiffuser-2, have\ndemonstrated promising results in producing images with embedded text.\nTextDiffuser-2 effectively generates bounding box layouts that guide the\nrendering of visual text, achieving high fidelity and coherence. However,\nexisting approaches often rely on resource-intensive processes and are limited\nin their ability to run efficiently on both CPU and GPU platforms. To address\nthese challenges, we propose a novel two-stage pipeline that integrates\nreinforcement learning (RL) for rapid and optimized text layout generation with\na diffusion-based image synthesis model. Our RL-based approach significantly\naccelerates the bounding box prediction step while reducing overlaps, allowing\nthe system to run efficiently on both CPUs and GPUs. Extensive evaluations\ndemonstrate that our framework maintains or surpasses TextDiffuser-2's quality\nin text placement and image synthesis, with markedly faster runtime and\nincreased flexibility. Extensive evaluations demonstrate that our framework\nmaintains or surpasses TextDiffuser-2's quality in text placement and image\nsynthesis, with markedly faster runtime and increased flexibility. Our approach\nhas been evaluated on the MARIOEval benchmark, achieving OCR and CLIPScore\nmetrics close to state-of-the-art models, while being 97.64% more faster and\nrequiring only 2MB of memory to run."}
{"id": "2505.19297", "pdf": "https://arxiv.org/pdf/2505.19297", "abs": "https://arxiv.org/abs/2505.19297", "authors": ["Valerii Startsev", "Alexander Ustyuzhanin", "Alexey Kirillov", "Dmitry Baranchuk", "Sergey Kastryulin"], "title": "Alchemist: Turning Public Text-to-Image Data into Generative Gold", "categories": ["cs.CV"], "comment": null, "summary": "Pre-training equips text-to-image (T2I) models with broad world knowledge,\nbut this alone is often insufficient to achieve high aesthetic quality and\nalignment. Consequently, supervised fine-tuning (SFT) is crucial for further\nrefinement. However, its effectiveness highly depends on the quality of the\nfine-tuning dataset. Existing public SFT datasets frequently target narrow\ndomains (e.g., anime or specific art styles), and the creation of high-quality,\ngeneral-purpose SFT datasets remains a significant challenge. Current curation\nmethods are often costly and struggle to identify truly impactful samples. This\nchallenge is further complicated by the scarcity of public general-purpose\ndatasets, as leading models often rely on large, proprietary, and poorly\ndocumented internal data, hindering broader research progress. This paper\nintroduces a novel methodology for creating general-purpose SFT datasets by\nleveraging a pre-trained generative model as an estimator of high-impact\ntraining samples. We apply this methodology to construct and release Alchemist,\na compact (3,350 samples) yet highly effective SFT dataset. Experiments\ndemonstrate that Alchemist substantially improves the generative quality of\nfive public T2I models while preserving diversity and style. Additionally, we\nrelease the fine-tuned models' weights to the public."}
{"id": "2505.19319", "pdf": "https://arxiv.org/pdf/2505.19319", "abs": "https://arxiv.org/abs/2505.19319", "authors": ["Qiang Hu", "Qimei Wang", "Jia Chen", "Xuantao Ji", "Qiang Li", "Zhiwei Wang"], "title": "Holistic White-light Polyp Classification via Alignment-free Dense Distillation of Auxiliary Optical Chromoendoscopy", "categories": ["cs.CV"], "comment": "Early Accepted by MICCAI 2025. Code and models:\n  https://github.com/Huster-Hq/ADD", "summary": "White Light Imaging (WLI) and Narrow Band Imaging (NBI) are the two main\ncolonoscopic modalities for polyp classification. While NBI, as optical\nchromoendoscopy, offers valuable vascular details, WLI remains the most common\nand often the only available modality in resource-limited settings. However,\nWLI-based methods typically underperform, limiting their clinical\napplicability. Existing approaches transfer knowledge from NBI to WLI through\nglobal feature alignment but often rely on cropped lesion regions, which are\nsusceptible to detection errors and neglect contextual and subtle diagnostic\ncues. To address this, this paper proposes a novel holistic classification\nframework that leverages full-image diagnosis without requiring polyp\nlocalization. The key innovation lies in the Alignment-free Dense Distillation\n(ADD) module, which enables fine-grained cross-domain knowledge distillation\nregardless of misalignment between WLI and NBI images. Without resorting to\nexplicit image alignment, ADD learns pixel-wise cross-domain affinities to\nestablish correspondences between feature maps, guiding the distillation along\nthe most relevant pixel connections. To further enhance distillation\nreliability, ADD incorporates Class Activation Mapping (CAM) to filter\ncross-domain affinities, ensuring the distillation path connects only those\nsemantically consistent regions with equal contributions to polyp diagnosis.\nExtensive results on public and in-house datasets show that our method achieves\nstate-of-the-art performance, relatively outperforming the other approaches by\nat least 2.5% and 16.2% in AUC, respectively. Code is available at:\nhttps://github.com/Huster-Hq/ADD."}
{"id": "2505.19328", "pdf": "https://arxiv.org/pdf/2505.19328", "abs": "https://arxiv.org/abs/2505.19328", "authors": ["Manuela González-González", "Soufiane Belharbi", "Muhammad Osama Zeeshan", "Masoumeh Sharafi", "Muhammad Haseeb Aslam", "Marco Pedersoli", "Alessandro Lameiras Koerich", "Simon L Bacon", "Eric Granger"], "title": "BAH Dataset for Ambivalence/Hesitancy Recognition in Videos for Behavioural Change", "categories": ["cs.CV", "cs.LG"], "comment": "41 pages, 13 figures, under review", "summary": "Recognizing complex emotions linked to ambivalence and hesitancy (A/H) can\nplay a critical role in the personalization and effectiveness of digital\nbehaviour change interventions. These subtle and conflicting emotions are\nmanifested by a discord between multiple modalities, such as facial and vocal\nexpressions, and body language. Although experts can be trained to identify\nA/H, integrating them into digital interventions is costly and less effective.\nAutomatic learning systems provide a cost-effective alternative that can adapt\nto individual users, and operate seamlessly within real-time, and\nresource-limited environments. However, there are currently no datasets\navailable for the design of ML models to recognize A/H. This paper introduces a\nfirst Behavioural Ambivalence/Hesitancy (BAH) dataset collected for\nsubject-based multimodal recognition of A/H in videos. It contains videos from\n224 participants captured across 9 provinces in Canada, with different age, and\nethnicity. Through our web platform, we recruited participants to answer 7\nquestions, some of which were designed to elicit A/H while recording themselves\nvia webcam with microphone. BAH amounts to 1,118 videos for a total duration of\n8.26 hours with 1.5 hours of A/H. Our behavioural team annotated timestamp\nsegments to indicate where A/H occurs, and provide frame- and video-level\nannotations with the A/H cues. Video transcripts and their timestamps are also\nincluded, along with cropped and aligned faces in each frame, and a variety of\nparticipants meta-data. We include results baselines for BAH at frame- and\nvideo-level recognition in multi-modal setups, in addition to zero-shot\nprediction, and for personalization using unsupervised domain adaptation. The\nlimited performance of baseline models highlights the challenges of recognizing\nA/H in real-world videos. The data, code, and pretrained weights are available."}
{"id": "2505.19352", "pdf": "https://arxiv.org/pdf/2505.19352", "abs": "https://arxiv.org/abs/2505.19352", "authors": ["Chenrui Ma", "Xi Xiao", "Tianyang Wang", "Yanning Shen"], "title": "Beyond Editing Pairs: Fine-Grained Instructional Image Editing via Multi-Scale Learnable Regions", "categories": ["cs.CV"], "comment": null, "summary": "Current text-driven image editing methods typically follow one of two\ndirections: relying on large-scale, high-quality editing pair datasets to\nimprove editing precision and diversity, or exploring alternative dataset-free\ntechniques. However, constructing large-scale editing datasets requires\ncarefully designed pipelines, is time-consuming, and often results in\nunrealistic samples or unwanted artifacts. Meanwhile, dataset-free methods may\nsuffer from limited instruction comprehension and restricted editing\ncapabilities. Faced with these challenges, the present work develops a novel\nparadigm for instruction-driven image editing that leverages widely available\nand enormous text-image pairs, instead of relying on editing pair datasets. Our\napproach introduces a multi-scale learnable region to localize and guide the\nediting process. By treating the alignment between images and their textual\ndescriptions as supervision and learning to generate task-specific editing\nregions, our method achieves high-fidelity, precise, and instruction-consistent\nimage editing. Extensive experiments demonstrate that the proposed approach\nattains state-of-the-art performance across various tasks and benchmarks, while\nexhibiting strong adaptability to various types of generative models."}
{"id": "2505.19373", "pdf": "https://arxiv.org/pdf/2505.19373", "abs": "https://arxiv.org/abs/2505.19373", "authors": ["Niloufar Alipour Talemi", "Hossein Kashiani", "Hossein R. Nowdeh", "Fatemeh Afghah"], "title": "DiSa: Directional Saliency-Aware Prompt Learning for Generalizable Vision-Language Models", "categories": ["cs.CV"], "comment": "Accepted at the 31st ACM SIGKDD Conference on Knowledge Discovery and\n  Data Mining (KDD 2025)", "summary": "Prompt learning has emerged as a powerful paradigm for adapting\nvision-language models such as CLIP to downstream tasks. However, existing\nmethods often overfit to seen data, leading to significant performance\ndegradation when generalizing to novel classes or unseen domains. To address\nthis limitation, we propose DiSa, a Directional Saliency-Aware Prompt Learning\nframework that integrates two complementary regularization strategies to\nenhance generalization. First, our Cross-Interactive Regularization (CIR)\nfosters cross-modal alignment by enabling cooperative learning between prompted\nand frozen encoders. Within CIR, a saliency-aware masking strategy guides the\nimage encoder to prioritize semantically critical image regions, reducing\nreliance on less informative patches. Second, we introduce a directional\nregularization strategy that aligns visual embeddings with class-wise prototype\nfeatures in a directional manner to prioritize consistency in feature\norientation over strict proximity. This approach ensures robust generalization\nby leveraging stable prototype directions derived from class-mean statistics.\nExtensive evaluations on 11 diverse image classification benchmarks demonstrate\nthat DiSa consistently outperforms state-of-the-art prompt learning methods\nacross various settings, including base-to-novel generalization, cross-dataset\ntransfer, domain generalization, and few-shot learning."}
{"id": "2505.19377", "pdf": "https://arxiv.org/pdf/2505.19377", "abs": "https://arxiv.org/abs/2505.19377", "authors": ["Zichong Meng", "Zeyu Han", "Xiaogang Peng", "Yiming Xie", "Huaizu Jiang"], "title": "Absolute Coordinates Make Motion Generation Easy", "categories": ["cs.CV"], "comment": "Preprint", "summary": "State-of-the-art text-to-motion generation models rely on the\nkinematic-aware, local-relative motion representation popularized by HumanML3D,\nwhich encodes motion relative to the pelvis and to the previous frame with\nbuilt-in redundancy. While this design simplifies training for earlier\ngeneration models, it introduces critical limitations for diffusion models and\nhinders applicability to downstream tasks. In this work, we revisit the motion\nrepresentation and propose a radically simplified and long-abandoned\nalternative for text-to-motion generation: absolute joint coordinates in global\nspace. Through systematic analysis of design choices, we show that this\nformulation achieves significantly higher motion fidelity, improved text\nalignment, and strong scalability, even with a simple Transformer backbone and\nno auxiliary kinematic-aware losses. Moreover, our formulation naturally\nsupports downstream tasks such as text-driven motion control and\ntemporal/spatial editing without additional task-specific reengineering and\ncostly classifier guidance generation from control signals. Finally, we\ndemonstrate promising generalization to directly generate SMPL-H mesh vertices\nin motion from text, laying a strong foundation for future research and\nmotion-related applications."}
{"id": "2505.19385", "pdf": "https://arxiv.org/pdf/2505.19385", "abs": "https://arxiv.org/abs/2505.19385", "authors": ["Jiaqi Guo", "Santiago Lopez-Tapia", "Aggelos K. Katsaggelos"], "title": "Advancing Limited-Angle CT Reconstruction Through Diffusion-Based Sinogram Completion", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted at the 2025 IEEE International Conference on Image\n  Processing (Oral)", "summary": "Limited Angle Computed Tomography (LACT) often faces significant challenges\ndue to missing angular information. Unlike previous methods that operate in the\nimage domain, we propose a new method that focuses on sinogram inpainting. We\nleverage MR-SDEs, a variant of diffusion models that characterize the diffusion\nprocess with mean-reverting stochastic differential equations, to fill in\nmissing angular data at the projection level. Furthermore, by combining\ndistillation with constraining the output of the model using the pseudo-inverse\nof the inpainting matrix, the diffusion process is accelerated and done in a\nstep, enabling efficient and accurate sinogram completion. A subsequent\npost-processing module back-projects the inpainted sinogram into the image\ndomain and further refines the reconstruction, effectively suppressing\nartifacts while preserving critical structural details. Quantitative\nexperimental results demonstrate that the proposed method achieves\nstate-of-the-art performance in both perceptual and fidelity quality, offering\na promising solution for LACT reconstruction in scientific and clinical\napplications."}
{"id": "2505.19386", "pdf": "https://arxiv.org/pdf/2505.19386", "abs": "https://arxiv.org/abs/2505.19386", "authors": ["Nate Gillman", "Charles Herrmann", "Michael Freeman", "Daksh Aggarwal", "Evan Luo", "Deqing Sun", "Chen Sun"], "title": "Force Prompting: Video Generation Models Can Learn and Generalize Physics-based Control Signals", "categories": ["cs.CV", "cs.AI"], "comment": "Project page: https://force-prompting.github.io/", "summary": "Recent advances in video generation models have sparked interest in world\nmodels capable of simulating realistic environments. While navigation has been\nwell-explored, physically meaningful interactions that mimic real-world forces\nremain largely understudied. In this work, we investigate using physical forces\nas a control signal for video generation and propose force prompts which enable\nusers to interact with images through both localized point forces, such as\npoking a plant, and global wind force fields, such as wind blowing on fabric.\nWe demonstrate that these force prompts can enable videos to respond\nrealistically to physical control signals by leveraging the visual and motion\nprior in the original pretrained model, without using any 3D asset or physics\nsimulator at inference. The primary challenge of force prompting is the\ndifficulty in obtaining high quality paired force-video training data, both in\nthe real world due to the difficulty of obtaining force signals, and in\nsynthetic data due to limitations in the visual quality and domain diversity of\nphysics simulators. Our key finding is that video generation models can\ngeneralize remarkably well when adapted to follow physical force conditioning\nfrom videos synthesized by Blender, even with limited demonstrations of few\nobjects. Our method can generate videos which simulate forces across diverse\ngeometries, settings, and materials. We also try to understand the source of\nthis generalization and perform ablations that reveal two key elements: visual\ndiversity and the use of specific text keywords during training. Our approach\nis trained on only around 15k training examples for a single day on four A100\nGPUs, and outperforms existing methods on force adherence and physics realism,\nbringing world models closer to real-world physics interactions. We release all\ndatasets, code, weights, and interactive video demos at our project page."}
{"id": "2505.19398", "pdf": "https://arxiv.org/pdf/2505.19398", "abs": "https://arxiv.org/abs/2505.19398", "authors": ["Yiwei Xie", "Ping Liu", "Zheng Zhang"], "title": "Erasing Concepts, Steering Generations: A Comprehensive Survey of Concept Suppression", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-Image (T2I) models have demonstrated impressive capabilities in\ngenerating high-quality and diverse visual content from natural language\nprompts. However, uncontrolled reproduction of sensitive, copyrighted, or\nharmful imagery poses serious ethical, legal, and safety challenges. To address\nthese concerns, the concept erasure paradigm has emerged as a promising\ndirection, enabling the selective removal of specific semantic concepts from\ngenerative models while preserving their overall utility. This survey provides\na comprehensive overview and in-depth synthesis of concept erasure techniques\nin T2I diffusion models. We systematically categorize existing approaches along\nthree key dimensions: intervention level, which identifies specific model\ncomponents targeted for concept removal; optimization structure, referring to\nthe algorithmic strategies employed to achieve suppression; and semantic scope,\nconcerning the complexity and nature of the concepts addressed. This\nmulti-dimensional taxonomy enables clear, structured comparisons across diverse\nmethodologies, highlighting fundamental trade-offs between erasure specificity,\ngeneralization, and computational complexity. We further discuss current\nevaluation benchmarks, standardized metrics, and practical datasets,\nemphasizing gaps that limit comprehensive assessment, particularly regarding\nrobustness and practical effectiveness. Finally, we outline major challenges\nand promising future directions, including disentanglement of concept\nrepresentations, adaptive and incremental erasure strategies, adversarial\nrobustness, and new generative architectures. This survey aims to guide\nresearchers toward safer, more ethically aligned generative models, providing\nfoundational knowledge and actionable recommendations to advance responsible\ndevelopment in generative AI."}
{"id": "2505.19415", "pdf": "https://arxiv.org/pdf/2505.19415", "abs": "https://arxiv.org/abs/2505.19415", "authors": ["Hang Hua", "Ziyun Zeng", "Yizhi Song", "Yunlong Tang", "Liu He", "Daniel Aliaga", "Wei Xiong", "Jiebo Luo"], "title": "MMIG-Bench: Towards Comprehensive and Explainable Evaluation of Multi-Modal Image Generation Models", "categories": ["cs.CV"], "comment": null, "summary": "Recent multimodal image generators such as GPT-4o, Gemini 2.0 Flash, and\nGemini 2.5 Pro excel at following complex instructions, editing images and\nmaintaining concept consistency. However, they are still evaluated by disjoint\ntoolkits: text-to-image (T2I) benchmarks that lacks multi-modal conditioning,\nand customized image generation benchmarks that overlook compositional\nsemantics and common knowledge. We propose MMIG-Bench, a comprehensive\nMulti-Modal Image Generation Benchmark that unifies these tasks by pairing\n4,850 richly annotated text prompts with 1,750 multi-view reference images\nacross 380 subjects, spanning humans, animals, objects, and artistic styles.\nMMIG-Bench is equipped with a three-level evaluation framework: (1) low-level\nmetrics for visual artifacts and identity preservation of objects; (2) novel\nAspect Matching Score (AMS): a VQA-based mid-level metric that delivers\nfine-grained prompt-image alignment and shows strong correlation with human\njudgments; and (3) high-level metrics for aesthetics and human preference.\nUsing MMIG-Bench, we benchmark 17 state-of-the-art models, including Gemini 2.5\nPro, FLUX, DreamBooth, and IP-Adapter, and validate our metrics with 32k human\nratings, yielding in-depth insights into architecture and data design. We will\nrelease the dataset and evaluation code to foster rigorous, unified evaluation\nand accelerate future innovations in multi-modal image generation."}
{"id": "2505.19420", "pdf": "https://arxiv.org/pdf/2505.19420", "abs": "https://arxiv.org/abs/2505.19420", "authors": ["Wenhua Wu", "Chenpeng Su", "Siting Zhu", "Tianchen Deng", "Zhe Liu", "Hesheng Wang"], "title": "ADD-SLAM: Adaptive Dynamic Dense SLAM with Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in Neural Radiance Fields (NeRF) and 3D Gaussian-based\nSimultaneous Localization and Mapping (SLAM) methods have demonstrated\nexceptional localization precision and remarkable dense mapping performance.\nHowever, dynamic objects introduce critical challenges by disrupting scene\nconsistency, leading to tracking drift and mapping artifacts. Existing methods\nthat employ semantic segmentation or object detection for dynamic\nidentification and filtering typically rely on predefined categorical priors,\nwhile discarding dynamic scene information crucial for robotic applications\nsuch as dynamic obstacle avoidance and environmental interaction. To overcome\nthese challenges, we propose ADD-SLAM: an Adaptive Dynamic Dense SLAM framework\nbased on Gaussian splitting. We design an adaptive dynamic identification\nmechanism grounded in scene consistency analysis, comparing geometric and\ntextural discrepancies between real-time observations and historical maps. Ours\nrequires no predefined semantic category priors and adaptively discovers scene\ndynamics. Precise dynamic object recognition effectively mitigates interference\nfrom moving targets during localization. Furthermore, we propose a\ndynamic-static separation mapping strategy that constructs a temporal Gaussian\nmodel to achieve online incremental dynamic modeling. Experiments conducted on\nmultiple dynamic datasets demonstrate our method's flexible and accurate\ndynamic segmentation capabilities, along with state-of-the-art performance in\nboth localization and mapping."}
{"id": "2505.19421", "pdf": "https://arxiv.org/pdf/2505.19421", "abs": "https://arxiv.org/abs/2505.19421", "authors": ["Bardia Safaei", "Vibashan VS", "Vishal M. Patel"], "title": "Certainty and Uncertainty Guided Active Domain Adaptation", "categories": ["cs.CV"], "comment": "Accepted at IEEE ICIP 2025", "summary": "Active Domain Adaptation (ADA) adapts models to target domains by selectively\nlabeling a few target samples. Existing ADA methods prioritize uncertain\nsamples but overlook confident ones, which often match ground-truth. We find\nthat incorporating confident predictions into the labeled set before active\nsampling reduces the search space and improves adaptation. To address this, we\npropose a collaborative framework that labels uncertain samples while treating\nhighly confident predictions as ground truth. Our method combines Gaussian\nProcess-based Active Sampling (GPAS) for identifying uncertain samples and\nPseudo-Label-based Certain Sampling (PLCS) for confident ones, progressively\nenhancing adaptation. PLCS refines the search space, and GPAS reduces the\ndomain gap, boosting the proportion of confident samples. Extensive experiments\non Office-Home and DomainNet show that our approach outperforms\nstate-of-the-art ADA methods."}
{"id": "2505.19422", "pdf": "https://arxiv.org/pdf/2505.19422", "abs": "https://arxiv.org/abs/2505.19422", "authors": ["Jiru Deng", "Tengjin Weng", "Tianyu Yang", "Wenhan Luo", "Zhiheng Li", "Wenhao Jiang"], "title": "LlamaSeg: Image Segmentation via Autoregressive Mask Generation", "categories": ["cs.CV"], "comment": null, "summary": "We present LlamaSeg, a visual autoregressive framework that unifies multiple\nimage segmentation tasks via natural language instructions. We reformulate\nimage segmentation as a visual generation problem, representing masks as\n\"visual\" tokens and employing a LLaMA-style Transformer to predict them\ndirectly from image inputs. By adhering to the next-token prediction paradigm,\nour approach naturally integrates segmentation tasks into autoregressive\narchitectures. To support large-scale training, we introduce a data annotation\npipeline and construct the SA-OVRS dataset, which contains 2M segmentation\nmasks annotated with over 5,800 open-vocabulary labels or diverse textual\ndescriptions, covering a wide spectrum of real-world scenarios. This enables\nour model to localize objects in images based on text prompts and to generate\nfine-grained masks. To more accurately evaluate the quality of masks produced\nby visual generative models, we further propose a composite metric that\ncombines Intersection over Union (IoU) with Average Hausdorff Distance (AHD),\noffering a more precise assessment of contour fidelity. Experimental results\ndemonstrate that our method surpasses existing generative models across\nmultiple datasets and yields more detailed segmentation masks."}
{"id": "2505.19425", "pdf": "https://arxiv.org/pdf/2505.19425", "abs": "https://arxiv.org/abs/2505.19425", "authors": ["Yuhao He", "Jinyu Tian", "Haiwei Wu", "Jianqing Li"], "title": "Structure Disruption: Subverting Malicious Diffusion-Based Inpainting via Self-Attention Query Perturbation", "categories": ["cs.CV", "cs.CR", "cs.LG"], "comment": null, "summary": "The rapid advancement of diffusion models has enhanced their image inpainting\nand editing capabilities but also introduced significant societal risks.\nAdversaries can exploit user images from social media to generate misleading or\nharmful content. While adversarial perturbations can disrupt inpainting, global\nperturbation-based methods fail in mask-guided editing tasks due to spatial\nconstraints. To address these challenges, we propose Structure Disruption\nAttack (SDA), a powerful protection framework for safeguarding sensitive image\nregions against inpainting-based editing. Building upon the contour-focused\nnature of self-attention mechanisms of diffusion models, SDA optimizes\nperturbations by disrupting queries in self-attention during the initial\ndenoising step to destroy the contour generation process. This targeted\ninterference directly disrupts the structural generation capability of\ndiffusion models, effectively preventing them from producing coherent images.\nWe validate our motivation through visualization techniques and extensive\nexperiments on public datasets, demonstrating that SDA achieves\nstate-of-the-art (SOTA) protection performance while maintaining strong\nrobustness."}
{"id": "2505.19434", "pdf": "https://arxiv.org/pdf/2505.19434", "abs": "https://arxiv.org/abs/2505.19434", "authors": ["X. Feng", "D. Zhang", "S. Hu", "X. Li", "M. Wu", "J. Zhang", "X. Chen", "K. Huang"], "title": "CSTrack: Enhancing RGB-X Tracking via Compact Spatiotemporal Features", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by ICML25!", "summary": "Effectively modeling and utilizing spatiotemporal features from RGB and other\nmodalities (\\eg, depth, thermal, and event data, denoted as X) is the core of\nRGB-X tracker design. Existing methods often employ two parallel branches to\nseparately process the RGB and X input streams, requiring the model to\nsimultaneously handle two dispersed feature spaces, which complicates both the\nmodel structure and computation process. More critically, intra-modality\nspatial modeling within each dispersed space incurs substantial computational\noverhead, limiting resources for inter-modality spatial modeling and temporal\nmodeling. To address this, we propose a novel tracker, CSTrack, which focuses\non modeling Compact Spatiotemporal features to achieve simple yet effective\ntracking. Specifically, we first introduce an innovative Spatial Compact Module\nthat integrates the RGB-X dual input streams into a compact spatial feature,\nenabling thorough intra- and inter-modality spatial modeling. Additionally, we\ndesign an efficient Temporal Compact Module that compactly represents temporal\nfeatures by constructing the refined target distribution heatmap. Extensive\nexperiments validate the effectiveness of our compact spatiotemporal modeling\nmethod, with CSTrack achieving new SOTA results on mainstream RGB-X benchmarks.\nThe code and models will be released at:\nhttps://github.com/XiaokunFeng/CSTrack."}
{"id": "2505.19455", "pdf": "https://arxiv.org/pdf/2505.19455", "abs": "https://arxiv.org/abs/2505.19455", "authors": ["Xu Li", "Fan Lyu"], "title": "MM-Prompt: Cross-Modal Prompt Tuning for Continual Visual Question Answering", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Continual Visual Question Answering (CVQA) based on pre-trained models(PTMs)\nhas achieved promising progress by leveraging prompt tuning to enable continual\nmulti-modal learning. However, most existing methods adopt cross-modal prompt\nisolation, constructing visual and textual prompts separately, which\nexacerbates modality imbalance and leads to degraded performance over time. To\ntackle this issue, we propose MM-Prompt, a novel framework incorporating\ncross-modal prompt query and cross-modal prompt recovery. The former enables\nbalanced prompt selection by incorporating cross-modal signals during query\nformation, while the latter promotes joint prompt reconstruction through\niterative cross-modal interactions, guided by an alignment loss to prevent\nrepresentational drift. Extensive experiments show that MM-Prompt surpasses\nprior approaches in accuracy and knowledge retention, while maintaining\nbalanced modality engagement throughout continual learning."}
{"id": "2505.19479", "pdf": "https://arxiv.org/pdf/2505.19479", "abs": "https://arxiv.org/abs/2505.19479", "authors": ["Lakshmi Aishwarya Malladi", "Navarun Gupta", "Ahmed El-Sayed", "Xingguo Xiong"], "title": "Revolutionizing Wildfire Detection with Convolutional Neural Networks: A VGG16 Model Approach", "categories": ["cs.CV", "cs.LG"], "comment": "Conference at ASEE 2025", "summary": "Over 8,024 wildfire incidents have been documented in 2024 alone, affecting\nthousands of fatalities and significant damage to infrastructure and\necosystems. Wildfires in the United States have inflicted devastating losses.\nWildfires are becoming more frequent and intense, which highlights how urgently\nefficient warning systems are needed to avoid disastrous outcomes. The goal of\nthis study is to enhance the accuracy of wildfire detection by using\nConvolutional Neural Network (CNN) built on the VGG16 architecture. The D-FIRE\ndataset, which includes several kinds of wildfire and non-wildfire images, was\nemployed in the study. Low-resolution images, dataset imbalance, and the\nnecessity for real-time applicability are some of the main challenges. These\nproblems were resolved by enriching the dataset using data augmentation\ntechniques and optimizing the VGG16 model for binary classification. The model\nproduced a low false negative rate, which is essential for reducing unexplored\nfires, despite dataset boundaries. In order to help authorities execute fast\nresponses, this work shows that deep learning models such as VGG16 can offer a\nreliable, automated approach for early wildfire recognition. For the purpose of\nreducing the impact of wildfires, our future work will concentrate on\nconnecting to systems with real-time surveillance networks and enlarging the\ndataset to cover more varied fire situations."}
{"id": "2505.19487", "pdf": "https://arxiv.org/pdf/2505.19487", "abs": "https://arxiv.org/abs/2505.19487", "authors": ["Zhuoheng Gao", "Yihao Li", "Jiyao Zhang", "Rui Zhao", "Tong Wu", "Hao Tang", "Zhaofei Yu", "Hao Dong", "Guozhang Chen", "Tiejun Huang"], "title": "SpikeStereoNet: A Brain-Inspired Framework for Stereo Depth Estimation from Spike Streams", "categories": ["cs.CV"], "comment": null, "summary": "Conventional frame-based cameras often struggle with stereo depth estimation\nin rapidly changing scenes. In contrast, bio-inspired spike cameras emit\nasynchronous events at microsecond-level resolution, providing an alternative\nsensing modality. However, existing methods lack specialized stereo algorithms\nand benchmarks tailored to the spike data. To address this gap, we propose\nSpikeStereoNet, a brain-inspired framework and the first to estimate stereo\ndepth directly from raw spike streams. The model fuses raw spike streams from\ntwo viewpoints and iteratively refines depth estimation through a recurrent\nspiking neural network (RSNN) update module. To benchmark our approach, we\nintroduce a large-scale synthetic spike stream dataset and a real-world stereo\nspike dataset with dense depth annotations. SpikeStereoNet outperforms existing\nmethods on both datasets by leveraging spike streams' ability to capture subtle\nedges and intensity shifts in challenging regions such as textureless surfaces\nand extreme lighting conditions. Furthermore, our framework exhibits strong\ndata efficiency, maintaining high accuracy even with substantially reduced\ntraining data. The source code and datasets will be publicly available."}
{"id": "2505.19492", "pdf": "https://arxiv.org/pdf/2505.19492", "abs": "https://arxiv.org/abs/2505.19492", "authors": ["Chuang Wang", "Haitao Zhou", "Ling Luo", "Qian Yu"], "title": "ViewCraft3D: High-Fidelity and View-Consistent 3D Vector Graphics Synthesis", "categories": ["cs.CV"], "comment": null, "summary": "3D vector graphics play a crucial role in various applications including 3D\nshape retrieval, conceptual design, and virtual reality interactions due to\ntheir ability to capture essential structural information with minimal\nrepresentation. While recent approaches have shown promise in generating 3D\nvector graphics, they often suffer from lengthy processing times and struggle\nto maintain view consistency. To address these limitations, we propose\nViewCraft3D (VC3D), an efficient method that leverages 3D priors to generate 3D\nvector graphics. Specifically, our approach begins with 3D object analysis,\nemploys a geometric extraction algorithm to fit 3D vector graphics to the\nunderlying structure, and applies view-consistent refinement process to enhance\nvisual quality. Our comprehensive experiments demonstrate that VC3D outperforms\nprevious methods in both qualitative and quantitative evaluations, while\nsignificantly reducing computational overhead. The resulting 3D sketches\nmaintain view consistency and effectively capture the essential characteristics\nof the original objects."}
{"id": "2505.19495", "pdf": "https://arxiv.org/pdf/2505.19495", "abs": "https://arxiv.org/abs/2505.19495", "authors": ["Wei Li", "Dezhao Luo", "Dongbao Yang", "Zhenhang Li", "Weiping Wang", "Yu Zhou"], "title": "The Role of Video Generation in Enhancing Data-Limited Action Understanding", "categories": ["cs.CV"], "comment": "IJCAI2025", "summary": "Video action understanding tasks in real-world scenarios always suffer data\nlimitations. In this paper, we address the data-limited action understanding\nproblem by bridging data scarcity. We propose a novel method that employs a\ntext-to-video diffusion transformer to generate annotated data for model\ntraining. This paradigm enables the generation of realistic annotated data on\nan infinite scale without human intervention. We proposed the information\nenhancement strategy and the uncertainty-based label smoothing tailored to\ngenerate sample training. Through quantitative and qualitative analysis, we\nobserved that real samples generally contain a richer level of information than\ngenerated samples. Based on this observation, the information enhancement\nstrategy is proposed to enhance the informative content of the generated\nsamples from two aspects: the environments and the characters. Furthermore, we\nobserved that some low-quality generated samples might negatively affect model\ntraining. To address this, we devised the uncertainty-based label smoothing\nstrategy to increase the smoothing of these samples, thus reducing their\nimpact. We demonstrate the effectiveness of the proposed method on four\ndatasets across five tasks and achieve state-of-the-art performance for\nzero-shot action recognition."}
{"id": "2505.19498", "pdf": "https://arxiv.org/pdf/2505.19498", "abs": "https://arxiv.org/abs/2505.19498", "authors": ["Nanxing Hu", "Xiaoyue Duan", "Jinchao Zhang", "Guoliang Kang"], "title": "Enhancing Visual Reliance in Text Generation: A Bayesian Perspective on Mitigating Hallucination in Large Vision-Language Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Large Vision-Language Models (LVLMs) usually generate texts which satisfy\ncontext coherence but don't match the visual input. Such a hallucination issue\nhinders LVLMs' applicability in the real world. The key to solving\nhallucination in LVLM is to make the text generation rely more on the visual\ncontent. Most previous works choose to enhance/adjust the features/output of a\nspecific modality (i.e., visual or textual) to alleviate hallucinations in\nLVLM, which do not explicitly or systematically enhance the visual reliance. In\nthis paper, we comprehensively investigate the factors which may degenerate the\nvisual reliance in text generation of LVLM from a Bayesian perspective. Based\non our observations, we propose to mitigate hallucination in LVLM from three\naspects. Firstly, we observe that not all visual tokens are informative in\ngenerating meaningful texts. We propose to evaluate and remove redundant visual\ntokens to avoid their disturbance. Secondly, LVLM may encode inappropriate\nprior information, making it lean toward generating unexpected words. We\npropose a simple yet effective way to rectify the prior from a Bayesian\nperspective. Thirdly, we observe that starting from certain steps, the\nposterior of next-token prediction conditioned on visual tokens may collapse to\na prior distribution which does not depend on any informative visual tokens at\nall. Thus, we propose to stop further text generation to avoid hallucination.\nExtensive experiments on three benchmarks including POPE, CHAIR, and MME\ndemonstrate that our method can consistently mitigate the hallucination issue\nof LVLM and performs favorably against previous state-of-the-arts."}
{"id": "2505.19500", "pdf": "https://arxiv.org/pdf/2505.19500", "abs": "https://arxiv.org/abs/2505.19500", "authors": ["Shogo Sato", "Masaru Tsuchida", "Mariko Yamaguchi", "Takuhiro Kaneko", "Kazuhiko Murasaki", "Taiga Yoshida", "Ryuichi Tanida"], "title": "Objective, Absolute and Hue-aware Metrics for Intrinsic Image Decomposition on Real-World Scenes: A Proof of Concept", "categories": ["cs.CV"], "comment": "copyright 2025 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "Intrinsic image decomposition (IID) is the task of separating an image into\nalbedo and shade. In real-world scenes, it is difficult to quantitatively\nassess IID quality due to the unavailability of ground truth. The existing\nmethod provides the relative reflection intensities based on human-judged\nannotations. However, these annotations have challenges in subjectivity,\nrelative evaluation, and hue non-assessment. To address these, we propose a\nconcept of quantitative evaluation with a calculated albedo from a\nhyperspectral imaging and light detection and ranging (LiDAR) intensity.\nAdditionally, we introduce an optional albedo densification approach based on\nspectral similarity. This paper conducted a concept verification in a\nlaboratory environment, and suggested the feasibility of an objective,\nabsolute, and hue-aware assessment. (This paper is accepted by IEEE ICIP 2025.\n)"}
{"id": "2505.19503", "pdf": "https://arxiv.org/pdf/2505.19503", "abs": "https://arxiv.org/abs/2505.19503", "authors": ["Sanghyun Kim", "Deunsol Jung", "Minsu Cho"], "title": "Locality-Aware Zero-Shot Human-Object Interaction Detection", "categories": ["cs.CV"], "comment": "Accepted to CVPR2025; Code is available at:\n  https://github.com/OreoChocolate/LAIN", "summary": "Recent methods for zero-shot Human-Object Interaction (HOI) detection\ntypically leverage the generalization ability of large Vision-Language Model\n(VLM), i.e., CLIP, on unseen categories, showing impressive results on various\nzero-shot settings. However, existing methods struggle to adapt CLIP\nrepresentations for human-object pairs, as CLIP tends to overlook fine-grained\ninformation necessary for distinguishing interactions. To address this issue,\nwe devise, LAIN, a novel zero-shot HOI detection framework enhancing the\nlocality and interaction awareness of CLIP representations. The locality\nawareness, which involves capturing fine-grained details and the spatial\nstructure of individual objects, is achieved by aggregating the information and\nspatial priors of adjacent neighborhood patches. The interaction awareness,\nwhich involves identifying whether and how a human is interacting with an\nobject, is achieved by capturing the interaction pattern between the human and\nthe object. By infusing locality and interaction awareness into CLIP\nrepresentation, LAIN captures detailed information about the human-object\npairs. Our extensive experiments on existing benchmarks show that LAIN\noutperforms previous methods on various zero-shot settings, demonstrating the\nimportance of locality and interaction awareness for effective zero-shot HOI\ndetection."}
{"id": "2505.19507", "pdf": "https://arxiv.org/pdf/2505.19507", "abs": "https://arxiv.org/abs/2505.19507", "authors": ["Chenyu Lu", "Shiliang Sun", "Jing Zhao", "Nan Zhang", "Tengfei Song", "Hao Yang"], "title": "Multimodal Machine Translation with Visual Scene Graph Pruning", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Multimodal machine translation (MMT) seeks to address the challenges posed by\nlinguistic polysemy and ambiguity in translation tasks by incorporating visual\ninformation. A key bottleneck in current MMT research is the effective\nutilization of visual data. Previous approaches have focused on extracting\nglobal or region-level image features and using attention or gating mechanisms\nfor multimodal information fusion. However, these methods have not adequately\ntackled the issue of visual information redundancy in MMT, nor have they\nproposed effective solutions. In this paper, we introduce a novel\napproach--multimodal machine translation with visual Scene Graph Pruning (PSG),\nwhich leverages language scene graph information to guide the pruning of\nredundant nodes in visual scene graphs, thereby reducing noise in downstream\ntranslation tasks. Through extensive comparative experiments with\nstate-of-the-art methods and ablation studies, we demonstrate the effectiveness\nof the PSG model. Our results also highlight the promising potential of visual\ninformation pruning in advancing the field of MMT."}
{"id": "2505.19518", "pdf": "https://arxiv.org/pdf/2505.19518", "abs": "https://arxiv.org/abs/2505.19518", "authors": ["Nakul Poudel", "Zixin Yang", "Kelly Merrell", "Richard Simon", "Cristian A. Linte"], "title": "Toward Patient-specific Partial Point Cloud to Surface Completion for Pre- to Intra-operative Registration in Image-guided Liver Interventions", "categories": ["cs.CV"], "comment": null, "summary": "Intra-operative data captured during image-guided surgery lacks sub-surface\ninformation, where key regions of interest, such as vessels and tumors, reside.\nImage-to-physical registration enables the fusion of pre-operative information\nand intra-operative data, typically represented as a point cloud. However, this\nregistration process struggles due to partial visibility of the intra-operative\npoint cloud. In this research, we propose a patient-specific point cloud\ncompletion approach to assist with the registration process. Specifically, we\nleverage VN-OccNet to generate a complete liver surface from a partial\nintra-operative point cloud. The network is trained in a patient-specific\nmanner, where simulated deformations from the pre-operative model are used to\ntrain the model. First, we conduct an in-depth analysis of VN-OccNet's\nrotation-equivariant property and its effectiveness in recovering complete\nsurfaces from partial intra-operative surfaces. Next, we integrate the\ncompleted intra-operative surface into the Go-ICP registration algorithm to\ndemonstrate its utility in improving initial rigid registration outcomes. Our\nresults highlight the promise of this patient-specific completion approach in\nmitigating the challenges posed by partial intra-operative visibility. The\nrotation equivariant and surface generation capabilities of VN-OccNet hold\nstrong promise for developing robust registration frameworks for variations of\nthe intra-operative point cloud."}
{"id": "2505.19519", "pdf": "https://arxiv.org/pdf/2505.19519", "abs": "https://arxiv.org/abs/2505.19519", "authors": ["Gihoon Kim", "Hyungjin Park", "Taesup Kim"], "title": "Regularized Personalization of Text-to-Image Diffusion Models without Distributional Drift", "categories": ["cs.CV"], "comment": null, "summary": "Personalization using text-to-image diffusion models involves adapting a\npretrained model to novel subjects with only a few image examples. This task\npresents a fundamental challenge, as the model must not only learn the new\nsubject effectively but also preserve its ability to generate diverse and\ncoherent outputs across a wide range of prompts. In other words, successful\npersonalization requires integrating new concepts without forgetting previously\nlearned generative capabilities. Forgetting denotes unintended distributional\ndrift, where the model's output distribution deviates from that of the original\npretrained model. In this paper, we provide an analysis of this issue and\nidentify a mismatch between standard training objectives and the goals of\npersonalization. To address this, we propose a new training objective based on\na Lipschitz-bounded formulation that explicitly constrains deviation from the\npretrained distribution. Our method provides improved control over\ndistributional drift and performs well even in data-scarce scenarios.\nExperimental results demonstrate that our approach consistently outperforms\nexisting personalization methods, achieving higher CLIP-T, CLIP-I, and DINO\nscores."}
{"id": "2505.19522", "pdf": "https://arxiv.org/pdf/2505.19522", "abs": "https://arxiv.org/abs/2505.19522", "authors": ["Jiyu Hu", "Haijiang Zeng", "Zhen Tian"], "title": "Applications and Effect Evaluation of Generative Adversarial Networks in Semi-Supervised Learning", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "In recent years, image classification, as a core task in computer vision,\nrelies on high-quality labelled data, which restricts the wide application of\ndeep learning models in practical scenarios. To alleviate the problem of\ninsufficient labelled samples, semi-supervised learning has gradually become a\nresearch hotspot. In this paper, we construct a semi-supervised image\nclassification model based on Generative Adversarial Networks (GANs), and\nthrough the introduction of the collaborative training mechanism of generators,\ndiscriminators and classifiers, we achieve the effective use of limited\nlabelled data and a large amount of unlabelled data, improve the quality of\nimage generation and classification accuracy, and provide an effective solution\nfor the task of image recognition in complex environments."}
{"id": "2505.19535", "pdf": "https://arxiv.org/pdf/2505.19535", "abs": "https://arxiv.org/abs/2505.19535", "authors": ["Juntong Wang", "Jiarui Wang", "Huiyu Duan", "Guangtao Zhai", "Xiongkuo Min"], "title": "TDVE-Assessor: Benchmarking and Evaluating the Quality of Text-Driven Video Editing with LMMs", "categories": ["cs.CV"], "comment": "25 pages, 14 figures, 8 tables", "summary": "Text-driven video editing is rapidly advancing, yet its rigorous evaluation\nremains challenging due to the absence of dedicated video quality assessment\n(VQA) models capable of discerning the nuances of editing quality. To address\nthis critical gap, we introduce TDVE-DB, a large-scale benchmark dataset for\ntext-driven video editing. TDVE-DB consists of 3,857 edited videos generated\nfrom 12 diverse models across 8 editing categories, and is annotated with\n173,565 human subjective ratings along three crucial dimensions, i.e., edited\nvideo quality, editing alignment, and structural consistency. Based on TDVE-DB,\nwe first conduct a comprehensive evaluation for the 12 state-of-the-art editing\nmodels revealing the strengths and weaknesses of current video techniques, and\nthen benchmark existing VQA methods in the context of text-driven video editing\nevaluation. Building on these insights, we propose TDVE-Assessor, a novel VQA\nmodel specifically designed for text-driven video editing assessment.\nTDVE-Assessor integrates both spatial and temporal video features into a large\nlanguage model (LLM) for rich contextual understanding to provide comprehensive\nquality assessment. Extensive experiments demonstrate that TDVE-Assessor\nsubstantially outperforms existing VQA models on TDVE-DB across all three\nevaluation dimensions, setting a new state-of-the-art. Both TDVE-DB and\nTDVE-Assessor will be released upon the publication."}
{"id": "2505.19536", "pdf": "https://arxiv.org/pdf/2505.19536", "abs": "https://arxiv.org/abs/2505.19536", "authors": ["Jintao Tong", "Wenwei Jin", "Pengda Qin", "Anqi Li", "Yixiong Zou", "Yuhong Li", "Yuhua Li", "Ruixuan Li"], "title": "FlowCut: Rethinking Redundancy via Information Flow for Efficient Vision-Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "19 pages, 11 figures", "summary": "Large vision-language models (LVLMs) excel at multimodal understanding but\nsuffer from high computational costs due to redundant vision tokens. Existing\npruning methods typically rely on single-layer attention scores to rank and\nprune redundant visual tokens to solve this inefficiency. However, as the\ninteraction between tokens and layers is complicated, this raises a basic\nquestion: Is such a simple single-layer criterion sufficient to identify\nredundancy? To answer this question, we rethink the emergence of redundant\nvisual tokens from a fundamental perspective: information flow, which models\nthe interaction between tokens and layers by capturing how information moves\nbetween tokens across layers. We find (1) the CLS token acts as an information\nrelay, which can simplify the complicated flow analysis; (2) the redundancy\nemerges progressively and dynamically via layer-wise attention concentration;\nand (3) relying solely on attention scores from single layers can lead to\ncontradictory redundancy identification. Based on this, we propose FlowCut, an\ninformation-flow-aware pruning framework, mitigating the insufficiency of the\ncurrent criterion for identifying redundant tokens and better aligning with the\nmodel's inherent behaviors. Extensive experiments show that FlowCut achieves\nsuperior results, outperforming SoTA by 1.6% on LLaVA-1.5-7B with 88.9% token\nreduction, and by 4.3% on LLaVA-NeXT-7B with 94.4% reduction, delivering 3.2x\nspeed-up in the prefilling stage. Our code is available at\nhttps://github.com/TungChintao/FlowCut"}
{"id": "2505.19546", "pdf": "https://arxiv.org/pdf/2505.19546", "abs": "https://arxiv.org/abs/2505.19546", "authors": ["Ali Bahri", "Moslem Yazdanpanah", "Sahar Dastani", "Mehrdad Noori", "Gustavo Adolfo Vargas Hakim", "David Osowiechi", "Farzad Beizaee", "Ismail Ben Ayed", "Christian Desrosiers"], "title": "SMART-PC: Skeletal Model Adaptation for Robust Test-Time Training in Point Clouds", "categories": ["cs.CV"], "comment": null, "summary": "Test-Time Training (TTT) has emerged as a promising solution to address\ndistribution shifts in 3D point cloud classification. However, existing methods\noften rely on computationally expensive backpropagation during adaptation,\nlimiting their applicability in real-world, time-sensitive scenarios. In this\npaper, we introduce SMART-PC, a skeleton-based framework that enhances\nresilience to corruptions by leveraging the geometric structure of 3D point\nclouds. During pre-training, our method predicts skeletal representations,\nenabling the model to extract robust and meaningful geometric features that are\nless sensitive to corruptions, thereby improving adaptability to test-time\ndistribution shifts. Unlike prior approaches, SMART-PC achieves real-time\nadaptation by eliminating backpropagation and updating only BatchNorm\nstatistics, resulting in a lightweight and efficient framework capable of\nachieving high frame-per-second rates while maintaining superior classification\nperformance. Extensive experiments on benchmark datasets, including\nModelNet40-C, ShapeNet-C, and ScanObjectNN-C, demonstrate that SMART-PC\nachieves state-of-the-art results, outperforming existing methods such as MATE\nin terms of both accuracy and computational efficiency. The implementation is\navailable at: https://github.com/AliBahri94/SMART-PC."}
{"id": "2505.19554", "pdf": "https://arxiv.org/pdf/2505.19554", "abs": "https://arxiv.org/abs/2505.19554", "authors": ["Jiongchao Jin", "Shengchu Zhao", "Dajun Chen", "Wei Jiang", "Yong Li"], "title": "Aggregated Structural Representation with Large Language Models for Human-Centric Layout Generation", "categories": ["cs.CV"], "comment": null, "summary": "Time consumption and the complexity of manual layout design make automated\nlayout generation a critical task, especially for multiple applications across\ndifferent mobile devices. Existing graph-based layout generation approaches\nsuffer from limited generative capability, often resulting in unreasonable and\nincompatible outputs. Meanwhile, vision based generative models tend to\noverlook the original structural information, leading to component\nintersections and overlaps. To address these challenges, we propose an\nAggregation Structural Representation (ASR) module that integrates graph\nnetworks with large language models (LLMs) to preserve structural information\nwhile enhancing generative capability. This novel pipeline utilizes graph\nfeatures as hierarchical prior knowledge, replacing the traditional Vision\nTransformer (ViT) module in multimodal large language models (MLLM) to predict\nfull layout information for the first time. Moreover, the intermediate graph\nmatrix used as input for the LLM is human editable, enabling progressive, human\ncentric design generation. A comprehensive evaluation on the RICO dataset\ndemonstrates the strong performance of ASR, both quantitatively using mean\nIntersection over Union (mIoU), and qualitatively through a crowdsourced user\nstudy. Additionally, sampling on relational features ensures diverse layout\ngeneration, further enhancing the adaptability and creativity of the proposed\napproach."}
{"id": "2505.19564", "pdf": "https://arxiv.org/pdf/2505.19564", "abs": "https://arxiv.org/abs/2505.19564", "authors": ["Haofan Ren", "Zunjie Zhu", "Xiang Chen", "Ming Lu", "Rongfeng Lu", "Chenggang Yan"], "title": "K-Buffers: A Plug-in Method for Enhancing Neural Fields with Multiple Buffers", "categories": ["cs.CV"], "comment": "15 pages, 9 figures, IJCAI 2025", "summary": "Neural fields are now the central focus of research in 3D vision and computer\ngraphics. Existing methods mainly focus on various scene representations, such\nas neural points and 3D Gaussians. However, few works have studied the\nrendering process to enhance the neural fields. In this work, we propose a\nplug-in method named K-Buffers that leverages multiple buffers to improve the\nrendering performance. Our method first renders K buffers from scene\nrepresentations and constructs K pixel-wise feature maps. Then, We introduce a\nK-Feature Fusion Network (KFN) to merge the K pixel-wise feature maps. Finally,\nwe adopt a feature decoder to generate the rendering image. We also introduce\nan acceleration strategy to improve rendering speed and quality. We apply our\nmethod to well-known radiance field baselines, including neural point fields\nand 3D Gaussian Splatting (3DGS). Extensive experiments demonstrate that our\nmethod effectively enhances the rendering performance of neural point fields\nand 3DGS."}
{"id": "2505.19565", "pdf": "https://arxiv.org/pdf/2505.19565", "abs": "https://arxiv.org/abs/2505.19565", "authors": ["George Karantaidis", "Athanasios Pantsios", "Ioannis Kompatsiaris", "Symeon Papadopoulos"], "title": "Few-Shot Class-Incremental Learning For Efficient SAR Automatic Target Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Synthetic aperture radar automatic target recognition (SAR-ATR) systems have\nrapidly evolved to tackle incremental recognition challenges in operational\nsettings. Data scarcity remains a major hurdle that conventional SAR-ATR\ntechniques struggle to address. To cope with this challenge, we propose a\nfew-shot class-incremental learning (FSCIL) framework based on a dual-branch\narchitecture that focuses on local feature extraction and leverages the\ndiscrete Fourier transform and global filters to capture long-term spatial\ndependencies. This incorporates a lightweight cross-attention mechanism that\nfuses domain-specific features with global dependencies to ensure robust\nfeature interaction, while maintaining computational efficiency by introducing\nminimal scale-shift parameters. The framework combines focal loss for class\ndistinction under imbalance and center loss for compact intra-class\ndistributions to enhance class separation boundaries. Experimental results on\nthe MSTAR benchmark dataset demonstrate that the proposed framework\nconsistently outperforms state-of-the-art methods in FSCIL SAR-ATR, attesting\nto its effectiveness in real-world scenarios."}
{"id": "2505.19569", "pdf": "https://arxiv.org/pdf/2505.19569", "abs": "https://arxiv.org/abs/2505.19569", "authors": ["Jianghang Lin", "Yue Hu", "Jiangtao Shen", "Yunhang Shen", "Liujuan Cao", "Shengchuan Zhang", "Rongrong Ji"], "title": "What You Perceive Is What You Conceive: A Cognition-Inspired Framework for Open Vocabulary Image Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Open vocabulary image segmentation tackles the challenge of recognizing\ndynamically adjustable, predefined novel categories at inference time by\nleveraging vision-language alignment. However, existing paradigms typically\nperform class-agnostic region segmentation followed by category matching, which\ndeviates from the human visual system's process of recognizing objects based on\nsemantic concepts, leading to poor alignment between region segmentation and\ntarget concepts. To bridge this gap, we propose a novel Cognition-Inspired\nFramework for open vocabulary image segmentation that emulates the human visual\nrecognition process: first forming a conceptual understanding of an object,\nthen perceiving its spatial extent. The framework consists of three core\ncomponents: (1) A Generative Vision-Language Model (G-VLM) that mimics human\ncognition by generating object concepts to provide semantic guidance for region\nsegmentation. (2) A Concept-Aware Visual Enhancer Module that fuses textual\nconcept features with global visual representations, enabling adaptive visual\nperception based on target concepts. (3) A Cognition-Inspired Decoder that\nintegrates local instance features with G-VLM-provided semantic cues, allowing\nselective classification over a subset of relevant categories. Extensive\nexperiments demonstrate that our framework achieves significant improvements,\nreaching $27.2$ PQ, $17.0$ mAP, and $35.3$ mIoU on A-150. It further attains\n$56.2$, $28.2$, $15.4$, $59.2$, $18.7$, and $95.8$ mIoU on Cityscapes,\nMapillary Vistas, A-847, PC-59, PC-459, and PAS-20, respectively. In addition,\nour framework supports vocabulary-free segmentation, offering enhanced\nflexibility in recognizing unseen categories. Code will be public."}
{"id": "2505.19571", "pdf": "https://arxiv.org/pdf/2505.19571", "abs": "https://arxiv.org/abs/2505.19571", "authors": ["Hu Xiaobin", "Liang Yujie", "Luo Donghao", "Peng Xu", "Zhang Jiangning", "Zhu Junwei", "Wang Chengjie", "Fu Yanwei"], "title": "VTBench: Comprehensive Benchmark Suite Towards Real-World Virtual Try-on Models", "categories": ["cs.CV"], "comment": "Project Websit: \\url{https://github.com/HUuxiaobin/VTBench}", "summary": "While virtual try-on has achieved significant progress, evaluating these\nmodels towards real-world scenarios remains a challenge. A comprehensive\nbenchmark is essential for three key reasons:(1) Current metrics inadequately\nreflect human perception, particularly in unpaired try-on settings;(2)Most\nexisting test sets are limited to indoor scenarios, lacking complexity for\nreal-world evaluation; and (3) An ideal system should guide future advancements\nin virtual try-on generation. To address these needs, we introduce VTBench, a\nhierarchical benchmark suite that systematically decomposes virtual image\ntry-on into hierarchical, disentangled dimensions, each equipped with tailored\ntest sets and evaluation criteria. VTBench exhibits three key advantages:1)\nMulti-Dimensional Evaluation Framework: The benchmark encompasses five critical\ndimensions for virtual try-on generation (e.g., overall image quality, texture\npreservation, complex background consistency, cross-category size adaptability,\nand hand-occlusion handling). Granular evaluation metrics of corresponding test\nsets pinpoint model capabilities and limitations across diverse, challenging\nscenarios.2) Human Alignment: Human preference annotations are provided for\neach test set, ensuring the benchmark's alignment with perceptual quality\nacross all evaluation dimensions. (3) Valuable Insights: Beyond standard indoor\nsettings, we analyze model performance variations across dimensions and\ninvestigate the disparity between indoor and real-world try-on scenarios. To\nfoster the field of virtual try-on towards challenging real-world scenario,\nVTBench will be open-sourced, including all test sets, evaluation protocols,\ngenerated results, and human annotations."}
{"id": "2505.19582", "pdf": "https://arxiv.org/pdf/2505.19582", "abs": "https://arxiv.org/abs/2505.19582", "authors": ["Kaiqing Lin", "Zhiyuan Yan", "Ke-Yue Zhang", "Li Hao", "Yue Zhou", "Yuzhen Lin", "Weixiang Li", "Taiping Yao", "Shouhong Ding", "Bin Li"], "title": "Guard Me If You Know Me: Protecting Specific Face-Identity from Deepfakes", "categories": ["cs.CV"], "comment": null, "summary": "Securing personal identity against deepfake attacks is increasingly critical\nin the digital age, especially for celebrities and political figures whose\nfaces are easily accessible and frequently targeted. Most existing deepfake\ndetection methods focus on general-purpose scenarios and often ignore the\nvaluable prior knowledge of known facial identities, e.g., \"VIP individuals\"\nwhose authentic facial data are already available. In this paper, we propose\n\\textbf{VIPGuard}, a unified multimodal framework designed to capture\nfine-grained and comprehensive facial representations of a given identity,\ncompare them against potentially fake or similar-looking faces, and reason over\nthese comparisons to make accurate and explainable predictions. Specifically,\nour framework consists of three main stages. First, fine-tune a multimodal\nlarge language model (MLLM) to learn detailed and structural facial attributes.\nSecond, we perform identity-level discriminative learning to enable the model\nto distinguish subtle differences between highly similar faces, including real\nand fake variations. Finally, we introduce user-specific customization, where\nwe model the unique characteristics of the target face identity and perform\nsemantic reasoning via MLLM to enable personalized and explainable deepfake\ndetection. Our framework shows clear advantages over previous detection works,\nwhere traditional detectors mainly rely on low-level visual cues and provide no\nhuman-understandable explanations, while other MLLM-based models often lack a\ndetailed understanding of specific face identities. To facilitate the\nevaluation of our method, we built a comprehensive identity-aware benchmark\ncalled \\textbf{VIPBench} for personalized deepfake detection, involving the\nlatest 7 face-swapping and 7 entire face synthesis techniques for generation."}
{"id": "2505.19585", "pdf": "https://arxiv.org/pdf/2505.19585", "abs": "https://arxiv.org/abs/2505.19585", "authors": ["Jiameng Li", "Teodora Popordanoska", "Sebastian G. Gruber", "Frederik Maes", "Matthew B. Blaschko"], "title": "Beyond Segmentation: Confidence-Aware and Debiased Estimation of Ratio-based Biomarkers", "categories": ["cs.CV"], "comment": "9 pages", "summary": "Ratio-based biomarkers -- such as the proportion of necrotic tissue within a\ntumor -- are widely used in clinical practice to support diagnosis, prognosis\nand treatment planning. These biomarkers are typically estimated from soft\nsegmentation outputs by computing region-wise ratios. Despite the high-stakes\nnature of clinical decision making, existing methods provide only point\nestimates, offering no measure of uncertainty. In this work, we propose a\nunified \\textit{confidence-aware} framework for estimating ratio-based\nbiomarkers. We conduct a systematic analysis of error propagation in the\nsegmentation-to-biomarker pipeline and identify model miscalibration as the\ndominant source of uncertainty. To mitigate this, we incorporate a lightweight,\npost-hoc calibration module that can be applied using internal hospital data\nwithout retraining. We leverage a tunable parameter $Q$ to control the\nconfidence level of the derived bounds, allowing adaptation towards clinical\npractice. Extensive experiments show that our method produces statistically\nsound confidence intervals, with tunable confidence levels, enabling more\ntrustworthy application of predictive biomarkers in clinical workflows."}
{"id": "2505.19603", "pdf": "https://arxiv.org/pdf/2505.19603", "abs": "https://arxiv.org/abs/2505.19603", "authors": ["Ho Hin Lee", "Quan Liu", "Shunxing Bao", "Yuankai Huo", "Bennett A. Landman"], "title": "Rep3D: Re-parameterize Large 3D Kernels with Low-Rank Receptive Modeling for Medical Imaging", "categories": ["cs.CV", "cs.LG"], "comment": "14 pages", "summary": "In contrast to vision transformers, which model long-range dependencies\nthrough global self-attention, large kernel convolutions provide a more\nefficient and scalable alternative, particularly in high-resolution 3D\nvolumetric settings. However, naively increasing kernel size often leads to\noptimization instability and degradation in performance. Motivated by the\nspatial bias observed in effective receptive fields (ERFs), we hypothesize that\ndifferent kernel elements converge at variable rates during training. To\nsupport this, we derive a theoretical connection between element-wise gradients\nand first-order optimization, showing that structurally re-parameterized\nconvolution blocks inherently induce spatially varying learning rates. Building\non this insight, we introduce Rep3D, a 3D convolutional framework that\nincorporates a learnable spatial prior into large kernel training. A\nlightweight two-stage modulation network generates a receptive-biased scaling\nmask, adaptively re-weighting kernel updates and enabling local-to-global\nconvergence behavior. Rep3D adopts a plain encoder design with large depthwise\nconvolutions, avoiding the architectural complexity of multi-branch\ncompositions. We evaluate Rep3D on five challenging 3D segmentation benchmarks\nand demonstrate consistent improvements over state-of-the-art baselines,\nincluding transformer-based and fixed-prior re-parameterization methods. By\nunifying spatial inductive bias with optimization-aware learning, Rep3D offers\nan interpretable, and scalable solution for 3D medical image analysis. The\nsource code is publicly available at https://github.com/leeh43/Rep3D."}
{"id": "2505.19610", "pdf": "https://arxiv.org/pdf/2505.19610", "abs": "https://arxiv.org/abs/2505.19610", "authors": ["Jiaxin Song", "Yixu Wang", "Jie Li", "Rui Yu", "Yan Teng", "Xingjun Ma", "Yingchun Wang"], "title": "JailBound: Jailbreaking Internal Safety Boundaries of Vision-Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Vision-Language Models (VLMs) exhibit impressive performance, yet the\nintegration of powerful vision encoders has significantly broadened their\nattack surface, rendering them increasingly susceptible to jailbreak attacks.\nHowever, lacking well-defined attack objectives, existing jailbreak methods\noften struggle with gradient-based strategies prone to local optima and lacking\nprecise directional guidance, and typically decouple visual and textual\nmodalities, thereby limiting their effectiveness by neglecting crucial\ncross-modal interactions. Inspired by the Eliciting Latent Knowledge (ELK)\nframework, we posit that VLMs encode safety-relevant information within their\ninternal fusion-layer representations, revealing an implicit safety decision\nboundary in the latent space. This motivates exploiting boundary to steer model\nbehavior. Accordingly, we propose JailBound, a novel latent space jailbreak\nframework comprising two stages: (1) Safety Boundary Probing, which addresses\nthe guidance issue by approximating decision boundary within fusion layer's\nlatent space, thereby identifying optimal perturbation directions towards the\ntarget region; and (2) Safety Boundary Crossing, which overcomes the\nlimitations of decoupled approaches by jointly optimizing adversarial\nperturbations across both image and text inputs. This latter stage employs an\ninnovative mechanism to steer the model's internal state towards\npolicy-violating outputs while maintaining cross-modal semantic consistency.\nExtensive experiments on six diverse VLMs demonstrate JailBound's efficacy,\nachieves 94.32% white-box and 67.28% black-box attack success averagely, which\nare 6.17% and 21.13% higher than SOTA methods, respectively. Our findings\nexpose a overlooked safety risk in VLMs and highlight the urgent need for more\nrobust defenses. Warning: This paper contains potentially sensitive, harmful\nand offensive content."}
{"id": "2505.19611", "pdf": "https://arxiv.org/pdf/2505.19611", "abs": "https://arxiv.org/abs/2505.19611", "authors": ["Ruolin Shen", "Xiaozhong Ji", "Kai WU", "Jiangning Zhang", "Yijun He", "HaiHua Yang", "Xiaobin Hu", "Xiaoyu Sun"], "title": "Align and Surpass Human Camouflaged Perception: Visual Refocus Reinforcement Fine-Tuning", "categories": ["cs.CV", "cs.AI"], "comment": "Project Website: \\url{https://github.com/HUuxiaobin/VRRF}", "summary": "Current multi-modal models exhibit a notable misalignment with the human\nvisual system when identifying objects that are visually assimilated into the\nbackground. Our observations reveal that these multi-modal models cannot\ndistinguish concealed objects, demonstrating an inability to emulate human\ncognitive processes which effectively utilize foreground-background similarity\nprinciples for visual analysis. To analyze this hidden human-model visual\nthinking discrepancy, we build a visual system that mimicks human visual\ncamouflaged perception to progressively and iteratively `refocus' visual\nconcealed content. The refocus is a progressive guidance mechanism enabling\nmodels to logically localize objects in visual images through stepwise\nreasoning. The localization process of concealed objects requires hierarchical\nattention shifting with dynamic adjustment and refinement of prior cognitive\nknowledge. In this paper, we propose a visual refocus reinforcement framework\nvia the policy optimization algorithm to encourage multi-modal models to think\nand refocus more before answering, and achieve excellent reasoning abilities to\nalign and even surpass human camouflaged perception systems. Our extensive\nexperiments on camouflaged perception successfully demonstrate the emergence of\nrefocus visual phenomena, characterized by multiple reasoning tokens and\ndynamic adjustment of the detection box. Besides, experimental results on both\ncamouflaged object classification and detection tasks exhibit significantly\nsuperior performance compared to Supervised Fine-Tuning (SFT) baselines."}
{"id": "2505.19613", "pdf": "https://arxiv.org/pdf/2505.19613", "abs": "https://arxiv.org/abs/2505.19613", "authors": ["Amira Guesmi", "Bassem Ouni", "Muhammad Shafique"], "title": "TESSER: Transfer-Enhancing Adversarial Attacks from Vision Transformers via Spectral and Semantic Regularization", "categories": ["cs.CV"], "comment": null, "summary": "Adversarial transferability remains a critical challenge in evaluating the\nrobustness of deep neural networks. In security-critical applications,\ntransferability enables black-box attacks without access to model internals,\nmaking it a key concern for real-world adversarial threat assessment. While\nVision Transformers (ViTs) have demonstrated strong adversarial performance,\nexisting attacks often fail to transfer effectively across architectures,\nespecially from ViTs to Convolutional Neural Networks (CNNs) or hybrid models.\nIn this paper, we introduce \\textbf{TESSER} -- a novel adversarial attack\nframework that enhances transferability via two key strategies: (1)\n\\textit{Feature-Sensitive Gradient Scaling (FSGS)}, which modulates gradients\nbased on token-wise importance derived from intermediate feature activations,\nand (2) \\textit{Spectral Smoothness Regularization (SSR)}, which suppresses\nhigh-frequency noise in perturbations using a differentiable Gaussian prior.\nThese components work in tandem to generate perturbations that are both\nsemantically meaningful and spectrally smooth. Extensive experiments on\nImageNet across 12 diverse architectures demonstrate that TESSER achieves\n+10.9\\% higher attack succes rate (ASR) on CNNs and +7.2\\% on ViTs compared to\nthe state-of-the-art Adaptive Token Tuning (ATT) method. Moreover, TESSER\nsignificantly improves robustness against defended models, achieving 53.55\\%\nASR on adversarially trained CNNs. Qualitative analysis shows strong alignment\nbetween TESSER's perturbations and salient visual regions identified via\nGrad-CAM, while frequency-domain analysis reveals a 12\\% reduction in\nhigh-frequency energy, confirming the effectiveness of spectral regularization."}
{"id": "2505.19618", "pdf": "https://arxiv.org/pdf/2505.19618", "abs": "https://arxiv.org/abs/2505.19618", "authors": ["Hanze Liu", "Jiahong Fu", "Qi Xie", "Deyu Meng"], "title": "Rotation-Equivariant Self-Supervised Method in Image Denoising", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Self-supervised image denoising methods have garnered significant research\nattention in recent years, for this kind of method reduces the requirement of\nlarge training datasets. Compared to supervised methods, self-supervised\nmethods rely more on the prior embedded in deep networks themselves. As a\nresult, most of the self-supervised methods are designed with Convolution\nNeural Networks (CNNs) architectures, which well capture one of the most\nimportant image prior, translation equivariant prior. Inspired by the great\nsuccess achieved by the introduction of translational equivariance, in this\npaper, we explore the way to further incorporate another important image prior.\nSpecifically, we first apply high-accuracy rotation equivariant convolution to\nself-supervised image denoising. Through rigorous theoretical analysis, we have\nproved that simply replacing all the convolution layers with rotation\nequivariant convolution layers would modify the network into its rotation\nequivariant version. To the best of our knowledge, this is the first time that\nrotation equivariant image prior is introduced to self-supervised image\ndenoising at the network architecture level with a comprehensive theoretical\nanalysis of equivariance errors, which offers a new perspective to the field of\nself-supervised image denoising. Moreover, to further improve the performance,\nwe design a new mask mechanism to fusion the output of rotation equivariant\nnetwork and vanilla CNN-based network, and construct an adaptive rotation\nequivariant framework. Through extensive experiments on three typical methods,\nwe have demonstrated the effectiveness of the proposed method."}
{"id": "2505.19624", "pdf": "https://arxiv.org/pdf/2505.19624", "abs": "https://arxiv.org/abs/2505.19624", "authors": ["Pusheng Xu", "Xia Gong", "Xiaolan Chen", "Weiyi Zhang", "Jiancheng Yang", "Bingjie Yan", "Meng Yuan", "Yalin Zheng", "Mingguang He", "Danli Shi"], "title": "Benchmarking Large Multimodal Models for Ophthalmic Visual Question Answering with OphthalWeChat", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Purpose: To develop a bilingual multimodal visual question answering (VQA)\nbenchmark for evaluating VLMs in ophthalmology. Methods: Ophthalmic image posts\nand associated captions published between January 1, 2016, and December 31,\n2024, were collected from WeChat Official Accounts. Based on these captions,\nbilingual question-answer (QA) pairs in Chinese and English were generated\nusing GPT-4o-mini. QA pairs were categorized into six subsets by question type\nand language: binary (Binary_CN, Binary_EN), single-choice (Single-choice_CN,\nSingle-choice_EN), and open-ended (Open-ended_CN, Open-ended_EN). The benchmark\nwas used to evaluate the performance of three VLMs: GPT-4o, Gemini 2.0 Flash,\nand Qwen2.5-VL-72B-Instruct. Results: The final OphthalWeChat dataset included\n3,469 images and 30,120 QA pairs across 9 ophthalmic subspecialties, 548\nconditions, 29 imaging modalities, and 68 modality combinations. Gemini 2.0\nFlash achieved the highest overall accuracy (0.548), outperforming GPT-4o\n(0.522, P < 0.001) and Qwen2.5-VL-72B-Instruct (0.514, P < 0.001). It also led\nin both Chinese (0.546) and English subsets (0.550). Subset-specific\nperformance showed Gemini 2.0 Flash excelled in Binary_CN (0.687),\nSingle-choice_CN (0.666), and Single-choice_EN (0.646), while GPT-4o ranked\nhighest in Binary_EN (0.717), Open-ended_CN (BLEU-1: 0.301; BERTScore: 0.382),\nand Open-ended_EN (BLEU-1: 0.183; BERTScore: 0.240). Conclusions: This study\npresents the first bilingual VQA benchmark for ophthalmology, distinguished by\nits real-world context and inclusion of multiple examinations per patient. The\ndataset reflects authentic clinical decision-making scenarios and enables\nquantitative evaluation of VLMs, supporting the development of accurate,\nspecialized, and trustworthy AI systems for eye care."}
{"id": "2505.19638", "pdf": "https://arxiv.org/pdf/2505.19638", "abs": "https://arxiv.org/abs/2505.19638", "authors": ["Ming Meng", "Qi Dong", "Jiajie Li", "Zhe Zhu", "Xingyu Wang", "Zhaoxin Fan", "Wei Zhao", "Wenjun Wu"], "title": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment", "categories": ["cs.CV"], "comment": null, "summary": "Virtual try-on technology has become increasingly important in the fashion\nand retail industries, enabling the generation of high-fidelity garment images\nthat adapt seamlessly to target human models. While existing methods have\nachieved notable progress, they still face significant challenges in\nmaintaining consistency across different poses. Specifically, geometric\ndistortions lead to a lack of spatial consistency, mismatches in garment\nstructure and texture across poses result in semantic inconsistency, and the\nloss or distortion of fine-grained details diminishes visual fidelity. To\naddress these challenges, we propose HF-VTON, a novel framework that ensures\nhigh-fidelity virtual try-on performance across diverse poses. HF-VTON consists\nof three key modules: (1) the Appearance-Preserving Warp Alignment Module\n(APWAM), which aligns garments to human poses, addressing geometric\ndeformations and ensuring spatial consistency; (2) the Semantic Representation\nand Comprehension Module (SRCM), which captures fine-grained garment attributes\nand multi-pose data to enhance semantic representation, maintaining structural,\ntextural, and pattern consistency; and (3) the Multimodal Prior-Guided\nAppearance Generation Module (MPAGM), which integrates multimodal features and\nprior knowledge from pre-trained models to optimize appearance generation,\nensuring both semantic and geometric consistency. Additionally, to overcome\ndata limitations in existing benchmarks, we introduce the SAMP-VTONS dataset,\nfeaturing multi-pose pairs and rich textual annotations for a more\ncomprehensive evaluation. Experimental results demonstrate that HF-VTON\noutperforms state-of-the-art methods on both VITON-HD and SAMP-VTONS, excelling\nin visual fidelity, semantic consistency, and detail preservation."}
{"id": "2505.19650", "pdf": "https://arxiv.org/pdf/2505.19650", "abs": "https://arxiv.org/abs/2505.19650", "authors": ["Fanheng Kong", "Jingyuan Zhang", "Yahui Liu", "Hongzhi Zhang", "Shi Feng", "Xiaocui Yang", "Daling Wang", "Yu Tian", "Qi Wang", "Fuzheng Zhang", "Guorui Zhou"], "title": "Modality Curation: Building Universal Embeddings for Advanced Multimodal Information Retrieval", "categories": ["cs.CV", "cs.IR", "cs.MM"], "comment": "26 pages, project page: https://friedrichor.github.io/projects/UNITE", "summary": "Multimodal information retrieval (MIR) faces inherent challenges due to the\nheterogeneity of data sources and the complexity of cross-modal alignment.\nWhile previous studies have identified modal gaps in feature spaces, a\nsystematic approach to address these challenges remains unexplored. In this\nwork, we introduce UNITE, a universal framework that tackles these challenges\nthrough two critical yet underexplored aspects: data curation and\nmodality-aware training configurations. Our work provides the first\ncomprehensive analysis of how modality-specific data properties influence\ndownstream task performance across diverse scenarios. Moreover, we propose\nModal-Aware Masked Contrastive Learning (MAMCL) to mitigate the competitive\nrelationships among the instances of different modalities. Our framework\nachieves state-of-the-art results on multiple multimodal retrieval benchmarks,\noutperforming existing methods by notable margins. Through extensive\nexperiments, we demonstrate that strategic modality curation and tailored\ntraining protocols are pivotal for robust cross-modal representation learning.\nThis work not only advances MIR performance but also provides a foundational\nblueprint for future research in multimodal systems. Our project is available\nat https://friedrichor.github.io/projects/UNITE."}
{"id": "2505.19656", "pdf": "https://arxiv.org/pdf/2505.19656", "abs": "https://arxiv.org/abs/2505.19656", "authors": ["Tianren Ma", "Xiaosong Zhang", "Boyu Yang", "Junlan Feng", "Qixiang Ye"], "title": "ReDDiT: Rehashing Noise for Discrete Visual Generation", "categories": ["cs.CV"], "comment": "Preprint, under development", "summary": "Discrete diffusion models are gaining traction in the visual generative area\nfor their efficiency and compatibility. However, the pioneered attempts still\nfall behind the continuous counterparts, which we attribute to the noise\n(absorbing state) design and sampling heuristics. In this study, we propose the\nrehashing noise framework for discrete diffusion transformer, termed ReDDiT, to\nextend absorbing states and improve expressive capacity of discrete diffusion\nmodels. ReDDiT enriches the potential paths that latent variables can traverse\nduring training with randomized multi-index corruption. The derived rehash\nsampler, which reverses the randomized absorbing paths, guarantees the\ndiversity and low discrepancy of the generation process. These reformulations\nlead to more consistent and competitive generation quality, mitigating the need\nfor heavily tuned randomness. Experiments show that ReDDiT significantly\noutperforms the baseline (reducing gFID from 6.18 to 1.61) and is on par with\nthe continuous counterparts with higher efficiency."}
{"id": "2505.19659", "pdf": "https://arxiv.org/pdf/2505.19659", "abs": "https://arxiv.org/abs/2505.19659", "authors": ["Piyush Tiwary", "Kinjawl Bhattacharyya", "Prathosh A. P"], "title": "LangDAug: Langevin Data Augmentation for Multi-Source Domain Generalization in Medical Image Segmentation", "categories": ["cs.CV"], "comment": "Accepted at ICML 2025", "summary": "Medical image segmentation models often struggle to generalize across\ndifferent domains due to various reasons. Domain Generalization (DG) methods\novercome this either through representation learning or data augmentation\n(DAug). While representation learning methods seek domain-invariant features,\nthey often rely on ad-hoc techniques and lack formal guarantees. DAug methods,\nwhich enrich model representations through synthetic samples, have shown\ncomparable or superior performance to representation learning approaches. We\npropose LangDAug, a novel $\\textbf{Lang}$evin $\\textbf{D}$ata\n$\\textbf{Aug}$mentation for multi-source domain generalization in 2D medical\nimage segmentation. LangDAug leverages Energy-Based Models (EBMs) trained via\ncontrastive divergence to traverse between source domains, generating\nintermediate samples through Langevin dynamics. Theoretical analysis shows that\nLangDAug induces a regularization effect, and for GLMs, it upper-bounds the\nRademacher complexity by the intrinsic dimensionality of the data manifold.\nThrough extensive experiments on Fundus segmentation and 2D MRI prostate\nsegmentation benchmarks, we show that LangDAug outperforms state-of-the-art\ndomain generalization methods and effectively complements existing\ndomain-randomization approaches. The codebase for our method is available at\nhttps://github.com/backpropagator/LangDAug."}
{"id": "2505.19668", "pdf": "https://arxiv.org/pdf/2505.19668", "abs": "https://arxiv.org/abs/2505.19668", "authors": ["Tengda Huang", "Yu Zhang", "Tianren Li", "Yufu Qu", "Fulin Liu", "Zhenzhong Wei"], "title": "Burst Image Super-Resolution via Multi-Cross Attention Encoding and Multi-Scan State-Space Decoding", "categories": ["cs.CV"], "comment": "32 pages, 13 figures, submitted to 'Image and Vision Computing'", "summary": "Multi-image super-resolution (MISR) can achieve higher image quality than\nsingle-image super-resolution (SISR) by aggregating sub-pixel information from\nmultiple spatially shifted frames. Among MISR tasks, burst super-resolution\n(BurstSR) has gained significant attention due to its wide range of\napplications. Recent methods have increasingly adopted Transformers over\nconvolutional neural networks (CNNs) in super-resolution tasks, due to their\nsuperior ability to capture both local and global context. However, most\nexisting approaches still rely on fixed and narrow attention windows that\nrestrict the perception of features beyond the local field. This limitation\nhampers alignment and feature aggregation, both of which are crucial for\nhigh-quality super-resolution. To address these limitations, we propose a novel\nfeature extractor that incorporates two newly designed attention mechanisms:\noverlapping cross-window attention and cross-frame attention, enabling more\nprecise and efficient extraction of sub-pixel information across multiple\nframes. Furthermore, we introduce a Multi-scan State-Space Module with the\ncross-frame attention mechanism to enhance feature aggregation. Extensive\nexperiments on both synthetic and real-world benchmarks demonstrate the\nsuperiority of our approach. Additional evaluations on ISO 12233 resolution\ntest charts further confirm its enhanced super-resolution performance."}
{"id": "2505.19684", "pdf": "https://arxiv.org/pdf/2505.19684", "abs": "https://arxiv.org/abs/2505.19684", "authors": ["Bingrui Sima", "Linhua Cong", "Wenxuan Wang", "Kun He"], "title": "VisCRA: A Visual Chain Reasoning Attack for Jailbreaking Multimodal Large Language Models", "categories": ["cs.CV"], "comment": null, "summary": "The emergence of Multimodal Large Language Models (MLRMs) has enabled\nsophisticated visual reasoning capabilities by integrating reinforcement\nlearning and Chain-of-Thought (CoT) supervision. However, while these enhanced\nreasoning capabilities improve performance, they also introduce new and\nunderexplored safety risks. In this work, we systematically investigate the\nsecurity implications of advanced visual reasoning in MLRMs. Our analysis\nreveals a fundamental trade-off: as visual reasoning improves, models become\nmore vulnerable to jailbreak attacks. Motivated by this critical finding, we\nintroduce VisCRA (Visual Chain Reasoning Attack), a novel jailbreak framework\nthat exploits the visual reasoning chains to bypass safety mechanisms. VisCRA\ncombines targeted visual attention masking with a two-stage reasoning induction\nstrategy to precisely control harmful outputs. Extensive experiments\ndemonstrate VisCRA's significant effectiveness, achieving high attack success\nrates on leading closed-source MLRMs: 76.48% on Gemini 2.0 Flash Thinking,\n68.56% on QvQ-Max, and 56.60% on GPT-4o. Our findings highlight a critical\ninsight: the very capability that empowers MLRMs -- their visual reasoning --\ncan also serve as an attack vector, posing significant security risks."}
{"id": "2505.19692", "pdf": "https://arxiv.org/pdf/2505.19692", "abs": "https://arxiv.org/abs/2505.19692", "authors": ["Wenchao Sun", "Xuewu Lin", "Keyu Chen", "Zixiang Pei", "Yining Shi", "Chuang Zhang", "Sifa Zheng"], "title": "DriveCamSim: Generalizable Camera Simulation via Explicit Camera Modeling for Autonomous Driving", "categories": ["cs.CV"], "comment": null, "summary": "Camera sensor simulation serves as a critical role for autonomous driving\n(AD), e.g. evaluating vision-based AD algorithms. While existing approaches\nhave leveraged generative models for controllable image/video generation, they\nremain constrained to generating multi-view video sequences with fixed camera\nviewpoints and video frequency, significantly limiting their downstream\napplications. To address this, we present a generalizable camera simulation\nframework DriveCamSim, whose core innovation lies in the proposed Explicit\nCamera Modeling (ECM) mechanism. Instead of implicit interaction through\nvanilla attention, ECM establishes explicit pixel-wise correspondences across\nmulti-view and multi-frame dimensions, decoupling the model from overfitting to\nthe specific camera configurations (intrinsic/extrinsic parameters, number of\nviews) and temporal sampling rates presented in the training data. For\ncontrollable generation, we identify the issue of information loss inherent in\nexisting conditional encoding and injection pipelines, proposing an\ninformation-preserving control mechanism. This control mechanism not only\nimproves conditional controllability, but also can be extended to be\nidentity-aware to enhance temporal consistency in foreground object rendering.\nWith above designs, our model demonstrates superior performance in both visual\nquality and controllability, as well as generalization capability across\nspatial-level (camera parameters variations) and temporal-level (video frame\nrate variations), enabling flexible user-customizable camera simulation\ntailored to diverse application scenarios. Code will be avaliable at\nhttps://github.com/swc-17/DriveCamSim for facilitating future research."}
{"id": "2505.19694", "pdf": "https://arxiv.org/pdf/2505.19694", "abs": "https://arxiv.org/abs/2505.19694", "authors": ["Wen Yin", "Yong Wang", "Guiduo Duan", "Dongyang Zhang", "Xin Hu", "Yuan-Fang Li", "Tao He"], "title": "Knowledge-Aligned Counterfactual-Enhancement Diffusion Perception for Unsupervised Cross-Domain Visual Emotion Recognition", "categories": ["cs.CV"], "comment": "Accepted at CVPR 2025", "summary": "Visual Emotion Recognition (VER) is a critical yet challenging task aimed at\ninferring emotional states of individuals based on visual cues. However,\nexisting works focus on single domains, e.g., realistic images or stickers,\nlimiting VER models' cross-domain generalizability. To fill this gap, we\nintroduce an Unsupervised Cross-Domain Visual Emotion Recognition (UCDVER)\ntask, which aims to generalize visual emotion recognition from the source\ndomain (e.g., realistic images) to the low-resource target domain (e.g.,\nstickers) in an unsupervised manner. Compared to the conventional unsupervised\ndomain adaptation problems, UCDVER presents two key challenges: a significant\nemotional expression variability and an affective distribution shift. To\nmitigate these issues, we propose the Knowledge-aligned\nCounterfactual-enhancement Diffusion Perception (KCDP) framework. Specifically,\nKCDP leverages a VLM to align emotional representations in a shared knowledge\nspace and guides diffusion models for improved visual affective perception.\nFurthermore, a Counterfactual-Enhanced Language-image Emotional Alignment\n(CLIEA) method generates high-quality pseudo-labels for the target domain.\nExtensive experiments demonstrate that our model surpasses SOTA models in both\nperceptibility and generalization, e.g., gaining 12% improvements over the SOTA\nVER model TGCA-PVT. The project page is at https://yinwen2019.github.io/ucdver."}
{"id": "2505.19696", "pdf": "https://arxiv.org/pdf/2505.19696", "abs": "https://arxiv.org/abs/2505.19696", "authors": ["Mohamed Amine Kerkouri", "Marouane Tliba", "Aladine Chetouani", "Nour Aburaed", "Alessandro Bruno"], "title": "Modeling Beyond MOS: Quality Assessment Models Must Integrate Context, Reasoning, and Multimodality", "categories": ["cs.CV", "cs.MM", "eess.IV"], "comment": "Under review", "summary": "This position paper argues that Mean Opinion Score (MOS), while historically\nfoundational, is no longer sufficient as the sole supervisory signal for\nmultimedia quality assessment models. MOS reduces rich, context-sensitive human\njudgments to a single scalar, obscuring semantic failures, user intent, and the\nrationale behind quality decisions. We contend that modern quality assessment\nmodels must integrate three interdependent capabilities: (1) context-awareness,\nto adapt evaluations to task-specific goals and viewing conditions; (2)\nreasoning, to produce interpretable, evidence-grounded justifications for\nquality judgments; and (3) multimodality, to align perceptual and semantic cues\nusing vision-language models. We critique the limitations of current\nMOS-centric benchmarks and propose a roadmap for reform: richer datasets with\ncontextual metadata and expert rationales, and new evaluation metrics that\nassess semantic alignment, reasoning fidelity, and contextual sensitivity. By\nreframing quality assessment as a contextual, explainable, and multimodal\nmodeling task, we aim to catalyze a shift toward more robust, human-aligned,\nand trustworthy evaluation systems."}
{"id": "2505.19702", "pdf": "https://arxiv.org/pdf/2505.19702", "abs": "https://arxiv.org/abs/2505.19702", "authors": ["Minheng Ni", "Zhengyuan Yang", "Linjie Li", "Chung-Ching Lin", "Kevin Lin", "Wangmeng Zuo", "Lijuan Wang"], "title": "Point-RFT: Improving Multimodal Reasoning with Visually Grounded Reinforcement Finetuning", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in large language models have significantly improved textual\nreasoning through the effective use of Chain-of-Thought (CoT) and reinforcement\nlearning. However, extending these successes to vision-language tasks remains\nchallenging due to inherent limitations in text-only CoT, such as visual\nhallucinations and insufficient multimodal integration. In this paper, we\nintroduce Point-RFT, a multimodal reasoning framework explicitly designed to\nleverage visually grounded CoT reasoning for visual document understanding. Our\napproach consists of two stages: First, we conduct format finetuning using a\ncurated dataset of 71K diverse visual reasoning problems, each annotated with\ndetailed, step-by-step rationales explicitly grounded to corresponding visual\nelements. Second, we employ reinforcement finetuning targeting visual document\nunderstanding. On ChartQA, our approach improves accuracy from 70.88%\n(format-finetuned baseline) to 90.04%, surpassing the 83.92% accuracy achieved\nby reinforcement finetuning relying solely on text-based CoT. The result shows\nthat our grounded CoT is more effective for multimodal reasoning compared with\nthe text-only CoT. Moreover, Point-RFT exhibits superior generalization\ncapability across several out-of-domain visual document reasoning benchmarks,\nincluding CharXiv, PlotQA, IconQA, TabMWP, etc., and highlights its potential\nin complex real-world scenarios."}
{"id": "2505.19707", "pdf": "https://arxiv.org/pdf/2505.19707", "abs": "https://arxiv.org/abs/2505.19707", "authors": ["Rong-Cheng Tu", "Zhao Jin", "Jingyi Liao", "Xiao Luo", "Yingjie Wang", "Li Shen", "Dacheng Tao"], "title": "MLLM-Guided VLM Fine-Tuning with Joint Inference for Zero-Shot Composed Image Retrieval", "categories": ["cs.CV", "cs.IR"], "comment": null, "summary": "Existing Zero-Shot Composed Image Retrieval (ZS-CIR) methods typically train\nadapters that convert reference images into pseudo-text tokens, which are\nconcatenated with the modifying text and processed by frozen text encoders in\npretrained VLMs or LLMs. While this design leverages the strengths of large\npretrained models, it only supervises the adapter to produce encoder-compatible\ntokens that loosely preserve visual semantics. Crucially, it does not directly\noptimize the composed query representation to capture the full intent of the\ncomposition or to align with the target semantics, thereby limiting retrieval\nperformance, particularly in cases involving fine-grained or complex visual\ntransformations. To address this problem, we propose MLLM-Guided VLM\nFine-Tuning with Joint Inference (MVFT-JI), a novel approach that leverages a\npretrained multimodal large language model (MLLM) to construct two\ncomplementary training tasks using only unlabeled images: target text retrieval\ntaskand text-to-image retrieval task. By jointly optimizing these tasks, our\nmethod enables the VLM to inherently acquire robust compositional retrieval\ncapabilities, supported by the provided theoretical justifications and\nempirical validation. Furthermore, during inference, we further prompt the MLLM\nto generate target texts from composed queries and compute retrieval scores by\nintegrating similarities between (i) the composed query and candidate images,\nand (ii) the MLLM-generated target text and candidate images. This strategy\neffectively combines the VLM's semantic alignment strengths with the MLLM's\nreasoning capabilities."}
{"id": "2505.19733", "pdf": "https://arxiv.org/pdf/2505.19733", "abs": "https://arxiv.org/abs/2505.19733", "authors": ["Alou Diakite", "Cheng Li", "Lei Xie", "Yuanjing Feng", "Ruoyou Wu", "Jianzhong He", "Hairong Zheng", "Shanshan Wang"], "title": "Cross-Sequence Semi-Supervised Learning for Multi-Parametric MRI-Based Visual Pathway Delineation", "categories": ["cs.CV", "cs.CE"], "comment": null, "summary": "Accurately delineating the visual pathway (VP) is crucial for understanding\nthe human visual system and diagnosing related disorders. Exploring\nmulti-parametric MR imaging data has been identified as an important way to\ndelineate VP. However, due to the complex cross-sequence relationships,\nexisting methods cannot effectively model the complementary information from\ndifferent MRI sequences. In addition, these existing methods heavily rely on\nlarge training data with labels, which is labor-intensive and time-consuming to\nobtain. In this work, we propose a novel semi-supervised multi-parametric\nfeature decomposition framework for VP delineation. Specifically, a\ncorrelation-constrained feature decomposition (CFD) is designed to handle the\ncomplex cross-sequence relationships by capturing the unique characteristics of\neach MRI sequence and easing the multi-parametric information fusion process.\nFurthermore, a consistency-based sample enhancement (CSE) module is developed\nto address the limited labeled data issue, by generating and promoting\nmeaningful edge information from unlabeled data. We validate our framework\nusing two public datasets, and one in-house Multi-Shell Diffusion MRI (MDM)\ndataset. Experimental results demonstrate the superiority of our approach in\nterms of delineation performance when compared to seven state-of-the-art\napproaches."}
{"id": "2505.19742", "pdf": "https://arxiv.org/pdf/2505.19742", "abs": "https://arxiv.org/abs/2505.19742", "authors": ["Jue Gong", "Tingyu Yang", "Jingkai Wang", "Zheng Chen", "Xing Liu", "Hong Gu", "Yulun Zhang", "Xiaokang Yang"], "title": "HAODiff: Human-Aware One-Step Diffusion via Dual-Prompt Guidance", "categories": ["cs.CV"], "comment": "9 pages, 8 figures. The code and model will be available at\n  https://github.com/gobunu/HAODiff", "summary": "Human-centered images often suffer from severe generic degradation during\ntransmission and are prone to human motion blur (HMB), making restoration\nchallenging. Existing research lacks sufficient focus on these issues, as both\nproblems often coexist in practice. To address this, we design a degradation\npipeline that simulates the coexistence of HMB and generic noise, generating\nsynthetic degraded data to train our proposed HAODiff, a human-aware one-step\ndiffusion. Specifically, we propose a triple-branch dual-prompt guidance (DPG),\nwhich leverages high-quality images, residual noise (LQ minus HQ), and HMB\nsegmentation masks as training targets. It produces a positive-negative prompt\npair for classifier-free guidance (CFG) in a single diffusion step. The\nresulting adaptive dual prompts let HAODiff exploit CFG more effectively,\nboosting robustness against diverse degradations. For fair evaluation, we\nintroduce MPII-Test, a benchmark rich in combined noise and HMB cases.\nExtensive experiments show that our HAODiff surpasses existing state-of-the-art\n(SOTA) methods in terms of both quantitative metrics and visual quality on\nsynthetic and real-world datasets, including our introduced MPII-Test. Code is\navailable at: https://github.com/gobunu/HAODiff."}
{"id": "2505.19746", "pdf": "https://arxiv.org/pdf/2505.19746", "abs": "https://arxiv.org/abs/2505.19746", "authors": ["Jakov Samardžija", "Donik Vršnak", "Sven Lončarić"], "title": "Improving Heart Rejection Detection in XPCI Images Using Synthetic Data Augmentation", "categories": ["cs.CV"], "comment": null, "summary": "Accurate identification of acute cellular rejection (ACR) in endomyocardial\nbiopsies is essential for effective management of heart transplant patients.\nHowever, the rarity of high-grade rejection cases (3R) presents a significant\nchallenge for training robust deep learning models. This work addresses the\nclass imbalance problem by leveraging synthetic data generation using StyleGAN\nto augment the limited number of real 3R images. Prior to GAN training,\nhistogram equalization was applied to standardize image appearance and improve\nthe consistency of tissue representation. StyleGAN was trained on available 3R\nbiopsy patches and subsequently used to generate 10,000 realistic synthetic\nimages. These were combined with real 0R samples, that is samples without\nrejection, in various configurations to train ResNet-18 classifiers for binary\nrejection classification.\n  Three classifier variants were evaluated: one trained on real 0R and\nsynthetic 3R images, another using both synthetic and additional real samples,\nand a third trained solely on real data. All models were tested on an\nindependent set of real biopsy images. Results demonstrate that synthetic data\nimproves classification performance, particularly when used in combination with\nreal samples. The highest-performing model, which used both real and synthetic\nimages, achieved strong precision and recall for both classes. These findings\nunderscore the value of hybrid training strategies and highlight the potential\nof GAN-based data augmentation in biomedical image analysis, especially in\ndomains constrained by limited annotated datasets."}
{"id": "2505.19750", "pdf": "https://arxiv.org/pdf/2505.19750", "abs": "https://arxiv.org/abs/2505.19750", "authors": ["Huaiyuan Zhang", "Hang Chen", "Yu Cheng", "Shunyi Wu", "Linghao Sun", "Linao Han", "Zeyu Shi", "Lei Qi"], "title": "SuperAD: A Training-free Anomaly Classification and Segmentation Method for CVPR 2025 VAND 3.0 Workshop Challenge Track 1: Adapt & Detect", "categories": ["cs.CV"], "comment": null, "summary": "In this technical report, we present our solution to the CVPR 2025 Visual\nAnomaly and Novelty Detection (VAND) 3.0 Workshop Challenge Track 1: Adapt &\nDetect: Robust Anomaly Detection in Real-World Applications. In real-world\nindustrial anomaly detection, it is crucial to accurately identify anomalies\nwith physical complexity, such as transparent or reflective surfaces,\nocclusions, and low-contrast contaminations. The recently proposed MVTec AD 2\ndataset significantly narrows the gap between publicly available benchmarks and\nanomalies found in real-world industrial environments. To address the\nchallenges posed by this dataset--such as complex and varying lighting\nconditions and real anomalies with large scale differences--we propose a fully\ntraining-free anomaly detection and segmentation method based on feature\nextraction using the DINOv2 model named SuperAD. Our method carefully selects a\nsmall number of normal reference images and constructs a memory bank by\nleveraging the strong representational power of DINOv2. Anomalies are then\nsegmented by performing nearest neighbor matching between test image features\nand the memory bank. Our method achieves competitive results on both test sets\nof the MVTec AD 2 dataset."}
{"id": "2505.19751", "pdf": "https://arxiv.org/pdf/2505.19751", "abs": "https://arxiv.org/abs/2505.19751", "authors": ["Hala Djeghim", "Nathan Piasco", "Luis Roldão", "Moussab Bennehar", "Dzmitry Tsishkou", "Céline Loscos", "Désiré Sidibé"], "title": "SAIL: Self-supervised Albedo Estimation from Real Images with a Latent Diffusion Model", "categories": ["cs.CV"], "comment": null, "summary": "Intrinsic image decomposition aims at separating an image into its underlying\nalbedo and shading components, isolating the base color from lighting effects\nto enable downstream applications such as virtual relighting and scene editing.\nDespite the rise and success of learning-based approaches, intrinsic image\ndecomposition from real-world images remains a significant challenging task due\nto the scarcity of labeled ground-truth data. Most existing solutions rely on\nsynthetic data as supervised setups, limiting their ability to generalize to\nreal-world scenes. Self-supervised methods, on the other hand, often produce\nalbedo maps that contain reflections and lack consistency under different\nlighting conditions. To address this, we propose SAIL, an approach designed to\nestimate albedo-like representations from single-view real-world images. We\nrepurpose the prior knowledge of a latent diffusion model for unconditioned\nscene relighting as a surrogate objective for albedo estimation. To extract the\nalbedo, we introduce a novel intrinsic image decomposition fully formulated in\nthe latent space. To guide the training of our latent diffusion model, we\nintroduce regularization terms that constrain both the lighting-dependent and\nindependent components of our latent image decomposition. SAIL predicts stable\nalbedo under varying lighting conditions and generalizes to multiple scenes,\nusing only unlabeled multi-illumination data available online."}
{"id": "2505.19793", "pdf": "https://arxiv.org/pdf/2505.19793", "abs": "https://arxiv.org/abs/2505.19793", "authors": ["Li Fang", "Hao Zhu", "Longlong Chen", "Fei Hu", "Long Ye", "Zhan Ma"], "title": "Depth-Guided Bundle Sampling for Efficient Generalizable Neural Radiance Field Reconstruction", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Recent advancements in generalizable novel view synthesis have achieved\nimpressive quality through interpolation between nearby views. However,\nrendering high-resolution images remains computationally intensive due to the\nneed for dense sampling of all rays. Recognizing that natural scenes are\ntypically piecewise smooth and sampling all rays is often redundant, we propose\na novel depth-guided bundle sampling strategy to accelerate rendering. By\ngrouping adjacent rays into a bundle and sampling them collectively, a shared\nrepresentation is generated for decoding all rays within the bundle. To further\noptimize efficiency, our adaptive sampling strategy dynamically allocates\nsamples based on depth confidence, concentrating more samples in complex\nregions while reducing them in smoother areas. When applied to ENeRF, our\nmethod achieves up to a 1.27 dB PSNR improvement and a 47% increase in FPS on\nthe DTU dataset. Extensive experiments on synthetic and real-world datasets\ndemonstrate state-of-the-art rendering quality and up to 2x faster rendering\ncompared to existing generalizable methods. Code is available at\nhttps://github.com/KLMAV-CUC/GDB-NeRF."}
{"id": "2505.19795", "pdf": "https://arxiv.org/pdf/2505.19795", "abs": "https://arxiv.org/abs/2505.19795", "authors": ["Sajjad Shahabodini", "Mobina Mansoori", "Farnoush Bayatmakou", "Jamshid Abouei", "Konstantinos N. Plataniotis", "Arash Mohammadi"], "title": "The Missing Point in Vision Transformers for Universal Image Segmentation", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV"], "comment": null, "summary": "Image segmentation remains a challenging task in computer vision, demanding\nrobust mask generation and precise classification. Recent mask-based approaches\nyield high-quality masks by capturing global context. However, accurately\nclassifying these masks, especially in the presence of ambiguous boundaries and\nimbalanced class distributions, remains an open challenge. In this work, we\nintroduce ViT-P, a novel two-stage segmentation framework that decouples mask\ngeneration from classification. The first stage employs a proposal generator to\nproduce class-agnostic mask proposals, while the second stage utilizes a\npoint-based classification model built on the Vision Transformer (ViT) to\nrefine predictions by focusing on mask central points. ViT-P serves as a\npre-training-free adapter, allowing the integration of various pre-trained\nvision transformers without modifying their architecture, ensuring adaptability\nto dense prediction tasks. Furthermore, we demonstrate that coarse and bounding\nbox annotations can effectively enhance classification without requiring\nadditional training on fine annotation datasets, reducing annotation costs\nwhile maintaining strong performance. Extensive experiments across COCO,\nADE20K, and Cityscapes datasets validate the effectiveness of ViT-P, achieving\nstate-of-the-art results with 54.0 PQ on ADE20K panoptic segmentation, 87.4\nmIoU on Cityscapes semantic segmentation, and 63.6 mIoU on ADE20K semantic\nsegmentation. The code and pretrained models are available at:\nhttps://github.com/sajjad-sh33/ViT-P}{https://github.com/sajjad-sh33/ViT-P."}
{"id": "2505.19799", "pdf": "https://arxiv.org/pdf/2505.19799", "abs": "https://arxiv.org/abs/2505.19799", "authors": ["Yulu Bai", "Jiahong Fu", "Qi Xie", "Deyu Meng"], "title": "A Regularization-Guided Equivariant Approach for Image Restoration", "categories": ["cs.CV"], "comment": null, "summary": "Equivariant and invariant deep learning models have been developed to exploit\nintrinsic symmetries in data, demonstrating significant effectiveness in\ncertain scenarios. However, these methods often suffer from limited\nrepresentation accuracy and rely on strict symmetry assumptions that may not\nhold in practice. These limitations pose a significant drawback for image\nrestoration tasks, which demands high accuracy and precise symmetry\nrepresentation. To address these challenges, we propose a rotation-equivariant\nregularization strategy that adaptively enforces the appropriate symmetry\nconstraints on the data while preserving the network's representational\naccuracy. Specifically, we introduce EQ-Reg, a regularizer designed to enhance\nrotation equivariance, which innovatively extends the insights of\ndata-augmentation-based and equivariant-based methodologies. This is achieved\nthrough self-supervised learning and the spatial rotation and cyclic channel\nshift of feature maps deduce in the equivariant framework. Our approach firstly\nenables a non-strictly equivariant network suitable for image restoration,\nproviding a simple and adaptive mechanism for adjusting equivariance based on\ntask. Extensive experiments across three low-level tasks demonstrate the\nsuperior accuracy and generalization capability of our method, outperforming\nstate-of-the-art approaches."}
{"id": "2505.19805", "pdf": "https://arxiv.org/pdf/2505.19805", "abs": "https://arxiv.org/abs/2505.19805", "authors": ["Jérémy Scanvic", "Quentin Barthélemy", "Julián Tachella"], "title": "Translation-Equivariance of Normalization Layers and Aliasing in Convolutional Neural Networks", "categories": ["cs.CV"], "comment": null, "summary": "The design of convolutional neural architectures that are exactly equivariant\nto continuous translations is an active field of research. It promises to\nbenefit scientific computing, notably by making existing imaging systems more\nphysically accurate. Most efforts focus on the design of downsampling/pooling\nlayers, upsampling layers and activation functions, but little attention is\ndedicated to normalization layers. In this work, we present a novel theoretical\nframework for understanding the equivariance of normalization layers to\ndiscrete shifts and continuous translations. We also determine necessary and\nsufficient conditions for normalization layers to be equivariant in terms of\nthe dimensions they operate on. Using real feature maps from ResNet-18 and\nImageNet, we test those theoretical results empirically and find that they are\nconsistent with our predictions."}
{"id": "2505.19812", "pdf": "https://arxiv.org/pdf/2505.19812", "abs": "https://arxiv.org/abs/2505.19812", "authors": ["Zehong Ma", "Shiliang Zhang", "Longhui Wei", "Qi Tian"], "title": "Efficient Multi-modal Long Context Learning for Training-free Adaptation", "categories": ["cs.CV"], "comment": "Accepted to ICML2025", "summary": "Traditional approaches to adapting multi-modal large language models (MLLMs)\nto new tasks have relied heavily on fine-tuning. This paper introduces\nEfficient Multi-Modal Long Context Learning (EMLoC), a novel training-free\nalternative that embeds demonstration examples directly into the model input.\nEMLoC offers a more efficient, flexible, and scalable solution for task\nadaptation. Because extremely lengthy inputs introduce prohibitive\ncomputational and memory overhead, EMLoC contributes a chunk-wise compression\nmechanism combined with layer-wise adaptive pruning. It condenses long-context\nmultimodal inputs into compact, task-specific memory representations. By\nadaptively pruning tokens at each layer under a Jensen-Shannon divergence\nconstraint, our method achieves a dramatic reduction in inference complexity\nwithout sacrificing performance. This approach is the first to seamlessly\nintegrate compression and pruning techniques for multi-modal long-context\nlearning, offering a scalable and efficient solution for real-world\napplications. Extensive experiments on diverse vision-language benchmarks\ndemonstrate that EMLoC achieves performance on par with or superior to naive\nlong-context approaches. Our results highlight the potential of EMLoC as a\ngroundbreaking framework for efficient and flexible adaptation of multi-modal\nmodels in resource-constrained environments. Codes are publicly available at\nhttps://github.com/Zehong-Ma/EMLoC."}
{"id": "2505.19813", "pdf": "https://arxiv.org/pdf/2505.19813", "abs": "https://arxiv.org/abs/2505.19813", "authors": ["You Wang", "Li Fang", "Hao Zhu", "Fei Hu", "Long Ye", "Zhan Ma"], "title": "GoLF-NRT: Integrating Global Context and Local Geometry for Few-Shot View Synthesis", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Neural Radiance Fields (NeRF) have transformed novel view synthesis by\nmodeling scene-specific volumetric representations directly from images. While\ngeneralizable NeRF models can generate novel views across unknown scenes by\nlearning latent ray representations, their performance heavily depends on a\nlarge number of multi-view observations. However, with limited input views,\nthese methods experience significant degradation in rendering quality. To\naddress this limitation, we propose GoLF-NRT: a Global and Local feature\nFusion-based Neural Rendering Transformer. GoLF-NRT enhances generalizable\nneural rendering from few input views by leveraging a 3D transformer with\nefficient sparse attention to capture global scene context. In parallel, it\nintegrates local geometric features extracted along the epipolar line, enabling\nhigh-quality scene reconstruction from as few as 1 to 3 input views.\nFurthermore, we introduce an adaptive sampling strategy based on attention\nweights and kernel regression, improving the accuracy of transformer-based\nneural rendering. Extensive experiments on public datasets show that GoLF-NRT\nachieves state-of-the-art performance across varying numbers of input views,\nhighlighting the effectiveness and superiority of our approach. Code is\navailable at https://github.com/KLMAV-CUC/GoLF-NRT."}
{"id": "2505.19846", "pdf": "https://arxiv.org/pdf/2505.19846", "abs": "https://arxiv.org/abs/2505.19846", "authors": ["Nagito Saito", "Shintaro Ito", "Koichi Ito", "Takafumi Aoki"], "title": "Zero-Shot Pseudo Labels Generation Using SAM and CLIP for Semi-Supervised Semantic Segmentation", "categories": ["cs.CV"], "comment": "Accepted to ICIP 2025", "summary": "Semantic segmentation is a fundamental task in medical image analysis and\nautonomous driving and has a problem with the high cost of annotating the\nlabels required in training. To address this problem, semantic segmentation\nmethods based on semi-supervised learning with a small number of labeled data\nhave been proposed. For example, one approach is to train a semantic\nsegmentation model using images with annotated labels and pseudo labels. In\nthis approach, the accuracy of the semantic segmentation model depends on the\nquality of the pseudo labels, and the quality of the pseudo labels depends on\nthe performance of the model to be trained and the amount of data with\nannotated labels. In this paper, we generate pseudo labels using zero-shot\nannotation with the Segment Anything Model (SAM) and Contrastive Language-Image\nPretraining (CLIP), improve the accuracy of the pseudo labels using the Unified\nDual-Stream Perturbations Approach (UniMatch), and use them as enhanced labels\nto train a semantic segmentation model. The effectiveness of the proposed\nmethod is demonstrated through the experiments using the public datasets:\nPASCAL and MS COCO."}
{"id": "2505.19853", "pdf": "https://arxiv.org/pdf/2505.19853", "abs": "https://arxiv.org/abs/2505.19853", "authors": ["Miaoyu Li", "Qin Chao", "Boyang Li"], "title": "Two Causally Related Needles in a Video Haystack", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Evaluating the video understanding capabilities of Video-Language Models\n(VLMs) remains a significant challenge. We propose a long-context video\nunderstanding benchmark, Causal2Needles, that assesses two crucial abilities\ninsufficiently evaluated by existing benchmarks: (1) the ability to extract\ninformation from two separate locations in a long video and understand them\njointly, and (2) the ability to model the world in terms of cause and effect in\nhuman behaviors. Specifically, Causal2Needles introduces 2-needle questions,\nwhich require extracting information from both the cause and effect\nhuman-behavior events in a long video and the associated narration text. To\nprevent textual bias, these questions comprise two complementary formats: one\nasking to identify the video clip containing the answer, and one asking for the\ntextual description of an unrelated visual detail from that video clip. Our\nexperiments reveal that models excelling in pre-existing benchmarks struggle\nwith 2-needle visual grounding, and the model performance is negatively\ncorrelated with the distance between the two needles. These findings highlight\ncritical limitations in current VLMs."}
{"id": "2505.19854", "pdf": "https://arxiv.org/pdf/2505.19854", "abs": "https://arxiv.org/abs/2505.19854", "authors": ["Natsuki Takama", "Shintaro Ito", "Koichi Ito", "Hwann-Tzong Chen", "Takafumi Aoki"], "title": "Sparse2DGS: Sparse-View Surface Reconstruction using 2D Gaussian Splatting with Dense Point Cloud", "categories": ["cs.CV"], "comment": "Accepted to ICIP 2025", "summary": "Gaussian Splatting (GS) has gained attention as a fast and effective method\nfor novel view synthesis. It has also been applied to 3D reconstruction using\nmulti-view images and can achieve fast and accurate 3D reconstruction. However,\nGS assumes that the input contains a large number of multi-view images, and\ntherefore, the reconstruction accuracy significantly decreases when only a\nlimited number of input images are available. One of the main reasons is the\ninsufficient number of 3D points in the sparse point cloud obtained through\nStructure from Motion (SfM), which results in a poor initialization for\noptimizing the Gaussian primitives. We propose a new 3D reconstruction method,\ncalled Sparse2DGS, to enhance 2DGS in reconstructing objects using only three\nimages. Sparse2DGS employs DUSt3R, a fundamental model for stereo images, along\nwith COLMAP MVS to generate highly accurate and dense 3D point clouds, which\nare then used to initialize 2D Gaussians. Through experiments on the DTU\ndataset, we show that Sparse2DGS can accurately reconstruct the 3D shapes of\nobjects using just three images."}
{"id": "2505.19858", "pdf": "https://arxiv.org/pdf/2505.19858", "abs": "https://arxiv.org/abs/2505.19858", "authors": ["Zixiang Zhao", "Haowen Bai", "Bingxin Ke", "Yukun Cui", "Lilun Deng", "Yulun Zhang", "Kai Zhang", "Konrad Schindler"], "title": "A Unified Solution to Video Fusion: From Multi-Frame Learning to Benchmarking", "categories": ["cs.CV"], "comment": null, "summary": "The real world is dynamic, yet most image fusion methods process static\nframes independently, ignoring temporal correlations in videos and leading to\nflickering and temporal inconsistency. To address this, we propose Unified\nVideo Fusion (UniVF), a novel framework for temporally coherent video fusion\nthat leverages multi-frame learning and optical flow-based feature warping for\ninformative, temporally coherent video fusion. To support its development, we\nalso introduce Video Fusion Benchmark (VF-Bench), the first comprehensive\nbenchmark covering four video fusion tasks: multi-exposure, multi-focus,\ninfrared-visible, and medical fusion. VF-Bench provides high-quality,\nwell-aligned video pairs obtained through synthetic data generation and\nrigorous curation from existing datasets, with a unified evaluation protocol\nthat jointly assesses the spatial quality and temporal consistency of video\nfusion. Extensive experiments show that UniVF achieves state-of-the-art results\nacross all tasks on VF-Bench. Project page: https://vfbench.github.io."}
{"id": "2505.19863", "pdf": "https://arxiv.org/pdf/2505.19863", "abs": "https://arxiv.org/abs/2505.19863", "authors": ["Lukas Meyer", "Andrei-Timotei Ardelean", "Tim Weyrich", "Marc Stamminger"], "title": "FruitNeRF++: A Generalized Multi-Fruit Counting Method Utilizing Contrastive Learning and Neural Radiance Fields", "categories": ["cs.CV", "cs.LG"], "comment": "for project website, see https://meyerls.github.io/fruit_nerfpp", "summary": "We introduce FruitNeRF++, a novel fruit-counting approach that combines\ncontrastive learning with neural radiance fields to count fruits from\nunstructured input photographs of orchards. Our work is based on FruitNeRF,\nwhich employs a neural semantic field combined with a fruit-specific clustering\napproach. The requirement for adaptation for each fruit type limits the\napplicability of the method, and makes it difficult to use in practice. To lift\nthis limitation, we design a shape-agnostic multi-fruit counting framework,\nthat complements the RGB and semantic data with instance masks predicted by a\nvision foundation model. The masks are used to encode the identity of each\nfruit as instance embeddings into a neural instance field. By volumetrically\nsampling the neural fields, we extract a point cloud embedded with the instance\nfeatures, which can be clustered in a fruit-agnostic manner to obtain the fruit\ncount. We evaluate our approach using a synthetic dataset containing apples,\nplums, lemons, pears, peaches, and mangoes, as well as a real-world benchmark\napple dataset. Our results demonstrate that FruitNeRF++ is easier to control\nand compares favorably to other state-of-the-art methods."}
{"id": "2505.19868", "pdf": "https://arxiv.org/pdf/2505.19868", "abs": "https://arxiv.org/abs/2505.19868", "authors": ["Junhong Lee", "Seungwook Kim", "Minsu Cho"], "title": "Harnessing the Power of Training-Free Techniques in Text-to-2D Generation for Text-to-3D Generation via Score Distillation Sampling", "categories": ["cs.CV"], "comment": null, "summary": "Recent studies show that simple training-free techniques can dramatically\nimprove the quality of text-to-2D generation outputs, e.g. Classifier-Free\nGuidance (CFG) or FreeU. However, these training-free techniques have been\nunderexplored in the lens of Score Distillation Sampling (SDS), which is a\npopular and effective technique to leverage the power of pretrained text-to-2D\ndiffusion models for various tasks. In this paper, we aim to shed light on the\neffect such training-free techniques have on SDS, via a particular application\nof text-to-3D generation via 2D lifting. We present our findings, which show\nthat varying the scales of CFG presents a trade-off between object size and\nsurface smoothness, while varying the scales of FreeU presents a trade-off\nbetween texture details and geometric errors. Based on these findings, we\nprovide insights into how we can effectively harness training-free techniques\nfor SDS, via a strategic scaling of such techniques in a dynamic manner with\nrespect to the timestep or optimization iteration step. We show that using our\nproposed scheme strikes a favorable balance between texture details and surface\nsmoothness in text-to-3D generations, while preserving the size of the output\nand mitigating the occurrence of geometric defects."}
{"id": "2505.19873", "pdf": "https://arxiv.org/pdf/2505.19873", "abs": "https://arxiv.org/abs/2505.19873", "authors": ["Yanqi Cheng", "Tieyong Zeng", "Pietro Lio", "Carola-Bibiane Schönlieb", "Angelica I Aviles-Rivero"], "title": "Deep Spectral Prior", "categories": ["cs.CV", "cs.NA", "math.NA"], "comment": null, "summary": "We introduce Deep Spectral Prior (DSP), a new formulation of Deep Image Prior\n(DIP) that redefines image reconstruction as a frequency-domain alignment\nproblem. Unlike traditional DIP, which relies on pixel-wise loss and early\nstopping to mitigate overfitting, DSP directly matches Fourier coefficients\nbetween the network output and observed measurements. This shift introduces an\nexplicit inductive bias towards spectral coherence, aligning with the known\nfrequency structure of images and the spectral bias of convolutional neural\nnetworks. We provide a rigorous theoretical framework demonstrating that DSP\nacts as an implicit spectral regulariser, suppressing high-frequency noise by\ndesign and eliminating the need for early stopping. Our analysis spans four\ncore dimensions establishing smooth convergence dynamics, local stability, and\nfavourable bias-variance tradeoffs. We further show that DSP naturally projects\nreconstructions onto a frequency-consistent manifold, enhancing\ninterpretability and robustness. These theoretical guarantees are supported by\nempirical results across denoising, inpainting, and super-resolution tasks,\nwhere DSP consistently outperforms classical DIP and other unsupervised\nbaselines."}
{"id": "2505.19874", "pdf": "https://arxiv.org/pdf/2505.19874", "abs": "https://arxiv.org/abs/2505.19874", "authors": ["Yi Wu", "Lingting Zhu", "Shengju Qian", "Lei Liu", "Wandi Qiao", "Lequan Yu", "Bin Li"], "title": "StyleAR: Customizing Multimodal Autoregressive Model for Style-Aligned Text-to-Image Generation", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": null, "summary": "In the current research landscape, multimodal autoregressive (AR) models have\nshown exceptional capabilities across various domains, including visual\nunderstanding and generation. However, complex tasks such as style-aligned\ntext-to-image generation present significant challenges, particularly in data\nacquisition. In analogy to instruction-following tuning for image editing of AR\nmodels, style-aligned generation requires a reference style image and prompt,\nresulting in a text-image-to-image triplet where the output shares the style\nand semantics of the input. However, acquiring large volumes of such triplet\ndata with specific styles is considerably more challenging than obtaining\nconventional text-to-image data used for training generative models. To address\nthis issue, we propose StyleAR, an innovative approach that combines a\nspecially designed data curation method with our proposed AR models to\neffectively utilize text-to-image binary data for style-aligned text-to-image\ngeneration. Our method synthesizes target stylized data using a reference style\nimage and prompt, but only incorporates the target stylized image as the image\nmodality to create high-quality binary data. To facilitate binary data\ntraining, we introduce a CLIP image encoder with a perceiver resampler that\ntranslates the image input into style tokens aligned with multimodal tokens in\nAR models and implement a style-enhanced token technique to prevent content\nleakage which is a common issue in previous work. Furthermore, we mix raw\nimages drawn from large-scale text-image datasets with stylized images to\nenhance StyleAR's ability to extract richer stylistic features and ensure style\nconsistency. Extensive qualitative and quantitative experiments demonstrate our\nsuperior performance."}
{"id": "2505.19877", "pdf": "https://arxiv.org/pdf/2505.19877", "abs": "https://arxiv.org/abs/2505.19877", "authors": ["Chao Huang", "Benfeng Wang", "Jie Wen", "Chengliang Liu", "Wei Wang", "Li Shen", "Xiaochun Cao"], "title": "Vad-R1: Towards Video Anomaly Reasoning via Perception-to-Cognition Chain-of-Thought", "categories": ["cs.CV"], "comment": "9 pages, 4 figures", "summary": "Recent advancements in reasoning capability of Multimodal Large Language\nModels (MLLMs) demonstrate its effectiveness in tackling complex visual tasks.\nHowever, existing MLLM-based Video Anomaly Detection (VAD) methods remain\nlimited to shallow anomaly descriptions without deep reasoning. In this paper,\nwe propose a new task named Video Anomaly Reasoning (VAR), which aims to enable\ndeep analysis and understanding of anomalies in the video by requiring MLLMs to\nthink explicitly before answering. To this end, we propose Vad-R1, an\nend-to-end MLLM-based framework for VAR. Specifically, we design a\nPerception-to-Cognition Chain-of-Thought (P2C-CoT) that simulates the human\nprocess of recognizing anomalies, guiding the MLLM to reason anomaly\nstep-by-step. Based on the structured P2C-CoT, we construct Vad-Reasoning, a\ndedicated dataset for VAR. Furthermore, we propose an improved reinforcement\nlearning algorithm AVA-GRPO, which explicitly incentivizes the anomaly\nreasoning capability of MLLMs through a self-verification mechanism with\nlimited annotations. Experimental results demonstrate that Vad-R1 achieves\nsuperior performance, outperforming both open-source and proprietary models on\nVAD and VAR tasks. Codes and datasets will be released at\nhttps://github.com/wbfwonderful/Vad-R1."}
{"id": "2505.19883", "pdf": "https://arxiv.org/pdf/2505.19883", "abs": "https://arxiv.org/abs/2505.19883", "authors": ["Shintaro Ito", "Natsuki Takama", "Koichi Ito", "Hwann-Tzong Chen", "Takafumi Aoki"], "title": "ErpGS: Equirectangular Image Rendering enhanced with 3D Gaussian Regularization", "categories": ["cs.CV"], "comment": "Accepted to ICIP2025", "summary": "The use of multi-view images acquired by a 360-degree camera can reconstruct\na 3D space with a wide area. There are 3D reconstruction methods from\nequirectangular images based on NeRF and 3DGS, as well as Novel View Synthesis\n(NVS) methods. On the other hand, it is necessary to overcome the large\ndistortion caused by the projection model of a 360-degree camera when\nequirectangular images are used. In 3DGS-based methods, the large distortion of\nthe 360-degree camera model generates extremely large 3D Gaussians, resulting\nin poor rendering accuracy. We propose ErpGS, which is Omnidirectional GS based\non 3DGS to realize NVS addressing the problems. ErpGS introduce some rendering\naccuracy improvement techniques: geometric regularization, scale\nregularization, and distortion-aware weights and a mask to suppress the effects\nof obstacles in equirectangular images. Through experiments on public datasets,\nwe demonstrate that ErpGS can render novel view images more accurately than\nconventional methods."}
{"id": "2505.19889", "pdf": "https://arxiv.org/pdf/2505.19889", "abs": "https://arxiv.org/abs/2505.19889", "authors": ["David Schneider", "Zdravko Marinov", "Rafael Baur", "Zeyun Zhong", "Rodi Düger", "Rainer Stiefelhagen"], "title": "OmniFall: A Unified Staged-to-Wild Benchmark for Human Fall Detection", "categories": ["cs.CV", "I.2.10; I.5.4"], "comment": null, "summary": "Current video-based fall detection research mostly relies on small, staged\ndatasets with significant domain biases concerning background, lighting, and\ncamera setup resulting in unknown real-world performance. We introduce\nOmniFall, unifying eight public fall detection datasets (roughly 14 h of\nrecordings, roughly 42 h of multiview data, 101 subjects, 29 camera views)\nunder a consistent ten-class taxonomy with standardized evaluation protocols.\nOur benchmark provides complete video segmentation labels and enables fair\ncross-dataset comparison previously impossible with incompatible annotation\nschemes. For real-world evaluation we curate OOPS-Fall from genuine accident\nvideos and establish a staged-to-wild protocol measuring generalization from\ncontrolled to uncontrolled environments. Experiments with frozen pre-trained\nbackbones such as I3D or VideoMAE reveal significant performance gaps between\nin-distribution and in-the-wild scenarios, highlighting critical challenges in\ndeveloping robust fall detection systems. OmniFall Dataset at\nhttps://huggingface.co/datasets/simplexsigil2/omnifall , Code at\nhttps://github.com/simplexsigil/omnifall-experiments"}
{"id": "2505.19895", "pdf": "https://arxiv.org/pdf/2505.19895", "abs": "https://arxiv.org/abs/2505.19895", "authors": ["Afrah Shaahid", "Muzammil Behzad"], "title": "Underwater Diffusion Attention Network with Contrastive Language-Image Joint Learning for Underwater Image Enhancement", "categories": ["cs.CV"], "comment": null, "summary": "Underwater images are often affected by complex degradations such as light\nabsorption, scattering, color casts, and artifacts, making enhancement critical\nfor effective object detection, recognition, and scene understanding in aquatic\nenvironments. Existing methods, especially diffusion-based approaches,\ntypically rely on synthetic paired datasets due to the scarcity of real\nunderwater references, introducing bias and limiting generalization.\nFurthermore, fine-tuning these models can degrade learned priors, resulting in\nunrealistic enhancements due to domain shifts. To address these challenges, we\npropose UDAN-CLIP, an image-to-image diffusion framework pre-trained on\nsynthetic underwater datasets and enhanced with a customized classifier based\non vision-language model, a spatial attention module, and a novel\nCLIP-Diffusion loss. The classifier preserves natural in-air priors and\nsemantically guides the diffusion process, while the spatial attention module\nfocuses on correcting localized degradations such as haze and low contrast. The\nproposed CLIP-Diffusion loss further strengthens visual-textual alignment and\nhelps maintain semantic consistency during enhancement. The proposed\ncontributions empower our UDAN-CLIP model to perform more effective underwater\nimage enhancement, producing results that are not only visually compelling but\nalso more realistic and detail-preserving. These improvements are consistently\nvalidated through both quantitative metrics and qualitative visual comparisons,\ndemonstrating the model's ability to correct distortions and restore natural\nappearance in challenging underwater conditions."}
{"id": "2505.19901", "pdf": "https://arxiv.org/pdf/2505.19901", "abs": "https://arxiv.org/abs/2505.19901", "authors": ["Peng Liu", "Xiaoming Ren", "Fengkai Liu", "Qingsong Xie", "Quanlong Zheng", "Yanhao Zhang", "Haonan Lu", "Yujiu Yang"], "title": "Dynamic-I2V: Exploring Image-to-Video Generaion Models via Multimodal LLM", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in image-to-video (I2V) generation have shown promising\nperformance in conventional scenarios. However, these methods still encounter\nsignificant challenges when dealing with complex scenes that require a deep\nunderstanding of nuanced motion and intricate object-action relationships. To\naddress these challenges, we present Dynamic-I2V, an innovative framework that\nintegrates Multimodal Large Language Models (MLLMs) to jointly encode visual\nand textual conditions for a diffusion transformer (DiT) architecture. By\nleveraging the advanced multimodal understanding capabilities of MLLMs, our\nmodel significantly improves motion controllability and temporal coherence in\nsynthesized videos. The inherent multimodality of Dynamic-I2V further enables\nflexible support for diverse conditional inputs, extending its applicability to\nvarious downstream generation tasks. Through systematic analysis, we identify a\ncritical limitation in current I2V benchmarks: a significant bias towards\nfavoring low-dynamic videos, stemming from an inadequate balance between motion\ncomplexity and visual quality metrics. To resolve this evaluation gap, we\npropose DIVE - a novel assessment benchmark specifically designed for\ncomprehensive dynamic quality measurement in I2V generation. In conclusion,\nextensive quantitative and qualitative experiments confirm that Dynamic-I2V\nattains state-of-the-art performance in image-to-video generation, particularly\nrevealing significant improvements of 42.5%, 7.9%, and 11.8% in dynamic range,\ncontrollability, and quality, respectively, as assessed by the DIVE metric in\ncomparison to existing methods."}
{"id": "2505.19911", "pdf": "https://arxiv.org/pdf/2505.19911", "abs": "https://arxiv.org/abs/2505.19911", "authors": ["Xiaosen Wang", "Shaokang Wang", "Zhijin Ge", "Yuyang Luo", "Shudong Zhang"], "title": "Attention! You Vision Language Model Could Be Maliciously Manipulated", "categories": ["cs.CV"], "comment": null, "summary": "Large Vision-Language Models (VLMs) have achieved remarkable success in\nunderstanding complex real-world scenarios and supporting data-driven\ndecision-making processes. However, VLMs exhibit significant vulnerability\nagainst adversarial examples, either text or image, which can lead to various\nadversarial outcomes, e.g., jailbreaking, hijacking, and hallucination, etc. In\nthis work, we empirically and theoretically demonstrate that VLMs are\nparticularly susceptible to image-based adversarial examples, where\nimperceptible perturbations can precisely manipulate each output token. To this\nend, we propose a novel attack called Vision-language model Manipulation Attack\n(VMA), which integrates first-order and second-order momentum optimization\ntechniques with a differentiable transformation mechanism to effectively\noptimize the adversarial perturbation. Notably, VMA can be a double-edged\nsword: it can be leveraged to implement various attacks, such as jailbreaking,\nhijacking, privacy breaches, Denial-of-Service, and the generation of sponge\nexamples, etc, while simultaneously enabling the injection of watermarks for\ncopyright protection. Extensive empirical evaluations substantiate the efficacy\nand generalizability of VMA across diverse scenarios and datasets."}
{"id": "2505.19919", "pdf": "https://arxiv.org/pdf/2505.19919", "abs": "https://arxiv.org/abs/2505.19919", "authors": ["Chen Sang", "Yeqiang Qian", "Jiale Zhang", "Chunxiang Wang", "Ming Yang"], "title": "Weather-Magician: Reconstruction and Rendering Framework for 4D Weather Synthesis In Real Time", "categories": ["cs.CV"], "comment": "Project homepage: https://weathermagician.github.io", "summary": "For tasks such as urban digital twins, VR/AR/game scene design, or creating\nsynthetic films, the traditional industrial approach often involves manually\nmodeling scenes and using various rendering engines to complete the rendering\nprocess. This approach typically requires high labor costs and hardware\ndemands, and can result in poor quality when replicating complex real-world\nscenes. A more efficient approach is to use data from captured real-world\nscenes, then apply reconstruction and rendering algorithms to quickly recreate\nthe authentic scene. However, current algorithms are unable to effectively\nreconstruct and render real-world weather effects. To address this, we propose\na framework based on gaussian splatting, that can reconstruct real scenes and\nrender them under synthesized 4D weather effects. Our work can simulate various\ncommon weather effects by applying Gaussians modeling and rendering techniques.\nIt supports continuous dynamic weather changes and can easily control the\ndetails of the effects. Additionally, our work has low hardware requirements\nand achieves real-time rendering performance. The result demos can be accessed\non our project homepage: weathermagician.github.io"}
{"id": "2505.19920", "pdf": "https://arxiv.org/pdf/2505.19920", "abs": "https://arxiv.org/abs/2505.19920", "authors": ["Sebastian Groß", "Stefan Heindorf", "Philipp Terhörst"], "title": "A Responsible Face Recognition Approach for Small and Mid-Scale Systems Through Personalized Neural Networks", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Traditional face recognition systems rely on extracting fixed face\nrepresentations, known as templates, to store and verify identities. These\nrepresentations are typically generated by neural networks that often lack\nexplainability and raise concerns regarding fairness and privacy. In this work,\nwe propose a novel model-template (MOTE) approach that replaces vector-based\nface templates with small personalized neural networks. This design enables\nmore responsible face recognition for small and medium-scale systems. During\nenrollment, MOTE creates a dedicated binary classifier for each identity,\ntrained to determine whether an input face matches the enrolled identity. Each\nclassifier is trained using only a single reference sample, along with\nsynthetically balanced samples to allow adjusting fairness at the level of a\nsingle individual during enrollment. Extensive experiments across multiple\ndatasets and recognition systems demonstrate substantial improvements in\nfairness and particularly in privacy. Although the method increases inference\ntime and storage requirements, it presents a strong solution for small- and\nmid-scale applications where fairness and privacy are critical."}
{"id": "2505.19928", "pdf": "https://arxiv.org/pdf/2505.19928", "abs": "https://arxiv.org/abs/2505.19928", "authors": ["Gabriele Lagani", "Fabrizio Falchi", "Claudio Gennaro", "Giuseppe Amato"], "title": "CA3D: Convolutional-Attentional 3D Nets for Efficient Video Activity Recognition on the Edge", "categories": ["cs.CV"], "comment": null, "summary": "In this paper, we introduce a deep learning solution for video activity\nrecognition that leverages an innovative combination of convolutional layers\nwith a linear-complexity attention mechanism. Moreover, we introduce a novel\nquantization mechanism to further improve the efficiency of our model during\nboth training and inference. Our model maintains a reduced computational cost,\nwhile preserving robust learning and generalization capabilities. Our approach\naddresses the issues related to the high computing requirements of current\nmodels, with the goal of achieving competitive accuracy on consumer and edge\ndevices, enabling smart home and smart healthcare applications where efficiency\nand privacy issues are of concern. We experimentally validate our model on\ndifferent established and publicly available video activity recognition\nbenchmarks, improving accuracy over alternative models at a competitive\ncomputing cost."}
{"id": "2505.19938", "pdf": "https://arxiv.org/pdf/2505.19938", "abs": "https://arxiv.org/abs/2505.19938", "authors": ["Wenrui Li", "Penghong Wang", "Xingtao Wang", "Wangmeng Zuo", "Xiaopeng Fan", "Yonghong Tian"], "title": "Multi-Timescale Motion-Decoupled Spiking Transformer for Audio-Visual Zero-Shot Learning", "categories": ["cs.CV"], "comment": "Accepted by IEEE TCSVT", "summary": "Audio-visual zero-shot learning (ZSL) has been extensively researched for its\ncapability to classify video data from unseen classes during training.\nNevertheless, current methodologies often struggle with background scene biases\nand inadequate motion detail. This paper proposes a novel dual-stream\nMulti-Timescale Motion-Decoupled Spiking Transformer (MDST++), which decouples\ncontextual semantic information and sparse dynamic motion information. The\nrecurrent joint learning unit is proposed to extract contextual semantic\ninformation and capture joint knowledge across various modalities to understand\nthe environment of actions. By converting RGB images to events, our method\ncaptures motion information more accurately and mitigates background scene\nbiases. Moreover, we introduce a discrepancy analysis block to model audio\nmotion information. To enhance the robustness of SNNs in extracting temporal\nand motion cues, we dynamically adjust the threshold of Leaky\nIntegrate-and-Fire neurons based on global motion and contextual semantic\ninformation. Our experiments validate the effectiveness of MDST++,\ndemonstrating their consistent superiority over state-of-the-art methods on\nmainstream benchmarks. Additionally, incorporating motion and multi-timescale\ninformation significantly improves HM and ZSL accuracy by 26.2\\% and 39.9\\%."}
{"id": "2505.19944", "pdf": "https://arxiv.org/pdf/2505.19944", "abs": "https://arxiv.org/abs/2505.19944", "authors": ["Naoyuki Terashita", "Yusuke Tozaki", "Hideaki Omote", "Congkha Nguyen", "Ryosuke Nakamoto", "Yuta Koreeda", "Hiroaki Ozaki"], "title": "Can Visual Encoder Learn to See Arrows?", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "This work has been accepted for poster presentation at the Second\n  Workshop on Visual Concepts in CVPR 2025", "summary": "The diagram is a visual representation of a relationship illustrated with\nedges (lines or arrows), which is widely used in industrial and scientific\ncommunication. Although recognizing diagrams is essential for vision language\nmodels (VLMs) to comprehend domain-specific knowledge, recent studies reveal\nthat many VLMs fail to identify edges in images. We hypothesize that these\nfailures stem from an over-reliance on textual and positional biases,\npreventing VLMs from learning explicit edge features. Based on this idea, we\nempirically investigate whether the image encoder in VLMs can learn edge\nrepresentation through training on a diagram dataset in which edges are biased\nneither by textual nor positional information. To this end, we conduct\ncontrastive learning on an artificially generated diagram--caption dataset to\ntrain an image encoder and evaluate its diagram-related features on three\ntasks: probing, image retrieval, and captioning. Our results show that the\nfinetuned model outperforms pretrained CLIP in all tasks and surpasses\nzero-shot GPT-4o and LLaVA-Mistral in the captioning task. These findings\nconfirm that eliminating textual and positional biases fosters accurate edge\nrecognition in VLMs, offering a promising path for advancing diagram\nunderstanding."}
{"id": "2505.19948", "pdf": "https://arxiv.org/pdf/2505.19948", "abs": "https://arxiv.org/abs/2505.19948", "authors": ["Gokul Adethya", "Bhanu Pratyush Mantha", "Tianyang Wang", "Xingjian Li", "Min Xu"], "title": "SaSi: A Self-augmented and Self-interpreted Deep Learning Approach for Few-shot Cryo-ET Particle Detection", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Cryo-electron tomography (cryo-ET) has emerged as a powerful technique for\nimaging macromolecular complexes in their near-native states. However, the\nlocalization of 3D particles in cellular environments still presents a\nsignificant challenge due to low signal-to-noise ratios and missing wedge\nartifacts. Deep learning approaches have shown great potential, but they need\nhuge amounts of data, which can be a challenge in cryo-ET scenarios where\nlabeled data is often scarce. In this paper, we propose a novel Self-augmented\nand Self-interpreted (SaSi) deep learning approach towards few-shot particle\ndetection in 3D cryo-ET images. Our method builds upon self-augmentation\ntechniques to further boost data utilization and introduces a self-interpreted\nsegmentation strategy for alleviating dependency on labeled data, hence\nimproving generalization and robustness. As demonstrated by experiments\nconducted on both simulated and real-world cryo-ET datasets, the SaSi approach\nsignificantly outperforms existing state-of-the-art methods for particle\nlocalization. This research increases understanding of how to detect particles\nwith very few labels in cryo-ET and thus sets a new benchmark for few-shot\nlearning in structural biology."}
{"id": "2505.19952", "pdf": "https://arxiv.org/pdf/2505.19952", "abs": "https://arxiv.org/abs/2505.19952", "authors": ["Rong-Cheng Tu", "Wenhao Sun", "Hanzhe You", "Yingjie Wang", "Jiaxing Huang", "Li Shen", "Dacheng Tao"], "title": "Multimodal Reasoning Agent for Zero-Shot Composed Image Retrieval", "categories": ["cs.CV", "cs.IR"], "comment": null, "summary": "Zero-Shot Composed Image Retrieval (ZS-CIR) aims to retrieve target images\ngiven a compositional query, consisting of a reference image and a modifying\ntext-without relying on annotated training data. Existing approaches often\ngenerate a synthetic target text using large language models (LLMs) to serve as\nan intermediate anchor between the compositional query and the target image.\nModels are then trained to align the compositional query with the generated\ntext, and separately align images with their corresponding texts using\ncontrastive learning. However, this reliance on intermediate text introduces\nerror propagation, as inaccuracies in query-to-text and text-to-image mappings\naccumulate, ultimately degrading retrieval performance. To address these\nproblems, we propose a novel framework by employing a Multimodal Reasoning\nAgent (MRA) for ZS-CIR. MRA eliminates the dependence on textual intermediaries\nby directly constructing triplets, <reference image, modification text, target\nimage>, using only unlabeled image data. By training on these synthetic\ntriplets, our model learns to capture the relationships between compositional\nqueries and candidate images directly. Extensive experiments on three standard\nCIR benchmarks demonstrate the effectiveness of our approach. On the FashionIQ\ndataset, our method improves Average R@10 by at least 7.5\\% over existing\nbaselines; on CIRR, it boosts R@1 by 9.6\\%; and on CIRCO, it increases mAP@5 by\n9.5\\%."}
{"id": "2505.19958", "pdf": "https://arxiv.org/pdf/2505.19958", "abs": "https://arxiv.org/abs/2505.19958", "authors": ["Yong Liu", "Jinshan Pan", "Yinchuan Li", "Qingji Dong", "Chao Zhu", "Yu Guo", "Fei Wang"], "title": "UltraVSR: Achieving Ultra-Realistic Video Super-Resolution with Efficient One-Step Diffusion Space", "categories": ["cs.CV"], "comment": "Under review, 10 pages, 7 figures", "summary": "Diffusion models have shown great potential in generating realistic image\ndetail. However, adapting these models to video super-resolution (VSR) remains\nchallenging due to their inherent stochasticity and lack of temporal modeling.\nIn this paper, we propose UltraVSR, a novel framework that enables\nultra-realistic and temporal-coherent VSR through an efficient one-step\ndiffusion space. A central component of UltraVSR is the Degradation-aware\nRestoration Schedule (DRS), which estimates a degradation factor from the\nlow-resolution input and transforms iterative denoising process into a\nsingle-step reconstruction from from low-resolution to high-resolution videos.\nThis design eliminates randomness from diffusion noise and significantly speeds\nup inference. To ensure temporal consistency, we propose a lightweight yet\neffective Recurrent Temporal Shift (RTS) module, composed of an RTS-convolution\nunit and an RTS-attention unit. By partially shifting feature components along\nthe temporal dimension, these two units collaboratively facilitate effective\nfeature propagation, fusion, and alignment across neighboring frames, without\nrelying on explicit temporal layers. The RTS module is integrated into a\npretrained text-to-image diffusion model and is further enhanced through\nSpatio-temporal Joint Distillation (SJD), which improves temporal coherence\nwhile preserving realistic details. Additionally, we introduce a Temporally\nAsynchronous Inference (TAI) strategy to capture long-range temporal\ndependencies under limited memory constraints. Extensive experiments show that\nUltraVSR achieves state-of-the-art performance, both qualitatively and\nquantitatively, in a single sampling step."}
{"id": "2505.19972", "pdf": "https://arxiv.org/pdf/2505.19972", "abs": "https://arxiv.org/abs/2505.19972", "authors": ["Kanglei Zhou", "Hubert P. H. Shum", "Frederick W. B. Li", "Xingxing Zhang", "Xiaohui Liang"], "title": "PHI: Bridging Domain Shift in Long-Term Action Quality Assessment via Progressive Hierarchical Instruction", "categories": ["cs.CV"], "comment": "Accepted by IEEE Transactions on Image Processing", "summary": "Long-term Action Quality Assessment (AQA) aims to evaluate the quantitative\nperformance of actions in long videos. However, existing methods face\nchallenges due to domain shifts between the pre-trained large-scale action\nrecognition backbones and the specific AQA task, thereby hindering their\nperformance. This arises since fine-tuning resource-intensive backbones on\nsmall AQA datasets is impractical. We address this by identifying two levels of\ndomain shift: task-level, regarding differences in task objectives, and\nfeature-level, regarding differences in important features. For feature-level\nshifts, which are more detrimental, we propose Progressive Hierarchical\nInstruction (PHI) with two strategies. First, Gap Minimization Flow (GMF)\nleverages flow matching to progressively learn a fast flow path that reduces\nthe domain gap between initial and desired features across shallow to deep\nlayers. Additionally, a temporally-enhanced attention module captures\nlong-range dependencies essential for AQA. Second, List-wise Contrastive\nRegularization (LCR) facilitates coarse-to-fine alignment by comprehensively\ncomparing batch pairs to learn fine-grained cues while mitigating domain shift.\nIntegrating these modules, PHI offers an effective solution. Experiments\ndemonstrate that PHI achieves state-of-the-art performance on three\nrepresentative long-term AQA datasets, proving its superiority in addressing\nthe domain shift for long-term AQA."}
{"id": "2505.19985", "pdf": "https://arxiv.org/pdf/2505.19985", "abs": "https://arxiv.org/abs/2505.19985", "authors": ["Jianqiao Zheng", "Xueqian Li", "Hemanth Saratchandran", "Simon Lucey"], "title": "Structured Initialization for Vision Transformers", "categories": ["cs.CV"], "comment": null, "summary": "Convolutional Neural Networks (CNNs) inherently encode strong inductive\nbiases, enabling effective generalization on small-scale datasets. In this\npaper, we propose integrating this inductive bias into ViTs, not through an\narchitectural intervention but solely through initialization. The motivation\nhere is to have a ViT that can enjoy strong CNN-like performance when data\nassets are small, but can still scale to ViT-like performance as the data\nexpands. Our approach is motivated by our empirical results that random impulse\nfilters can achieve commensurate performance to learned filters within a CNN.\nWe improve upon current ViT initialization strategies, which typically rely on\nempirical heuristics such as using attention weights from pretrained models or\nfocusing on the distribution of attention weights without enforcing structures.\nEmpirical results demonstrate that our method significantly outperforms\nstandard ViT initialization across numerous small and medium-scale benchmarks,\nincluding Food-101, CIFAR-10, CIFAR-100, STL-10, Flowers, and Pets, while\nmaintaining comparative performance on large-scale datasets such as\nImageNet-1K. Moreover, our initialization strategy can be easily integrated\ninto various transformer-based architectures such as Swin Transformer and\nMLP-Mixer with consistent improvements in performance."}
{"id": "2505.19990", "pdf": "https://arxiv.org/pdf/2505.19990", "abs": "https://arxiv.org/abs/2505.19990", "authors": ["Jack Hong", "Shilin Yan", "Zehao Xiao", "Jiayin Cai", "Xiaolong Jiang", "Yao Hu", "Henghui Ding"], "title": "Progressive Scaling Visual Object Tracking", "categories": ["cs.CV"], "comment": null, "summary": "In this work, we propose a progressive scaling training strategy for visual\nobject tracking, systematically analyzing the influence of training data\nvolume, model size, and input resolution on tracking performance. Our empirical\nstudy reveals that while scaling each factor leads to significant improvements\nin tracking accuracy, naive training suffers from suboptimal optimization and\nlimited iterative refinement. To address this issue, we introduce DT-Training,\na progressive scaling framework that integrates small teacher transfer and\ndual-branch alignment to maximize model potential. The resulting scaled tracker\nconsistently outperforms state-of-the-art methods across multiple benchmarks,\ndemonstrating strong generalization and transferability of the proposed method.\nFurthermore, we validate the broader applicability of our approach to\nadditional tasks, underscoring its versatility beyond tracking."}
{"id": "2505.20001", "pdf": "https://arxiv.org/pdf/2505.20001", "abs": "https://arxiv.org/abs/2505.20001", "authors": ["Shihao Li", "Chenglong Li", "Aihua Zheng", "Andong Lu", "Jin Tang", "Jixin Ma"], "title": "NEXT: Multi-Grained Mixture of Experts via Text-Modulation for Multi-Modal Object Re-ID", "categories": ["cs.CV"], "comment": null, "summary": "Multi-modal object re-identification (ReID) aims to extract identity features\nacross heterogeneous spectral modalities to enable accurate recognition and\nretrieval in complex real-world scenarios. However, most existing methods rely\non implicit feature fusion structures, making it difficult to model\nfine-grained recognition strategies under varying challenging conditions.\nBenefiting from the powerful semantic understanding capabilities of Multi-modal\nLarge Language Models (MLLMs), the visual appearance of an object can be\neffectively translated into descriptive text. In this paper, we propose a\nreliable multi-modal caption generation method based on attribute confidence,\nwhich significantly reduces the unknown recognition rate of MLLMs in\nmulti-modal semantic generation and improves the quality of generated text.\nAdditionally, we propose a novel ReID framework NEXT, the Multi-grained Mixture\nof Experts via Text-Modulation for Multi-modal Object Re-Identification.\nSpecifically, we decouple the recognition problem into semantic and structural\nexpert branches to separately capture modality-specific appearance and\nintrinsic structure. For semantic recognition, we propose the Text-Modulated\nSemantic-sampling Experts (TMSE), which leverages randomly sampled high-quality\nsemantic texts to modulate expert-specific sampling of multi-modal features and\nmining intra-modality fine-grained semantic cues. Then, to recognize\ncoarse-grained structure features, we propose the Context-Shared\nStructure-aware Experts (CSSE) that focuses on capturing the holistic object\nstructure across modalities and maintains inter-modality structural consistency\nthrough a soft routing mechanism. Finally, we propose the Multi-Modal Feature\nAggregation (MMFA), which adopts a unified feature fusion strategy to simply\nand effectively integrate semantic and structural expert outputs into the final\nidentity representations."}
{"id": "2505.20021", "pdf": "https://arxiv.org/pdf/2505.20021", "abs": "https://arxiv.org/abs/2505.20021", "authors": ["Hyunsik Chae", "Seungwoo Yoon", "Jaden Park", "Chloe Yewon Chun", "Yongin Cho", "Mu Cai", "Yong Jae Lee", "Ernest K. Ryu"], "title": "Decomposing Complex Visual Comprehension into Atomic Visual Skills for Vision Language Models", "categories": ["cs.CV", "cs.AI"], "comment": "69 pages, 16 figures", "summary": "Recent Vision-Language Models (VLMs) have demonstrated impressive multimodal\ncomprehension and reasoning capabilities, yet they often struggle with\ntrivially simple visual tasks. In this work, we focus on the domain of basic 2D\nEuclidean geometry and systematically categorize the fundamental, indivisible\nvisual perception skills, which we refer to as atomic visual skills. We then\nintroduce the Atomic Visual Skills Dataset (AVSD) for evaluating VLMs on the\natomic visual skills. Using AVSD, we benchmark state-of-the-art VLMs and find\nthat they struggle with these tasks, despite being trivial for adult humans.\nOur findings highlight the need for purpose-built datasets to train and\nevaluate VLMs on atomic, rather than composite, visual perception tasks."}
{"id": "2505.20024", "pdf": "https://arxiv.org/pdf/2505.20024", "abs": "https://arxiv.org/abs/2505.20024", "authors": ["Xueyi Liu", "Zuodong Zhong", "Yuxin Guo", "Yun-Fu Liu", "Zhiguo Su", "Qichao Zhang", "Junli Wang", "Yinfeng Gao", "Yupeng Zheng", "Qiao Lin", "Huiyong Chen", "Dongbin Zhao"], "title": "ReasonPlan: Unified Scene Prediction and Decision Reasoning for Closed-loop Autonomous Driving", "categories": ["cs.CV", "cs.AI", "cs.RO", "68T40(Primary), 68T45, 68T50(Secondary)", "I.2.9; I.2.10; I.5.1"], "comment": "18 pages; 9 figures; https://github.com/Liuxueyi/ReasonPlan", "summary": "Due to the powerful vision-language reasoning and generalization abilities,\nmultimodal large language models (MLLMs) have garnered significant attention in\nthe field of end-to-end (E2E) autonomous driving. However, their application to\nclosed-loop systems remains underexplored, and current MLLM-based methods have\nnot shown clear superiority to mainstream E2E imitation learning approaches. In\nthis work, we propose ReasonPlan, a novel MLLM fine-tuning framework designed\nfor closed-loop driving through holistic reasoning with a self-supervised Next\nScene Prediction task and supervised Decision Chain-of-Thought process. This\ndual mechanism encourages the model to align visual representations with\nactionable driving context, while promoting interpretable and causally grounded\ndecision making. We curate a planning-oriented decision reasoning dataset,\nnamely PDR, comprising 210k diverse and high-quality samples. Our method\noutperforms the mainstream E2E imitation learning method by a large margin of\n19% L2 and 16.1 driving score on Bench2Drive benchmark. Furthermore, ReasonPlan\ndemonstrates strong zero-shot generalization on unseen DOS benchmark,\nhighlighting its adaptability in handling zero-shot corner cases. Code and\ndataset will be found in https://github.com/Liuxueyi/ReasonPlan."}
{"id": "2505.20032", "pdf": "https://arxiv.org/pdf/2505.20032", "abs": "https://arxiv.org/abs/2505.20032", "authors": ["Fotios Lygerakis", "Ozan Özdenizci", "Elmar Rückert"], "title": "ViTaPEs: Visuotactile Position Encodings for Cross-Modal Alignment in Multimodal Transformers", "categories": ["cs.CV", "cs.LG", "cs.RO"], "comment": null, "summary": "Tactile sensing provides local essential information that is complementary to\nvisual perception, such as texture, compliance, and force. Despite recent\nadvances in visuotactile representation learning, challenges remain in fusing\nthese modalities and generalizing across tasks and environments without heavy\nreliance on pre-trained vision-language models. Moreover, existing methods do\nnot study positional encodings, thereby overlooking the multi-scale spatial\nreasoning needed to capture fine-grained visuotactile correlations. We\nintroduce ViTaPEs, a transformer-based framework that robustly integrates\nvisual and tactile input data to learn task-agnostic representations for\nvisuotactile perception. Our approach exploits a novel multi-scale positional\nencoding scheme to capture intra-modal structures, while simultaneously\nmodeling cross-modal cues. Unlike prior work, we provide provable guarantees in\nvisuotactile fusion, showing that our encodings are injective,\nrigid-motion-equivariant, and information-preserving, validating these\nproperties empirically. Experiments on multiple large-scale real-world datasets\nshow that ViTaPEs not only surpasses state-of-the-art baselines across various\nrecognition tasks but also demonstrates zero-shot generalization to unseen,\nout-of-domain scenarios. We further demonstrate the transfer-learning strength\nof ViTaPEs in a robotic grasping task, where it outperforms state-of-the-art\nbaselines in predicting grasp success. Project page:\nhttps://sites.google.com/view/vitapes"}
{"id": "2505.20033", "pdf": "https://arxiv.org/pdf/2505.20033", "abs": "https://arxiv.org/abs/2505.20033", "authors": ["Christoph Schuhmann", "Robert Kaczmarczyk", "Gollam Rabby", "Maurice Kraus", "Felix Friedrich", "Huu Nguyen", "Krishna Kalyan", "Kourosh Nadi", "Kristian Kersting", "Sören Auer"], "title": "EmoNet-Face: An Expert-Annotated Benchmark for Synthetic Emotion Recognition", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Effective human-AI interaction relies on AI's ability to accurately perceive\nand interpret human emotions. Current benchmarks for vision and vision-language\nmodels are severely limited, offering a narrow emotional spectrum that\noverlooks nuanced states (e.g., bitterness, intoxication) and fails to\ndistinguish subtle differences between related feelings (e.g., shame vs.\nembarrassment). Existing datasets also often use uncontrolled imagery with\noccluded faces and lack demographic diversity, risking significant bias. To\naddress these critical gaps, we introduce EmoNet Face, a comprehensive\nbenchmark suite. EmoNet Face features: (1) A novel 40-category emotion\ntaxonomy, meticulously derived from foundational research to capture finer\ndetails of human emotional experiences. (2) Three large-scale, AI-generated\ndatasets (EmoNet HQ, Binary, and Big) with explicit, full-face expressions and\ncontrolled demographic balance across ethnicity, age, and gender. (3) Rigorous,\nmulti-expert annotations for training and high-fidelity evaluation. (4) We\nbuild Empathic Insight Face, a model achieving human-expert-level performance\non our benchmark. The publicly released EmoNet Face suite - taxonomy, datasets,\nand model - provides a robust foundation for developing and evaluating AI\nsystems with a deeper understanding of human emotions."}
{"id": "2505.20041", "pdf": "https://arxiv.org/pdf/2505.20041", "abs": "https://arxiv.org/abs/2505.20041", "authors": ["Jianxin Huang", "Jiahang Li", "Sergey Vityazev", "Alexander Dvorkovich", "Rui Fan"], "title": "DepthMatch: Semi-Supervised RGB-D Scene Parsing through Depth-Guided Regularization", "categories": ["cs.CV"], "comment": "5 pages, 2 figures, accepted by IEEE Signal Processing Letters", "summary": "RGB-D scene parsing methods effectively capture both semantic and geometric\nfeatures of the environment, demonstrating great potential under challenging\nconditions such as extreme weather and low lighting. However, existing RGB-D\nscene parsing methods predominantly rely on supervised training strategies,\nwhich require a large amount of manually annotated pixel-level labels that are\nboth time-consuming and costly. To overcome these limitations, we introduce\nDepthMatch, a semi-supervised learning framework that is specifically designed\nfor RGB-D scene parsing. To make full use of unlabeled data, we propose\ncomplementary patch mix-up augmentation to explore the latent relationships\nbetween texture and spatial features in RGB-D image pairs. We also design a\nlightweight spatial prior injector to replace traditional complex fusion\nmodules, improving the efficiency of heterogeneous feature fusion. Furthermore,\nwe introduce depth-guided boundary loss to enhance the model's boundary\nprediction capabilities. Experimental results demonstrate that DepthMatch\nexhibits high applicability in both indoor and outdoor scenes, achieving\nstate-of-the-art results on the NYUv2 dataset and ranking first on the KITTI\nSemantics benchmark."}
{"id": "2505.20049", "pdf": "https://arxiv.org/pdf/2505.20049", "abs": "https://arxiv.org/abs/2505.20049", "authors": ["Hongsong Wang", "Ao Sun", "Jie Gui", "Liang Wang"], "title": "Data-Free Class-Incremental Gesture Recognition with Prototype-Guided Pseudo Feature Replay", "categories": ["cs.CV"], "comment": "Code is on https://github.com/sunao-101/PGPFR-3/", "summary": "Gesture recognition is an important research area in the field of computer\nvision. Most gesture recognition efforts focus on close-set scenarios, thereby\nlimiting the capacity to effectively handle unseen or novel gestures. We aim to\naddress class-incremental gesture recognition, which entails the ability to\naccommodate new and previously unseen gestures over time. Specifically, we\nintroduce a Prototype-Guided Pseudo Feature Replay (PGPFR) framework for\ndata-free class-incremental gesture recognition. This framework comprises four\ncomponents: Pseudo Feature Generation with Batch Prototypes (PFGBP),\nVariational Prototype Replay (VPR) for old classes, Truncated Cross-Entropy\n(TCE) for new classes, and Continual Classifier Re-Training (CCRT). To tackle\nthe issue of catastrophic forgetting, the PFGBP dynamically generates a\ndiversity of pseudo features in an online manner, leveraging class prototypes\nof old classes along with batch class prototypes of new classes. Furthermore,\nthe VPR enforces consistency between the classifier's weights and the\nprototypes of old classes, leveraging class prototypes and covariance matrices\nto enhance robustness and generalization capabilities. The TCE mitigates the\nimpact of domain differences of the classifier caused by pseudo features.\nFinally, the CCRT training strategy is designed to prevent overfitting to new\nclasses and ensure the stability of features extracted from old classes.\nExtensive experiments conducted on two widely used gesture recognition\ndatasets, namely SHREC 2017 3D and EgoGesture 3D, demonstrate that our approach\noutperforms existing state-of-the-art methods by 11.8\\% and 12.8\\% in terms of\nmean global accuracy, respectively. The code is available on\nhttps://github.com/sunao-101/PGPFR-3/."}
{"id": "2505.20053", "pdf": "https://arxiv.org/pdf/2505.20053", "abs": "https://arxiv.org/abs/2505.20053", "authors": ["Zheqi Lv", "Junhao Chen", "Qi Tian", "Keting Yin", "Shengyu Zhang", "Fei Wu"], "title": "Multimodal LLM-Guided Semantic Correction in Text-to-Image Diffusion", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "comment": null, "summary": "Diffusion models have become the mainstream architecture for text-to-image\ngeneration, achieving remarkable progress in visual quality and prompt\ncontrollability. However, current inference pipelines generally lack\ninterpretable semantic supervision and correction mechanisms throughout the\ndenoising process. Most existing approaches rely solely on post-hoc scoring of\nthe final image, prompt filtering, or heuristic resampling strategies-making\nthem ineffective in providing actionable guidance for correcting the generative\ntrajectory. As a result, models often suffer from object confusion, spatial\nerrors, inaccurate counts, and missing semantic elements, severely compromising\nprompt-image alignment and image quality. To tackle these challenges, we\npropose MLLM Semantic-Corrected Ping-Pong-Ahead Diffusion (PPAD), a novel\nframework that, for the first time, introduces a Multimodal Large Language\nModel (MLLM) as a semantic observer during inference. PPAD performs real-time\nanalysis on intermediate generations, identifies latent semantic\ninconsistencies, and translates feedback into controllable signals that\nactively guide the remaining denoising steps. The framework supports both\ninference-only and training-enhanced settings, and performs semantic correction\nat only extremely few diffusion steps, offering strong generality and\nscalability. Extensive experiments demonstrate PPAD's significant improvements."}
{"id": "2505.20056", "pdf": "https://arxiv.org/pdf/2505.20056", "abs": "https://arxiv.org/abs/2505.20056", "authors": ["Hongsong Wang", "Yin Zhu", "Qiuxia Lai", "Yang Zhang", "Guo-Sen Xie", "Xin Geng"], "title": "PAMD: Plausibility-Aware Motion Diffusion Model for Long Dance Generation", "categories": ["cs.CV"], "comment": "This project page is available at:\n  https://mucunzhuzhu.github.io/PAMD-page/", "summary": "Computational dance generation is crucial in many areas, such as art,\nhuman-computer interaction, virtual reality, and digital entertainment,\nparticularly for generating coherent and expressive long dance sequences.\nDiffusion-based music-to-dance generation has made significant progress, yet\nexisting methods still struggle to produce physically plausible motions. To\naddress this, we propose Plausibility-Aware Motion Diffusion (PAMD), a\nframework for generating dances that are both musically aligned and physically\nrealistic. The core of PAMD lies in the Plausible Motion Constraint (PMC),\nwhich leverages Neural Distance Fields (NDFs) to model the actual pose manifold\nand guide generated motions toward a physically valid pose manifold. To provide\nmore effective guidance during generation, we incorporate Prior Motion Guidance\n(PMG), which uses standing poses as auxiliary conditions alongside music\nfeatures. To further enhance realism for complex movements, we introduce the\nMotion Refinement with Foot-ground Contact (MRFC) module, which addresses\nfoot-skating artifacts by bridging the gap between the optimization objective\nin linear joint position space and the data representation in nonlinear\nrotation space. Extensive experiments show that PAMD significantly improves\nmusical alignment and enhances the physical plausibility of generated motions.\nThis project page is available at: https://mucunzhuzhu.github.io/PAMD-page/."}
{"id": "2505.20058", "pdf": "https://arxiv.org/pdf/2505.20058", "abs": "https://arxiv.org/abs/2505.20058", "authors": ["Yihong Lin", "Xianjia Wu", "Xilai Wang", "Jianqiao Hu", "Songju Lei", "Xiandong Li", "Wenxiong Kang"], "title": "M3DHMR: Monocular 3D Hand Mesh Recovery", "categories": ["cs.CV"], "comment": "9 pages, 5 figures", "summary": "Monocular 3D hand mesh recovery is challenging due to high degrees of freedom\nof hands, 2D-to-3D ambiguity and self-occlusion. Most existing methods are\neither inefficient or less straightforward for predicting the position of 3D\nmesh vertices. Thus, we propose a new pipeline called Monocular 3D Hand Mesh\nRecovery (M3DHMR) to directly estimate the positions of hand mesh vertices.\nM3DHMR provides 2D cues for 3D tasks from a single image and uses a new spiral\ndecoder consist of several Dynamic Spiral Convolution (DSC) Layers and a Region\nof Interest (ROI) Layer. On the one hand, DSC Layers adaptively adjust the\nweights based on the vertex positions and extract the vertex features in both\nspatial and channel dimensions. On the other hand, ROI Layer utilizes the\nphysical information and refines mesh vertices in each predefined hand region\nseparately. Extensive experiments on popular dataset FreiHAND demonstrate that\nM3DHMR significantly outperforms state-of-the-art real-time methods."}
{"id": "2505.20100", "pdf": "https://arxiv.org/pdf/2505.20100", "abs": "https://arxiv.org/abs/2505.20100", "authors": ["Fengyuan Sun", "Leqi Shen", "Hui Chen", "Sicheng Zhao", "Jungong Han", "Guiguang Ding"], "title": "AdaTP: Attention-Debiased Token Pruning for Video Large Language Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Video Large Language Models (Video LLMs) have achieved remarkable results in\nvideo understanding tasks. However, they often suffer from heavy computational\noverhead due to the large number of visual tokens generated from multiple video\nframes. Existing visual token compression methods often rely on attention\nscores from language models as guidance. However, these scores exhibit inherent\nbiases: global bias reflects a tendency to focus on the two ends of the visual\ntoken sequence, while local bias leads to an over-concentration on the same\nspatial positions across different frames. To address the issue of attention\nbias, we propose $\\textbf{A}$ttention-$\\textbf{D}$ebi$\\textbf{a}$sed\n$\\textbf{T}$oken $\\textbf{P}$runing for Video Large Language Models\n($\\textbf{AdaTP}$), a novel token pruning pipeline for Video LLMs. AdaTP\nintegrates two dedicated debiasing modules into the pipeline, targeting global\nattention bias and local attention bias, respectively. Without the need for\nadditional training, our method significantly reduces the computational\noverhead of Video LLMs while retaining the performance of vanilla models.\nExtensive evaluation shows that AdaTP achieves state-of-the-art performance in\nvarious commonly used video understanding benchmarks. In particular, on\nLLaVA-OneVision-7B, AdaTP maintains performance without degradation while using\nonly up to $27.3\\%$ FLOPs compared to the vanilla model. Our code will be\nreleased soon."}
{"id": "2505.20106", "pdf": "https://arxiv.org/pdf/2505.20106", "abs": "https://arxiv.org/abs/2505.20106", "authors": ["Zuyao Chen", "Jinlin Wu", "Zhen Lei", "Chang Wen Chen"], "title": "From Data to Modeling: Fully Open-vocabulary Scene Graph Generation", "categories": ["cs.CV"], "comment": null, "summary": "We present OvSGTR, a novel transformer-based framework for fully\nopen-vocabulary scene graph generation that overcomes the limitations of\ntraditional closed-set models. Conventional methods restrict both object and\nrelationship recognition to a fixed vocabulary, hindering their applicability\nto real-world scenarios where novel concepts frequently emerge. In contrast,\nour approach jointly predicts objects (nodes) and their inter-relationships\n(edges) beyond predefined categories. OvSGTR leverages a DETR-like architecture\nfeaturing a frozen image backbone and text encoder to extract high-quality\nvisual and semantic features, which are then fused via a transformer decoder\nfor end-to-end scene graph prediction. To enrich the model's understanding of\ncomplex visual relations, we propose a relation-aware pre-training strategy\nthat synthesizes scene graph annotations in a weakly supervised manner.\nSpecifically, we investigate three pipelines--scene parser-based, LLM-based,\nand multimodal LLM-based--to generate transferable supervision signals with\nminimal manual annotation. Furthermore, we address the common issue of\ncatastrophic forgetting in open-vocabulary settings by incorporating a\nvisual-concept retention mechanism coupled with a knowledge distillation\nstrategy, ensuring that the model retains rich semantic cues during\nfine-tuning. Extensive experiments on the VG150 benchmark demonstrate that\nOvSGTR achieves state-of-the-art performance across multiple settings,\nincluding closed-set, open-vocabulary object detection-based, relation-based,\nand fully open-vocabulary scenarios. Our results highlight the promise of\nlarge-scale relation-aware pre-training and transformer architectures for\nadvancing scene graph generation towards more generalized and reliable visual\nunderstanding."}
{"id": "2505.20122", "pdf": "https://arxiv.org/pdf/2505.20122", "abs": "https://arxiv.org/abs/2505.20122", "authors": ["Anh Thai", "Stefan Stojanov", "Zixuan Huang", "Bikram Boote", "James M. Rehg"], "title": "MEBench: A Novel Benchmark for Understanding Mutual Exclusivity Bias in Vision-Language Models", "categories": ["cs.CV"], "comment": null, "summary": "This paper introduces MEBench, a novel benchmark for evaluating mutual\nexclusivity (ME) bias, a cognitive phenomenon observed in children during word\nlearning. Unlike traditional ME tasks, MEBench further incorporates spatial\nreasoning to create more challenging and realistic evaluation settings. We\nassess the performance of state-of-the-art vision-language models (VLMs) on\nthis benchmark using novel evaluation metrics that capture key aspects of\nME-based reasoning. To facilitate controlled experimentation, we also present a\nflexible and scalable data generation pipeline that supports the construction\nof diverse annotated scenes."}
{"id": "2505.20124", "pdf": "https://arxiv.org/pdf/2505.20124", "abs": "https://arxiv.org/abs/2505.20124", "authors": ["Fanheng Kong", "Jingyuan Zhang", "Hongzhi Zhang", "Shi Feng", "Daling Wang", "Linhao Yu", "Xingguang Ji", "Yu Tian", "Qi Wang", "Fuzheng Zhang"], "title": "TUNA: Comprehensive Fine-grained Temporal Understanding Evaluation on Dense Dynamic Videos", "categories": ["cs.CV", "cs.DB", "cs.MM"], "comment": "Accepted to CVPR 2025 Main. Project page:\n  https://friedrichor.github.io/projects/TUNA", "summary": "Videos are unique in their integration of temporal elements, including\ncamera, scene, action, and attribute, along with their dynamic relationships\nover time. However, existing benchmarks for video understanding often treat\nthese properties separately or narrowly focus on specific aspects, overlooking\nthe holistic nature of video content. To address this, we introduce TUNA, a\ntemporal-oriented benchmark for fine-grained understanding on dense dynamic\nvideos, with two complementary tasks: captioning and QA. Our TUNA features\ndiverse video scenarios and dynamics, assisted by interpretable and robust\nevaluation criteria. We evaluate several leading models on our benchmark,\nproviding fine-grained performance assessments across various dimensions. This\nevaluation reveals key challenges in video temporal understanding, such as\nlimited action description, inadequate multi-subject understanding, and\ninsensitivity to camera motion, offering valuable insights for improving video\nunderstanding models. The data and code are available at\nhttps://friedrichor.github.io/projects/TUNA."}
{"id": "2505.20126", "pdf": "https://arxiv.org/pdf/2505.20126", "abs": "https://arxiv.org/abs/2505.20126", "authors": ["Shintaro Ito", "Natsuki Takama", "Toshiki Watanabe", "Koichi Ito", "Hwann-Tzong Chen", "Takafumi Aoki"], "title": "OB3D: A New Dataset for Benchmarking Omnidirectional 3D Reconstruction Using Blender", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in radiance field rendering, exemplified by Neural\nRadiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), have significantly\nprogressed 3D modeling and reconstruction. The use of multiple 360-degree\nomnidirectional images for these tasks is increasingly favored due to\nadvantages in data acquisition and comprehensive scene capture. However, the\ninherent geometric distortions in common omnidirectional representations, such\nas equirectangular projection (particularly severe in polar regions and varying\nwith latitude), pose substantial challenges to achieving high-fidelity 3D\nreconstructions. Current datasets, while valuable, often lack the specific\nfocus, scene composition, and ground truth granularity required to\nsystematically benchmark and drive progress in overcoming these\nomnidirectional-specific challenges. To address this critical gap, we introduce\nOmnidirectional Blender 3D (OB3D), a new synthetic dataset curated for\nadvancing 3D reconstruction from multiple omnidirectional images. OB3D features\ndiverse and complex 3D scenes generated from Blender 3D projects, with a\ndeliberate emphasis on challenging scenarios. The dataset provides\ncomprehensive ground truth, including omnidirectional RGB images, precise\nomnidirectional camera parameters, and pixel-aligned equirectangular maps for\ndepth and normals, alongside evaluation metrics. By offering a controlled yet\nchallenging environment, OB3Daims to facilitate the rigorous evaluation of\nexisting methods and prompt the development of new techniques to enhance the\naccuracy and reliability of 3D reconstruction from omnidirectional images."}
{"id": "2505.20129", "pdf": "https://arxiv.org/pdf/2505.20129", "abs": "https://arxiv.org/abs/2505.20129", "authors": ["Xinhang Liu", "Yu-Wing Tai", "Chi-Keung Tang"], "title": "Agentic 3D Scene Generation with Spatially Contextualized VLMs", "categories": ["cs.CV", "cs.GR"], "comment": null, "summary": "Despite recent advances in multimodal content generation enabled by\nvision-language models (VLMs), their ability to reason about and generate\nstructured 3D scenes remains largely underexplored. This limitation constrains\ntheir utility in spatially grounded tasks such as embodied AI, immersive\nsimulations, and interactive 3D applications. We introduce a new paradigm that\nenables VLMs to generate, understand, and edit complex 3D environments by\ninjecting a continually evolving spatial context. Constructed from multimodal\ninput, this context consists of three components: a scene portrait that\nprovides a high-level semantic blueprint, a semantically labeled point cloud\ncapturing object-level geometry, and a scene hypergraph that encodes rich\nspatial relationships, including unary, binary, and higher-order constraints.\nTogether, these components provide the VLM with a structured, geometry-aware\nworking memory that integrates its inherent multimodal reasoning capabilities\nwith structured 3D understanding for effective spatial reasoning. Building on\nthis foundation, we develop an agentic 3D scene generation pipeline in which\nthe VLM iteratively reads from and updates the spatial context. The pipeline\nfeatures high-quality asset generation with geometric restoration, environment\nsetup with automatic verification, and ergonomic adjustment guided by the scene\nhypergraph. Experiments show that our framework can handle diverse and\nchallenging inputs, achieving a level of generalization not observed in prior\nwork. Further results demonstrate that injecting spatial context enables VLMs\nto perform downstream tasks such as interactive scene editing and path\nplanning, suggesting strong potential for spatially intelligent systems in\ncomputer graphics, 3D vision, and embodied applications."}
{"id": "2505.20147", "pdf": "https://arxiv.org/pdf/2505.20147", "abs": "https://arxiv.org/abs/2505.20147", "authors": ["Jin Wang", "Yao Lai", "Aoxue Li", "Shifeng Zhang", "Jiacheng Sun", "Ning Kang", "Chengyue Wu", "Zhenguo Li", "Ping Luo"], "title": "FUDOKI: Discrete Flow-based Unified Understanding and Generation via Kinetic-Optimal Velocities", "categories": ["cs.CV"], "comment": "37 pages, 12 figures", "summary": "The rapid progress of large language models (LLMs) has catalyzed the\nemergence of multimodal large language models (MLLMs) that unify visual\nunderstanding and image generation within a single framework. However, most\nexisting MLLMs rely on autoregressive (AR) architectures, which impose inherent\nlimitations on future development, such as the raster-scan order in image\ngeneration and restricted reasoning abilities in causal context modeling. In\nthis work, we challenge the dominance of AR-based approaches by introducing\nFUDOKI, a unified multimodal model purely based on discrete flow matching, as\nan alternative to conventional AR paradigms. By leveraging metric-induced\nprobability paths with kinetic optimal velocities, our framework goes beyond\nthe previous masking-based corruption process, enabling iterative refinement\nwith self-correction capability and richer bidirectional context integration\nduring generation. To mitigate the high cost of training from scratch, we\ninitialize FUDOKI from pre-trained AR-based MLLMs and adaptively transition to\nthe discrete flow matching paradigm. Experimental results show that FUDOKI\nachieves performance comparable to state-of-the-art AR-based MLLMs across both\nvisual understanding and image generation tasks, highlighting its potential as\na foundation for next-generation unified multimodal models. Furthermore, we\nshow that applying test-time scaling techniques to FUDOKI yields significant\nperformance gains, further underscoring its promise for future enhancement\nthrough reinforcement learning."}
{"id": "2505.20152", "pdf": "https://arxiv.org/pdf/2505.20152", "abs": "https://arxiv.org/abs/2505.20152", "authors": ["Kai Sun", "Yushi Bai", "Zhen Yang", "Jiajie Zhang", "Ji Qi", "Lei Hou", "Juanzi Li"], "title": "Hard Negative Contrastive Learning for Fine-Grained Geometric Understanding in Large Multimodal Models", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Benefiting from contrastively trained visual encoders on large-scale natural\nscene images, Large Multimodal Models (LMMs) have achieved remarkable\nperformance across various visual perception tasks. However, the inherent\nlimitations of contrastive learning upon summarized descriptions fundamentally\nrestrict the capabilities of models in meticulous reasoning, particularly in\ncrucial scenarios of geometric problem-solving. To enhance geometric\nunderstanding, we propose a novel hard negative contrastive learning framework\nfor the vision encoder, which combines image-based contrastive learning using\ngeneration-based hard negatives created by perturbing diagram generation code,\nand text-based contrastive learning using rule-based negatives derived from\nmodified geometric descriptions and retrieval-based negatives selected based on\ncaption similarity. We train CLIP using our strong negative learning method,\nnamely MMCLIP (Multimodal Math CLIP), and subsequently train an LMM for\ngeometric problem-solving. Experiments show that our trained model, MMGeoLM,\nsignificantly outperforms other open-source models on three geometric reasoning\nbenchmarks. Even with a size of 7B, it can rival powerful closed-source models\nlike GPT-4o. We further study the impact of different negative sample\nconstruction methods and the number of negative samples on the geometric\nreasoning performance of LMM, yielding fruitful conclusions. The code and\ndataset are available at https://github.com/THU-KEG/MMGeoLM."}
{"id": "2505.20156", "pdf": "https://arxiv.org/pdf/2505.20156", "abs": "https://arxiv.org/abs/2505.20156", "authors": ["Yi Chen", "Sen Liang", "Zixiang Zhou", "Ziyao Huang", "Yifeng Ma", "Junshu Tang", "Qin Lin", "Yuan Zhou", "Qinglin Lu"], "title": "HunyuanVideo-Avatar: High-Fidelity Audio-Driven Human Animation for Multiple Characters", "categories": ["cs.CV"], "comment": null, "summary": "Recent years have witnessed significant progress in audio-driven human\nanimation. However, critical challenges remain in (i) generating highly dynamic\nvideos while preserving character consistency, (ii) achieving precise emotion\nalignment between characters and audio, and (iii) enabling multi-character\naudio-driven animation. To address these challenges, we propose\nHunyuanVideo-Avatar, a multimodal diffusion transformer (MM-DiT)-based model\ncapable of simultaneously generating dynamic, emotion-controllable, and\nmulti-character dialogue videos. Concretely, HunyuanVideo-Avatar introduces\nthree key innovations: (i) A character image injection module is designed to\nreplace the conventional addition-based character conditioning scheme,\neliminating the inherent condition mismatch between training and inference.\nThis ensures the dynamic motion and strong character consistency; (ii) An Audio\nEmotion Module (AEM) is introduced to extract and transfer the emotional cues\nfrom an emotion reference image to the target generated video, enabling\nfine-grained and accurate emotion style control; (iii) A Face-Aware Audio\nAdapter (FAA) is proposed to isolate the audio-driven character with\nlatent-level face mask, enabling independent audio injection via\ncross-attention for multi-character scenarios. These innovations empower\nHunyuanVideo-Avatar to surpass state-of-the-art methods on benchmark datasets\nand a newly proposed wild dataset, generating realistic avatars in dynamic,\nimmersive scenarios."}
{"id": "2505.20171", "pdf": "https://arxiv.org/pdf/2505.20171", "abs": "https://arxiv.org/abs/2505.20171", "authors": ["Ryan Po", "Yotam Nitzan", "Richard Zhang", "Berlin Chen", "Tri Dao", "Eli Shechtman", "Gordon Wetzstein", "Xun Huang"], "title": "Long-Context State-Space Video World Models", "categories": ["cs.CV"], "comment": "Project website: https://ryanpo.com/ssm_wm", "summary": "Video diffusion models have recently shown promise for world modeling through\nautoregressive frame prediction conditioned on actions. However, they struggle\nto maintain long-term memory due to the high computational cost associated with\nprocessing extended sequences in attention layers. To overcome this limitation,\nwe propose a novel architecture leveraging state-space models (SSMs) to extend\ntemporal memory without compromising computational efficiency. Unlike previous\napproaches that retrofit SSMs for non-causal vision tasks, our method fully\nexploits the inherent advantages of SSMs in causal sequence modeling. Central\nto our design is a block-wise SSM scanning scheme, which strategically trades\noff spatial consistency for extended temporal memory, combined with dense local\nattention to ensure coherence between consecutive frames. We evaluate the\nlong-term memory capabilities of our model through spatial retrieval and\nreasoning tasks over extended horizons. Experiments on Memory Maze and\nMinecraft datasets demonstrate that our approach surpasses baselines in\npreserving long-range memory, while maintaining practical inference speeds\nsuitable for interactive applications."}
{"id": "2505.20202", "pdf": "https://arxiv.org/pdf/2505.20202", "abs": "https://arxiv.org/abs/2505.20202", "authors": ["Jiabo Ma", "Yingxue Xu", "Fengtao Zhou", "Yihui Wang", "Cheng Jin", "Zhengrui Guo", "Jianfeng Wu", "On Ki Tang", "Huajun Zhou", "Xi Wang", "Luyang Luo", "Zhengyu Zhang", "Du Cai", "Zizhao Gao", "Wei Wang", "Yueping Liu", "Jiankun He", "Jing Cui", "Zhenhui Li", "Jing Zhang", "Feng Gao", "Xiuming Zhang", "Li Liang", "Ronald Cheong Kin Chan", "Zhe Wang", "Hao Chen"], "title": "PathBench: A comprehensive comparison benchmark for pathology foundation models towards precision oncology", "categories": ["cs.CV"], "comment": "35 pages, 9 figures", "summary": "The emergence of pathology foundation models has revolutionized computational\nhistopathology, enabling highly accurate, generalized whole-slide image\nanalysis for improved cancer diagnosis, and prognosis assessment. While these\nmodels show remarkable potential across cancer diagnostics and prognostics,\ntheir clinical translation faces critical challenges including variability in\noptimal model across cancer types, potential data leakage in evaluation, and\nlack of standardized benchmarks. Without rigorous, unbiased evaluation, even\nthe most advanced PFMs risk remaining confined to research settings, delaying\ntheir life-saving applications. Existing benchmarking efforts remain limited by\nnarrow cancer-type focus, potential pretraining data overlaps, or incomplete\ntask coverage. We present PathBench, the first comprehensive benchmark\naddressing these gaps through: multi-center in-hourse datasets spanning common\ncancers with rigorous leakage prevention, evaluation across the full clinical\nspectrum from diagnosis to prognosis, and an automated leaderboard system for\ncontinuous model assessment. Our framework incorporates large-scale data,\nenabling objective comparison of PFMs while reflecting real-world clinical\ncomplexity. All evaluation data comes from private medical providers, with\nstrict exclusion of any pretraining usage to avoid data leakage risks. We have\ncollected 15,888 WSIs from 8,549 patients across 10 hospitals, encompassing\nover 64 diagnosis and prognosis tasks. Currently, our evaluation of 19 PFMs\nshows that Virchow2 and H-Optimus-1 are the most effective models overall. This\nwork provides researchers with a robust platform for model development and\noffers clinicians actionable insights into PFM performance across diverse\nclinical scenarios, ultimately accelerating the translation of these\ntransformative technologies into routine pathology practice."}
{"id": "2505.20236", "pdf": "https://arxiv.org/pdf/2505.20236", "abs": "https://arxiv.org/abs/2505.20236", "authors": ["Weihao Xuan", "Qingcheng Zeng", "Heli Qi", "Junjue Wang", "Naoto Yokoya"], "title": "Seeing is Believing, but How Much? A Comprehensive Analysis of Verbalized Calibration in Vision-Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Uncertainty quantification is essential for assessing the reliability and\ntrustworthiness of modern AI systems. Among existing approaches, verbalized\nuncertainty, where models express their confidence through natural language,\nhas emerged as a lightweight and interpretable solution in large language\nmodels (LLMs). However, its effectiveness in vision-language models (VLMs)\nremains insufficiently studied. In this work, we conduct a comprehensive\nevaluation of verbalized confidence in VLMs, spanning three model categories,\nfour task domains, and three evaluation scenarios. Our results show that\ncurrent VLMs often display notable miscalibration across diverse tasks and\nsettings. Notably, visual reasoning models (i.e., thinking with images)\nconsistently exhibit better calibration, suggesting that modality-specific\nreasoning is critical for reliable uncertainty estimation. To further address\ncalibration challenges, we introduce Visual Confidence-Aware Prompting, a\ntwo-stage prompting strategy that improves confidence alignment in multimodal\nsettings. Overall, our study highlights the inherent miscalibration in VLMs\nacross modalities. More broadly, our findings underscore the fundamental\nimportance of modality alignment and model faithfulness in advancing reliable\nmultimodal systems."}
{"id": "2505.20255", "pdf": "https://arxiv.org/pdf/2505.20255", "abs": "https://arxiv.org/abs/2505.20255", "authors": ["Muyao Niu", "Mingdeng Cao", "Yifan Zhan", "Qingtian Zhu", "Mingze Ma", "Jiancheng Zhao", "Yanhong Zeng", "Zhihang Zhong", "Xiao Sun", "Yinqiang Zheng"], "title": "AniCrafter: Customizing Realistic Human-Centric Animation via Avatar-Background Conditioning in Video Diffusion Models", "categories": ["cs.CV"], "comment": "Github: https://github.com/MyNiuuu/AniCrafter", "summary": "Recent advances in video diffusion models have significantly improved\ncharacter animation techniques. However, current approaches rely on basic\nstructural conditions such as DWPose or SMPL-X to animate character images,\nlimiting their effectiveness in open-domain scenarios with dynamic backgrounds\nor challenging human poses. In this paper, we introduce $\\textbf{AniCrafter}$,\na diffusion-based human-centric animation model that can seamlessly integrate\nand animate a given character into open-domain dynamic backgrounds while\nfollowing given human motion sequences. Built on cutting-edge Image-to-Video\n(I2V) diffusion architectures, our model incorporates an innovative\n\"avatar-background\" conditioning mechanism that reframes open-domain\nhuman-centric animation as a restoration task, enabling more stable and\nversatile animation outputs. Experimental results demonstrate the superior\nperformance of our method. Codes will be available at\nhttps://github.com/MyNiuuu/AniCrafter."}
{"id": "2505.20256", "pdf": "https://arxiv.org/pdf/2505.20256", "abs": "https://arxiv.org/abs/2505.20256", "authors": ["Hao Zhong", "Muzhi Zhu", "Zongze Du", "Zheng Huang", "Canyu Zhao", "Mingyu Liu", "Wen Wang", "Hao Chen", "Chunhua Shen"], "title": "Omni-R1: Reinforcement Learning for Omnimodal Reasoning via Two-System Collaboration", "categories": ["cs.CV"], "comment": "Project page: https://aim-uofa.github.io/OmniR1", "summary": "Long-horizon video-audio reasoning and fine-grained pixel understanding\nimpose conflicting requirements on omnimodal models: dense temporal coverage\ndemands many low-resolution frames, whereas precise grounding calls for\nhigh-resolution inputs. We tackle this trade-off with a two-system\narchitecture: a Global Reasoning System selects informative keyframes and\nrewrites the task at low spatial cost, while a Detail Understanding System\nperforms pixel-level grounding on the selected high-resolution snippets.\nBecause ``optimal'' keyframe selection and reformulation are ambiguous and hard\nto supervise, we formulate them as a reinforcement learning (RL) problem and\npresent Omni-R1, an end-to-end RL framework built on Group Relative Policy\nOptimization. Omni-R1 trains the Global Reasoning System through hierarchical\nrewards obtained via online collaboration with the Detail Understanding System,\nrequiring only one epoch of RL on small task splits.\n  Experiments on two challenging benchmarks, namely Referring Audio-Visual\nSegmentation (RefAVS) and Reasoning Video Object Segmentation (REVOS), show\nthat Omni-R1 not only surpasses strong supervised baselines but also\noutperforms specialized state-of-the-art models, while substantially improving\nout-of-domain generalization and mitigating multimodal hallucination. Our\nresults demonstrate the first successful application of RL to large-scale\nomnimodal reasoning and highlight a scalable path toward universally foundation\nmodels."}
{"id": "2505.20267", "pdf": "https://arxiv.org/pdf/2505.20267", "abs": "https://arxiv.org/abs/2505.20267", "authors": ["Changjian Jiang", "Kerui Ren", "Linning Xu", "Jiong Chen", "Jiangmiao Pang", "Yu Zhang", "Bo Dai", "Mulin Yu"], "title": "HaloGS: Loose Coupling of Compact Geometry and Gaussian Splats for 3D Scenes", "categories": ["cs.CV"], "comment": null, "summary": "High fidelity 3D reconstruction and rendering hinge on capturing precise\ngeometry while preserving photo realistic detail. Most existing methods either\nfuse these goals into a single cumbersome model or adopt hybrid schemes whose\nuniform primitives lead to a trade off between efficiency and fidelity. In this\npaper, we introduce HaloGS, a dual representation that loosely couples coarse\ntriangles for geometry with Gaussian primitives for appearance, motivated by\nthe lightweight classic geometry representations and their proven efficiency in\nreal world applications. Our design yields a compact yet expressive model\ncapable of photo realistic rendering across both indoor and outdoor\nenvironments, seamlessly adapting to varying levels of scene complexity.\nExperiments on multiple benchmark datasets demonstrate that our method yields\nboth compact, accurate geometry and high fidelity renderings, especially in\nchallenging scenarios where robust geometric structure make a clear difference."}
{"id": "2505.20270", "pdf": "https://arxiv.org/pdf/2505.20270", "abs": "https://arxiv.org/abs/2505.20270", "authors": ["Jinsheng Quan", "Chunshi Wang", "Yawei Luo"], "title": "ParticleGS: Particle-Based Dynamics Modeling of 3D Gaussians for Prior-free Motion Extrapolation", "categories": ["cs.CV"], "comment": null, "summary": "This paper aims to model the dynamics of 3D Gaussians from visual\nobservations to support temporal extrapolation. Existing dynamic 3D\nreconstruction methods often struggle to effectively learn underlying dynamics\nor rely heavily on manually defined physical priors, which limits their\nextrapolation capabilities. To address this issue, we propose a novel dynamic\n3D Gaussian Splatting prior-free motion extrapolation framework based on\nparticle dynamics systems. The core advantage of our method lies in its ability\nto learn differential equations that describe the dynamics of 3D Gaussians, and\nfollow them during future frame extrapolation. Instead of simply fitting to the\nobserved visual frame sequence, we aim to more effectively model the gaussian\nparticle dynamics system. To this end, we introduce a dynamics latent state\nvector into the standard Gaussian kernel and design a dynamics latent space\nencoder to extract initial state. Subsequently, we introduce a Neural\nODEs-based dynamics module that models the temporal evolution of Gaussian in\ndynamics latent space. Finally, a Gaussian kernel space decoder is used to\ndecode latent state at the specific time step into the deformation.\nExperimental results demonstrate that the proposed method achieves comparable\nrendering quality with existing approaches in reconstruction tasks, and\nsignificantly outperforms them in future frame extrapolation. Our code is\navailable at https://github.com/QuanJinSheng/ParticleGS."}
{"id": "2505.20271", "pdf": "https://arxiv.org/pdf/2505.20271", "abs": "https://arxiv.org/abs/2505.20271", "authors": ["Yu Xu", "Fan Tang", "You Wu", "Lin Gao", "Oliver Deussen", "Hongbin Yan", "Jintao Li", "Juan Cao", "Tong-Yee Lee"], "title": "In-Context Brush: Zero-shot Customized Subject Insertion with Context-Aware Latent Space Manipulation", "categories": ["cs.CV", "cs.AI", "cs.GR"], "comment": null, "summary": "Recent advances in diffusion models have enhanced multimodal-guided visual\ngeneration, enabling customized subject insertion that seamlessly \"brushes\"\nuser-specified objects into a given image guided by textual prompts. However,\nexisting methods often struggle to insert customized subjects with high\nfidelity and align results with the user's intent through textual prompts. In\nthis work, we propose \"In-Context Brush\", a zero-shot framework for customized\nsubject insertion by reformulating the task within the paradigm of in-context\nlearning. Without loss of generality, we formulate the object image and the\ntextual prompts as cross-modal demonstrations, and the target image with the\nmasked region as the query. The goal is to inpaint the target image with the\nsubject aligning textual prompts without model tuning. Building upon a\npretrained MMDiT-based inpainting network, we perform test-time enhancement via\ndual-level latent space manipulation: intra-head \"latent feature shifting\"\nwithin each attention head that dynamically shifts attention outputs to reflect\nthe desired subject semantics and inter-head \"attention reweighting\" across\ndifferent heads that amplifies prompt controllability through differential\nattention prioritization. Extensive experiments and applications demonstrate\nthat our approach achieves superior identity preservation, text alignment, and\nimage quality compared to existing state-of-the-art methods, without requiring\ndedicated training or additional data collection."}
{"id": "2505.20272", "pdf": "https://arxiv.org/pdf/2505.20272", "abs": "https://arxiv.org/abs/2505.20272", "authors": ["Meng Cao", "Haoze Zhao", "Can Zhang", "Xiaojun Chang", "Ian Reid", "Xiaodan Liang"], "title": "Ground-R1: Incentivizing Grounded Visual Reasoning via Reinforcement Learning", "categories": ["cs.CV"], "comment": null, "summary": "Large Vision-Language Models (LVLMs) have demonstrated impressive general\ncapabilities across a wide range of multi-modal tasks. However, the reasoning\nprocesses of LVLMs often suffer from unreliable outputs and limited\ninterpretability. To address this, grounded visual reasoning has emerged as a\npromising paradigm that enforces responses anchored on salient visual evidence\nregions. However, existing approaches typically rely on costly supervision such\nas bounding box annotations, chain-of-thought rationale or external tool calls,\nlimiting their scalability. In this work, we propose Ground-R1, a reinforcement\nlearning framework that enables grounded visual reasoning without requiring\nexplicit evidence or rationale annotations. Ground-R1 consists of a grounding\nphase that generates evidence region rollouts based on format constraints, and\nan answering phase that produces responses guided by both answer correctness\nand format adherence rewards. Extensive experiments across multiple visual\nreasoning benchmarks manifest that Ground-R1 achieves superior performance and\nexhibits emergent cognitive behaviors such as uncertainty awareness, spatial\nperception, and iterative refinement, offering a scalable and interpretable\nalternative to existing approaches."}
{"id": "2505.20275", "pdf": "https://arxiv.org/pdf/2505.20275", "abs": "https://arxiv.org/abs/2505.20275", "authors": ["Yang Ye", "Xianyi He", "Zongjian Li", "Bin Lin", "Shenghai Yuan", "Zhiyuan Yan", "Bohan Hou", "Li Yuan"], "title": "ImgEdit: A Unified Image Editing Dataset and Benchmark", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in generative models have enabled high-fidelity\ntext-to-image generation. However, open-source image-editing models still lag\nbehind their proprietary counterparts, primarily due to limited high-quality\ndata and insufficient benchmarks. To overcome these limitations, we introduce\nImgEdit, a large-scale, high-quality image-editing dataset comprising 1.2\nmillion carefully curated edit pairs, which contain both novel and complex\nsingle-turn edits, as well as challenging multi-turn tasks. To ensure the data\nquality, we employ a multi-stage pipeline that integrates a cutting-edge\nvision-language model, a detection model, a segmentation model, alongside\ntask-specific in-painting procedures and strict post-processing. ImgEdit\nsurpasses existing datasets in both task novelty and data quality. Using\nImgEdit, we train ImgEdit-E1, an editing model using Vision Language Model to\nprocess the reference image and editing prompt, which outperforms existing\nopen-source models on multiple tasks, highlighting the value of ImgEdit and\nmodel design. For comprehensive evaluation, we introduce ImgEdit-Bench, a\nbenchmark designed to evaluate image editing performance in terms of\ninstruction adherence, editing quality, and detail preservation. It includes a\nbasic testsuite, a challenging single-turn suite, and a dedicated multi-turn\nsuite. We evaluate both open-source and proprietary models, as well as\nImgEdit-E1, providing deep analysis and actionable insights into the current\nbehavior of image-editing models. The source data are publicly available on\nhttps://github.com/PKU-YuanGroup/ImgEdit."}
{"id": "2505.20279", "pdf": "https://arxiv.org/pdf/2505.20279", "abs": "https://arxiv.org/abs/2505.20279", "authors": ["Zhiwen Fan", "Jian Zhang", "Renjie Li", "Junge Zhang", "Runjin Chen", "Hezhen Hu", "Kevin Wang", "Huaizhi Qu", "Dilin Wang", "Zhicheng Yan", "Hongyu Xu", "Justin Theiss", "Tianlong Chen", "Jiachen Li", "Zhengzhong Tu", "Zhangyang Wang", "Rakesh Ranjan"], "title": "VLM-3R: Vision-Language Models Augmented with Instruction-Aligned 3D Reconstruction", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "The rapid advancement of Large Multimodal Models (LMMs) for 2D images and\nvideos has motivated extending these models to understand 3D scenes, aiming for\nhuman-like visual-spatial intelligence. Nevertheless, achieving deep spatial\nunderstanding comparable to human capabilities poses significant challenges in\nmodel encoding and data acquisition. Existing methods frequently depend on\nexternal depth sensors for geometry capture or utilize off-the-shelf algorithms\nfor pre-constructing 3D maps, thereby limiting their scalability, especially\nwith prevalent monocular video inputs and for time-sensitive applications. In\nthis work, we introduce VLM-3R, a unified framework for Vision-Language Models\n(VLMs) that incorporates 3D Reconstructive instruction tuning. VLM-3R processes\nmonocular video frames by employing a geometry encoder to derive implicit 3D\ntokens that represent spatial understanding. Leveraging our Spatial-Visual-View\nFusion and over 200K curated 3D reconstructive instruction tuning\nquestion-answer (QA) pairs, VLM-3R effectively aligns real-world spatial\ncontext with language instructions. This enables monocular 3D spatial\nassistance and embodied reasoning. To facilitate the evaluation of temporal\nreasoning, we introduce the Vision-Spatial-Temporal Intelligence benchmark,\nfeaturing over 138.6K QA pairs across five distinct tasks focused on evolving\nspatial relationships. Extensive experiments demonstrate that our model,\nVLM-3R, not only facilitates robust visual-spatial reasoning but also enables\nthe understanding of temporal 3D context changes, excelling in both accuracy\nand scalability."}
{"id": "2505.20283", "pdf": "https://arxiv.org/pdf/2505.20283", "abs": "https://arxiv.org/abs/2505.20283", "authors": ["Guangzhao He", "Chen Geng", "Shangzhe Wu", "Jiajun Wu"], "title": "Category-Agnostic Neural Object Rigging", "categories": ["cs.CV", "I.2.10"], "comment": "Accepted to CVPR 2025. Project Page: https://guangzhaohe.com/canor", "summary": "The motion of deformable 4D objects lies in a low-dimensional manifold. To\nbetter capture the low dimensionality and enable better controllability,\ntraditional methods have devised several heuristic-based methods, i.e.,\nrigging, for manipulating dynamic objects in an intuitive fashion. However,\nsuch representations are not scalable due to the need for expert knowledge of\nspecific categories. Instead, we study the automatic exploration of such\nlow-dimensional structures in a purely data-driven manner. Specifically, we\ndesign a novel representation that encodes deformable 4D objects into a sparse\nset of spatially grounded blobs and an instance-aware feature volume to\ndisentangle the pose and instance information of the 3D shape. With such a\nrepresentation, we can manipulate the pose of 3D objects intuitively by\nmodifying the parameters of the blobs, while preserving rich instance-specific\ninformation. We evaluate the proposed method on a variety of object categories\nand demonstrate the effectiveness of the proposed framework. Project page:\nhttps://guangzhaohe.com/canor"}
{"id": "2505.20287", "pdf": "https://arxiv.org/pdf/2505.20287", "abs": "https://arxiv.org/abs/2505.20287", "authors": ["Zhongwei Zhang", "Fuchen Long", "Zhaofan Qiu", "Yingwei Pan", "Wu Liu", "Ting Yao", "Tao Mei"], "title": "MotionPro: A Precise Motion Controller for Image-to-Video Generation", "categories": ["cs.CV", "cs.MM"], "comment": "CVPR 2025. Project page: https://zhw-zhang.github.io/MotionPro-page/", "summary": "Animating images with interactive motion control has garnered popularity for\nimage-to-video (I2V) generation. Modern approaches typically rely on large\nGaussian kernels to extend motion trajectories as condition without explicitly\ndefining movement region, leading to coarse motion control and failing to\ndisentangle object and camera moving. To alleviate these, we present MotionPro,\na precise motion controller that novelly leverages region-wise trajectory and\nmotion mask to regulate fine-grained motion synthesis and identify target\nmotion category (i.e., object or camera moving), respectively. Technically,\nMotionPro first estimates the flow maps on each training video via a tracking\nmodel, and then samples the region-wise trajectories to simulate inference\nscenario. Instead of extending flow through large Gaussian kernels, our\nregion-wise trajectory approach enables more precise control by directly\nutilizing trajectories within local regions, thereby effectively characterizing\nfine-grained movements. A motion mask is simultaneously derived from the\npredicted flow maps to capture the holistic motion dynamics of the movement\nregions. To pursue natural motion control, MotionPro further strengthens video\ndenoising by incorporating both region-wise trajectories and motion mask\nthrough feature modulation. More remarkably, we meticulously construct a\nbenchmark, i.e., MC-Bench, with 1.1K user-annotated image-trajectory pairs, for\nthe evaluation of both fine-grained and object-level I2V motion control.\nExtensive experiments conducted on WebVid-10M and MC-Bench demonstrate the\neffectiveness of MotionPro. Please refer to our project page for more results:\nhttps://zhw-zhang.github.io/MotionPro-page/."}
{"id": "2505.20288", "pdf": "https://arxiv.org/pdf/2505.20288", "abs": "https://arxiv.org/abs/2505.20288", "authors": ["Guangting Zheng", "Yehao Li", "Yingwei Pan", "Jiajun Deng", "Ting Yao", "Yanyong Zhang", "Tao Mei"], "title": "Hierarchical Masked Autoregressive Models with Low-Resolution Token Pivots", "categories": ["cs.CV", "cs.MM"], "comment": "ICML 2025. Source code is available at\n  https://github.com/HiDream-ai/himar", "summary": "Autoregressive models have emerged as a powerful generative paradigm for\nvisual generation. The current de-facto standard of next token prediction\ncommonly operates over a single-scale sequence of dense image tokens, and is\nincapable of utilizing global context especially for early tokens prediction.\nIn this paper, we introduce a new autoregressive design to model a hierarchy\nfrom a few low-resolution image tokens to the typical dense image tokens, and\ndelve into a thorough hierarchical dependency across multi-scale image tokens.\nTechnically, we present a Hierarchical Masked Autoregressive models (Hi-MAR)\nthat pivot on low-resolution image tokens to trigger hierarchical\nautoregressive modeling in a multi-phase manner. Hi-MAR learns to predict a few\nimage tokens in low resolution, functioning as intermediary pivots to reflect\nglobal structure, in the first phase. Such pivots act as the additional\nguidance to strengthen the next autoregressive modeling phase by shaping global\nstructural awareness of typical dense image tokens. A new Diffusion Transformer\nhead is further devised to amplify the global context among all tokens for mask\ntoken prediction. Extensive evaluations on both class-conditional and\ntext-to-image generation tasks demonstrate that Hi-MAR outperforms typical AR\nbaselines, while requiring fewer computational costs. Code is available at\nhttps://github.com/HiDream-ai/himar."}
{"id": "2505.20289", "pdf": "https://arxiv.org/pdf/2505.20289", "abs": "https://arxiv.org/abs/2505.20289", "authors": ["Zeyi Huang", "Yuyang Ji", "Anirudh Sundara Rajan", "Zefan Cai", "Wen Xiao", "Junjie Hu", "Yong Jae Lee"], "title": "VisualToolAgent (VisTA): A Reinforcement Learning Framework for Visual Tool Selection", "categories": ["cs.CV"], "comment": null, "summary": "We introduce VisTA, a new reinforcement learning framework that empowers\nvisual agents to dynamically explore, select, and combine tools from a diverse\nlibrary based on empirical performance. Existing methods for tool-augmented\nreasoning either rely on training-free prompting or large-scale fine-tuning;\nboth lack active tool exploration and typically assume limited tool diversity,\nand fine-tuning methods additionally demand extensive human supervision. In\ncontrast, VisTA leverages end-to-end reinforcement learning to iteratively\nrefine sophisticated, query-specific tool selection strategies, using task\noutcomes as feedback signals. Through Group Relative Policy Optimization\n(GRPO), our framework enables an agent to autonomously discover effective\ntool-selection pathways without requiring explicit reasoning supervision.\nExperiments on the ChartQA, Geometry3K, and BlindTest benchmarks demonstrate\nthat VisTA achieves substantial performance gains over training-free baselines,\nespecially on out-of-distribution examples. These results highlight VisTA's\nability to enhance generalization, adaptively utilize diverse tools, and pave\nthe way for flexible, experience-driven visual reasoning systems."}
{"id": "2505.20291", "pdf": "https://arxiv.org/pdf/2505.20291", "abs": "https://arxiv.org/abs/2505.20291", "authors": ["Di Wu", "Yixin Wan", "Kai-Wei Chang"], "title": "Visualized Text-to-Image Retrieval", "categories": ["cs.CV", "cs.CL"], "comment": "Work in Progress", "summary": "We propose Visualize-then-Retrieve (VisRet), a new paradigm for Text-to-Image\n(T2I) retrieval that mitigates the limitations of cross-modal similarity\nalignment of existing multi-modal embeddings. VisRet first projects textual\nqueries into the image modality via T2I generation. Then, it performs retrieval\nwithin the image modality to bypass the weaknesses of cross-modal retrievers in\nrecognizing subtle visual-spatial features. Experiments on three\nknowledge-intensive T2I retrieval benchmarks, including a newly introduced\nmulti-entity benchmark, demonstrate that VisRet consistently improves T2I\nretrieval by 24.5% to 32.7% NDCG@10 across different embedding models. VisRet\nalso significantly benefits downstream visual question answering accuracy when\nused in retrieval-augmented generation pipelines. The method is plug-and-play\nand compatible with off-the-shelf retrievers, making it an effective module for\nknowledge-intensive multi-modal systems. Our code and the new benchmark are\npublicly available at https://github.com/xiaowu0162/Visualize-then-Retrieve."}
{"id": "2505.20292", "pdf": "https://arxiv.org/pdf/2505.20292", "abs": "https://arxiv.org/abs/2505.20292", "authors": ["Shenghai Yuan", "Xianyi He", "Yufan Deng", "Yang Ye", "Jinfa Huang", "Bin Lin", "Chongyang Ma", "Jiebo Luo", "Li Yuan"], "title": "OpenS2V-Nexus: A Detailed Benchmark and Million-Scale Dataset for Subject-to-Video Generation", "categories": ["cs.CV", "cs.AI"], "comment": "Code and Dataset: https://github.com/PKU-YuanGroup/OpenS2V-Nexus", "summary": "Subject-to-Video (S2V) generation aims to create videos that faithfully\nincorporate reference content, providing enhanced flexibility in the production\nof videos. To establish the infrastructure for S2V generation, we propose\nOpenS2V-Nexus, consisting of (i) OpenS2V-Eval, a fine-grained benchmark, and\n(ii) OpenS2V-5M, a million-scale dataset. In contrast to existing S2V\nbenchmarks inherited from VBench that focus on global and coarse-grained\nassessment of generated videos, OpenS2V-Eval focuses on the model's ability to\ngenerate subject-consistent videos with natural subject appearance and identity\nfidelity. For these purposes, OpenS2V-Eval introduces 180 prompts from seven\nmajor categories of S2V, which incorporate both real and synthetic test data.\nFurthermore, to accurately align human preferences with S2V benchmarks, we\npropose three automatic metrics, NexusScore, NaturalScore and GmeScore, to\nseparately quantify subject consistency, naturalness, and text relevance in\ngenerated videos. Building on this, we conduct a comprehensive evaluation of 16\nrepresentative S2V models, highlighting their strengths and weaknesses across\ndifferent content. Moreover, we create the first open-source large-scale S2V\ngeneration dataset OpenS2V-5M, which consists of five million high-quality 720P\nsubject-text-video triples. Specifically, we ensure subject-information\ndiversity in our dataset by (1) segmenting subjects and building pairing\ninformation via cross-video associations and (2) prompting GPT-Image-1 on raw\nframes to synthesize multi-view representations. Through OpenS2V-Nexus, we\ndeliver a robust infrastructure to accelerate future S2V generation research."}
{"id": "2505.20294", "pdf": "https://arxiv.org/pdf/2505.20294", "abs": "https://arxiv.org/abs/2505.20294", "authors": ["Xiao Chen", "Tai Wang", "Quanyi Li", "Tao Huang", "Jiangmiao Pang", "Tianfan Xue"], "title": "GLEAM: Learning Generalizable Exploration Policy for Active Mapping in Complex 3D Indoor Scenes", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "Project page: https://xiao-chen.tech/gleam/", "summary": "Generalizable active mapping in complex unknown environments remains a\ncritical challenge for mobile robots. Existing methods, constrained by\ninsufficient training data and conservative exploration strategies, exhibit\nlimited generalizability across scenes with diverse layouts and complex\nconnectivity. To enable scalable training and reliable evaluation, we introduce\nGLEAM-Bench, the first large-scale benchmark designed for generalizable active\nmapping with 1,152 diverse 3D scenes from synthetic and real-scan datasets.\nBuilding upon this foundation, we propose GLEAM, a unified generalizable\nexploration policy for active mapping. Its superior generalizability comes\nmainly from our semantic representations, long-term navigable goals, and\nrandomized strategies. It significantly outperforms state-of-the-art methods,\nachieving 66.50% coverage (+9.49%) with efficient trajectories and improved\nmapping accuracy on 128 unseen complex scenes. Project page:\nhttps://xiao-chen.tech/gleam/."}
{"id": "2505.20297", "pdf": "https://arxiv.org/pdf/2505.20297", "abs": "https://arxiv.org/abs/2505.20297", "authors": ["Qinyu Zhao", "Jaskirat Singh", "Ming Xu", "Akshay Asthana", "Stephen Gould", "Liang Zheng"], "title": "DiSA: Diffusion Step Annealing in Autoregressive Image Generation", "categories": ["cs.CV", "cs.CL"], "comment": "Our code is available at https://github.com/Qinyu-Allen-Zhao/DiSA", "summary": "An increasing number of autoregressive models, such as MAR, FlowAR, xAR, and\nHarmon adopt diffusion sampling to improve the quality of image generation.\nHowever, this strategy leads to low inference efficiency, because it usually\ntakes 50 to 100 steps for diffusion to sample a token. This paper explores how\nto effectively address this issue. Our key motivation is that as more tokens\nare generated during the autoregressive process, subsequent tokens follow more\nconstrained distributions and are easier to sample. To intuitively explain, if\na model has generated part of a dog, the remaining tokens must complete the dog\nand thus are more constrained. Empirical evidence supports our motivation: at\nlater generation stages, the next tokens can be well predicted by a multilayer\nperceptron, exhibit low variance, and follow closer-to-straight-line denoising\npaths from noise to tokens. Based on our finding, we introduce diffusion step\nannealing (DiSA), a training-free method which gradually uses fewer diffusion\nsteps as more tokens are generated, e.g., using 50 steps at the beginning and\ngradually decreasing to 5 steps at later stages. Because DiSA is derived from\nour finding specific to diffusion in autoregressive models, it is complementary\nto existing acceleration methods designed for diffusion alone. DiSA can be\nimplemented in only a few lines of code on existing models, and albeit simple,\nachieves $5-10\\times$ faster inference for MAR and Harmon and $1.4-2.5\\times$\nfor FlowAR and xAR, while maintaining the generation quality."}
{"id": "2505.18175", "pdf": "https://arxiv.org/pdf/2505.18175", "abs": "https://arxiv.org/abs/2505.18175", "authors": ["Natia Kukhilava", "Tatia Tsmindashvili", "Rapael Kalandadze", "Anchit Gupta", "Sofio Katamadze", "François Brémond", "Laura M. Ferrari", "Philipp Müller", "Benedikt Emanuel Wirth"], "title": "Evaluation in EEG Emotion Recognition: State-of-the-Art Review and Unified Framework", "categories": ["eess.SP", "cs.AI", "cs.CV", "cs.HC", "cs.LG"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Electroencephalography-based Emotion Recognition (EEG-ER) has become a\ngrowing research area in recent years. Analyzing 216 papers published between\n2018 and 2023, we uncover that the field lacks a unified evaluation protocol,\nwhich is essential to fairly define the state of the art, compare new\napproaches and to track the field's progress. We report the main\ninconsistencies between the used evaluation protocols, which are related to\nground truth definition, evaluation metric selection, data splitting types\n(e.g., subject-dependent or subject-independent) and the use of different\ndatasets. Capitalizing on this state-of-the-art research, we propose a unified\nevaluation protocol, EEGain (https://github.com/EmotionLab/EEGain), which\nenables an easy and efficient evaluation of new methods and datasets. EEGain is\na novel open source software framework, offering the capability to compare -\nand thus define - state-of-the-art results. EEGain includes standardized\nmethods for data pre-processing, data splitting, evaluation metrics, and the\nability to load the six most relevant datasets (i.e., AMIGOS, DEAP, DREAMER,\nMAHNOB-HCI, SEED, SEED-IV) in EEG-ER with only a single line of code. In\naddition, we have assessed and validated EEGain using these six datasets on the\nfour most common publicly available methods (EEGNet, DeepConvNet,\nShallowConvNet, TSception). This is a significant step to make research on\nEEG-ER more reproducible and comparable, thereby accelerating the overall\nprogress of the field."}
{"id": "2505.18184", "pdf": "https://arxiv.org/pdf/2505.18184", "abs": "https://arxiv.org/abs/2505.18184", "authors": ["Hania Ghouse", "Juveria Tanveen", "Abdul Muqtadir Ahmed", "Uma N. Dulhare"], "title": "AI- Enhanced Stethoscope in Remote Diagnostics for Cardiopulmonary Diseases", "categories": ["eess.SP", "cs.CV"], "comment": null, "summary": "The increase in cardiac and pulmonary diseases presents an alarming and\npervasive health challenge on a global scale responsible for unexpected and\npremature mortalities. In spite of how serious these conditions are, existing\nmethods of detection and treatment encounter challenges, particularly in\nachieving timely diagnosis for effective medical intervention. Manual screening\nprocesses commonly used for primary detection of cardiac and respiratory\nproblems face inherent limitations, increased by a scarcity of skilled medical\npractitioners in remote or under-resourced areas. To address this, our study\nintroduces an innovative yet efficient model which integrates AI for diagnosing\nlung and heart conditions concurrently using the auscultation sounds. Unlike\nthe already high-priced digital stethoscope, our proposed model has been\nparticularly designed to deploy on low-cost embedded devices and thus ensure\napplicability in under-developed regions that actually face an issue of\naccessing medical care. Our proposed model incorporates MFCC feature extraction\nand engineering techniques to ensure that the signal is well analyzed for\naccurate diagnostics through the hybrid model combining Gated Recurrent Unit\nwith CNN in processing audio signals recorded from the low-cost stethoscope.\nBeyond its diagnostic capabilities, the model generates digital audio records\nthat facilitate in classifying six pulmonary and five cardiovascular diseases.\nHence, the integration of a cost effective stethoscope with an efficient AI\nempowered model deployed on a web app providing real-time analysis, represents\na transformative step towards standardized healthcare"}
{"id": "2505.18194", "pdf": "https://arxiv.org/pdf/2505.18194", "abs": "https://arxiv.org/abs/2505.18194", "authors": ["Yubo Peng", "Luping Xiang", "Bingxin Zhang", "Kun Yang"], "title": "Large Language Model-Driven Distributed Integrated Multimodal Sensing and Semantic Communications", "categories": ["eess.SP", "cs.AI", "cs.CV"], "comment": null, "summary": "Traditional single-modal sensing systems-based solely on either radio\nfrequency (RF) or visual data-struggle to cope with the demands of complex and\ndynamic environments. Furthermore, single-device systems are constrained by\nlimited perspectives and insufficient spatial coverage, which impairs their\neffectiveness in urban or non-line-of-sight scenarios. To overcome these\nchallenges, we propose a novel large language model (LLM)-driven distributed\nintegrated multimodal sensing and semantic communication (LLM-DiSAC) framework.\nSpecifically, our system consists of multiple collaborative sensing devices\nequipped with RF and camera modules, working together with an aggregation\ncenter to enhance sensing accuracy. First, on sensing devices, LLM-DiSAC\ndevelops an RF-vision fusion network (RVFN), which employs specialized feature\nextractors for RF and visual data, followed by a cross-attention module for\neffective multimodal integration. Second, a LLM-based semantic transmission\nnetwork (LSTN) is proposed to enhance communication efficiency, where the\nLLM-based decoder leverages known channel parameters, such as transceiver\ndistance and signal-to-noise ratio (SNR), to mitigate semantic distortion.\nThird, at the aggregation center, a transformer-based aggregation model (TRAM)\nwith an adaptive aggregation attention mechanism is developed to fuse\ndistributed features and enhance sensing accuracy. To preserve data privacy, a\ntwo-stage distributed learning strategy is introduced, allowing local model\ntraining at the device level and centralized aggregation model training using\nintermediate features. Finally, evaluations on a synthetic multi-view RF-visual\ndataset generated by the Genesis simulation engine show that LLM-DiSAC achieves\na good performance."}
{"id": "2505.18365", "pdf": "https://arxiv.org/pdf/2505.18365", "abs": "https://arxiv.org/abs/2505.18365", "authors": ["Zhangxing Bian", "Shuwen Wei", "Xiao Liang", "Yuan-Chiao Lu", "Samuel W. Remedios", "Fangxu Xing", "Jonghye Woo", "Dzung L. Pham", "Aaron Carass", "Philip V. Bayly", "Jiachen Zhuo", "Ahmed Alshareef", "Jerry L. Prince"], "title": "Brightness-Invariant Tracking Estimation in Tagged MRI", "categories": ["eess.IV", "cs.CV"], "comment": "Accepted by IPMI 2025", "summary": "Magnetic resonance (MR) tagging is an imaging technique for noninvasively\ntracking tissue motion in vivo by creating a visible pattern of magnetization\nsaturation (tags) that deforms with the tissue. Due to longitudinal relaxation\nand progression to steady-state, the tags and tissue brightnesses change over\ntime, which makes tracking with optical flow methods error-prone. Although\nFourier methods can alleviate these problems, they are also sensitive to\nbrightness changes as well as spectral spreading due to motion. To address\nthese problems, we introduce the brightness-invariant tracking estimation\n(BRITE) technique for tagged MRI. BRITE disentangles the anatomy from the tag\npattern in the observed tagged image sequence and simultaneously estimates the\nLagrangian motion. The inherent ill-posedness of this problem is addressed by\nleveraging the expressive power of denoising diffusion probabilistic models to\nrepresent the probabilistic distribution of the underlying anatomy and the\nflexibility of physics-informed neural networks to estimate\nbiologically-plausible motion. A set of tagged MR images of a gel phantom was\nacquired with various tag periods and imaging flip angles to demonstrate the\nimpact of brightness variations and to validate our method. The results show\nthat BRITE achieves more accurate motion and strain estimates as compared to\nother state of the art methods, while also being resistant to tag fading."}
{"id": "2505.18413", "pdf": "https://arxiv.org/pdf/2505.18413", "abs": "https://arxiv.org/abs/2505.18413", "authors": ["Toshiaki Koike-Akino", "Xiangyu Chen", "Jing Liu", "Ye Wang", "Pu", "Wang", "Matthew Brand"], "title": "LatentLLM: Attention-Aware Joint Tensor Compression", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": "37 pages, 16 figures", "summary": "Modern foundation models such as large language models (LLMs) and large\nmulti-modal models (LMMs) require a massive amount of computational and memory\nresources. We propose a new framework to convert such LLMs/LMMs into a\nreduced-dimension latent structure. Our method extends a local activation-aware\ntensor decomposition to a global attention-aware joint tensor de-composition.\nOur framework can significantly improve the model accuracy over the existing\nmodel compression methods when reducing the latent dimension to realize\ncomputationally/memory-efficient LLMs/LLMs. We show the benefit on several\nbenchmark including multi-modal reasoning tasks."}
{"id": "2505.18424", "pdf": "https://arxiv.org/pdf/2505.18424", "abs": "https://arxiv.org/abs/2505.18424", "authors": ["Tianyi Ren", "Juampablo E. Heras Rivera", "Hitender Oswal", "Yutong Pan", "William Henry", "Jacob Ruzevick", "Mehmet Kurt"], "title": "How We Won the ISLES'24 Challenge by Preprocessing", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Stroke is among the top three causes of death worldwide, and accurate\nidentification of stroke lesion boundaries is critical for diagnosis and\ntreatment. Supervised deep learning methods have emerged as the leading\nsolution for stroke lesion segmentation but require large, diverse, and\nannotated datasets. The ISLES'24 challenge addresses this need by providing\nlongitudinal stroke imaging data, including CT scans taken on arrival to the\nhospital and follow-up MRI taken 2-9 days from initial arrival, with\nannotations derived from follow-up MRI. Importantly, models submitted to the\nISLES'24 challenge are evaluated using only CT inputs, requiring prediction of\nlesion progression that may not be visible in CT scans for segmentation. Our\nwinning solution shows that a carefully designed preprocessing pipeline\nincluding deep-learning-based skull stripping and custom intensity windowing is\nbeneficial for accurate segmentation. Combined with a standard large residual\nnnU-Net architecture for segmentation, this approach achieves a mean test Dice\nof 28.5 with a standard deviation of 21.27."}
{"id": "2505.18487", "pdf": "https://arxiv.org/pdf/2505.18487", "abs": "https://arxiv.org/abs/2505.18487", "authors": ["Junlin Wang", "Zhiyun Lin"], "title": "Grounding Bodily Awareness in Visual Representations for Efficient Policy Learning", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": "A preprint version", "summary": "Learning effective visual representations for robotic manipulation remains a\nfundamental challenge due to the complex body dynamics involved in action\nexecution. In this paper, we study how visual representations that carry\nbody-relevant cues can enable efficient policy learning for downstream robotic\nmanipulation tasks. We present $\\textbf{I}$nter-token $\\textbf{Con}$trast\n($\\textbf{ICon}$), a contrastive learning method applied to the token-level\nrepresentations of Vision Transformers (ViTs). ICon enforces a separation in\nthe feature space between agent-specific and environment-specific tokens,\nresulting in agent-centric visual representations that embed body-specific\ninductive biases. This framework can be seamlessly integrated into end-to-end\npolicy learning by incorporating the contrastive loss as an auxiliary\nobjective. Our experiments show that ICon not only improves policy performance\nacross various manipulation tasks but also facilitates policy transfer across\ndifferent robots. The project website: https://github.com/HenryWJL/icon"}
{"id": "2505.18531", "pdf": "https://arxiv.org/pdf/2505.18531", "abs": "https://arxiv.org/abs/2505.18531", "authors": ["Jiayi Zhou", "Jiaming Ji", "Boyuan Chen", "Jiapeng Sun", "Wenqi Chen", "Donghai Hong", "Sirui Han", "Yike Guo", "Yaodong Yang"], "title": "Generative RLHF-V: Learning Principles from Multi-modal Human Preference", "categories": ["cs.AI", "cs.CV"], "comment": "9 pages, 8 figures", "summary": "Training multi-modal large language models (MLLMs) that align with human\nintentions is a long-term challenge. Traditional score-only reward models for\nalignment suffer from low accuracy, weak generalization, and poor\ninterpretability, blocking the progress of alignment methods, e.g.,\nreinforcement learning from human feedback (RLHF). Generative reward models\n(GRMs) leverage MLLMs' intrinsic reasoning capabilities to discriminate\npair-wise responses, but their pair-wise paradigm makes it hard to generalize\nto learnable rewards. We introduce Generative RLHF-V, a novel alignment\nframework that integrates GRMs with multi-modal RLHF. We propose a two-stage\npipeline: $\\textbf{multi-modal generative reward modeling from RL}$, where RL\nguides GRMs to actively capture human intention, then predict the correct\npair-wise scores; and $\\textbf{RL optimization from grouped comparison}$, which\nenhances multi-modal RL scoring precision by grouped responses comparison.\nExperimental results demonstrate that, besides out-of-distribution\ngeneralization of RM discrimination, our framework improves 4 MLLMs'\nperformance across 7 benchmarks by $18.1\\%$, while the baseline RLHF is only\n$5.3\\%$. We further validate that Generative RLHF-V achieves a near-linear\nimprovement with an increasing number of candidate responses. Our code and\nmodels can be found at https://generative-rlhf-v.github.io."}
{"id": "2505.18536", "pdf": "https://arxiv.org/pdf/2505.18536", "abs": "https://arxiv.org/abs/2505.18536", "authors": ["Haoyuan Sun", "Jiaqi Wu", "Bo Xia", "Yifu Luo", "Yifei Zhao", "Kai Qin", "Xufei Lv", "Tiantian Zhang", "Yongzhe Chang", "Xueqian Wang"], "title": "Reinforcement Fine-Tuning Powers Reasoning Capability of Multimodal Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Standing in 2025, at a critical juncture in the pursuit of Artificial General\nIntelligence (AGI), reinforcement fine-tuning (RFT) has demonstrated\nsignificant potential in enhancing the reasoning capability of large language\nmodels (LLMs) and has led to the development of cutting-edge AI models such as\nOpenAI-o1 and DeepSeek-R1. Moreover, the efficient application of RFT to\nenhance the reasoning capability of multimodal large language models (MLLMs)\nhas attracted widespread attention from the community. In this position paper,\nwe argue that reinforcement fine-tuning powers the reasoning capability of\nmultimodal large language models. To begin with, we provide a detailed\nintroduction to the fundamental background knowledge that researchers\ninterested in this field should be familiar with. Furthermore, we meticulously\nsummarize the improvements of RFT in powering reasoning capability of MLLMs\ninto five key points: diverse modalities, diverse tasks and domains, better\ntraining algorithms, abundant benchmarks and thriving engineering frameworks.\nFinally, we propose five promising directions for future research that the\ncommunity might consider. We hope that this position paper will provide\nvaluable insights to the community at this pivotal stage in the advancement\ntoward AGI. Summary of works done on RFT for MLLMs is available at\nhttps://github.com/Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs."}
{"id": "2505.18546", "pdf": "https://arxiv.org/pdf/2505.18546", "abs": "https://arxiv.org/abs/2505.18546", "authors": ["Dristi Datta", "Manoranjan Paul", "Manzur Murshed", "Shyh Wei Teng", "Leigh M. Schmidtke"], "title": "ReflectGAN: Modeling Vegetation Effects for Soil Carbon Estimation from Satellite Imagery", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "Soil organic carbon (SOC) is a critical indicator of soil health, but its\naccurate estimation from satellite imagery is hindered in vegetated regions due\nto spectral contamination from plant cover, which obscures soil reflectance and\nreduces model reliability. This study proposes the Reflectance Transformation\nGenerative Adversarial Network (ReflectGAN), a novel paired GAN-based framework\ndesigned to reconstruct accurate bare soil reflectance from vegetated soil\nsatellite observations. By learning the spectral transformation between\nvegetated and bare soil reflectance, ReflectGAN facilitates more precise SOC\nestimation under mixed land cover conditions. Using the LUCAS 2018 dataset and\ncorresponding Landsat 8 imagery, we trained multiple learning-based models on\nboth original and ReflectGAN-reconstructed reflectance inputs. Models trained\non ReflectGAN outputs consistently outperformed those using existing vegetation\ncorrection methods. For example, the best-performing model (RF) achieved an\n$R^2$ of 0.54, RMSE of 3.95, and RPD of 2.07 when applied to the\nReflectGAN-generated signals, representing a 35\\% increase in $R^2$, a 43\\%\nreduction in RMSE, and a 43\\% improvement in RPD compared to the best existing\nmethod (PMM-SU). The performance of the models with ReflectGAN is also better\ncompared to their counterparts when applied to another dataset, i.e.,\nSentinel-2 imagery. These findings demonstrate the potential of ReflectGAN to\nimprove SOC estimation accuracy in vegetated landscapes, supporting more\nreliable soil monitoring."}
{"id": "2505.18547", "pdf": "https://arxiv.org/pdf/2505.18547", "abs": "https://arxiv.org/abs/2505.18547", "authors": ["Min Cheng", "Fatemeh Doudi", "Dileep Kalathil", "Mohammad Ghavamzadeh", "Panganamala R. Kumar"], "title": "Diffusion Blend: Inference-Time Multi-Preference Alignment for Diffusion Models", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "Reinforcement learning (RL) algorithms have been used recently to align\ndiffusion models with downstream objectives such as aesthetic quality and\ntext-image consistency by fine-tuning them to maximize a single reward function\nunder a fixed KL regularization. However, this approach is inherently\nrestrictive in practice, where alignment must balance multiple, often\nconflicting objectives. Moreover, user preferences vary across prompts,\nindividuals, and deployment contexts, with varying tolerances for deviation\nfrom a pre-trained base model. We address the problem of inference-time\nmulti-preference alignment: given a set of basis reward functions and a\nreference KL regularization strength, can we design a fine-tuning procedure so\nthat, at inference time, it can generate images aligned with any user-specified\nlinear combination of rewards and regularization, without requiring additional\nfine-tuning? We propose Diffusion Blend, a novel approach to solve\ninference-time multi-preference alignment by blending backward diffusion\nprocesses associated with fine-tuned models, and we instantiate this approach\nwith two algorithms: DB-MPA for multi-reward alignment and DB-KLA for KL\nregularization control. Extensive experiments show that Diffusion Blend\nalgorithms consistently outperform relevant baselines and closely match or\nexceed the performance of individually fine-tuned models, enabling efficient,\nuser-driven alignment at inference-time. The code is available at\nhttps://github.com/bluewoods127/DB-2025}{github.com/bluewoods127/DB-2025."}
{"id": "2505.18568", "pdf": "https://arxiv.org/pdf/2505.18568", "abs": "https://arxiv.org/abs/2505.18568", "authors": ["Zhikang Chen", "Abudukelimu Wuerkaixi", "Sen Cui", "Haoxuan Li", "Ding Li", "Jingfeng Zhang", "Bo Han", "Gang Niu", "Houfang Liu", "Yi Yang", "Sifan Yang", "Changshui Zhang", "Tianling Ren"], "title": "Learning without Isolation: Pathway Protection for Continual Learning", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "23 pages", "summary": "Deep networks are prone to catastrophic forgetting during sequential task\nlearning, i.e., losing the knowledge about old tasks upon learning new tasks.\nTo this end, continual learning(CL) has emerged, whose existing methods focus\nmostly on regulating or protecting the parameters associated with the previous\ntasks. However, parameter protection is often impractical, since the size of\nparameters for storing the old-task knowledge increases linearly with the\nnumber of tasks, otherwise it is hard to preserve the parameters related to the\nold-task knowledge. In this work, we bring a dual opinion from neuroscience and\nphysics to CL: in the whole networks, the pathways matter more than the\nparameters when concerning the knowledge acquired from the old tasks. Following\nthis opinion, we propose a novel CL framework, learning without isolation(LwI),\nwhere model fusion is formulated as graph matching and the pathways occupied by\nthe old tasks are protected without being isolated. Thanks to the sparsity of\nactivation channels in a deep network, LwI can adaptively allocate available\npathways for a new task, realizing pathway protection and addressing\ncatastrophic forgetting in a parameter-efficient manner. Experiments on popular\nbenchmark datasets demonstrate the superiority of the proposed LwI."}
{"id": "2505.18603", "pdf": "https://arxiv.org/pdf/2505.18603", "abs": "https://arxiv.org/abs/2505.18603", "authors": ["Ye Mo", "Zirui Shao", "Kai Ye", "Xianwei Mao", "Bo Zhang", "Hangdi Xing", "Peng Ye", "Gang Huang", "Kehan Chen", "Zhou Huan", "Zixu Yan", "Sheng Zhou"], "title": "Doc-CoB: Enhancing Multi-Modal Document Understanding with Visual Chain-of-Boxes Reasoning", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "Multimodal large language models (MLLMs) have made significant progress in\ndocument understanding. However, the information-dense nature of document\nimages still poses challenges, as most queries depend on only a few relevant\nregions, with the rest being redundant. Existing one-pass MLLMs process entire\ndocument images without considering query relevance, often failing to focus on\ncritical regions and producing unfaithful responses. Inspired by the human\ncoarse-to-fine reading pattern, we introduce Doc-CoB (Chain-of-Box), a\nsimple-yet-effective mechanism that integrates human-style visual reasoning\ninto MLLM without modifying its architecture. Our method allows the model to\nautonomously select the set of regions (boxes) most relevant to the query, and\nthen focus attention on them for further understanding. We first design a fully\nautomatic pipeline, integrating a commercial MLLM with a layout analyzer, to\ngenerate 249k training samples with intermediate visual reasoning supervision.\nThen we incorporate two enabling tasks that improve box identification and\nbox-query reasoning, which together enhance document understanding. Extensive\nexperiments on seven benchmarks with four popular models show that Doc-CoB\nsignificantly improves performance, demonstrating its effectiveness and wide\napplicability. All code, data, and models will be released publicly."}
{"id": "2505.18625", "pdf": "https://arxiv.org/pdf/2505.18625", "abs": "https://arxiv.org/abs/2505.18625", "authors": ["Shivam Kumar Jha S", "Jaya NN Iyer"], "title": "Tropical Geometry Based Edge Detection Using Min-Plus and Max-Plus Algebra", "categories": ["math.AG", "cs.CV", "14T90, 14-04"], "comment": null, "summary": "This paper proposes a tropical geometry-based edge detection framework that\nreformulates convolution and gradient computations using min-plus and max-plus\nalgebra. The tropical formulation emphasizes dominant intensity variations,\ncontributing to sharper and more continuous edge representations. Three\nvariants are explored: an adaptive threshold-based method, a multi-kernel\nmin-plus method, and a max-plus method emphasizing structural continuity. The\nframework integrates multi-scale processing, Hessian filtering, and wavelet\nshrinkage to enhance edge transitions while maintaining computational\nefficiency. Experiments on MATLAB built-in grayscale and color images suggest\nthat tropical formulations integrated with classical operators, such as Canny\nand LoG, can improve boundary detection in low-contrast and textured regions.\nQuantitative evaluation using standard edge metrics indicates favorable edge\nclarity and structural coherence. These results highlight the potential of\ntropical algebra as a scalable and noise-aware formulation for edge detection\nin practical image analysis tasks."}
{"id": "2505.18664", "pdf": "https://arxiv.org/pdf/2505.18664", "abs": "https://arxiv.org/abs/2505.18664", "authors": ["Evgeny Ugolkov", "Xupeng He", "Hyung Kwak", "Hussein Hoteit"], "title": "Memory-Efficient Super-Resolution of 3D Micro-CT Images Using Octree-Based GANs: Enhancing Resolution and Segmentation Accuracy", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": "31 pages, 15 figures", "summary": "We present a memory-efficient algorithm for significantly enhancing the\nquality of segmented 3D micro-Computed Tomography (micro-CT) images of rocks\nusing a generative model. The proposed model achieves a 16x increase in\nresolution and corrects inaccuracies in segmentation caused by the overlapping\nX-ray attenuation in micro-CT measurements across different minerals. The\ngenerative model employed is a 3D Octree-based convolutional Wasserstein\ngenerative adversarial network with gradient penalty. To address the challenge\nof high memory consumption inherent in standard 3D convolutional layers, we\nimplemented an Octree structure within the 3D progressive growing generator\nmodel. This enabled the use of memory-efficient 3D Octree-based convolutional\nlayers. The approach is pivotal in overcoming the long-standing memory\nbottleneck in volumetric deep learning, making it possible to reach 16x\nsuper-resolution in 3D, a scale that is challenging to attain due to cubic\nmemory scaling. For training, we utilized segmented 3D low-resolution micro-CT\nimages along with unpaired segmented complementary 2D high-resolution laser\nscanning microscope images. Post-training, resolution improved from 7 to 0.44\nmicro-m/voxel with accurate segmentation of constituent minerals. Validated on\nBerea sandstone, this framework demonstrates substantial improvements in pore\ncharacterization and mineral differentiation, offering a robust solution to one\nof the primary computational limitations in modern geoscientific imaging."}
{"id": "2505.18772", "pdf": "https://arxiv.org/pdf/2505.18772", "abs": "https://arxiv.org/abs/2505.18772", "authors": ["Michal Edelstein", "Hsueh-Ti Derek Liu", "Mirela Ben-Chen"], "title": "CageNet: A Meta-Framework for Learning on Wild Meshes", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": "11 pages, 13 figures (excluding supplementary material)", "summary": "Learning on triangle meshes has recently proven to be instrumental to a\nmyriad of tasks, from shape classification, to segmentation, to deformation and\nanimation, to mention just a few. While some of these applications are tackled\nthrough neural network architectures which are tailored to the application at\nhand, many others use generic frameworks for triangle meshes where the only\ncustomization required is the modification of the input features and the loss\nfunction. Our goal in this paper is to broaden the applicability of these\ngeneric frameworks to \"wild\", i.e. meshes in-the-wild which often have multiple\ncomponents, non-manifold elements, disrupted connectivity, or a combination of\nthese. We propose a configurable meta-framework based on the concept of caged\ngeometry: Given a mesh, a cage is a single component manifold triangle mesh\nthat envelopes it closely. Generalized barycentric coordinates map between\nfunctions on the cage, and functions on the mesh, allowing us to learn and test\non a variety of data, in different applications. We demonstrate this concept by\nlearning segmentation and skinning weights on difficult data, achieving better\nperformance to state of the art techniques on wild meshes."}
{"id": "2505.18825", "pdf": "https://arxiv.org/pdf/2505.18825", "abs": "https://arxiv.org/abs/2505.18825", "authors": ["Nicholas M. Boffi", "Michael S. Albergo", "Eric Vanden-Eijnden"], "title": "How to build a consistency model: Learning flow maps via self-distillation", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Building on the framework proposed in Boffi et al. (2024), we present a\nsystematic approach for learning flow maps associated with flow and diffusion\nmodels. Flow map-based models, commonly known as consistency models, encompass\nrecent efforts to improve the efficiency of generative models based on\nsolutions to differential equations. By exploiting a relationship between the\nvelocity field underlying a continuous-time flow and the instantaneous rate of\nchange of the flow map, we show how to convert existing distillation schemes\ninto direct training algorithms via self-distillation, eliminating the need for\npre-trained models. We empirically evaluate several instantiations of our\nframework, finding that high-dimensional tasks like image synthesis benefit\nfrom objective functions that avoid temporal and spatial derivatives of the\nflow map, while lower-dimensional tasks can benefit from objectives\nincorporating higher-order derivatives to capture sharp features."}
{"id": "2505.18842", "pdf": "https://arxiv.org/pdf/2505.18842", "abs": "https://arxiv.org/abs/2505.18842", "authors": ["Jiwan Chung", "Junhyeok Kim", "Siyeol Kim", "Jaeyoung Lee", "Min Soo Kim", "Youngjae Yu"], "title": "Don't Look Only Once: Towards Multimodal Interactive Reasoning with Selective Visual Revisitation", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "We present v1, a lightweight extension to Multimodal Large Language Models\n(MLLMs) that enables selective visual revisitation during inference. While\ncurrent MLLMs typically consume visual input only once and reason purely over\ninternal memory, v1 introduces a simple point-and-copy mechanism that allows\nthe model to dynamically retrieve relevant image regions throughout the\nreasoning process. This mechanism augments existing architectures with minimal\nmodifications, enabling contextual access to visual tokens based on the model's\nevolving hypotheses. To train this capability, we construct v1g, a dataset of\n300K multimodal reasoning traces with interleaved visual grounding annotations.\nExperiments on three multimodal mathematical reasoning benchmarks -- MathVista,\nMathVision, and MathVerse -- demonstrate that v1 consistently improves\nperformance over comparable baselines, particularly on tasks requiring\nfine-grained visual reference and multi-step reasoning. Our results suggest\nthat dynamic visual access is a promising direction for enhancing grounded\nmultimodal reasoning. Code, models, and data will be released to support future\nresearch."}
{"id": "2505.18884", "pdf": "https://arxiv.org/pdf/2505.18884", "abs": "https://arxiv.org/abs/2505.18884", "authors": ["Borna Khodabandeh", "Amirabbas Afzali", "Amirhossein Afsharrad", "Seyed Shahabeddin Mousavi", "Sanjay Lall", "Sajjad Amini", "Seyed-Mohsen Moosavi-Dezfooli"], "title": "LORE: Lagrangian-Optimized Robust Embeddings for Visual Encoders", "categories": ["cs.LG", "cs.AI", "cs.CV", "math.OC"], "comment": null, "summary": "Visual encoders have become fundamental components in modern computer vision\npipelines. However, ensuring robustness against adversarial perturbations\nremains a critical challenge. Recent efforts have explored both supervised and\nunsupervised adversarial fine-tuning strategies. We identify two key\nlimitations in these approaches: (i) they often suffer from instability,\nespecially during the early stages of fine-tuning, resulting in suboptimal\nconvergence and degraded performance on clean data, and (ii) they exhibit a\nsuboptimal trade-off between robustness and clean data accuracy, hindering the\nsimultaneous optimization of both objectives. To overcome these challenges, we\npropose Lagrangian-Optimized Robust Embeddings (LORE), a novel unsupervised\nadversarial fine-tuning framework. LORE utilizes constrained optimization,\nwhich offers a principled approach to balancing competing goals, such as\nimproving robustness while preserving nominal performance. By enforcing\nembedding-space proximity constraints, LORE effectively maintains clean data\nperformance throughout adversarial fine-tuning. Extensive experiments show that\nLORE significantly improves zero-shot adversarial robustness with minimal\ndegradation in clean data accuracy. Furthermore, we demonstrate the\neffectiveness of the adversarially fine-tuned CLIP image encoder in\nout-of-distribution generalization and enhancing the interpretability of image\nembeddings."}
{"id": "2505.18902", "pdf": "https://arxiv.org/pdf/2505.18902", "abs": "https://arxiv.org/abs/2505.18902", "authors": ["Laura Baracaldo", "Blythe King", "Haoran Yan", "Yizi Lin", "Nina Miolane", "Mengyang Gu"], "title": "Unsupervised cell segmentation by fast Gaussian Processes", "categories": ["stat.AP", "cs.CV"], "comment": null, "summary": "Cell boundary information is crucial for analyzing cell behaviors from\ntime-lapse microscopy videos. Existing supervised cell segmentation tools, such\nas ImageJ, require tuning various parameters and rely on restrictive\nassumptions about the shape of the objects. While recent supervised\nsegmentation tools based on convolutional neural networks enhance accuracy,\nthey depend on high-quality labelled images, making them unsuitable for\nsegmenting new types of objects not in the database. We developed a novel\nunsupervised cell segmentation algorithm based on fast Gaussian processes for\nnoisy microscopy images without the need for parameter tuning or restrictive\nassumptions about the shape of the object. We derived robust thresholding\ncriteria adaptive for heterogeneous images containing distinct brightness at\ndifferent parts to separate objects from the background, and employed watershed\nsegmentation to distinguish touching cell objects. Both simulated studies and\nreal-data analysis of large microscopy images demonstrate the scalability and\naccuracy of our approach compared with the alternatives."}
{"id": "2505.18983", "pdf": "https://arxiv.org/pdf/2505.18983", "abs": "https://arxiv.org/abs/2505.18983", "authors": ["Haotian Sun", "Yitong Li", "Yuchen Zhuang", "Niao He", "Hanjun Dai", "Bo Dai"], "title": "AmorLIP: Efficient Language-Image Pretraining via Amortization", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Contrastive Language-Image Pretraining (CLIP) has demonstrated strong\nzero-shot performance across diverse downstream text-image tasks. Existing CLIP\nmethods typically optimize a contrastive objective using negative samples drawn\nfrom each minibatch. To achieve robust representation learning, these methods\nrequire extremely large batch sizes and escalate computational demands to\nhundreds or even thousands of GPUs. Prior approaches to mitigate this issue\noften compromise downstream performance, prolong training duration, or face\nscalability challenges with very large datasets. To overcome these limitations,\nwe propose AmorLIP, an efficient CLIP pretraining framework that amortizes\nexpensive computations involved in contrastive learning through lightweight\nneural networks, which substantially improves training efficiency and\nperformance. Leveraging insights from a spectral factorization of energy-based\nmodels, we introduce novel amortization objectives along with practical\ntechniques to improve training stability. Extensive experiments across 38\ndownstream tasks demonstrate the superior zero-shot classification and\nretrieval capabilities of AmorLIP, consistently outperforming standard CLIP\nbaselines with substantial relative improvements of up to 12.24%."}
{"id": "2505.18985", "pdf": "https://arxiv.org/pdf/2505.18985", "abs": "https://arxiv.org/abs/2505.18985", "authors": ["Tianyu Zhang", "Xinyu Wang", "Zhenghan Tai", "Lu Li", "Jijun Chi", "Jingrui Tian", "Hailin He", "Suyuchen Wang"], "title": "STRICT: Stress Test of Rendering Images Containing Text", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": "13 pages", "summary": "While diffusion models have revolutionized text-to-image generation with\ntheir ability to synthesize realistic and diverse scenes, they continue to\nstruggle to generate consistent and legible text within images. This\nshortcoming is commonly attributed to the locality bias inherent in\ndiffusion-based generation, which limits their ability to model long-range\nspatial dependencies. In this paper, we introduce $\\textbf{STRICT}$, a\nbenchmark designed to systematically stress-test the ability of diffusion\nmodels to render coherent and instruction-aligned text in images. Our benchmark\nevaluates models across multiple dimensions: (1) the maximum length of readable\ntext that can be generated; (2) the correctness and legibility of the generated\ntext, and (3) the ratio of not following instructions for generating text. We\nevaluate several state-of-the-art models, including proprietary and open-source\nvariants, and reveal persistent limitations in long-range consistency and\ninstruction-following capabilities. Our findings provide insights into\narchitectural bottlenecks and motivate future research directions in multimodal\ngenerative modeling. We release our entire evaluation pipeline at\nhttps://github.com/tianyu-z/STRICT-Bench."}
{"id": "2505.19000", "pdf": "https://arxiv.org/pdf/2505.19000", "abs": "https://arxiv.org/abs/2505.19000", "authors": ["Yunxin Li", "Xinyu Chen", "Zitao Li", "Zhenyu Liu", "Longyue Wang", "Wenhan Luo", "Baotian Hu", "Min Zhang"], "title": "VerIPO: Cultivating Long Reasoning in Video-LLMs via Verifier-Gudied Iterative Policy Optimization", "categories": ["cs.CL", "cs.CV"], "comment": "19 pages, 9 figures, Project Link:\n  https://github.com/HITsz-TMG/VerIPO", "summary": "Applying Reinforcement Learning (RL) to Video Large Language Models\n(Video-LLMs) shows significant promise for complex video reasoning. However,\npopular Reinforcement Fine-Tuning (RFT) methods, such as outcome-based Group\nRelative Policy Optimization (GRPO), are limited by data preparation\nbottlenecks (e.g., noise or high cost) and exhibit unstable improvements in the\nquality of long chain-of-thoughts (CoTs) and downstream performance.To address\nthese limitations, we propose VerIPO, a Verifier-guided Iterative Policy\nOptimization method designed to gradually improve video LLMs' capacity for\ngenerating deep, long-term reasoning chains. The core component is\nRollout-Aware Verifier, positioned between the GRPO and Direct Preference\nOptimization (DPO) training phases to form the GRPO-Verifier-DPO training loop.\nThis verifier leverages small LLMs as a judge to assess the reasoning logic of\nrollouts, enabling the construction of high-quality contrastive data, including\nreflective and contextually consistent CoTs. These curated preference samples\ndrive the efficient DPO stage (7x faster than GRPO), leading to marked\nimprovements in reasoning chain quality, especially in terms of length and\ncontextual consistency. This training loop benefits from GRPO's expansive\nsearch and DPO's targeted optimization. Experimental results demonstrate: 1)\nSignificantly faster and more effective optimization compared to standard GRPO\nvariants, yielding superior performance; 2) Our trained models exceed the\ndirect inference of large-scale instruction-tuned Video-LLMs, producing long\nand contextually consistent CoTs on diverse video reasoning tasks; and 3) Our\nmodel with one iteration outperforms powerful LMMs (e.g., Kimi-VL) and long\nreasoning models (e.g., Video-R1), highlighting its effectiveness and\nstability."}
{"id": "2505.19017", "pdf": "https://arxiv.org/pdf/2505.19017", "abs": "https://arxiv.org/abs/2505.19017", "authors": ["Yaxuan Li", "Yichen Zhu", "Junjie Wen", "Chaomin Shen", "Yi Xu"], "title": "WorldEval: World Model as Real-World Robot Policies Evaluator", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": "The project page is available at https://worldeval.github.io", "summary": "The field of robotics has made significant strides toward developing\ngeneralist robot manipulation policies. However, evaluating these policies in\nreal-world scenarios remains time-consuming and challenging, particularly as\nthe number of tasks scales and environmental conditions change. In this work,\nwe demonstrate that world models can serve as a scalable, reproducible, and\nreliable proxy for real-world robot policy evaluation. A key challenge is\ngenerating accurate policy videos from world models that faithfully reflect the\nrobot actions. We observe that directly inputting robot actions or using\nhigh-dimensional encoding methods often fails to generate action-following\nvideos. To address this, we propose Policy2Vec, a simple yet effective approach\nto turn a video generation model into a world simulator that follows latent\naction to generate the robot video. We then introduce WorldEval, an automated\npipeline designed to evaluate real-world robot policies entirely online.\nWorldEval effectively ranks various robot policies and individual checkpoints\nwithin a single policy, and functions as a safety detector to prevent dangerous\nactions by newly developed robot models. Through comprehensive paired\nevaluations of manipulation policies in real-world environments, we demonstrate\na strong correlation between policy performance in WorldEval and real-world\nscenarios. Furthermore, our method significantly outperforms popular methods\nsuch as real-to-sim approach."}
{"id": "2505.19091", "pdf": "https://arxiv.org/pdf/2505.19091", "abs": "https://arxiv.org/abs/2505.19091", "authors": ["Benjamin Clavié", "Florian Brand"], "title": "ReadBench: Measuring the Dense Text Visual Reading Ability of Vision-Language Models", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Recent advancements in Large Vision-Language Models (VLMs), have greatly\nenhanced their capability to jointly process text and images. However, despite\nextensive benchmarks evaluating visual comprehension (e.g., diagrams, color\nschemes, OCR tasks...), there is limited assessment of VLMs' ability to read\nand reason about text-rich images effectively. To fill this gap, we introduce\nReadBench, a multimodal benchmark specifically designed to evaluate the reading\ncomprehension capabilities of VLMs. ReadBench transposes contexts from\nestablished text-only benchmarks into images of text while keeping textual\nprompts and questions intact. Evaluating leading VLMs with ReadBench, we find\nminimal-but-present performance degradation on short, text-image inputs, while\nperformance sharply declines for longer, multi-page contexts. Our experiments\nfurther reveal that text resolution has negligible effects on multimodal\nperformance. These findings highlight needed improvements in VLMs, particularly\ntheir reasoning over visually presented extensive textual content, a capability\ncritical for practical applications. ReadBench is available at\nhttps://github.com/answerdotai/ReadBench ."}
{"id": "2505.19100", "pdf": "https://arxiv.org/pdf/2505.19100", "abs": "https://arxiv.org/abs/2505.19100", "authors": ["Yeyuan Wang", "Dehong Gao", "Rujiao Long", "Lei Yi", "Linbo Jin", "Libin Yang", "Xiaoyan Cai"], "title": "ASPO: Adaptive Sentence-Level Preference Optimization for Fine-Grained Multimodal Reasoning", "categories": ["cs.CL", "cs.CV"], "comment": "Accepted by ACL 2025 findings", "summary": "Direct Preference Optimization (DPO) has gained significant attention for its\nsimplicity and computational efficiency in aligning large language models\n(LLMs). Recent advancements have extended DPO to multimodal scenarios,\nachieving strong performance. However, traditional DPO relies on binary\npreference optimization, rewarding or penalizing entire responses without\nconsidering fine-grained segment correctness, leading to suboptimal solutions.\nThe root of this issue lies in the absence of fine-grained supervision during\nthe optimization process. To address this, we propose Adaptive Sentence-level\nPreference Optimization (ASPO), which evaluates individual sentences for more\nprecise preference optimization. By dynamically calculating adaptive rewards at\nthe sentence level based on model predictions, ASPO enhances response content\nassessment without additional models or parameters. This significantly improves\nthe alignment of multimodal features. Extensive experiments show that ASPO\nsubstantially enhances the overall performance of multimodal models."}
{"id": "2505.19147", "pdf": "https://arxiv.org/pdf/2505.19147", "abs": "https://arxiv.org/abs/2505.19147", "authors": ["Xuyang Liu", "Zichen Wen", "Shaobo Wang", "Junjie Chen", "Zhishan Tao", "Yubo Wang", "Xiangqi Jin", "Chang Zou", "Yiyu Wang", "Chenfei Liao", "Xu Zheng", "Honggang Chen", "Weijia Li", "Xuming Hu", "Conghui He", "Linfeng Zhang"], "title": "Shifting AI Efficiency From Model-Centric to Data-Centric Compression", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "Project:\n  \\url{https://github.com/xuyang-liu16/Awesome-Token-level-Model-Compression}", "summary": "The rapid advancement of large language models (LLMs) and multi-modal LLMs\n(MLLMs) has historically relied on model-centric scaling through increasing\nparameter counts from millions to hundreds of billions to drive performance\ngains. However, as we approach hardware limits on model size, the dominant\ncomputational bottleneck has fundamentally shifted to the quadratic cost of\nself-attention over long token sequences, now driven by ultra-long text\ncontexts, high-resolution images, and extended videos. In this position paper,\n\\textbf{we argue that the focus of research for efficient AI is shifting from\nmodel-centric compression to data-centric compression}. We position token\ncompression as the new frontier, which improves AI efficiency via reducing the\nnumber of tokens during model training or inference. Through comprehensive\nanalysis, we first examine recent developments in long-context AI across\nvarious domains and establish a unified mathematical framework for existing\nmodel efficiency strategies, demonstrating why token compression represents a\ncrucial paradigm shift in addressing long-context overhead. Subsequently, we\nsystematically review the research landscape of token compression, analyzing\nits fundamental benefits and identifying its compelling advantages across\ndiverse scenarios. Furthermore, we provide an in-depth analysis of current\nchallenges in token compression research and outline promising future\ndirections. Ultimately, our work aims to offer a fresh perspective on AI\nefficiency, synthesize existing research, and catalyze innovative developments\nto address the challenges that increasing context lengths pose to the AI\ncommunity's advancement."}
{"id": "2505.19151", "pdf": "https://arxiv.org/pdf/2505.19151", "abs": "https://arxiv.org/abs/2505.19151", "authors": ["Shenggan Cheng", "Yuanxin Wei", "Lansong Diao", "Yong Liu", "Bujiao Chen", "Lianghua Huang", "Yu Liu", "Wenyuan Yu", "Jiangsu Du", "Wei Lin", "Yang You"], "title": "SRDiffusion: Accelerate Video Diffusion Inference via Sketching-Rendering Cooperation", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": "9 pages, 6 figures", "summary": "Leveraging the diffusion transformer (DiT) architecture, models like Sora,\nCogVideoX and Wan have achieved remarkable progress in text-to-video,\nimage-to-video, and video editing tasks. Despite these advances,\ndiffusion-based video generation remains computationally intensive, especially\nfor high-resolution, long-duration videos. Prior work accelerates its inference\nby skipping computation, usually at the cost of severe quality degradation. In\nthis paper, we propose SRDiffusion, a novel framework that leverages\ncollaboration between large and small models to reduce inference cost. The\nlarge model handles high-noise steps to ensure semantic and motion fidelity\n(Sketching), while the smaller model refines visual details in low-noise steps\n(Rendering). Experimental results demonstrate that our method outperforms\nexisting approaches, over 3$\\times$ speedup for Wan with nearly no quality loss\nfor VBench, and 2$\\times$ speedup for CogVideoX. Our method is introduced as a\nnew direction orthogonal to existing acceleration strategies, offering a\npractical solution for scalable video generation."}
{"id": "2505.19190", "pdf": "https://arxiv.org/pdf/2505.19190", "abs": "https://arxiv.org/abs/2505.19190", "authors": ["Jiayi Xin", "Sukwon Yun", "Jie Peng", "Inyoung Choi", "Jenna L. Ballard", "Tianlong Chen", "Qi Long"], "title": "I2MoE: Interpretable Multimodal Interaction-aware Mixture-of-Experts", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "ICML 2025 Poster", "summary": "Modality fusion is a cornerstone of multimodal learning, enabling information\nintegration from diverse data sources. However, vanilla fusion methods are\nlimited by (1) inability to account for heterogeneous interactions between\nmodalities and (2) lack of interpretability in uncovering the multimodal\ninteractions inherent in the data. To this end, we propose I2MoE (Interpretable\nMultimodal Interaction-aware Mixture of Experts), an end-to-end MoE framework\ndesigned to enhance modality fusion by explicitly modeling diverse multimodal\ninteractions, as well as providing interpretation on a local and global level.\nFirst, I2MoE utilizes different interaction experts with weakly supervised\ninteraction losses to learn multimodal interactions in a data-driven way.\nSecond, I2MoE deploys a reweighting model that assigns importance scores for\nthe output of each interaction expert, which offers sample-level and\ndataset-level interpretation. Extensive evaluation of medical and general\nmultimodal datasets shows that I2MoE is flexible enough to be combined with\ndifferent fusion techniques, consistently improves task performance, and\nprovides interpretation across various real-world scenarios. Code is available\nat https://github.com/Raina-Xin/I2MoE."}
{"id": "2505.19195", "pdf": "https://arxiv.org/pdf/2505.19195", "abs": "https://arxiv.org/abs/2505.19195", "authors": ["Shaohao Rui", "Haoyang Su", "Jinyi Xiang", "Lian-Ming Wu", "Xiaosong Wang"], "title": "CardioCoT: Hierarchical Reasoning for Multimodal Survival Analysis", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "Accurate prediction of major adverse cardiovascular events recurrence risk in\nacute myocardial infarction patients based on postoperative cardiac MRI and\nassociated clinical notes is crucial for precision treatment and personalized\nintervention. Existing methods primarily focus on risk stratification\ncapability while overlooking the need for intermediate robust reasoning and\nmodel interpretability in clinical practice. Moreover, end-to-end risk\nprediction using LLM/VLM faces significant challenges due to data limitations\nand modeling complexity. To bridge this gap, we propose CardioCoT, a novel\ntwo-stage hierarchical reasoning-enhanced survival analysis framework designed\nto enhance both model interpretability and predictive performance. In the first\nstage, we employ an evidence-augmented self-refinement mechanism to guide\nLLM/VLMs in generating robust hierarchical reasoning trajectories based on\nassociated radiological findings. In the second stage, we integrate the\nreasoning trajectories with imaging data for risk model training and\nprediction. CardioCoT demonstrates superior performance in MACE recurrence risk\nprediction while providing interpretable reasoning processes, offering valuable\ninsights for clinical decision-making."}
{"id": "2505.19225", "pdf": "https://arxiv.org/pdf/2505.19225", "abs": "https://arxiv.org/abs/2505.19225", "authors": ["Chenglong Ma", "Yuanfeng Ji", "Jin Ye", "Zilong Li", "Chenhui Wang", "Junzhi Ning", "Wei Li", "Lihao Liu", "Qiushan Guo", "Tianbin Li", "Junjun He", "Hongming Shan"], "title": "MedITok: A Unified Tokenizer for Medical Image Synthesis and Interpretation", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Advanced autoregressive models have reshaped multimodal AI. However, their\ntransformative potential in medical imaging remains largely untapped due to the\nabsence of a unified visual tokenizer -- one capable of capturing fine-grained\nvisual structures for faithful image reconstruction and realistic image\nsynthesis, as well as rich semantics for accurate diagnosis and image\ninterpretation. To this end, we present MedITok, the first unified tokenizer\ntailored for medical images, encoding both low-level structural details and\nhigh-level clinical semantics within a unified latent space. To balance these\ncompeting objectives, we introduce a novel two-stage training framework: a\nvisual representation alignment stage that cold-starts the tokenizer\nreconstruction learning with a visual semantic constraint, followed by a\ntextual semantic representation alignment stage that infuses detailed clinical\nsemantics into the latent space. Trained on the meticulously collected\nlarge-scale dataset with over 30 million medical images and 2 million\nimage-caption pairs, MedITok achieves state-of-the-art performance on more than\n30 datasets across 9 imaging modalities and 4 different tasks. By providing a\nunified token space for autoregressive modeling, MedITok supports a wide range\nof tasks in clinical diagnostics and generative healthcare applications. Model\nand code will be made publicly available at:\nhttps://github.com/Masaaki-75/meditok."}
{"id": "2505.19235", "pdf": "https://arxiv.org/pdf/2505.19235", "abs": "https://arxiv.org/abs/2505.19235", "authors": ["Qinsi Wang", "Hancheng Ye", "Ming-Yu Chung", "Yudong Liu", "Yueqian Lin", "Martin Kuo", "Mingyuan Ma", "Jianyi Zhang", "Yiran Chen"], "title": "CoreMatching: A Co-adaptive Sparse Inference Framework with Token and Neuron Pruning for Comprehensive Acceleration of Vision-Language Models", "categories": ["cs.LG", "cs.CV"], "comment": "ICML 2025", "summary": "Vision-Language Models (VLMs) excel across diverse tasks but suffer from high\ninference costs in time and memory. Token sparsity mitigates inefficiencies in\ntoken usage, while neuron sparsity reduces high-dimensional computations, both\noffering promising solutions to enhance efficiency. Recently, these two\nsparsity paradigms have evolved largely in parallel, fostering the prevailing\nassumption that they function independently. However, a fundamental yet\nunderexplored question remains: Do they truly operate in isolation, or is there\na deeper underlying interplay that has yet to be uncovered? In this paper, we\nconduct the first comprehensive investigation into this question. By\nintroducing and analyzing the matching mechanism between Core Neurons and Core\nTokens, we found that key neurons and tokens for inference mutually influence\nand reinforce each other. Building on this insight, we propose CoreMatching, a\nco-adaptive sparse inference framework, which leverages the synergy between\ntoken and neuron sparsity to enhance inference efficiency. Through theoretical\nanalysis and efficiency evaluations, we demonstrate that the proposed method\nsurpasses state-of-the-art baselines on ten image understanding tasks and three\nhardware devices. Notably, on the NVIDIA Titan Xp, it achieved 5x FLOPs\nreduction and a 10x overall speedup. Code is released at\nhttps://github.com/wangqinsi1/2025-ICML-CoreMatching/tree/main."}
{"id": "2505.19249", "pdf": "https://arxiv.org/pdf/2505.19249", "abs": "https://arxiv.org/abs/2505.19249", "authors": ["Mir Sazzat Hossain", "Khan Muhammad Bin Asad", "Payaswini Saikia", "Adrita Khan", "Md Akil Raihan Iftee", "Rakibul Hasan Rajib", "Arshad Momen", "Md Ashraful Amin", "Amin Ahsan Ali", "AKM Mahbubur Rahman"], "title": "RGC-Bent: A Novel Dataset for Bent Radio Galaxy Classification", "categories": ["astro-ph.GA", "cs.CV"], "comment": "6 pages, 3 figures, 2 tables, Accepted In ICIP 2025", "summary": "We introduce a novel machine learning dataset tailored for the classification\nof bent radio active galactic nuclei (AGN) in astronomical observations. Bent\nradio AGN, distinguished by their curved jet structures, provide critical\ninsights into galaxy cluster dynamics, interactions within the intracluster\nmedium, and the broader physics of AGN. Despite their astrophysical\nsignificance, the classification of bent radio AGN remains a challenge due to\nthe scarcity of specialized datasets and benchmarks. To address this, we\npresent a dataset, derived from a well-recognized radio astronomy survey, that\nis designed to support the classification of NAT (Narrow-Angle Tail) and WAT\n(Wide-Angle Tail) categories, along with detailed data processing steps. We\nfurther evaluate the performance of state-of-the-art deep learning models on\nthe dataset, including Convolutional Neural Networks (CNNs), and\ntransformer-based architectures. Our results demonstrate the effectiveness of\nadvanced machine learning models in classifying bent radio AGN, with ConvNeXT\nachieving the highest F1-scores for both NAT and WAT sources. By sharing this\ndataset and benchmarks, we aim to facilitate the advancement of research in AGN\nclassification, galaxy cluster environments and galaxy evolution."}
{"id": "2505.19306", "pdf": "https://arxiv.org/pdf/2505.19306", "abs": "https://arxiv.org/abs/2505.19306", "authors": ["Weiming Zhi", "Ziyong Ma", "Tianyi Zhang", "Matthew Johnson-Roberson"], "title": "From Single Images to Motion Policies via Video-Generation Environment Representations", "categories": ["cs.RO", "cs.CV", "cs.GR", "cs.LG"], "comment": null, "summary": "Autonomous robots typically need to construct representations of their\nsurroundings and adapt their motions to the geometry of their environment.\nHere, we tackle the problem of constructing a policy model for collision-free\nmotion generation, consistent with the environment, from a single input RGB\nimage. Extracting 3D structures from a single image often involves monocular\ndepth estimation. Developments in depth estimation have given rise to large\npre-trained models such as DepthAnything. However, using outputs of these\nmodels for downstream motion generation is challenging due to frustum-shaped\nerrors that arise. Instead, we propose a framework known as Video-Generation\nEnvironment Representation (VGER), which leverages the advances of large-scale\nvideo generation models to generate a moving camera video conditioned on the\ninput image. Frames of this video, which form a multiview dataset, are then\ninput into a pre-trained 3D foundation model to produce a dense point cloud. We\nthen introduce a multi-scale noise approach to train an implicit representation\nof the environment structure and build a motion generation model that complies\nwith the geometry of the representation. We extensively evaluate VGER over a\ndiverse set of indoor and outdoor environments. We demonstrate its ability to\nproduce smooth motions that account for the captured geometry of a scene, all\nfrom a single RGB input image."}
{"id": "2505.19354", "pdf": "https://arxiv.org/pdf/2505.19354", "abs": "https://arxiv.org/abs/2505.19354", "authors": ["Mohammad Mahdi Moradi", "Sudhir Mudur"], "title": "GC-KBVQA: A New Four-Stage Framework for Enhancing Knowledge Based Visual Question Answering Performance", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Knowledge-Based Visual Question Answering (KB-VQA) methods focus on tasks\nthat demand reasoning with information extending beyond the explicit content\ndepicted in the image. Early methods relied on explicit knowledge bases to\nprovide this auxiliary information. Recent approaches leverage Large Language\nModels (LLMs) as implicit knowledge sources. While KB-VQA methods have\ndemonstrated promising results, their potential remains constrained as the\nauxiliary text provided may not be relevant to the question context, and may\nalso include irrelevant information that could misguide the answer predictor.\nWe introduce a novel four-stage framework called Grounding Caption-Guided\nKnowledge-Based Visual Question Answering (GC-KBVQA), which enables LLMs to\neffectively perform zero-shot VQA tasks without the need for end-to-end\nmultimodal training. Innovations include grounding question-aware caption\ngeneration to move beyond generic descriptions and have compact, yet detailed\nand context-rich information. This is combined with knowledge from external\nsources to create highly informative prompts for the LLM. GC-KBVQA can address\na variety of VQA tasks, and does not require task-specific fine-tuning, thus\nreducing both costs and deployment complexity by leveraging general-purpose,\npre-trained LLMs. Comparison with competing KB-VQA methods shows significantly\nimproved performance. Our code will be made public."}
{"id": "2505.19361", "pdf": "https://arxiv.org/pdf/2505.19361", "abs": "https://arxiv.org/abs/2505.19361", "authors": ["Mario Leiva", "Noel Ngu", "Joshua Shay Kricheli", "Aditya Taparia", "Ransalu Senanayake", "Paulo Shakarian", "Nathaniel Bastian", "John Corcoran", "Gerardo Simari"], "title": "Consistency-based Abductive Reasoning over Perceptual Errors of Multiple Pre-trained Models in Novel Environments", "categories": ["cs.AI", "cs.CV", "cs.LG", "cs.LO"], "comment": null, "summary": "The deployment of pre-trained perception models in novel environments often\nleads to performance degradation due to distributional shifts. Although recent\nartificial intelligence approaches for metacognition use logical rules to\ncharacterize and filter model errors, improving precision often comes at the\ncost of reduced recall. This paper addresses the hypothesis that leveraging\nmultiple pre-trained models can mitigate this recall reduction. We formulate\nthe challenge of identifying and managing conflicting predictions from various\nmodels as a consistency-based abduction problem. The input predictions and the\nlearned error detection rules derived from each model are encoded in a logic\nprogram. We then seek an abductive explanation--a subset of model\npredictions--that maximizes prediction coverage while ensuring the rate of\nlogical inconsistencies (derived from domain constraints) remains below a\nspecified threshold. We propose two algorithms for this knowledge\nrepresentation task: an exact method based on Integer Programming (IP) and an\nefficient Heuristic Search (HS). Through extensive experiments on a simulated\naerial imagery dataset featuring controlled, complex distributional shifts, we\ndemonstrate that our abduction-based framework outperforms individual models\nand standard ensemble baselines, achieving, for instance, average relative\nimprovements of approximately 13.6% in F1-score and 16.6% in accuracy across 15\ndiverse test datasets when compared to the best individual model. Our results\nvalidate the use of consistency-based abduction as an effective mechanism to\nrobustly integrate knowledge from multiple imperfect reasoners in challenging,\nnovel scenarios."}
{"id": "2505.19381", "pdf": "https://arxiv.org/pdf/2505.19381", "abs": "https://arxiv.org/abs/2505.19381", "authors": ["Anqing Jiang", "Yu Gao", "Zhigang Sun", "Yiru Wang", "Jijun Wang", "Jinghao Chai", "Qian Cao", "Yuweng Heng", "Hao Jiang", "Zongzheng Zhang", "Xianda Guo", "Hao Sun", "Hao Zhao"], "title": "DiffVLA: Vision-Language Guided Diffusion Planning for Autonomous Driving", "categories": ["cs.AI", "cs.CV", "cs.RO"], "comment": "4pages", "summary": "Research interest in end-to-end autonomous driving has surged owing to its\nfully differentiable design integrating modular tasks, i.e. perception,\nprediction and planing, which enables optimization in pursuit of the ultimate\ngoal. Despite the great potential of the end-to-end paradigm, existing methods\nsuffer from several aspects including expensive BEV (bird's eye view)\ncomputation, action diversity, and sub-optimal decision in complex real-world\nscenarios. To address these challenges, we propose a novel hybrid sparse-dense\ndiffusion policy, empowered by a Vision-Language Model (VLM), called Diff-VLA.\nWe explore the sparse diffusion representation for efficient multi-modal\ndriving behavior. Moreover, we rethink the effectiveness of VLM driving\ndecision and improve the trajectory generation guidance through deep\ninteraction across agent, map instances and VLM output. Our method shows\nsuperior performance in Autonomous Grand Challenge 2025 which contains\nchallenging real and reactive synthetic scenarios. Our methods achieves 45.0\nPDMS."}
{"id": "2505.19404", "pdf": "https://arxiv.org/pdf/2505.19404", "abs": "https://arxiv.org/abs/2505.19404", "authors": ["Yuta Ono", "Hiroshi Nakamura", "Hideki Takase"], "title": "Exploring the Possibility of TypiClust for Low-Budget Federated Active Learning", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "6 pages. Accepted at COMPSAC 2025", "summary": "Federated Active Learning (FAL) seeks to reduce the burden of annotation\nunder the realistic constraints of federated learning by leveraging Active\nLearning (AL). As FAL settings make it more expensive to obtain ground truth\nlabels, FAL strategies that work well in low-budget regimes, where the amount\nof annotation is very limited, are needed. In this work, we investigate the\neffectiveness of TypiClust, a successful low-budget AL strategy, in low-budget\nFAL settings. Our empirical results show that TypiClust works well even in\nlow-budget FAL settings contrasted with relatively low performances of other\nmethods, although these settings present additional challenges, such as data\nheterogeneity, compared to AL. In addition, we show that FAL settings cause\ndistribution shifts in terms of typicality, but TypiClust is not very\nvulnerable to the shifts. We also analyze the sensitivity of TypiClust to\nfeature extraction methods, and it suggests a way to perform FAL even in\nlimited data situations."}
{"id": "2505.19447", "pdf": "https://arxiv.org/pdf/2505.19447", "abs": "https://arxiv.org/abs/2505.19447", "authors": ["Hengtong Shen", "Haiyan Gu", "Haitao Li", "Yi Yang", "Agen qiu"], "title": "A Contrastive Learning Foundation Model Based on Perfectly Aligned Sample Pairs for Remote Sensing Images", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Self-Supervised Learning (SSL) enables us to pre-train foundation models\nwithout costly labeled data. Among SSL methods, Contrastive Learning (CL)\nmethods are better at obtaining accurate semantic representations in noise\ninterference. However, due to the significant domain gap, while CL methods have\nachieved great success in many computer vision tasks, they still require\nspecific adaptation for Remote Sensing (RS) images. To this end, we present a\nnovel self-supervised method called PerA, which produces all-purpose RS\nfeatures through semantically Perfectly Aligned sample pairs. Specifically,\nPerA obtains features from sampled views by applying spatially disjoint masks\nto augmented images rather than random cropping. With disjoint masks, we divide\npatches from different views into different parts that are semantically aligned\nbut inconsistent in appearance. Our framework provides high-quality features by\nensuring consistency between teacher and student and predicting learnable mask\ntokens. Compared to previous contrastive methods, our method demonstrates\nhigher memory efficiency and can be trained with larger batches due to its\nsparse inputs. We also collect an unlabeled pre-training dataset, which\ncontains about 5 million RS images. We conducted experiments on multiple\ndownstream task datasets and achieved performance comparable to previous\nstate-of-the-art methods with a limited model scale, which verified the\nsuperiority of our method. We hope this work will contribute to practical\nremote sensing interpretation works."}
{"id": "2505.19469", "pdf": "https://arxiv.org/pdf/2505.19469", "abs": "https://arxiv.org/abs/2505.19469", "authors": ["Mingzhuo Li", "Guang Li", "Jiafeng Mao", "Takahiro Ogawa", "Miki Haseyama"], "title": "Diversity-Driven Generative Dataset Distillation Based on Diffusion Model with Self-Adaptive Memory", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "Accepted by ICIP 2025", "summary": "Dataset distillation enables the training of deep neural networks with\ncomparable performance in significantly reduced time by compressing large\ndatasets into small and representative ones. Although the introduction of\ngenerative models has made great achievements in this field, the distributions\nof their distilled datasets are not diverse enough to represent the original\nones, leading to a decrease in downstream validation accuracy. In this paper,\nwe present a diversity-driven generative dataset distillation method based on a\ndiffusion model to solve this problem. We introduce self-adaptive memory to\nalign the distribution between distilled and real datasets, assessing the\nrepresentativeness. The degree of alignment leads the diffusion model to\ngenerate more diverse datasets during the distillation process. Extensive\nexperiments show that our method outperforms existing state-of-the-art methods\nin most situations, proving its ability to tackle dataset distillation tasks."}
{"id": "2505.19587", "pdf": "https://arxiv.org/pdf/2505.19587", "abs": "https://arxiv.org/abs/2505.19587", "authors": ["Shadi Alijani", "Homayoun Najjaran"], "title": "WQLCP: Weighted Adaptive Conformal Prediction for Robust Uncertainty Quantification Under Distribution Shifts", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Conformal prediction (CP) provides a framework for constructing prediction\nsets with guaranteed coverage, assuming exchangeable data. However, real-world\nscenarios often involve distribution shifts that violate exchangeability,\nleading to unreliable coverage and inflated prediction sets. To address this\nchallenge, we first introduce Reconstruction Loss-Scaled Conformal Prediction\n(RLSCP), which utilizes reconstruction losses derived from a Variational\nAutoencoder (VAE) as an uncertainty metric to scale score functions. While\nRLSCP demonstrates performance improvements, mainly resulting in better\ncoverage, it quantifies quantiles based on a fixed calibration dataset without\nconsidering the discrepancies between test and train datasets in an\nunexchangeable setting. In the next step, we propose Weighted Quantile\nLoss-scaled Conformal Prediction (WQLCP), which refines RLSCP by incorporating\na weighted notion of exchangeability, adjusting the calibration quantile\nthreshold based on weights with respect to the ratio of calibration and test\nloss values. This approach improves the CP-generated prediction set outputs in\nthe presence of distribution shifts. Experiments on large-scale datasets,\nincluding ImageNet variants, demonstrate that WQLCP outperforms existing\nbaselines by consistently maintaining coverage while reducing prediction set\nsizes, providing a robust solution for CP under distribution shifts."}
{"id": "2505.19614", "pdf": "https://arxiv.org/pdf/2505.19614", "abs": "https://arxiv.org/abs/2505.19614", "authors": ["Sanghyuk Chun"], "title": "Multiplicity is an Inevitable and Inherent Challenge in Multimodal Learning", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Multimodal learning has seen remarkable progress, particularly with the\nemergence of large-scale pre-training across various modalities. However, most\ncurrent approaches are built on the assumption of a deterministic, one-to-one\nalignment between modalities. This oversimplifies real-world multimodal\nrelationships, where their nature is inherently many-to-many. This phenomenon,\nnamed multiplicity, is not a side-effect of noise or annotation error, but an\ninevitable outcome of semantic abstraction, representational asymmetry, and\ntask-dependent ambiguity in multimodal tasks. This position paper argues that\nmultiplicity is a fundamental bottleneck that manifests across all stages of\nthe multimodal learning pipeline: from data construction to training and\nevaluation. This paper examines the causes and consequences of multiplicity,\nand highlights how multiplicity introduces training uncertainty, unreliable\nevaluation, and low dataset quality. This position calls for new research\ndirections on multimodal learning: novel multiplicity-aware learning frameworks\nand dataset construction protocols considering multiplicity."}
{"id": "2505.19616", "pdf": "https://arxiv.org/pdf/2505.19616", "abs": "https://arxiv.org/abs/2505.19616", "authors": ["Rui Cai", "Bangzheng Li", "Xiaofei Wen", "Muhao Chen", "Zhe Zhao"], "title": "Diagnosing and Mitigating Modality Interference in Multimodal Large Language Models", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have demonstrated impressive\ncapabilities across tasks, yet they often exhibit difficulty in distinguishing\ntask-relevant from irrelevant signals, particularly in tasks like Visual\nQuestion Answering (VQA), which can lead to susceptibility to misleading or\nspurious inputs. We refer to this broader limitation as the Cross-Modality\nCompetency Problem: the model's inability to fairly evaluate all modalities.\nThis vulnerability becomes more evident in modality-specific tasks such as\nimage classification or pure text question answering, where models are expected\nto rely solely on one modality. In such tasks, spurious information from\nirrelevant modalities often leads to significant performance degradation. We\nrefer to this failure as Modality Interference, which serves as a concrete and\nmeasurable instance of the cross-modality competency problem. We further design\na perturbation-based causal diagnostic experiment to verify and quantify this\nproblem. To mitigate modality interference, we propose a novel framework to\nfine-tune MLLMs, including perturbation-based data augmentations with both\nheuristic perturbations and adversarial perturbations via Projected Gradient\nDescent (PGD), and a consistency regularization strategy applied to model\noutputs with original and perturbed inputs. Experiments on multiple benchmark\ndatasets (image-heavy, text-heavy, and VQA tasks) and multiple model families\nwith different scales demonstrate significant improvements in robustness and\ncross-modality competency, indicating our method's effectiveness in boosting\nunimodal reasoning ability while enhancing performance on multimodal tasks."}
{"id": "2505.19662", "pdf": "https://arxiv.org/pdf/2505.19662", "abs": "https://arxiv.org/abs/2505.19662", "authors": ["Atsunori Moteki", "Shoichi Masui", "Fan Yang", "Yueqi Song", "Yonatan Bisk", "Graham Neubig", "Ikuo Kusajima", "Yasuto Watanabe", "Hiroyuki Ishida", "Jun Takahashi", "Shan Jiang"], "title": "FieldWorkArena: Agentic AI Benchmark for Real Field Work Tasks", "categories": ["cs.AI", "cs.CV"], "comment": "6 pages, 2 figures, 4 tables", "summary": "This paper proposes FieldWorkArena, a benchmark for agentic AI targeting\nreal-world field work. With the recent increase in demand for agentic AI, they\nare required to monitor and report safety and health incidents, as well as\nmanufacturing-related incidents, that may occur in real-world work\nenvironments. Existing agentic AI benchmarks have been limited to evaluating\nweb tasks and are insufficient for evaluating agents in real-world work\nenvironments, where complexity increases significantly. In this paper, we\ndefine a new action space that agentic AI should possess for real world work\nenvironment benchmarks and improve the evaluation function from previous\nmethods to assess the performance of agentic AI in diverse real-world tasks.\nThe dataset consists of videos captured on-site and documents actually used in\nfactories and warehouses, and tasks were created based on interviews with\non-site workers and managers. Evaluation results confirmed that performance\nevaluation considering the characteristics of Multimodal LLM (MLLM) such as\nGPT-4o is feasible. Additionally, the effectiveness and limitations of the\nproposed new evaluation method were identified. The complete dataset\n(HuggingFace) and evaluation program (GitHub) can be downloaded from the\nfollowing website:\nhttps://en-documents.research.global.fujitsu.com/fieldworkarena/."}
{"id": "2505.19678", "pdf": "https://arxiv.org/pdf/2505.19678", "abs": "https://arxiv.org/abs/2505.19678", "authors": ["Hao Fang", "Changle Zhou", "Jiawei Kong", "Kuofeng Gao", "Bin Chen", "Tao Liang", "Guojun Ma", "Shu-Tao Xia"], "title": "Grounding Language with Vision: A Conditional Mutual Information Calibrated Decoding Strategy for Reducing Hallucinations in LVLMs", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Large Vision-Language Models (LVLMs) are susceptible to hallucinations, where\ngenerated responses seem semantically plausible yet exhibit little or no\nrelevance to the input image. Previous studies reveal that this issue primarily\nstems from LVLMs' over-reliance on language priors while disregarding the\nvisual information during decoding. To alleviate this issue, we introduce a\nnovel Conditional Pointwise Mutual Information (C-PMI) calibrated decoding\nstrategy, which adaptively strengthens the mutual dependency between generated\ntexts and input images to mitigate hallucinations. Unlike existing methods\nsolely focusing on text token sampling, we propose to jointly model the\ncontributions of visual and textual tokens to C-PMI, formulating hallucination\nmitigation as a bi-level optimization problem aimed at maximizing mutual\ninformation. To solve it, we design a token purification mechanism that\ndynamically regulates the decoding process by sampling text tokens remaining\nmaximally relevant to the given image, while simultaneously refining image\ntokens most pertinent to the generated response. Extensive experiments across\nvarious benchmarks reveal that the proposed method significantly reduces\nhallucinations in LVLMs while preserving decoding efficiency."}
{"id": "2505.19779", "pdf": "https://arxiv.org/pdf/2505.19779", "abs": "https://arxiv.org/abs/2505.19779", "authors": ["Mobina Mansoori", "Sajjad Shahabodini", "Farnoush Bayatmakou", "Jamshid Abouei", "Konstantinos N. Plataniotis", "Arash Mohammadi"], "title": "Advancements in Medical Image Classification through Fine-Tuning Natural Domain Foundation Models", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "Using massive datasets, foundation models are large-scale, pre-trained models\nthat perform a wide range of tasks. These models have shown consistently\nimproved results with the introduction of new methods. It is crucial to analyze\nhow these trends impact the medical field and determine whether these\nadvancements can drive meaningful change. This study investigates the\napplication of recent state-of-the-art foundation models, DINOv2, MAE, VMamba,\nCoCa, SAM2, and AIMv2, for medical image classification. We explore their\neffectiveness on datasets including CBIS-DDSM for mammography, ISIC2019 for\nskin lesions, APTOS2019 for diabetic retinopathy, and CHEXPERT for chest\nradiographs. By fine-tuning these models and evaluating their configurations,\nwe aim to understand the potential of these advancements in medical image\nclassification. The results indicate that these advanced models significantly\nenhance classification outcomes, demonstrating robust performance despite\nlimited labeled data. Based on our results, AIMv2, DINOv2, and SAM2 models\noutperformed others, demonstrating that progress in natural domain training has\npositively impacted the medical domain and improved classification outcomes.\nOur code is publicly available at:\nhttps://github.com/sajjad-sh33/Medical-Transfer-Learning."}
{"id": "2505.19802", "pdf": "https://arxiv.org/pdf/2505.19802", "abs": "https://arxiv.org/abs/2505.19802", "authors": ["Zhiyu Wang", "Yang Liu", "Hatice Gunes"], "title": "GraphAU-Pain: Graph-based Action Unit Representation for Pain Intensity Estimation", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Understanding pain-related facial behaviors is essential for digital\nhealthcare in terms of effective monitoring, assisted diagnostics, and\ntreatment planning, particularly for patients unable to communicate verbally.\nExisting data-driven methods of detecting pain from facial expressions are\nlimited due to interpretability and severity quantification. To this end, we\npropose GraphAU-Pain, leveraging a graph-based framework to model facial Action\nUnits (AUs) and their interrelationships for pain intensity estimation. AUs are\nrepresented as graph nodes, with co-occurrence relationships as edges, enabling\na more expressive depiction of pain-related facial behaviors. By utilizing a\nrelational graph neural network, our framework offers improved interpretability\nand significant performance gains. Experiments conducted on the publicly\navailable UNBC dataset demonstrate the effectiveness of the GraphAU-Pain,\nachieving an F1-score of 66.21% and accuracy of 87.61% in pain intensity\nestimation."}
{"id": "2505.19897", "pdf": "https://arxiv.org/pdf/2505.19897", "abs": "https://arxiv.org/abs/2505.19897", "authors": ["Qiushi Sun", "Zhoumianze Liu", "Chang Ma", "Zichen Ding", "Fangzhi Xu", "Zhangyue Yin", "Haiteng Zhao", "Zhenyu Wu", "Kanzhi Cheng", "Zhaoyang Liu", "Jianing Wang", "Qintong Li", "Xiangru Tang", "Tianbao Xie", "Xiachong Feng", "Xiang Li", "Ben Kao", "Wenhai Wang", "Biqing Qi", "Lingpeng Kong", "Zhiyong Wu"], "title": "ScienceBoard: Evaluating Multimodal Autonomous Agents in Realistic Scientific Workflows", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC"], "comment": "work in progress", "summary": "Large Language Models (LLMs) have extended their impact beyond Natural\nLanguage Processing, substantially fostering the development of\ninterdisciplinary research. Recently, various LLM-based agents have been\ndeveloped to assist scientific discovery progress across multiple aspects and\ndomains. Among these, computer-using agents, capable of interacting with\noperating systems as humans do, are paving the way to automated scientific\nproblem-solving and addressing routines in researchers' workflows. Recognizing\nthe transformative potential of these agents, we introduce ScienceBoard, which\nencompasses two complementary contributions: (i) a realistic, multi-domain\nenvironment featuring dynamic and visually rich scientific workflows with\nintegrated professional software, where agents can autonomously interact via\ndifferent interfaces to accelerate complex research tasks and experiments; and\n(ii) a challenging benchmark of 169 high-quality, rigorously validated\nreal-world tasks curated by humans, spanning scientific-discovery workflows in\ndomains such as biochemistry, astronomy, and geoinformatics. Extensive\nevaluations of agents with state-of-the-art backbones (e.g., GPT-4o, Claude\n3.7, UI-TARS) show that, despite some promising results, they still fall short\nof reliably assisting scientists in complex workflows, achieving only a 15%\noverall success rate. In-depth analysis further provides valuable insights for\naddressing current agent limitations and more effective design principles,\npaving the way to build more capable agents for scientific discovery. Our code,\nenvironment, and benchmark are at\nhttps://qiushisun.github.io/ScienceBoard-Home/."}
{"id": "2505.19983", "pdf": "https://arxiv.org/pdf/2505.19983", "abs": "https://arxiv.org/abs/2505.19983", "authors": ["Tong Wu", "Zhiyong Chen", "Dazhi He", "Feng Yang", "Meixia Tao", "Xiaodong Xu", "Wenjun Zhang", "Ping Zhang"], "title": "ICDM: Interference Cancellation Diffusion Models for Wireless Semantic Communications", "categories": ["cs.IT", "cs.AI", "cs.CV", "math.IT"], "comment": "submitted to IEEE journal", "summary": "Diffusion models (DMs) have recently achieved significant success in wireless\ncommunications systems due to their denoising capabilities. The broadcast\nnature of wireless signals makes them susceptible not only to Gaussian noise,\nbut also to unaware interference. This raises the question of whether DMs can\neffectively mitigate interference in wireless semantic communication systems.\nIn this paper, we model the interference cancellation problem as a maximum a\nposteriori (MAP) problem over the joint posterior probability of the signal and\ninterference, and theoretically prove that the solution provides excellent\nestimates for the signal and interference. To solve this problem, we develop an\ninterference cancellation diffusion model (ICDM), which decomposes the joint\nposterior into independent prior probabilities of the signal and interference,\nalong with the channel transition probablity. The log-gradients of these\ndistributions at each time step are learned separately by DMs and accurately\nestimated through deriving. ICDM further integrates these gradients with\nadvanced numerical iteration method, achieving accurate and rapid interference\ncancellation. Extensive experiments demonstrate that ICDM significantly reduces\nthe mean square error (MSE) and enhances perceptual quality compared to schemes\nwithout ICDM. For example, on the CelebA dataset under the Rayleigh fading\nchannel with a signal-to-noise ratio (SNR) of $20$ dB and signal to\ninterference plus noise ratio (SINR) of 0 dB, ICDM reduces the MSE by 4.54 dB\nand improves the learned perceptual image patch similarity (LPIPS) by 2.47 dB."}
{"id": "2505.19995", "pdf": "https://arxiv.org/pdf/2505.19995", "abs": "https://arxiv.org/abs/2505.19995", "authors": ["Marcel Aach", "Cyril Blanc", "Andreas Lintermann", "Kurt De Grave"], "title": "Optimizing edge AI models on HPC systems with the edge in the loop", "categories": ["cs.DC", "cs.CV", "I.2.6; D.1.3; I.2.8; I.5.1"], "comment": "13 pages, accepted for oral presentation at Computational Aspects of\n  Deep Learning 2025 (at ISC 2025)", "summary": "Artificial intelligence and machine learning models deployed on edge devices,\ne.g., for quality control in Additive Manufacturing (AM), are frequently small\nin size. Such models usually have to deliver highly accurate results within a\nshort time frame. Methods that are commonly employed in literature start out\nwith larger trained models and try to reduce their memory and latency footprint\nby structural pruning, knowledge distillation, or quantization. It is, however,\nalso possible to leverage hardware-aware Neural Architecture Search (NAS), an\napproach that seeks to systematically explore the architecture space to find\noptimized configurations. In this study, a hardware-aware NAS workflow is\nintroduced that couples an edge device located in Belgium with a powerful\nHigh-Performance Computing system in Germany, to train possible architecture\ncandidates as fast as possible while performing real-time latency measurements\non the target hardware. The approach is verified on a use case in the AM\ndomain, based on the open RAISE-LPBF dataset, achieving ~8.8 times faster\ninference speed while simultaneously enhancing model quality by a factor of\n~1.35, compared to a human-designed baseline."}
{"id": "2505.20038", "pdf": "https://arxiv.org/pdf/2505.20038", "abs": "https://arxiv.org/abs/2505.20038", "authors": ["Chang Liu", "Haomin Zhang", "Shiyu Xia", "Zihao Chen", "Chaofan Ding", "Xin Yue", "Huizhe Chen", "Xinhan Di"], "title": "Towards Video to Piano Music Generation with Chain-of-Perform Support Benchmarks", "categories": ["cs.SD", "cs.CV", "eess.AS"], "comment": "4 pages, 1 figure, accepted by CVPR 2025 MMFM Workshop", "summary": "Generating high-quality piano audio from video requires precise\nsynchronization between visual cues and musical output, ensuring accurate\nsemantic and temporal alignment.However, existing evaluation datasets do not\nfully capture the intricate synchronization required for piano music\ngeneration. A comprehensive benchmark is essential for two primary reasons: (1)\nexisting metrics fail to reflect the complexity of video-to-piano music\ninteractions, and (2) a dedicated benchmark dataset can provide valuable\ninsights to accelerate progress in high-quality piano music generation. To\naddress these challenges, we introduce the CoP Benchmark Dataset-a fully\nopen-sourced, multimodal benchmark designed specifically for video-guided piano\nmusic generation. The proposed Chain-of-Perform (CoP) benchmark offers several\ncompelling features: (1) detailed multimodal annotations, enabling precise\nsemantic and temporal alignment between video content and piano audio via\nstep-by-step Chain-of-Perform guidance; (2) a versatile evaluation framework\nfor rigorous assessment of both general-purpose and specialized video-to-piano\ngeneration tasks; and (3) full open-sourcing of the dataset, annotations, and\nevaluation protocols. The dataset is publicly available at\nhttps://github.com/acappemin/Video-to-Audio-and-Piano, with a continuously\nupdated leaderboard to promote ongoing research in this domain."}
{"id": "2505.20107", "pdf": "https://arxiv.org/pdf/2505.20107", "abs": "https://arxiv.org/abs/2505.20107", "authors": ["Ziyi Zhang", "Li Shen", "Deheng Ye", "Yong Luo", "Huangxuan Zhao", "Lefei Zhang"], "title": "Refining Few-Step Text-to-Multiview Diffusion via Reinforcement Learning", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Text-to-multiview (T2MV) generation, which produces coherent multiview images\nfrom a single text prompt, remains computationally intensive, while accelerated\nT2MV methods using few-step diffusion models often sacrifice image fidelity and\nview consistency. To address this, we propose a novel reinforcement learning\n(RL) finetuning framework tailored for few-step T2MV diffusion models to\njointly optimize per-view fidelity and cross-view consistency. Specifically, we\nfirst reformulate T2MV denoising across all views as a single unified Markov\ndecision process, enabling multiview-aware policy optimization driven by a\njoint-view reward objective. Next, we introduce ZMV-Sampling, a test-time T2MV\nsampling technique that adds an inversion-denoising pass to reinforce both\nviewpoint and text conditioning, resulting in improved T2MV generation at the\ncost of inference time. To internalize its performance gains into the base\nsampling policy, we develop MV-ZigAL, a novel policy optimization strategy that\nuses reward advantages of ZMV-Sampling over standard sampling as learning\nsignals for policy updates. Finally, noting that the joint-view reward\nobjective under-optimizes per-view fidelity but naively optimizing single-view\nmetrics neglects cross-view alignment, we reframe RL finetuning for T2MV\ndiffusion models as a constrained optimization problem that maximizes per-view\nfidelity subject to an explicit joint-view constraint, thereby enabling more\nefficient and balanced policy updates. By integrating this constrained\noptimization paradigm with MV-ZigAL, we establish our complete RL finetuning\nframework, referred to as MVC-ZigAL, which effectively refines the few-step\nT2MV diffusion baseline in both fidelity and consistency while preserving its\nfew-step efficiency."}
{"id": "2505.20123", "pdf": "https://arxiv.org/pdf/2505.20123", "abs": "https://arxiv.org/abs/2505.20123", "authors": ["Huijie Zhang", "Zijian Huang", "Siyi Chen", "Jinfan Zhou", "Zekai Zhang", "Peng Wang", "Qing Qu"], "title": "Understanding Generalization in Diffusion Models via Probability Flow Distance", "categories": ["cs.LG", "cs.CV"], "comment": "41 pages, 14 figures", "summary": "Diffusion models have emerged as a powerful class of generative models,\ncapable of producing high-quality samples that generalize beyond the training\ndata. However, evaluating this generalization remains challenging: theoretical\nmetrics are often impractical for high-dimensional data, while no practical\nmetrics rigorously measure generalization. In this work, we bridge this gap by\nintroducing probability flow distance ($\\texttt{PFD}$), a theoretically\ngrounded and computationally efficient metric to measure distributional\ngeneralization. Specifically, $\\texttt{PFD}$ quantifies the distance between\ndistributions by comparing their noise-to-data mappings induced by the\nprobability flow ODE. Moreover, by using $\\texttt{PFD}$ under a teacher-student\nevaluation protocol, we empirically uncover several key generalization\nbehaviors in diffusion models, including: (1) scaling behavior from\nmemorization to generalization, (2) early learning and double descent training\ndynamics, and (3) bias-variance decomposition. Beyond these insights, our work\nlays a foundation for future empirical and theoretical studies on\ngeneralization in diffusion models."}
{"id": "2505.20149", "pdf": "https://arxiv.org/pdf/2505.20149", "abs": "https://arxiv.org/abs/2505.20149", "authors": ["Cheng-Yu Tai", "Ching-Wen Chen", "Chi-Chin Wu", "Bo-Chen Chiu", "Cheng-Hung", "Lin", "Cheng-Kai Lu", "Jia-Kang Wang", "Tzu-Lun Huang"], "title": "Improvement Strategies for Few-Shot Learning in OCT Image Classification of Rare Retinal Diseases", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "This paper focuses on using few-shot learning to improve the accuracy of\nclassifying OCT diagnosis images with major and rare classes. We used the\nGAN-based augmentation strategy as a baseline and introduced several novel\nmethods to further enhance our model. The proposed strategy contains U-GAT-IT\nfor improving the generative part and uses the data balance technique to narrow\ndown the skew of accuracy between all categories. The best model obtained was\nbuilt with CBAM attention mechanism and fine-tuned InceptionV3, and achieved an\noverall accuracy of 97.85%, representing a significant improvement over the\noriginal baseline."}
{"id": "2505.20232", "pdf": "https://arxiv.org/pdf/2505.20232", "abs": "https://arxiv.org/abs/2505.20232", "authors": ["Pranav Poudel", "Aavash Chhetri", "Prashnna Gyawali", "Georgios Leontidis", "Binod Bhattarai"], "title": "Multimodal Federated Learning With Missing Modalities through Feature Imputation Network", "categories": ["cs.LG", "cs.CV"], "comment": "MIUA 2025", "summary": "Multimodal federated learning holds immense potential for collaboratively\ntraining models from multiple sources without sharing raw data, addressing both\ndata scarcity and privacy concerns, two key challenges in healthcare. A major\nchallenge in training multimodal federated models in healthcare is the presence\nof missing modalities due to multiple reasons, including variations in clinical\npractice, cost and accessibility constraints, retrospective data collection,\nprivacy concerns, and occasional technical or human errors. Previous methods\ntypically rely on publicly available real datasets or synthetic data to\ncompensate for missing modalities. However, obtaining real datasets for every\ndisease is impractical, and training generative models to synthesize missing\nmodalities is computationally expensive and prone to errors due to the high\ndimensionality of medical data. In this paper, we propose a novel, lightweight,\nlow-dimensional feature translator to reconstruct bottleneck features of the\nmissing modalities. Our experiments on three different datasets (MIMIC-CXR, NIH\nOpen-I, and CheXpert), in both homogeneous and heterogeneous settings\nconsistently improve the performance of competitive baselines. The code and\nimplementation details are available at:\nhttps://github.com/bhattarailab/FedFeatGen"}
{"id": "2505.20274", "pdf": "https://arxiv.org/pdf/2505.20274", "abs": "https://arxiv.org/abs/2505.20274", "authors": ["Kejing Lu", "Chuan Xiao", "Yoshiharu Ishikawa"], "title": "Probabilistic Kernel Function for Fast Angle Testing", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.DB", "cs.DS"], "comment": null, "summary": "In this paper, we study the angle testing problem in high-dimensional\nEuclidean spaces and propose two projection-based probabilistic kernel\nfunctions, one designed for angle comparison and the other for angle\nthresholding. Unlike existing approaches that rely on random projection vectors\ndrawn from Gaussian distributions, our approach leverages reference angles and\nemploys a deterministic structure for the projection vectors. Notably, our\nkernel functions do not require asymptotic assumptions, such as the number of\nprojection vectors tending to infinity, and can be both theoretically and\nexperimentally shown to outperform Gaussian-distribution-based kernel\nfunctions. We further apply the proposed kernel function to Approximate Nearest\nNeighbor Search (ANNS) and demonstrate that our approach achieves a 2.5X ~ 3X\nhigher query-per-second (QPS) throughput compared to the state-of-the-art\ngraph-based search algorithm HNSW."}
{"id": "2505.20277", "pdf": "https://arxiv.org/pdf/2505.20277", "abs": "https://arxiv.org/abs/2505.20277", "authors": ["Haonan Zhang", "Run Luo", "Xiong Liu", "Yuchuan Wu", "Ting-En Lin", "Pengpeng Zeng", "Qiang Qu", "Feiteng Fang", "Min Yang", "Lianli Gao", "Jingkuan Song", "Fei Huang", "Yongbin Li"], "title": "OmniCharacter: Towards Immersive Role-Playing Agents with Seamless Speech-Language Personality Interaction", "categories": ["cs.CL", "cs.CV"], "comment": "14 pages, 6 figures", "summary": "Role-Playing Agents (RPAs), benefiting from large language models, is an\nemerging interactive AI system that simulates roles or characters with diverse\npersonalities. However, existing methods primarily focus on mimicking dialogues\namong roles in textual form, neglecting the role's voice traits (e.g., voice\nstyle and emotions) as playing a crucial effect in interaction, which tends to\nbe more immersive experiences in realistic scenarios. Towards this goal, we\npropose OmniCharacter, a first seamless speech-language personality interaction\nmodel to achieve immersive RPAs with low latency. Specifically, OmniCharacter\nenables agents to consistently exhibit role-specific personality traits and\nvocal traits throughout the interaction, enabling a mixture of speech and\nlanguage responses. To align the model with speech-language scenarios, we\nconstruct a dataset named OmniCharacter-10K, which involves more distinctive\ncharacters (20), richly contextualized multi-round dialogue (10K), and dynamic\nspeech response (135K). Experimental results showcase that our method yields\nbetter responses in terms of both content and style compared to existing RPAs\nand mainstream speech-language models, with a response latency as low as 289ms.\nCode and dataset are available at\nhttps://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/OmniCharacter."}
{"id": "2505.20298", "pdf": "https://arxiv.org/pdf/2505.20298", "abs": "https://arxiv.org/abs/2505.20298", "authors": ["Jeonghun Baek", "Kazuki Egashira", "Shota Onohara", "Atsuyuki Miyai", "Yuki Imajuku", "Hikaru Ikuta", "Kiyoharu Aizawa"], "title": "MangaVQA and MangaLMM: A Benchmark and Specialized Model for Multimodal Manga Understanding", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "20 pages, 11 figures", "summary": "Manga, or Japanese comics, is a richly multimodal narrative form that blends\nimages and text in complex ways. Teaching large multimodal models (LMMs) to\nunderstand such narratives at a human-like level could help manga creators\nreflect on and refine their stories. To this end, we introduce two benchmarks\nfor multimodal manga understanding: MangaOCR, which targets in-page text\nrecognition, and MangaVQA, a novel benchmark designed to evaluate contextual\nunderstanding through visual question answering. MangaVQA consists of 526\nhigh-quality, manually constructed question-answer pairs, enabling reliable\nevaluation across diverse narrative and visual scenarios. Building on these\nbenchmarks, we develop MangaLMM, a manga-specialized model finetuned from the\nopen-source LMM Qwen2.5-VL to jointly handle both tasks. Through extensive\nexperiments, including comparisons with proprietary models such as GPT-4o and\nGemini 2.5, we assess how well LMMs understand manga. Our benchmark and model\nprovide a comprehensive foundation for evaluating and advancing LMMs in the\nrichly narrative domain of manga."}
