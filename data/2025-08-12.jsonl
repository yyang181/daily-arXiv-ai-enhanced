{"id": "2508.06496", "pdf": "https://arxiv.org/pdf/2508.06496", "abs": "https://arxiv.org/abs/2508.06496", "authors": ["Rakesh Raj Madavan", "Akshat Kaimal", "Hashim Faisal", "Chandrakala S"], "title": "Med-GRIM: Enhanced Zero-Shot Medical VQA using prompt-embedded Multimodal Graph RAG", "categories": ["cs.CV", "cs.MA"], "comment": null, "summary": "An ensemble of trained multimodal encoders and vision-language models (VLMs)\nhas become a standard approach for visual question answering (VQA) tasks.\nHowever, such models often fail to produce responses with the detailed\nprecision necessary for complex, domain-specific applications such as medical\nVQA. Our representation model, BIND: BLIVA Integrated with Dense Encoding,\nextends prior multimodal work by refining the joint embedding space through\ndense, query-token-based encodings inspired by contrastive pretraining\ntechniques. This refined encoder powers Med-GRIM, a model designed for medical\nVQA tasks that leverages graph-based retrieval and prompt engineering to\nintegrate domain-specific knowledge. Rather than relying on compute-heavy\nfine-tuning of vision and language models on specific datasets, Med-GRIM\napplies a low-compute, modular workflow with small language models (SLMs) for\nefficiency. Med-GRIM employs prompt-based retrieval to dynamically inject\nrelevant knowledge, ensuring both accuracy and robustness in its responses. By\nassigning distinct roles to each agent within the VQA system, Med-GRIM achieves\nlarge language model performance at a fraction of the computational cost.\nAdditionally, to support scalable research in zero-shot multimodal medical\napplications, we introduce DermaGraph, a novel Graph-RAG dataset comprising\ndiverse dermatological conditions. This dataset facilitates both multimodal and\nunimodal querying. The code and dataset are available at:\nhttps://github.com/Rakesh-123-cryp/Med-GRIM.git"}
{"id": "2508.06511", "pdf": "https://arxiv.org/pdf/2508.06511", "abs": "https://arxiv.org/abs/2508.06511", "authors": ["He Feng", "Yongjia Ma", "Donglin Di", "Lei Fan", "Tonghua Su", "Xiangqian Wu"], "title": "DiTalker: A Unified DiT-based Framework for High-Quality and Speaking Styles Controllable Portrait Animation", "categories": ["cs.CV"], "comment": null, "summary": "Portrait animation aims to synthesize talking videos from a static reference\nface, conditioned on audio and style frame cues (e.g., emotion and head poses),\nwhile ensuring precise lip synchronization and faithful reproduction of\nspeaking styles. Existing diffusion-based portrait animation methods primarily\nfocus on lip synchronization or static emotion transformation, often\noverlooking dynamic styles such as head movements. Moreover, most of these\nmethods rely on a dual U-Net architecture, which preserves identity consistency\nbut incurs additional computational overhead. To this end, we propose DiTalker,\na unified DiT-based framework for speaking style-controllable portrait\nanimation. We design a Style-Emotion Encoding Module that employs two separate\nbranches: a style branch extracting identity-specific style information (e.g.,\nhead poses and movements), and an emotion branch extracting identity-agnostic\nemotion features. We further introduce an Audio-Style Fusion Module that\ndecouples audio and speaking styles via two parallel cross-attention layers,\nusing these features to guide the animation process. To enhance the quality of\nresults, we adopt and modify two optimization constraints: one to improve lip\nsynchronization and the other to preserve fine-grained identity and background\ndetails. Extensive experiments demonstrate the superiority of DiTalker in terms\nof lip synchronization and speaking style controllability. Project Page:\nhttps://thenameishope.github.io/DiTalker/"}
{"id": "2508.06515", "pdf": "https://arxiv.org/pdf/2508.06515", "abs": "https://arxiv.org/abs/2508.06515", "authors": ["Minh Duc Chu", "Kshitij Pawar", "Zihao He", "Roxanna Sharifi", "Ross Sonnenblick", "Magdalayna Curry", "Laura D'Adamo", "Lindsay Young", "Stuart B Murray", "Kristina Lerman"], "title": "BigTokDetect: A Clinically-Informed Vision-Language Model Framework for Detecting Pro-Bigorexia Videos on TikTok", "categories": ["cs.CV"], "comment": null, "summary": "Social media platforms increasingly struggle to detect harmful content that\npromotes muscle dysmorphic behaviors, particularly pro-bigorexia content that\ndisproportionately affects adolescent males. Unlike traditional eating disorder\ndetection focused on the \"thin ideal,\" pro-bigorexia material masquerades as\nlegitimate fitness content through complex multimodal combinations of visual\ndisplays, coded language, and motivational messaging that evade text-based\ndetection systems. We address this challenge by developing BigTokDetect, a\nclinically-informed detection framework for identifying pro-bigorexia content\non TikTok. We introduce BigTok, the first expert-annotated multimodal dataset\nof over 2,200 TikTok videos labeled by clinical psychologists and psychiatrists\nacross five primary categories spanning body image, nutrition, exercise,\nsupplements, and masculinity. Through a comprehensive evaluation of\nstate-of-the-art vision language models, we achieve 0.829% accuracy on primary\ncategory classification and 0.690% on subcategory detection via domain-specific\nfinetuning. Our ablation studies demonstrate that multimodal fusion improves\nperformance by 5-10% over text-only approaches, with video features providing\nthe most discriminative signals. These findings establish new benchmarks for\nmultimodal harmful content detection and provide both the computational tools\nand methodological framework needed for scalable content moderation in\nspecialized mental health domains."}
{"id": "2508.06517", "pdf": "https://arxiv.org/pdf/2508.06517", "abs": "https://arxiv.org/abs/2508.06517", "authors": ["Haoran Xi", "Chen Liu", "Xiaolin Li"], "title": "Frequency Prior Guided Matching: A Data Augmentation Approach for Generalizable Semi-Supervised Polyp Segmentation", "categories": ["cs.CV"], "comment": "19 pages, 8 figures, 6 tables", "summary": "Automated polyp segmentation is essential for early diagnosis of colorectal\ncancer, yet developing robust models remains challenging due to limited\nannotated data and significant performance degradation under domain shift.\nAlthough semi-supervised learning (SSL) reduces annotation requirements,\nexisting methods rely on generic augmentations that ignore polyp-specific\nstructural properties, resulting in poor generalization to new imaging centers\nand devices. To address this, we introduce Frequency Prior Guided Matching\n(FPGM), a novel augmentation framework built on a key discovery: polyp edges\nexhibit a remarkably consistent frequency signature across diverse datasets.\nFPGM leverages this intrinsic regularity in a two-stage process. It first\nlearns a domain-invariant frequency prior from the edge regions of labeled\npolyps. Then, it performs principled spectral perturbations on unlabeled\nimages, aligning their amplitude spectra with this learned prior while\npreserving phase information to maintain structural integrity. This targeted\nalignment normalizes domain-specific textural variations, thereby compelling\nthe model to learn the underlying, generalizable anatomical structure.\nValidated on six public datasets, FPGM establishes a new state-of-the-art\nagainst ten competing methods. It demonstrates exceptional zero-shot\ngeneralization capabilities, achieving over 10% absolute gain in Dice score in\ndata-scarce scenarios. By significantly enhancing cross-domain robustness, FPGM\npresents a powerful solution for clinically deployable polyp segmentation under\nlimited supervision."}
{"id": "2508.06525", "pdf": "https://arxiv.org/pdf/2508.06525", "abs": "https://arxiv.org/abs/2508.06525", "authors": ["Guoyuan An", "JaeYoon Kim", "SungEui Yoon"], "title": "Large Language Models Facilitate Vision Reflection in Image Classification", "categories": ["cs.CV"], "comment": null, "summary": "This paper presents several novel findings on the explainability of vision\nreflection in large multimodal models (LMMs). First, we show that prompting an\nLMM to verify the prediction of a specialized vision model can improve\nrecognition accuracy, even on benchmarks like ImageNet, despite prior evidence\nthat LMMs typically underperform dedicated vision encoders. Second, we analyze\nthe internal behavior of vision reflection and find that the vision-language\nconnector maps visual features into explicit textual concepts, allowing the\nlanguage model to reason about prediction plausibility using commonsense\nknowledge. We further observe that replacing a large number of vision tokens\nwith only a few text tokens still enables LLaVA to generate similar answers,\nsuggesting that LMMs may rely primarily on a compact set of distilled textual\nrepresentations rather than raw vision features. Third, we show that a\ntraining-free connector can enhance LMM performance in fine-grained recognition\ntasks, without extensive feature-alignment training. Together, these findings\noffer new insights into the explainability of vision-language models and\nsuggest that vision reflection is a promising strategy for achieving robust and\ninterpretable visual recognition."}
{"id": "2508.06528", "pdf": "https://arxiv.org/pdf/2508.06528", "abs": "https://arxiv.org/abs/2508.06528", "authors": ["Xiuliang Zhang", "Tadiwa Elisha Nyamasvisva", "Chuntao Liu"], "title": "A Framework Combining 3D CNN and Transformer for Video-Based Behavior Recognition", "categories": ["cs.CV", "cs.AI"], "comment": "9 pages,6 figures", "summary": "Video-based behavior recognition is essential in fields such as public\nsafety, intelligent surveillance, and human-computer interaction. Traditional\n3D Convolutional Neural Network (3D CNN) effectively capture local\nspatiotemporal features but struggle with modeling long-range dependencies.\nConversely, Transformers excel at learning global contextual information but\nface challenges with high computational costs. To address these limitations, we\npropose a hybrid framework combining 3D CNN and Transformer architectures. The\n3D CNN module extracts low-level spatiotemporal features, while the Transformer\nmodule captures long-range temporal dependencies, with a fusion mechanism\nintegrating both representations. Evaluated on benchmark datasets, the proposed\nmodel outperforms traditional 3D CNN and standalone Transformers, achieving\nhigher recognition accuracy with manageable complexity. Ablation studies\nfurther validate the complementary strengths of the two modules. This hybrid\nframework offers an effective and scalable solution for video-based behavior\nrecognition."}
{"id": "2508.06529", "pdf": "https://arxiv.org/pdf/2508.06529", "abs": "https://arxiv.org/abs/2508.06529", "authors": ["Jiayuan Wang", "Q. M. Jonathan Wu", "Katsuya Suto", "Ning Zhang"], "title": "RMT-PPAD: Real-time Multi-task Learning for Panoptic Perception in Autonomous Driving", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Autonomous driving systems rely on panoptic driving perception that requires\nboth precision and real-time performance. In this work, we propose RMT-PPAD, a\nreal-time, transformer-based multi-task model that jointly performs object\ndetection, drivable area segmentation, and lane line segmentation. We introduce\na lightweight module, a gate control with an adapter to adaptively fuse shared\nand task-specific features, effectively alleviating negative transfer between\ntasks. Additionally, we design an adaptive segmentation decoder to learn the\nweights over multi-scale features automatically during the training stage. This\navoids the manual design of task-specific structures for different segmentation\ntasks. We also identify and resolve the inconsistency between training and\ntesting labels in lane line segmentation. This allows fairer evaluation.\nExperiments on the BDD100K dataset demonstrate that RMT-PPAD achieves\nstate-of-the-art results with mAP50 of 84.9% and Recall of 95.4% for object\ndetection, mIoU of 92.6% for drivable area segmentation, and IoU of 56.8% and\naccuracy of 84.7% for lane line segmentation. The inference speed reaches 32.6\nFPS. Moreover, we introduce real-world scenarios to evaluate RMT-PPAD\nperformance in practice. The results show that RMT-PPAD consistently delivers\nstable performance. The source codes and pre-trained models are released at\nhttps://github.com/JiayuanWang-JW/RMT-PPAD."}
{"id": "2508.06530", "pdf": "https://arxiv.org/pdf/2508.06530", "abs": "https://arxiv.org/abs/2508.06530", "authors": ["Ming-Kun Xie", "Jia-Hao Xiao", "Gang Niu", "Lei Feng", "Zhiqiang Kou", "Min-Ling Zhang", "Masashi Sugiyama"], "title": "What Makes \"Good\" Distractors for Object Hallucination Evaluation in Large Vision-Language Models?", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Large Vision-Language Models (LVLMs), empowered by the success of Large\nLanguage Models (LLMs), have achieved impressive performance across domains.\nDespite the great advances in LVLMs, they still suffer from the unavailable\nobject hallucination issue, which tends to generate objects inconsistent with\nthe image content. The most commonly used Polling-based Object Probing\nEvaluation (POPE) benchmark evaluates this issue by sampling negative\ncategories according to category-level statistics, \\textit{e.g.}, category\nfrequencies and co-occurrence. However, with the continuous advancement of\nLVLMs, the POPE benchmark has shown diminishing effectiveness in assessing\nobject hallucination, as it employs a simplistic sampling strategy that\noverlooks image-specific information and restricts distractors to negative\nobject categories only. In this paper, we introduce the Hallucination\nsearching-based Object Probing Evaluation (HOPE) benchmark, aiming to generate\nthe most misleading distractors (\\textit{i.e.}, non-existent objects or\nincorrect image descriptions) that can trigger hallucination in LVLMs, which\nserves as a means to more rigorously assess their immunity to hallucination. To\nexplore the image-specific information, the content-aware hallucination\nsearching leverages Contrastive Language-Image Pre-Training (CLIP) to\napproximate the predictive behavior of LVLMs by selecting negative objects with\nthe highest predicted likelihood as distractors. To expand the scope of\nhallucination assessment, the description-based hallucination searching\nconstructs highly misleading distractors by pairing true objects with false\ndescriptions. Experimental results show that HOPE leads to a precision drop of\nat least 9\\% and up to 23\\% across various state-of-the-art LVLMs,\nsignificantly outperforming POPE in exposing hallucination vulnerabilities. The\ncode is available at https://github.com/xiemk/HOPE."}
{"id": "2508.06535", "pdf": "https://arxiv.org/pdf/2508.06535", "abs": "https://arxiv.org/abs/2508.06535", "authors": ["Faisal Ahmed"], "title": "Transfer Learning with EfficientNet for Accurate Leukemia Cell Classification", "categories": ["cs.CV", "cs.LG", "F.2.2; I.2.7"], "comment": "8 pages, 1 figure", "summary": "Accurate classification of Acute Lymphoblastic Leukemia (ALL) from peripheral\nblood smear images is essential for early diagnosis and effective treatment\nplanning. This study investigates the use of transfer learning with pretrained\nconvolutional neural networks (CNNs) to improve diagnostic performance. To\naddress the class imbalance in the dataset of 3,631 Hematologic and 7,644 ALL\nimages, we applied extensive data augmentation techniques to create a balanced\ntraining set of 10,000 images per class. We evaluated several models, including\nResNet50, ResNet101, and EfficientNet variants B0, B1, and B3. EfficientNet-B3\nachieved the best results, with an F1-score of 94.30%, accuracy of 92.02%,\nandAUCof94.79%,outperformingpreviouslyreported methods in the C-NMCChallenge.\nThesefindings demonstrate the effectiveness of combining data augmentation with\nadvanced transfer learning models, particularly EfficientNet-B3, in developing\naccurate and robust diagnostic tools for hematologic malignancy detection."}
{"id": "2508.06537", "pdf": "https://arxiv.org/pdf/2508.06537", "abs": "https://arxiv.org/abs/2508.06537", "authors": ["Shantanusinh Parmar"], "title": "Benchmarking Deep Learning-Based Object Detection Models on Feature Deficient Astrophotography Imagery Dataset", "categories": ["cs.CV", "astro-ph.IM"], "comment": null, "summary": "Object detection models are typically trained on datasets like ImageNet,\nCOCO, and PASCAL VOC, which focus on everyday objects. However, these lack\nsignal sparsity found in non-commercial domains. MobilTelesco, a\nsmartphone-based astrophotography dataset, addresses this by providing sparse\nnight-sky images. We benchmark several detection models on it, highlighting\nchallenges under feature-deficient conditions."}
{"id": "2508.06543", "pdf": "https://arxiv.org/pdf/2508.06543", "abs": "https://arxiv.org/abs/2508.06543", "authors": ["Jinghan Yu", "Zhiyuan Ma", "Yue Ma", "Kaiqi Liu", "Yuhan Wang", "Jianjun Li"], "title": "MILD: Multi-Layer Diffusion Strategy for Complex and Precise Multi-IP Aware Human Erasing", "categories": ["cs.CV"], "comment": null, "summary": "Recent years have witnessed the success of diffusion models in\nimage-customized tasks. Prior works have achieved notable progress on\nhuman-oriented erasing using explicit mask guidance and semantic-aware\ninpainting. However, they struggle under complex multi-IP scenarios involving\nhuman-human occlusions, human-object entanglements, and background\ninterferences. These challenges are mainly due to: 1) Dataset limitations, as\nexisting datasets rarely cover dense occlusions, camouflaged backgrounds, and\ndiverse interactions; 2) Lack of spatial decoupling, where foreground instances\ncannot be effectively disentangled, limiting clean background restoration. In\nthis work, we introduce a high-quality multi-IP human erasing dataset with\ndiverse pose variations and complex backgrounds. We then propose Multi-Layer\nDiffusion (MILD), a novel strategy that decomposes generation into semantically\nseparated pathways for each instance and the background. To enhance\nhuman-centric understanding, we introduce Human Morphology Guidance,\nintegrating pose, parsing, and spatial relations. We further present\nSpatially-Modulated Attention to better guide attention flow. Extensive\nexperiments show that MILD outperforms state-of-the-art methods on challenging\nhuman erasing benchmarks."}
{"id": "2508.06546", "pdf": "https://arxiv.org/pdf/2508.06546", "abs": "https://arxiv.org/abs/2508.06546", "authors": ["Qi Xun Yeo", "Yanyan Li", "Gim Hee Lee"], "title": "Statistical Confidence Rescoring for Robust 3D Scene Graph Generation from Multi-View Images", "categories": ["cs.CV", "eess.IV"], "comment": "This paper has been accepted in ICCV 25", "summary": "Modern 3D semantic scene graph estimation methods utilize ground truth 3D\nannotations to accurately predict target objects, predicates, and\nrelationships. In the absence of given 3D ground truth representations, we\nexplore leveraging only multi-view RGB images to tackle this task. To attain\nrobust features for accurate scene graph estimation, we must overcome the noisy\nreconstructed pseudo point-based geometry from predicted depth maps and reduce\nthe amount of background noise present in multi-view image features. The key is\nto enrich node and edge features with accurate semantic and spatial information\nand through neighboring relations. We obtain semantic masks to guide feature\naggregation to filter background features and design a novel method to\nincorporate neighboring node information to aid robustness of our scene graph\nestimates. Furthermore, we leverage on explicit statistical priors calculated\nfrom the training summary statistics to refine node and edge predictions based\non their one-hop neighborhood. Our experiments show that our method outperforms\ncurrent methods purely using multi-view images as the initial input. Our\nproject page is available at https://qixun1.github.io/projects/SCRSSG."}
{"id": "2508.06551", "pdf": "https://arxiv.org/pdf/2508.06551", "abs": "https://arxiv.org/abs/2508.06551", "authors": ["Ye Tao"], "title": "Slice or the Whole Pie? Utility Control for AI Models", "categories": ["cs.CV"], "comment": null, "summary": "Training deep neural networks (DNNs) has become an increasingly\nresource-intensive task, requiring large volumes of labeled data, substantial\ncomputational power, and considerable fine-tuning efforts to achieve optimal\nperformance across diverse use cases. Although pre-trained models offer a\nuseful starting point, adapting them to meet specific user needs often demands\nextensive customization, and infrastructure overhead. This challenge grows when\na single model must support diverse appli-cations with differing requirements\nfor performance. Traditional solutions often involve training multiple model\nversions to meet varying requirements, which can be inefficient and difficult\nto maintain. In order to overcome this challenge, we propose NNObfuscator, a\nnovel utility control mechanism that enables AI models to dynamically modify\ntheir performance according to predefined conditions. It is different from\ntraditional methods that need separate models for each user. Instead,\nNNObfuscator allows a single model to be adapted in real time, giving you\ncontrolled access to multiple levels of performance. This mechanism enables\nmodel owners set up tiered access, ensuring that free-tier users receive a\nbaseline level of performance while premium users benefit from enhanced\ncapabilities. The approach improves resource allocation, reduces unnecessary\ncomputation, and supports sustainable business models in AI deployment. To\nvalidate our approach, we conducted experiments on multiple tasks, including\nimage classification, semantic segmentation, and text to image generation,\nusing well-established models such as ResNet, DeepLab, VGG16, FCN and Stable\nDiffusion. Experimental results show that NNObfuscator successfully makes model\nmore adaptable, so that a single trained model can handle a broad range of\ntasks without requiring a lot of changes."}
{"id": "2508.06552", "pdf": "https://arxiv.org/pdf/2508.06552", "abs": "https://arxiv.org/abs/2508.06552", "authors": ["Unisha Joshi"], "title": "Age-Diverse Deepfake Dataset: Bridging the Age Gap in Deepfake Detection", "categories": ["cs.CV", "cs.LG"], "comment": "11 pages, 4 figures, and 7 tables", "summary": "The challenges associated with deepfake detection are increasing\nsignificantly with the latest advancements in technology and the growing\npopularity of deepfake videos and images. Despite the presence of numerous\ndetection models, demographic bias in the deepfake dataset remains largely\nunaddressed. This paper focuses on the mitigation of age-specific bias in the\ndeepfake dataset by introducing an age-diverse deepfake dataset that will\nimprove fairness across age groups. The dataset is constructed through a\nmodular pipeline incorporating the existing deepfake datasets Celeb-DF,\nFaceForensics++, and UTKFace datasets, and the creation of synthetic data to\nfill the age distribution gaps. The effectiveness and generalizability of this\ndataset are evaluated using three deepfake detection models: XceptionNet,\nEfficientNet, and LipForensics. Evaluation metrics, including AUC, pAUC, and\nEER, revealed that models trained on the age-diverse dataset demonstrated\nfairer performance across age groups, improved overall accuracy, and higher\ngeneralization across datasets. This study contributes a reproducible,\nfairness-aware deepfake dataset and model pipeline that can serve as a\nfoundation for future research in fairer deepfake detection. The complete\ndataset and implementation code are available at\nhttps://github.com/unishajoshi/age-diverse-deepfake-detection."}
{"id": "2508.06553", "pdf": "https://arxiv.org/pdf/2508.06553", "abs": "https://arxiv.org/abs/2508.06553", "authors": ["Jiahao Xiao", "Jianbo Zhang", "BoWen Yan", "Shengyu Guo", "Tongrui Ye", "Kaiwei Zhang", "Zicheng Zhang", "Xiaohong Liu", "Zhengxue Cheng", "Lei Fan", "Chuyi Li", "Guangtao Zhai"], "title": "Static and Plugged: Make Embodied Evaluation Simple", "categories": ["cs.CV"], "comment": null, "summary": "Embodied intelligence is advancing rapidly, driving the need for efficient\nevaluation. Current benchmarks typically rely on interactive simulated\nenvironments or real-world setups, which are costly, fragmented, and hard to\nscale. To address this, we introduce StaticEmbodiedBench, a plug-and-play\nbenchmark that enables unified evaluation using static scene representations.\nCovering 42 diverse scenarios and 8 core dimensions, it supports scalable and\ncomprehensive assessment through a simple interface. Furthermore, we evaluate\n19 Vision-Language Models (VLMs) and 11 Vision-Language-Action models (VLAs),\nestablishing the first unified static leaderboard for Embodied intelligence.\nMoreover, we release a subset of 200 samples from our benchmark to accelerate\nthe development of embodied intelligence."}
{"id": "2508.06555", "pdf": "https://arxiv.org/pdf/2508.06555", "abs": "https://arxiv.org/abs/2508.06555", "authors": ["Hongbo Ma", "Fei Shen", "Hongbin Xu", "Xiaoce Wang", "Gang Xu", "Jinkai Zheng", "Liangqiong Qu", "Ming Li"], "title": "StyleTailor: Towards Personalized Fashion Styling via Hierarchical Negative Feedback", "categories": ["cs.CV", "cs.CY", "cs.MA"], "comment": "24pages, 5 figures", "summary": "The advancement of intelligent agents has revolutionized problem-solving\nacross diverse domains, yet solutions for personalized fashion styling remain\nunderexplored, which holds immense promise for promoting shopping experiences.\nIn this work, we present StyleTailor, the first collaborative agent framework\nthat seamlessly unifies personalized apparel design, shopping recommendation,\nvirtual try-on, and systematic evaluation into a cohesive workflow. To this\nend, StyleTailor pioneers an iterative visual refinement paradigm driven by\nmulti-level negative feedback, enabling adaptive and precise user alignment.\nSpecifically, our framework features two core agents, i.e., Designer for\npersonalized garment selection and Consultant for virtual try-on, whose outputs\nare progressively refined via hierarchical vision-language model feedback\nspanning individual items, complete outfits, and try-on efficacy.\nCounterexamples are aggregated into negative prompts, forming a closed-loop\nmechanism that enhances recommendation quality.To assess the performance, we\nintroduce a comprehensive evaluation suite encompassing style consistency,\nvisual quality, face similarity, and artistic appraisal. Extensive experiments\ndemonstrate StyleTailor's superior performance in delivering personalized\ndesigns and recommendations, outperforming strong baselines without negative\nfeedback and establishing a new benchmark for intelligent fashion systems."}
{"id": "2508.06556", "pdf": "https://arxiv.org/pdf/2508.06556", "abs": "https://arxiv.org/abs/2508.06556", "authors": ["Sarina Penquitt", "Jonathan Klees", "Rinor Cakaj", "Daniel Kondermann", "Matthias Rottmann", "Lars Schmarje"], "title": "From Label Error Detection to Correction: A Modular Framework and Benchmark for Object Detection Datasets", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Object detection has advanced rapidly in recent years, driven by increasingly\nlarge and diverse datasets. However, label errors, defined as missing labels,\nincorrect classification or inaccurate localization, often compromise the\nquality of these datasets. This can have a significant impact on the outcomes\nof training and benchmark evaluations. Although several methods now exist for\ndetecting label errors in object detection datasets, they are typically\nvalidated only on synthetic benchmarks or limited manual inspection. How to\ncorrect such errors systemically and at scale therefore remains an open\nproblem. We introduce a semi-automated framework for label-error correction\ncalled REC$\\checkmark$D (Rechecked). Building on existing detectors, the\nframework pairs their error proposals with lightweight, crowd-sourced\nmicrotasks. These tasks enable multiple annotators to independently verify each\ncandidate bounding box, and their responses are aggregated to estimate\nambiguity and improve label quality. To demonstrate the effectiveness of\nREC$\\checkmark$D, we apply it to the class pedestrian in the KITTI dataset. Our\ncrowdsourced review yields high-quality corrected annotations, which indicate a\nrate of at least 24% of missing and inaccurate annotations in original\nannotations. This validated set will be released as a new real-world benchmark\nfor label error detection and correction. We show that current label error\ndetection methods, when combined with our correction framework, can recover\nhundreds of errors in the time it would take a human to annotate bounding boxes\nfrom scratch. However, even the best methods still miss up to 66% of the true\nerrors and with low quality labels introduce more errors than they find. This\nhighlights the urgent need for further research, now enabled by our released\nbenchmark."}
{"id": "2508.06558", "pdf": "https://arxiv.org/pdf/2508.06558", "abs": "https://arxiv.org/abs/2508.06558", "authors": ["Simon Baur", "Alexandra Benova", "Emilio Dolgener Cantú", "Jackie Ma"], "title": "On the effectiveness of multimodal privileged knowledge distillation in two vision transformer based diagnostic applications", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Deploying deep learning models in clinical practice often requires leveraging\nmultiple data modalities, such as images, text, and structured data, to achieve\nrobust and trustworthy decisions. However, not all modalities are always\navailable at inference time. In this work, we propose multimodal privileged\nknowledge distillation (MMPKD), a training strategy that utilizes additional\nmodalities available solely during training to guide a unimodal vision model.\nSpecifically, we used a text-based teacher model for chest radiographs\n(MIMIC-CXR) and a tabular metadata-based teacher model for mammography\n(CBIS-DDSM) to distill knowledge into a vision transformer student model. We\nshow that MMPKD can improve the resulting attention maps' zero-shot\ncapabilities of localizing ROI in input images, while this effect does not\ngeneralize across domains, as contrarily suggested by prior research."}
{"id": "2508.06564", "pdf": "https://arxiv.org/pdf/2508.06564", "abs": "https://arxiv.org/abs/2508.06564", "authors": ["Guanyu Hu", "Dimitrios Kollias", "Xinyu Yang"], "title": "Grounding Emotion Recognition with Visual Prototypes: VEGA -- Revisiting CLIP in MERC", "categories": ["cs.CV"], "comment": "accepted for publication at ACM Multimedia (ACM MM) 2025", "summary": "Multimodal Emotion Recognition in Conversations remains a challenging task\ndue to the complex interplay of textual, acoustic and visual signals. While\nrecent models have improved performance via advanced fusion strategies, they\noften lack psychologically meaningful priors to guide multimodal alignment. In\nthis paper, we revisit the use of CLIP and propose a novel Visual Emotion\nGuided Anchoring (VEGA) mechanism that introduces class-level visual semantics\ninto the fusion and classification process. Distinct from prior work that\nprimarily utilizes CLIP's textual encoder, our approach leverages its image\nencoder to construct emotion-specific visual anchors based on facial exemplars.\nThese anchors guide unimodal and multimodal features toward a perceptually\ngrounded and psychologically aligned representation space, drawing inspiration\nfrom cognitive theories (prototypical emotion categories and multisensory\nintegration). A stochastic anchor sampling strategy further enhances robustness\nby balancing semantic stability and intra-class diversity. Integrated into a\ndual-branch architecture with self-distillation, our VEGA-augmented model\nachieves sota performance on IEMOCAP and MELD. Code is available at:\nhttps://github.com/dkollias/VEGA."}
{"id": "2508.06565", "pdf": "https://arxiv.org/pdf/2508.06565", "abs": "https://arxiv.org/abs/2508.06565", "authors": ["Jing Zhang", "Xiaowei Yu", "Minheng Chen", "Lu Zhang", "Tong Chen", "Yan Zhuang", "Chao Cao", "Yanjun Lyu", "Li Su", "Tianming Liu", "Dajiang Zhu"], "title": "Bridging Brain Connectomes and Clinical Reports for Early Alzheimer's Disease Diagnosis", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Integrating brain imaging data with clinical reports offers a valuable\nopportunity to leverage complementary multimodal information for more effective\nand timely diagnosis in practical clinical settings. This approach has gained\nsignificant attention in brain disorder research, yet a key challenge remains:\nhow to effectively link objective imaging data with subjective text-based\nreports, such as doctors' notes. In this work, we propose a novel framework\nthat aligns brain connectomes with clinical reports in a shared cross-modal\nlatent space at both the subject and connectome levels, thereby enhancing\nrepresentation learning. The key innovation of our approach is that we treat\nbrain subnetworks as tokens of imaging data, rather than raw image patches, to\nalign with word tokens in clinical reports. This enables a more efficient\nidentification of system-level associations between neuroimaging findings and\nclinical observations, which is critical since brain disorders often manifest\nas network-level abnormalities rather than isolated regional alterations. We\napplied our method to mild cognitive impairment (MCI) using the ADNI dataset.\nOur approach not only achieves state-of-the-art predictive performance but also\nidentifies clinically meaningful connectome-text pairs, offering new insights\ninto the early mechanisms of Alzheimer's disease and supporting the development\nof clinically useful multimodal biomarkers."}
{"id": "2508.06566", "pdf": "https://arxiv.org/pdf/2508.06566", "abs": "https://arxiv.org/abs/2508.06566", "authors": ["Manish Kansana", "Elias Hossain", "Shahram Rahimi", "Noorbakhsh Amiri Golilarz"], "title": "Surformer v1: Transformer-Based Surface Classification Using Tactile and Vision Features", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Surface material recognition is a key component in robotic perception and\nphysical interaction, particularly when leveraging both tactile and visual\nsensory inputs. In this work, we propose Surformer v1, a transformer-based\narchitecture designed for surface classification using structured tactile\nfeatures and PCA-reduced visual embeddings extracted via ResNet-50. The model\nintegrates modality-specific encoders with cross-modal attention layers,\nenabling rich interactions between vision and touch. Currently,\nstate-of-the-art deep learning models for vision tasks have achieved remarkable\nperformance. With this in mind, our first set of experiments focused\nexclusively on tactile-only surface classification. Using feature engineering,\nwe trained and evaluated multiple machine learning models, assessing their\naccuracy and inference time. We then implemented an encoder-only Transformer\nmodel tailored for tactile features. This model not only achieved the highest\naccuracy but also demonstrated significantly faster inference time compared to\nother evaluated models, highlighting its potential for real-time applications.\nTo extend this investigation, we introduced a multimodal fusion setup by\ncombining vision and tactile inputs. We trained both Surformer v1 (using\nstructured features) and Multimodal CNN (using raw images) to examine the\nimpact of feature-based versus image-based multimodal learning on\nclassification accuracy and computational efficiency. The results showed that\nSurformer v1 achieved 99.4% accuracy with an inference time of 0.77 ms, while\nthe Multimodal CNN achieved slightly higher accuracy but required significantly\nmore inference time. These findings suggest Surformer v1 offers a compelling\nbalance between accuracy, efficiency, and computational cost for surface\nmaterial recognition."}
{"id": "2508.06570", "pdf": "https://arxiv.org/pdf/2508.06570", "abs": "https://arxiv.org/abs/2508.06570", "authors": ["Mohammad Zia Ur Rehman", "Anukriti Bhatnagar", "Omkar Kabde", "Shubhi Bansal", "Nagendra Kumar"], "title": "ImpliHateVid: A Benchmark Dataset and Two-stage Contrastive Learning Framework for Implicit Hate Speech Detection in Videos", "categories": ["cs.CV", "cs.LG"], "comment": "Published in ACL 2025", "summary": "The existing research has primarily focused on text and image-based hate\nspeech detection, video-based approaches remain underexplored. In this work, we\nintroduce a novel dataset, ImpliHateVid, specifically curated for implicit hate\nspeech detection in videos. ImpliHateVid consists of 2,009 videos comprising\n509 implicit hate videos, 500 explicit hate videos, and 1,000 non-hate videos,\nmaking it one of the first large-scale video datasets dedicated to implicit\nhate detection. We also propose a novel two-stage contrastive learning\nframework for hate speech detection in videos. In the first stage, we train\nmodality-specific encoders for audio, text, and image using contrastive loss by\nconcatenating features from the three encoders. In the second stage, we train\ncross-encoders using contrastive learning to refine multimodal representations.\nAdditionally, we incorporate sentiment, emotion, and caption-based features to\nenhance implicit hate detection. We evaluate our method on two datasets,\nImpliHateVid for implicit hate speech detection and another dataset for general\nhate speech detection in videos, HateMM dataset, demonstrating the\neffectiveness of the proposed multimodal contrastive learning for hateful\ncontent detection in videos and the significance of our dataset."}
{"id": "2508.06623", "pdf": "https://arxiv.org/pdf/2508.06623", "abs": "https://arxiv.org/abs/2508.06623", "authors": ["Sihan Ma", "Qiming Wu", "Ruotong Jiang", "Frank Burns"], "title": "ContextGuard-LVLM: Enhancing News Veracity through Fine-grained Cross-modal Contextual Consistency Verification", "categories": ["cs.CV"], "comment": null, "summary": "The proliferation of digital news media necessitates robust methods for\nverifying content veracity, particularly regarding the consistency between\nvisual and textual information. Traditional approaches often fall short in\naddressing the fine-grained cross-modal contextual consistency (FCCC) problem,\nwhich encompasses deeper alignment of visual narrative, emotional tone, and\nbackground information with text, beyond mere entity matching. To address this,\nwe propose ContextGuard-LVLM, a novel framework built upon advanced\nVision-Language Large Models (LVLMs) and integrating a multi-stage contextual\nreasoning mechanism. Our model is uniquely enhanced through reinforced or\nadversarial learning paradigms, enabling it to detect subtle contextual\nmisalignments that evade zero-shot baselines. We extend and augment three\nestablished datasets (TamperedNews-Ent, News400-Ent, MMG-Ent) with new\nfine-grained contextual annotations, including \"contextual sentiment,\" \"visual\nnarrative theme,\" and \"scene-event logical coherence,\" and introduce a\ncomprehensive CTXT (Contextual Coherence) entity type. Extensive experiments\ndemonstrate that ContextGuard-LVLM consistently outperforms state-of-the-art\nzero-shot LVLM baselines (InstructBLIP and LLaVA 1.5) across nearly all\nfine-grained consistency tasks, showing significant improvements in complex\nlogical reasoning and nuanced contextual understanding. Furthermore, our model\nexhibits superior robustness to subtle perturbations and a higher agreement\nrate with human expert judgments on challenging samples, affirming its efficacy\nin discerning sophisticated forms of context detachment."}
{"id": "2508.06624", "pdf": "https://arxiv.org/pdf/2508.06624", "abs": "https://arxiv.org/abs/2508.06624", "authors": ["Kexin Yu", "Zihan Xu", "Jialei Xie", "Carter Adams"], "title": "VL-MedGuide: A Visual-Linguistic Large Model for Intelligent and Explainable Skin Disease Auxiliary Diagnosis", "categories": ["cs.CV"], "comment": null, "summary": "Accurate diagnosis of skin diseases remains a significant challenge due to\nthe complex and diverse visual features present in dermatoscopic images, often\ncompounded by a lack of interpretability in existing purely visual diagnostic\nmodels. To address these limitations, this study introduces VL-MedGuide\n(Visual-Linguistic Medical Guide), a novel framework leveraging the powerful\nmulti-modal understanding and reasoning capabilities of Visual-Language Large\nModels (LVLMs) for intelligent and inherently interpretable auxiliary diagnosis\nof skin conditions. VL-MedGuide operates in two interconnected stages: a\nMulti-modal Concept Perception Module, which identifies and linguistically\ndescribes dermatologically relevant visual features through sophisticated\nprompt engineering, and an Explainable Disease Reasoning Module, which\nintegrates these concepts with raw visual information via Chain-of-Thought\nprompting to provide precise disease diagnoses alongside transparent\nrationales. Comprehensive experiments on the Derm7pt dataset demonstrate that\nVL-MedGuide achieves state-of-the-art performance in both disease diagnosis\n(83.55% BACC, 80.12% F1) and concept detection (76.10% BACC, 67.45% F1),\nsurpassing existing baselines. Furthermore, human evaluations confirm the high\nclarity, completeness, and trustworthiness of its generated explanations,\nbridging the gap between AI performance and clinical utility by offering\nactionable, explainable insights for dermatological practice."}
{"id": "2508.06625", "pdf": "https://arxiv.org/pdf/2508.06625", "abs": "https://arxiv.org/abs/2508.06625", "authors": ["Shilong Zou", "Yuhang Huang", "Renjiao Yi", "Chenyang Zhu", "Kai Xu"], "title": "CycleDiff: Cycle Diffusion Models for Unpaired Image-to-image Translation", "categories": ["cs.CV"], "comment": null, "summary": "We introduce a diffusion-based cross-domain image translator in the absence\nof paired training data. Unlike GAN-based methods, our approach integrates\ndiffusion models to learn the image translation process, allowing for more\ncoverable modeling of the data distribution and performance improvement of the\ncross-domain translation. However, incorporating the translation process within\nthe diffusion process is still challenging since the two processes are not\naligned exactly, i.e., the diffusion process is applied to the noisy signal\nwhile the translation process is conducted on the clean signal. As a result,\nrecent diffusion-based studies employ separate training or shallow integration\nto learn the two processes, yet this may cause the local minimal of the\ntranslation optimization, constraining the effectiveness of diffusion models.\nTo address the problem, we propose a novel joint learning framework that aligns\nthe diffusion and the translation process, thereby improving the global\noptimality. Specifically, we propose to extract the image components with\ndiffusion models to represent the clean signal and employ the translation\nprocess with the image components, enabling an end-to-end joint learning\nmanner. On the other hand, we introduce a time-dependent translation network to\nlearn the complex translation mapping, resulting in effective translation\nlearning and significant performance improvement. Benefiting from the design of\njoint learning, our method enables global optimization of both processes,\nenhancing the optimality and achieving improved fidelity and structural\nconsistency. We have conducted extensive experiments on RGB$\\leftrightarrow$RGB\nand diverse cross-modality translation tasks including\nRGB$\\leftrightarrow$Edge, RGB$\\leftrightarrow$Semantics and\nRGB$\\leftrightarrow$Depth, showcasing better generative performances than the\nstate of the arts."}
{"id": "2508.06632", "pdf": "https://arxiv.org/pdf/2508.06632", "abs": "https://arxiv.org/abs/2508.06632", "authors": ["Wenpeng Xing", "Jie Chen", "Zaifeng Yang", "Tiancheng Zhao", "Gaolei Li", "Changting Lin", "Yike Guo", "Meng Han"], "title": "CoDe-NeRF: Neural Rendering via Dynamic Coefficient Decomposition", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Neural Radiance Fields (NeRF) have shown impressive performance in novel view\nsynthesis, but challenges remain in rendering scenes with complex specular\nreflections and highlights. Existing approaches may produce blurry reflections\ndue to entanglement between lighting and material properties, or encounter\noptimization instability when relying on physically-based inverse rendering. In\nthis work, we present a neural rendering framework based on dynamic coefficient\ndecomposition, aiming to improve the modeling of view-dependent appearance. Our\napproach decomposes complex appearance into a shared, static neural basis that\nencodes intrinsic material properties, and a set of dynamic coefficients\ngenerated by a Coefficient Network conditioned on view and illumination. A\nDynamic Radiance Integrator then combines these components to synthesize the\nfinal radiance. Experimental results on several challenging benchmarks suggest\nthat our method can produce sharper and more realistic specular highlights\ncompared to existing techniques. We hope that this decomposition paradigm can\nprovide a flexible and effective direction for modeling complex appearance in\nneural scene representations."}
{"id": "2508.06640", "pdf": "https://arxiv.org/pdf/2508.06640", "abs": "https://arxiv.org/abs/2508.06640", "authors": ["Zheyuan Zhang", "Weihao Tang", "Hong Chen"], "title": "Rethinking Key-frame-based Micro-expression Recognition: A Robust and Accurate Framework Against Key-frame Errors", "categories": ["cs.CV"], "comment": null, "summary": "Micro-expression recognition (MER) is a highly challenging task in affective\ncomputing. With the reduced-sized micro-expression (ME) input that contains key\ninformation based on key-frame indexes, key-frame-based methods have\nsignificantly improved the performance of MER. However, most of these methods\nfocus on improving the performance with relatively accurate key-frame indexes,\nwhile ignoring the difficulty of obtaining accurate key-frame indexes and the\nobjective existence of key-frame index errors, which impedes them from moving\ntowards practical applications. In this paper, we propose CausalNet, a novel\nframework to achieve robust MER facing key-frame index errors while maintaining\naccurate recognition. To enhance robustness, CausalNet takes the representation\nof the entire ME sequence as the input. To address the information redundancy\nbrought by the complete ME range input and maintain accurate recognition,\nfirst, the Causal Motion Position Learning Module (CMPLM) is proposed to help\nthe model locate the muscle movement areas related to Action Units (AUs),\nthereby reducing the attention to other redundant areas. Second, the Causal\nAttention Block (CAB) is proposed to deeply learn the causal relationships\nbetween the muscle contraction and relaxation movements in MEs. Empirical\nexperiments have demonstrated that on popular ME benchmarks, the CausalNet has\nachieved robust MER under different levels of key-frame index noise. Meanwhile,\nit has surpassed state-of-the-art (SOTA) methods on several standard MER\nbenchmarks when using the provided annotated key-frames. Code is available at\nhttps://github.com/tony19980810/CausalNet."}
{"id": "2508.06656", "pdf": "https://arxiv.org/pdf/2508.06656", "abs": "https://arxiv.org/abs/2508.06656", "authors": ["Denis Lukovnikov", "Andreas Müller", "Erwin Quiring", "Asja Fischer"], "title": "Towards Robust Red-Green Watermarking for Autoregressive Image Generators", "categories": ["cs.CV"], "comment": null, "summary": "In-generation watermarking for detecting and attributing generated content\nhas recently been explored for latent diffusion models (LDMs), demonstrating\nhigh robustness. However, the use of in-generation watermarks in autoregressive\n(AR) image models has not been explored yet. AR models generate images by\nautoregressively predicting a sequence of visual tokens that are then decoded\ninto pixels using a vector-quantized decoder. Inspired by red-green watermarks\nfor large language models, we examine token-level watermarking schemes that\nbias the next-token prediction based on prior tokens. We find that a direct\ntransfer of these schemes works in principle, but the detectability of the\nwatermarks decreases considerably under common image perturbations. As a\nremedy, we propose two novel watermarking methods that rely on visual token\nclustering to assign similar tokens to the same set. Firstly, we investigate a\ntraining-free approach that relies on a cluster lookup table, and secondly, we\nfinetune VAE encoders to predict token clusters directly from perturbed images.\nOverall, our experiments show that cluster-level watermarks improve robustness\nagainst perturbations and regeneration attacks while preserving image quality.\nCluster classification further boosts watermark detectability, outperforming a\nset of baselines. Moreover, our methods offer fast verification runtime,\ncomparable to lightweight post-hoc watermarking methods."}
{"id": "2508.06696", "pdf": "https://arxiv.org/pdf/2508.06696", "abs": "https://arxiv.org/abs/2508.06696", "authors": ["Tianqin Li", "George Liu", "Tai Sing Lee"], "title": "Learning More by Seeing Less: Line Drawing Pretraining for Efficient, Transferable, and Human-Aligned Vision", "categories": ["cs.CV"], "comment": null, "summary": "Despite remarkable progress in computer vision, modern recognition systems\nremain limited by their dependence on rich, redundant visual inputs. In\ncontrast, humans can effortlessly understand sparse, minimal representations\nlike line drawings - suggesting that structure, rather than appearance,\nunderlies efficient visual understanding. In this work, we propose using line\ndrawings as a structure-first pretraining modality to induce more compact and\ngeneralizable visual representations. We show that models pretrained on line\ndrawings develop stronger shape bias, more focused attention, and greater data\nefficiency across classification, detection, and segmentation tasks. Notably,\nthese models also exhibit lower intrinsic dimensionality, requiring\nsignificantly fewer principal components to capture representational variance -\nechoing the similar observation in low dimensional efficient representation in\nthe brain. Beyond performance improvements, line drawing pretraining produces\nmore compressible representations, enabling better distillation into\nlightweight student models. Students distilled from line-pretrained teachers\nconsistently outperform those trained from color-supervised teachers,\nhighlighting the benefits of structurally compact knowledge. Finally, we\ndemonstrate that the pretraining with line-drawing can also be extended to\nunsupervised setting via our proposed method \"learning to draw\". Together, our\nresults support the view that structure-first visual learning fosters\nefficiency, generalization, and human-aligned inductive biases - offering a\nsimple yet powerful strategy for building more robust and adaptable vision\nsystems."}
{"id": "2508.06701", "pdf": "https://arxiv.org/pdf/2508.06701", "abs": "https://arxiv.org/abs/2508.06701", "authors": ["Md Rezwanul Haque", "Md. Milon Islam", "S M Taslim Uddin Raju", "Hamdi Altaheri", "Lobna Nassar", "Fakhri Karray"], "title": "MMFformer: Multimodal Fusion Transformer Network for Depression Detection", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.SD"], "comment": "Accepted for the 2025 IEEE International Conference on Systems, Man,\n  and Cybernetics (SMC), Vienna, Austria", "summary": "Depression is a serious mental health illness that significantly affects an\nindividual's well-being and quality of life, making early detection crucial for\nadequate care and treatment. Detecting depression is often difficult, as it is\nbased primarily on subjective evaluations during clinical interviews. Hence,\nthe early diagnosis of depression, thanks to the content of social networks,\nhas become a prominent research area. The extensive and diverse nature of\nuser-generated information poses a significant challenge, limiting the accurate\nextraction of relevant temporal information and the effective fusion of data\nacross multiple modalities. This paper introduces MMFformer, a multimodal\ndepression detection network designed to retrieve depressive spatio-temporal\nhigh-level patterns from multimodal social media information. The transformer\nnetwork with residual connections captures spatial features from videos, and a\ntransformer encoder is exploited to design important temporal dynamics in\naudio. Moreover, the fusion architecture fused the extracted features through\nlate and intermediate fusion strategies to find out the most relevant\nintermodal correlations among them. Finally, the proposed network is assessed\non two large-scale depression detection datasets, and the results clearly\nreveal that it surpasses existing state-of-the-art approaches, improving the\nF1-Score by 13.92% for D-Vlog dataset and 7.74% for LMVD dataset. The code is\nmade available publicly at\nhttps://github.com/rezwanh001/Large-Scale-Multimodal-Depression-Detection."}
{"id": "2508.06703", "pdf": "https://arxiv.org/pdf/2508.06703", "abs": "https://arxiv.org/abs/2508.06703", "authors": ["Justin London"], "title": "Fourier Optics and Deep Learning Methods for Fast 3D Reconstruction in Digital Holography", "categories": ["cs.CV"], "comment": null, "summary": "Computer-generated holography (CGH) is a promising method that modulates\nuser-defined waveforms with digital holograms. An efficient and fast pipeline\nframework is proposed to synthesize CGH using initial point cloud and MRI data.\nThis input data is reconstructed into volumetric objects that are then input\ninto non-convex Fourier optics optimization algorithms for phase-only hologram\n(POH) and complex-hologram (CH) generation using alternating projection, SGD,\nand quasi-Netwton methods. Comparison of reconstruction performance of these\nalgorithms as measured by MSE, RMSE, and PSNR is analyzed as well as to HoloNet\ndeep learning CGH. Performance metrics are shown to be improved by using 2D\nmedian filtering to remove artifacts and speckled noise during optimization."}
{"id": "2508.06715", "pdf": "https://arxiv.org/pdf/2508.06715", "abs": "https://arxiv.org/abs/2508.06715", "authors": ["Jixuan He", "Chieh Hubert Lin", "Lu Qi", "Ming-Hsuan Yang"], "title": "Restage4D: Reanimating Deformable 3D Reconstruction from a Single Video", "categories": ["cs.CV"], "comment": null, "summary": "Creating deformable 3D content has gained increasing attention with the rise\nof text-to-image and image-to-video generative models. While these models\nprovide rich semantic priors for appearance, they struggle to capture the\nphysical realism and motion dynamics needed for authentic 4D scene synthesis.\nIn contrast, real-world videos can provide physically grounded geometry and\narticulation cues that are difficult to hallucinate. One question is raised:\n\\textit{Can we generate physically consistent 4D content by leveraging the\nmotion priors of the real-world video}? In this work, we explore the task of\nreanimating deformable 3D scenes from a single video, using the original\nsequence as a supervisory signal to correct artifacts from synthetic motion. We\nintroduce \\textbf{Restage4D}, a geometry-preserving pipeline for\nvideo-conditioned 4D restaging. Our approach uses a video-rewinding training\nstrategy to temporally bridge a real base video and a synthetic driving video\nvia a shared motion representation. We further incorporate an occlusion-aware\nrigidity loss and a disocclusion backtracing mechanism to improve structural\nand geometry consistency under challenging motion. We validate Restage4D on\nDAVIS and PointOdyssey, demonstrating improved geometry consistency, motion\nquality, and 3D tracking performance. Our method not only preserves deformable\nstructure under novel motion, but also automatically corrects errors introduced\nby generative models, revealing the potential of video prior in 4D restaging\ntask. Source code and trained models will be released."}
{"id": "2508.06756", "pdf": "https://arxiv.org/pdf/2508.06756", "abs": "https://arxiv.org/abs/2508.06756", "authors": ["Somayeh Farahani", "Marjaneh Hejazi", "Antonio Di Ieva", "Sidong Liu"], "title": "FoundBioNet: A Foundation-Based Model for IDH Genotyping of Glioma from Multi-Parametric MRI", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted for oral and poster presentation at MICCAI 2025", "summary": "Accurate, noninvasive detection of isocitrate dehydrogenase (IDH) mutation is\nessential for effective glioma management. Traditional methods rely on invasive\ntissue sampling, which may fail to capture a tumor's spatial heterogeneity.\nWhile deep learning models have shown promise in molecular profiling, their\nperformance is often limited by scarce annotated data. In contrast, foundation\ndeep learning models offer a more generalizable approach for glioma imaging\nbiomarkers. We propose a Foundation-based Biomarker Network (FoundBioNet) that\nutilizes a SWIN-UNETR-based architecture to noninvasively predict IDH mutation\nstatus from multi-parametric MRI. Two key modules are incorporated: Tumor-Aware\nFeature Encoding (TAFE) for extracting multi-scale, tumor-focused features, and\nCross-Modality Differential (CMD) for highlighting subtle T2-FLAIR mismatch\nsignals associated with IDH mutation. The model was trained and validated on a\ndiverse, multi-center cohort of 1705 glioma patients from six public datasets.\nOur model achieved AUCs of 90.58%, 88.08%, 65.41%, and 80.31% on independent\ntest sets from EGD, TCGA, Ivy GAP, RHUH, and UPenn, consistently outperforming\nbaseline approaches (p <= 0.05). Ablation studies confirmed that both the TAFE\nand CMD modules are essential for improving predictive accuracy. By integrating\nlarge-scale pretraining and task-specific fine-tuning, FoundBioNet enables\ngeneralizable glioma characterization. This approach enhances diagnostic\naccuracy and interpretability, with the potential to enable more personalized\npatient care."}
{"id": "2508.06757", "pdf": "https://arxiv.org/pdf/2508.06757", "abs": "https://arxiv.org/abs/2508.06757", "authors": ["Yash Garg", "Saketh Bachu", "Arindam Dutta", "Rohit Lal", "Sarosij Bose", "Calvin-Khang Ta", "M. Salman Asif", "Amit Roy-Chowdhury"], "title": "VOccl3D: A Video Benchmark Dataset for 3D Human Pose and Shape Estimation under real Occlusions", "categories": ["cs.CV", "cs.GR"], "comment": null, "summary": "Human pose and shape (HPS) estimation methods have been extensively studied,\nwith many demonstrating high zero-shot performance on in-the-wild images and\nvideos. However, these methods often struggle in challenging scenarios\ninvolving complex human poses or significant occlusions. Although some studies\naddress 3D human pose estimation under occlusion, they typically evaluate\nperformance on datasets that lack realistic or substantial occlusions, e.g.,\nmost existing datasets introduce occlusions with random patches over the human\nor clipart-style overlays, which may not reflect real-world challenges. To\nbridge this gap in realistic occlusion datasets, we introduce a novel benchmark\ndataset, VOccl3D, a Video-based human Occlusion dataset with 3D body pose and\nshape annotations. Inspired by works such as AGORA and BEDLAM, we constructed\nthis dataset using advanced computer graphics rendering techniques,\nincorporating diverse real-world occlusion scenarios, clothing textures, and\nhuman motions. Additionally, we fine-tuned recent HPS methods, CLIFF and\nBEDLAM-CLIFF, on our dataset, demonstrating significant qualitative and\nquantitative improvements across multiple public datasets, as well as on the\ntest split of our dataset, while comparing its performance with other\nstate-of-the-art methods. Furthermore, we leveraged our dataset to enhance\nhuman detection performance under occlusion by fine-tuning an existing object\ndetector, YOLO11, thus leading to a robust end-to-end HPS estimation system\nunder occlusions. Overall, this dataset serves as a valuable resource for\nfuture research aimed at benchmarking methods designed to handle occlusions,\noffering a more realistic alternative to existing occlusion datasets. See the\nProject page for code and dataset:https://yashgarg98.github.io/VOccl3D-dataset/"}
{"id": "2508.06763", "pdf": "https://arxiv.org/pdf/2508.06763", "abs": "https://arxiv.org/abs/2508.06763", "authors": ["Zihao Sheng", "Zilin Huang", "Yen-Jung Chen", "Yansong Qu", "Yuhao Luo", "Yue Leng", "Sikai Chen"], "title": "SafePLUG: Empowering Multimodal LLMs with Pixel-Level Insight and Temporal Grounding for Traffic Accident Understanding", "categories": ["cs.CV", "cs.AI"], "comment": "The code, dataset, and model checkpoints will be made publicly\n  available at: https://zihaosheng.github.io/SafePLUG", "summary": "Multimodal large language models (MLLMs) have achieved remarkable progress\nacross a range of vision-language tasks and demonstrate strong potential for\ntraffic accident understanding. However, existing MLLMs in this domain\nprimarily focus on coarse-grained image-level or video-level comprehension and\noften struggle to handle fine-grained visual details or localized scene\ncomponents, limiting their applicability in complex accident scenarios. To\naddress these limitations, we propose SafePLUG, a novel framework that empowers\nMLLMs with both Pixel-Level Understanding and temporal Grounding for\ncomprehensive traffic accident analysis. SafePLUG supports both\narbitrary-shaped visual prompts for region-aware question answering and\npixel-level segmentation based on language instructions, while also enabling\nthe recognition of temporally anchored events in traffic accident scenarios. To\nadvance the development of MLLMs for traffic accident understanding, we curate\na new dataset containing multimodal question-answer pairs centered on diverse\naccident scenarios, with detailed pixel-level annotations and temporal event\nboundaries. Experimental results show that SafePLUG achieves strong performance\non multiple tasks, including region-based question answering, pixel-level\nsegmentation, temporal event localization, and accident event understanding.\nThese capabilities lay a foundation for fine-grained understanding of complex\ntraffic scenes, with the potential to improve driving safety and enhance\nsituational awareness in smart transportation systems. The code, dataset, and\nmodel checkpoints will be made publicly available at:\nhttps://zihaosheng.github.io/SafePLUG"}
{"id": "2508.06768", "pdf": "https://arxiv.org/pdf/2508.06768", "abs": "https://arxiv.org/abs/2508.06768", "authors": ["Noe Bertramo", "Gabriel Duguey", "Vivek Gopalakrishnan"], "title": "DiffUS: Differentiable Ultrasound Rendering from Volumetric Imaging", "categories": ["cs.CV", "cs.GR"], "comment": "10 pages, accepted to MICCAI ASMUS 25", "summary": "Intraoperative ultrasound imaging provides real-time guidance during numerous\nsurgical procedures, but its interpretation is complicated by noise, artifacts,\nand poor alignment with high-resolution preoperative MRI/CT scans. To bridge\nthe gap between reoperative planning and intraoperative guidance, we present\nDiffUS, a physics-based, differentiable ultrasound renderer that synthesizes\nrealistic B-mode images from volumetric imaging. DiffUS first converts MRI 3D\nscans into acoustic impedance volumes using a machine learning approach. Next,\nwe simulate ultrasound beam propagation using ray tracing with coupled\nreflection-transmission equations. DiffUS formulates wave propagation as a\nsparse linear system that captures multiple internal reflections. Finally, we\nreconstruct B-mode images via depth-resolved echo extraction across fan-shaped\nacquisition geometry, incorporating realistic artifacts including speckle noise\nand depth-dependent degradation. DiffUS is entirely implemented as\ndifferentiable tensor operations in PyTorch, enabling gradient-based\noptimization for downstream applications such as slice-to-volume registration\nand volumetric reconstruction. Evaluation on the ReMIND dataset demonstrates\nDiffUS's ability to generate anatomically accurate ultrasound images from brain\nMRI data."}
{"id": "2508.06805", "pdf": "https://arxiv.org/pdf/2508.06805", "abs": "https://arxiv.org/abs/2508.06805", "authors": ["Aarav Mehta", "Priya Deshmukh", "Vikram Singh", "Siddharth Malhotra", "Krishnan Menon Iyer", "Tanvi Iyer"], "title": "Edge Detection for Organ Boundaries via Top Down Refinement and SubPixel Upsampling", "categories": ["cs.CV"], "comment": "MICCAIA Workshop", "summary": "Accurate localization of organ boundaries is critical in medical imaging for\nsegmentation, registration, surgical planning, and radiotherapy. While deep\nconvolutional networks (ConvNets) have advanced general-purpose edge detection\nto near-human performance on natural images, their outputs often lack precise\nlocalization, a limitation that is particularly harmful in medical applications\nwhere millimeter-level accuracy is required. Building on a systematic analysis\nof ConvNet edge outputs, we propose a medically focused crisp edge detector\nthat adapts a novel top-down backward refinement architecture to medical images\n(2D and volumetric). Our method progressively upsamples and fuses high-level\nsemantic features with fine-grained low-level cues through a backward\nrefinement pathway, producing high-resolution, well-localized organ boundaries.\nWe further extend the design to handle anisotropic volumes by combining 2D\nslice-wise refinement with light 3D context aggregation to retain computational\nefficiency. Evaluations on several CT and MRI organ datasets demonstrate\nsubstantially improved boundary localization under strict criteria (boundary\nF-measure, Hausdorff distance) compared to baseline ConvNet detectors and\ncontemporary medical edge/contour methods. Importantly, integrating our crisp\nedge maps into downstream pipelines yields consistent gains in organ\nsegmentation (higher Dice scores, lower boundary errors), more accurate image\nregistration, and improved delineation of lesions near organ interfaces. The\nproposed approach produces clinically valuable, crisp organ edges that\nmaterially enhance common medical-imaging tasks."}
{"id": "2508.06816", "pdf": "https://arxiv.org/pdf/2508.06816", "abs": "https://arxiv.org/abs/2508.06816", "authors": ["Vikram Singh", "Kabir Malhotra", "Rohan Desai", "Ananya Shankaracharya", "Priyadarshini Chatterjee", "Krishnan Menon Iyer"], "title": "DualResolution Residual Architecture with Artifact Suppression for Melanocytic Lesion Segmentation", "categories": ["cs.CV"], "comment": "MICCAIA", "summary": "Accurate segmentation of melanocytic tumors in dermoscopic images is a\ncritical step for automated skin cancer screening and clinical decision\nsupport. Unlike natural scene segmentation, lesion delineation must reconcile\nsubtle texture and color variations, frequent artifacts (hairs, rulers,\nbubbles), and a strong need for precise boundary localization to support\ndownstream diagnosis. In this paper we introduce Our method, a novel ResNet\ninspired dual resolution architecture specifically designed for melanocytic\ntumor segmentation. Our method maintains a full resolution stream that\npreserves fine grained boundary information while a complementary pooled stream\naggregates multi scale contextual cues for robust lesion recognition. The\nstreams are tightly coupled by boundary aware residual connections that inject\nhigh frequency edge information into deep feature maps, and by a channel\nattention module that adapts color and texture sensitivity to dermoscopic\nappearance. To further address common imaging artifacts and the limited size of\nclinical datasets, we propose a lightweight artifact suppression block and a\nmulti task training objective that combines a Dice Tversky segmentation loss\nwith an explicit boundary loss and a contrastive regularizer for feature\nstability. The combined design yields pixel accurate masks without requiring\nheavy post processing or complex pre training protocols. Extensive experiments\non public dermoscopic benchmarks demonstrate that Our method significantly\nimproves boundary adherence and clinically relevant segmentation metrics\ncompared to standard encoder decoder baselines, making it a practical building\nblock for automated melanoma assessment systems."}
{"id": "2508.06819", "pdf": "https://arxiv.org/pdf/2508.06819", "abs": "https://arxiv.org/abs/2508.06819", "authors": ["Ayaan Nooruddin Siddiqui", "Mahnoor Zaidi", "Ayesha Nazneen Shahbaz", "Priyadarshini Chatterjee", "Krishnan Menon Iyer"], "title": "VesselRW: Weakly Supervised Subcutaneous Vessel Segmentation via Learned Random Walk Propagation", "categories": ["cs.CV"], "comment": null, "summary": "Accurate segmentation of subcutaneous vessels from clinical images is\nhampered by scarce, expensive ground truth and by low contrast, noisy\nappearance of vessels across patients and modalities. We present a novel weakly\nsupervised training framework tailored for subcutaneous vessel segmentation\nthat leverages inexpensive sparse annotations (e.g., centerline traces, dot\nmarkers, or short scribbles). Sparse labels are expanded into dense,\nprobabilistic supervision via a differentiable random walk label propagation\nmodel whose transition weights incorporate image driven vesselness cues and\ntubular continuity priors. The propagation yields per-pixel hitting\nprobabilities together with calibrated uncertainty estimates; these are\nincorporated into an uncertainty weighted loss to avoid over fitting to\nambiguous regions. Crucially, the label-propagator is learned jointly with a\nCNN based segmentation predictor, enabling the system to discover vessel edges\nand continuity constraints without explicit edge supervision. We further\nintroduce a topology aware regularizer that encourages centerline connectivity\nand penalizes spurious branches, improving clinical usability. In experiments\non clinical subcutaneous imaging datasets, our method consistently outperforms\nnaive training on sparse labels and conventional dense pseudo-labeling,\nproducing more complete vascular maps and better calibrated uncertainty for\ndownstream decision making. The approach substantially reduces annotation\nburden while preserving clinically relevant vessel topology."}
{"id": "2508.06831", "pdf": "https://arxiv.org/pdf/2508.06831", "abs": "https://arxiv.org/abs/2508.06831", "authors": ["Taha Mustapha Nehdi", "Nairouz Mrabah", "Atif Belal", "Marco Pedersoli", "Eric Granger"], "title": "Low-Rank Expert Merging for Multi-Source Domain Adaptation in Person Re-Identification", "categories": ["cs.CV"], "comment": null, "summary": "Adapting person re-identification (reID) models to new target environments\nremains a challenging problem that is typically addressed using unsupervised\ndomain adaptation (UDA) methods. Recent works show that when labeled data\noriginates from several distinct sources (e.g., datasets and cameras),\nconsidering each source separately and applying multi-source domain adaptation\n(MSDA) typically yields higher accuracy and robustness compared to blending the\nsources and performing conventional UDA. However, state-of-the-art MSDA methods\nlearn domain-specific backbone models or require access to source domain data\nduring adaptation, resulting in significant growth in training parameters and\ncomputational cost. In this paper, a Source-free Adaptive Gated Experts\n(SAGE-reID) method is introduced for person reID. Our SAGE-reID is a\ncost-effective, source-free MSDA method that first trains individual\nsource-specific low-rank adapters (LoRA) through source-free UDA. Next, a\nlightweight gating network is introduced and trained to dynamically assign\noptimal merging weights for fusion of LoRA experts, enabling effective\ncross-domain knowledge transfer. While the number of backbone parameters\nremains constant across source domains, LoRA experts scale linearly but remain\nnegligible in size (<= 2% of the backbone), reducing both the memory\nconsumption and risk of overfitting. Extensive experiments conducted on three\nchallenging benchmarks: Market-1501, DukeMTMC-reID, and MSMT17 indicate that\nSAGE-reID outperforms state-of-the-art methods while being computationally\nefficient."}
{"id": "2508.06845", "pdf": "https://arxiv.org/pdf/2508.06845", "abs": "https://arxiv.org/abs/2508.06845", "authors": ["Hamidreza Samadi", "Md Manjurul Ahsan", "Shivakumar Raman"], "title": "Hybrid Machine Learning Framework for Predicting Geometric Deviations from 3D Surface Metrology", "categories": ["cs.CV"], "comment": null, "summary": "This study addresses the challenge of accurately forecasting geometric\ndeviations in manufactured components using advanced 3D surface analysis.\nDespite progress in modern manufacturing, maintaining dimensional precision\nremains difficult, particularly for complex geometries. We present a\nmethodology that employs a high-resolution 3D scanner to acquire multi-angle\nsurface data from 237 components produced across different batches. The data\nwere processed through precise alignment, noise reduction, and merging\ntechniques to generate accurate 3D representations. A hybrid machine learning\nframework was developed, combining convolutional neural networks for feature\nextraction with gradient-boosted decision trees for predictive modeling. The\nproposed system achieved a prediction accuracy of 0.012 mm at a 95% confidence\nlevel, representing a 73% improvement over conventional statistical process\ncontrol methods. In addition to improved accuracy, the model revealed hidden\ncorrelations between manufacturing parameters and geometric deviations. This\napproach offers significant potential for automated quality control, predictive\nmaintenance, and design optimization in precision manufacturing, and the\nresulting dataset provides a strong foundation for future predictive modeling\nresearch."}
{"id": "2508.06853", "pdf": "https://arxiv.org/pdf/2508.06853", "abs": "https://arxiv.org/abs/2508.06853", "authors": ["L. D. M. S. Sai Teja", "Ashok Urlana", "Pruthwik Mishra"], "title": "AGIC: Attention-Guided Image Captioning to Improve Caption Relevance", "categories": ["cs.CV", "cs.AI"], "comment": "10 pages, 5 Figures", "summary": "Despite significant progress in image captioning, generating accurate and\ndescriptive captions remains a long-standing challenge. In this study, we\npropose Attention-Guided Image Captioning (AGIC), which amplifies salient\nvisual regions directly in the feature space to guide caption generation. We\nfurther introduce a hybrid decoding strategy that combines deterministic and\nprobabilistic sampling to balance fluency and diversity. To evaluate AGIC, we\nconduct extensive experiments on the Flickr8k and Flickr30k datasets. The\nresults show that AGIC matches or surpasses several state-of-the-art models\nwhile achieving faster inference. Moreover, AGIC demonstrates strong\nperformance across multiple evaluation metrics, offering a scalable and\ninterpretable solution for image captioning."}
{"id": "2508.06857", "pdf": "https://arxiv.org/pdf/2508.06857", "abs": "https://arxiv.org/abs/2508.06857", "authors": ["Mengxue Jia", "Zhihua Allen-Zhao", "You Zhao", "Sanyang Liu"], "title": "A Joint Sparse Self-Representation Learning Method for Multiview Clustering", "categories": ["cs.CV", "cs.DS"], "comment": null, "summary": "Multiview clustering (MC) aims to group samples using consistent and\ncomplementary information across various views. The subspace clustering, as a\nfundamental technique of MC, has attracted significant attention. In this\npaper, we propose a novel joint sparse self-representation learning model for\nMC, where a featured difference is the extraction of view-specific local\ninformation by introducing cardinality (i.e., $\\ell_0$-norm) constraints\ninstead of Graph-Laplacian regularization. Specifically, under each view,\ncardinality constraints directly restrict the samples used in the\nself-representation stage to extract reliable local and global structure\ninformation, while the low-rank constraint aids in revealing a global coherent\nstructure in the consensus affinity matrix during merging. The attendant\nchallenge is that Augmented Lagrange Method (ALM)-based alternating\nminimization algorithms cannot guarantee convergence when applied directly to\nour nonconvex, nonsmooth model, thus resulting in poor generalization ability.\nTo address it, we develop an alternating quadratic penalty (AQP) method with\nglobal convergence, where two subproblems are iteratively solved by closed-form\nsolutions. Empirical results on six standard datasets demonstrate the\nsuperiority of our model and AQP method, compared to eight state-of-the-art\nalgorithms."}
{"id": "2508.06869", "pdf": "https://arxiv.org/pdf/2508.06869", "abs": "https://arxiv.org/abs/2508.06869", "authors": ["Jianxiang He", "Shaoguang Wang", "Weiyu Guo", "Meisheng Hong", "Jungang Li", "Yijie Xu", "Ziyang Chen", "Hui Xiong"], "title": "VSI: Visual Subtitle Integration for Keyframe Selection to enhance Long Video Understanding", "categories": ["cs.CV", "cs.AI", "I.2.10"], "comment": "9 pages,3 figures", "summary": "Long video understanding presents a significant challenge to multimodal large\nlanguage models (MLLMs) primarily due to the immense data scale. A critical and\nwidely adopted strategy for making this task computationally tractable is\nkeyframe retrieval, which seeks to identify a sparse set of video frames that\nare most salient to a given textual query. However, the efficacy of this\napproach is hindered by weak multimodal alignment between textual queries and\nvisual content and fails to capture the complex temporal semantic information\nrequired for precise reasoning. To address this, we propose Visual-Subtitle\nIntegeration(VSI), a multimodal keyframe search method that integrates\nsubtitles, timestamps, and scene boundaries into a unified multimodal search\nprocess. The proposed method captures the visual information of video frames as\nwell as the complementary textual information through a dual-stream search\nmechanism by Video Search Stream as well as Subtitle Match Stream,\nrespectively, and improves the keyframe search accuracy through the interaction\nof the two search streams. Experimental results show that VSI achieve 40.00%\nkey frame localization accuracy on the text-relevant subset of LongVideoBench\nand 68.48% accuracy on downstream long Video-QA tasks, surpassing competitive\nbaselines by 20.35% and 15.79%, respectively. Furthermore, on the\nLongVideoBench, VSI achieved state-of-the-art(SOTA) in medium-to-long video-QA\ntasks, demonstrating the robustness and generalizability of the proposed\nmultimodal search strategy."}
{"id": "2508.06874", "pdf": "https://arxiv.org/pdf/2508.06874", "abs": "https://arxiv.org/abs/2508.06874", "authors": ["Shisheng Zhang", "Ramtin Gharleghi", "Sonit Singh", "Daniel Moses", "Dona Adikari", "Arcot Sowmya", "Susann Beier"], "title": "LWT-ARTERY-LABEL: A Lightweight Framework for Automated Coronary Artery Identification", "categories": ["cs.CV"], "comment": null, "summary": "Coronary artery disease (CAD) remains the leading cause of death globally,\nwith computed tomography coronary angiography (CTCA) serving as a key\ndiagnostic tool. However, coronary arterial analysis using CTCA, such as\nidentifying artery-specific features from computational modelling, is\nlabour-intensive and time-consuming. Automated anatomical labelling of coronary\narteries offers a potential solution, yet the inherent anatomical variability\nof coronary trees presents a significant challenge. Traditional knowledge-based\nlabelling methods fall short in leveraging data-driven insights, while recent\ndeep-learning approaches often demand substantial computational resources and\noverlook critical clinical knowledge. To address these limitations, we propose\na lightweight method that integrates anatomical knowledge with rule-based\ntopology constraints for effective coronary artery labelling. Our approach\nachieves state-of-the-art performance on benchmark datasets, providing a\npromising alternative for automated coronary artery labelling."}
{"id": "2508.06878", "pdf": "https://arxiv.org/pdf/2508.06878", "abs": "https://arxiv.org/abs/2508.06878", "authors": ["Maoxun Yuan", "Duanni Meng", "Ziteng Xi", "Tianyi Zhao", "Shiji Zhao", "Yimian Dai", "Xingxing Wei"], "title": "NS-FPN: Improving Infrared Small Target Detection and Segmentation from Noise Suppression Perspective", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Infrared small target detection and segmentation (IRSTDS) is a critical yet\nchallenging task in defense and civilian applications, owing to the dim,\nshapeless appearance of targets and severe background clutter. Recent CNN-based\nmethods have achieved promising target perception results, but they only focus\non enhancing feature representation to offset the impact of noise, which\nresults in the increased false alarms problem. In this paper, through analyzing\nthe problem from the frequency domain, we pioneer in improving performance from\nnoise suppression perspective and propose a novel noise-suppression feature\npyramid network (NS-FPN), which integrates a low-frequency guided feature\npurification (LFP) module and a spiral-aware feature sampling (SFS) module into\nthe original FPN structure. The LFP module suppresses the noise features by\npurifying high-frequency components to achieve feature enhancement devoid of\nnoise interference, while the SFS module further adopts spiral sampling to fuse\ntarget-relevant features in feature fusion process. Our NS-FPN is designed to\nbe lightweight yet effective and can be easily plugged into existing IRSTDS\nframeworks. Extensive experiments on the public IRSTDS datasets demonstrate\nthat our method significantly reduces false alarms and achieves superior\nperformance on IRSTDS tasks."}
{"id": "2508.06891", "pdf": "https://arxiv.org/pdf/2508.06891", "abs": "https://arxiv.org/abs/2508.06891", "authors": ["Melika Filvantorkaman", "Mohsen Piri", "Maral Filvan Torkaman", "Ashkan Zabihi", "Hamidreza Moradi"], "title": "Fusion-Based Brain Tumor Classification Using Deep Learning and Explainable AI, and Rule-Based Reasoning", "categories": ["cs.CV"], "comment": "37 pages, 6 figures", "summary": "Accurate and interpretable classification of brain tumors from magnetic\nresonance imaging (MRI) is critical for effective diagnosis and treatment\nplanning. This study presents an ensemble-based deep learning framework that\ncombines MobileNetV2 and DenseNet121 convolutional neural networks (CNNs) using\na soft voting strategy to classify three common brain tumor types: glioma,\nmeningioma, and pituitary adenoma. The models were trained and evaluated on the\nFigshare dataset using a stratified 5-fold cross-validation protocol. To\nenhance transparency and clinical trust, the framework integrates an\nExplainable AI (XAI) module employing Grad-CAM++ for class-specific saliency\nvisualization, alongside a symbolic Clinical Decision Rule Overlay (CDRO) that\nmaps predictions to established radiological heuristics. The ensemble\nclassifier achieved superior performance compared to individual CNNs, with an\naccuracy of 91.7%, precision of 91.9%, recall of 91.7%, and F1-score of 91.6%.\nGrad-CAM++ visualizations revealed strong spatial alignment between model\nattention and expert-annotated tumor regions, supported by Dice coefficients up\nto 0.88 and IoU scores up to 0.78. Clinical rule activation further validated\nmodel predictions in cases with distinct morphological features. A\nhuman-centered interpretability assessment involving five board-certified\nradiologists yielded high Likert-scale scores for both explanation usefulness\n(mean = 4.4) and heatmap-region correspondence (mean = 4.0), reinforcing the\nframework's clinical relevance. Overall, the proposed approach offers a robust,\ninterpretable, and generalizable solution for automated brain tumor\nclassification, advancing the integration of deep learning into clinical\nneurodiagnostics."}
{"id": "2508.06895", "pdf": "https://arxiv.org/pdf/2508.06895", "abs": "https://arxiv.org/abs/2508.06895", "authors": ["Jianting Tang", "Yubo Wang", "Haoyu Cao", "Linli Xu"], "title": "BASIC: Boosting Visual Alignment with Intrinsic Refined Embeddings in Multimodal Large Language Models", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to ICCV 2025", "summary": "Mainstream Multimodal Large Language Models (MLLMs) achieve visual\nunderstanding by using a vision projector to bridge well-pretrained vision\nencoders and large language models (LLMs). The inherent gap between visual and\ntextual modalities makes the embeddings from the vision projector critical for\nvisual comprehension. However, current alignment approaches treat visual\nembeddings as contextual cues and merely apply auto-regressive supervision to\ntextual outputs, neglecting the necessity of introducing equivalent direct\nvisual supervision, which hinders the potential finer alignment of visual\nembeddings. In this paper, based on our analysis of the refinement process of\nvisual embeddings in the LLM's shallow layers, we propose BASIC, a method that\nutilizes refined visual embeddings within the LLM as supervision to directly\nguide the projector in generating initial visual embeddings. Specifically, the\nguidance is conducted from two perspectives: (i) optimizing embedding\ndirections by reducing angles between initial and supervisory embeddings in\nsemantic space; (ii) improving semantic matching by minimizing disparities\nbetween the logit distributions of both visual embeddings. Without additional\nsupervisory models or artificial annotations, BASIC significantly improves the\nperformance of MLLMs across a wide range of benchmarks, demonstrating the\neffectiveness of our introduced direct visual supervision."}
{"id": "2508.06900", "pdf": "https://arxiv.org/pdf/2508.06900", "abs": "https://arxiv.org/abs/2508.06900", "authors": ["Weiran Chen", "Guiqian Zhu", "Ying Li", "Yi Ji", "Chunping Liu"], "title": "Advancements in Chinese font generation since deep learning era: A survey", "categories": ["cs.CV", "cs.AI"], "comment": "42 Pages, 25 figures", "summary": "Chinese font generation aims to create a new Chinese font library based on\nsome reference samples. It is a topic of great concern to many font designers\nand typographers. Over the past years, with the rapid development of deep\nlearning algorithms, various new techniques have achieved flourishing and\nthriving progress. Nevertheless, how to improve the overall quality of\ngenerated Chinese character images remains a tough issue. In this paper, we\nconduct a holistic survey of the recent Chinese font generation approaches\nbased on deep learning. To be specific, we first illustrate the research\nbackground of the task. Then, we outline our literature selection and analysis\nmethodology, and review a series of related fundamentals, including classical\ndeep learning architectures, font representation formats, public datasets, and\nfrequently-used evaluation metrics. After that, relying on the number of\nreference samples required to generate a new font, we categorize the existing\nmethods into two major groups: many-shot font generation and few-shot font\ngeneration methods. Within each category, representative approaches are\nsummarized, and their strengths and limitations are also discussed in detail.\nFinally, we conclude our paper with the challenges and future directions, with\nthe expectation to provide some valuable illuminations for the researchers in\nthis field."}
{"id": "2508.06902", "pdf": "https://arxiv.org/pdf/2508.06902", "abs": "https://arxiv.org/abs/2508.06902", "authors": ["Xuecheng Wu", "Dingkang Yang", "Danlei Huang", "Xinyi Yin", "Yifan Wang", "Jia Zhang", "Jiayu Nie", "Liangyu Fu", "Yang Liu", "Junxiao Xue", "Hadi Amirpour", "Wei Zhou"], "title": "eMotions: A Large-Scale Dataset and Audio-Visual Fusion Network for Emotion Analysis in Short-form Videos", "categories": ["cs.CV"], "comment": null, "summary": "Short-form videos (SVs) have become a vital part of our online routine for\nacquiring and sharing information. Their multimodal complexity poses new\nchallenges for video analysis, highlighting the need for video emotion analysis\n(VEA) within the community. Given the limited availability of SVs emotion data,\nwe introduce eMotions, a large-scale dataset consisting of 27,996 videos with\nfull-scale annotations. To ensure quality and reduce subjective bias, we\nemphasize better personnel allocation and propose a multi-stage annotation\nprocedure. Additionally, we provide the category-balanced and test-oriented\nvariants through targeted sampling to meet diverse needs. While there have been\nsignificant studies on videos with clear emotional cues (e.g., facial\nexpressions), analyzing emotions in SVs remains a challenging task. The\nchallenge arises from the broader content diversity, which introduces more\ndistinct semantic gaps and complicates the representations learning of\nemotion-related features. Furthermore, the prevalence of audio-visual\nco-expressions in SVs leads to the local biases and collective information gaps\ncaused by the inconsistencies in emotional expressions. To tackle this, we\npropose AV-CANet, an end-to-end audio-visual fusion network that leverages\nvideo transformer to capture semantically relevant representations. We further\nintroduce the Local-Global Fusion Module designed to progressively capture the\ncorrelations of audio-visual features. Besides, EP-CE Loss is constructed to\nglobally steer optimizations with tripolar penalties. Extensive experiments\nacross three eMotions-related datasets and four public VEA datasets demonstrate\nthe effectiveness of our proposed AV-CANet, while providing broad insights for\nfuture research. Moreover, we conduct ablation studies to examine the critical\ncomponents of our method. Dataset and code will be made available at Github."}
{"id": "2508.06904", "pdf": "https://arxiv.org/pdf/2508.06904", "abs": "https://arxiv.org/abs/2508.06904", "authors": ["Chao Yin", "Jide Li", "Xiaoqiang Li"], "title": "A Simple yet Powerful Instance-Aware Prompting Framework for Training-free Camouflaged Object Segmentation", "categories": ["cs.CV"], "comment": "under review", "summary": "Camouflaged Object Segmentation (COS) remains highly challenging due to the\nintrinsic visual similarity between target objects and their surroundings.\nWhile training-based COS methods achieve good performance, their performance\ndegrades rapidly with increased annotation sparsity. To circumvent this\nlimitation, recent studies have explored training-free COS methods, leveraging\nthe Segment Anything Model (SAM) by automatically generating visual prompts\nfrom a single task-generic prompt (\\textit{e.g.}, \"\\textit{camouflaged\nanimal}\") uniformly applied across all test images. However, these methods\ntypically produce only semantic-level visual prompts, causing SAM to output\ncoarse semantic masks and thus failing to handle scenarios with multiple\ndiscrete camouflaged instances effectively. To address this critical\nlimitation, we propose a simple yet powerful \\textbf{I}nstance-\\textbf{A}ware\n\\textbf{P}rompting \\textbf{F}ramework (IAPF), the first training-free COS\npipeline that explicitly converts a task-generic prompt into fine-grained\ninstance masks. Specifically, the IAPF comprises three steps: (1) Text Prompt\nGenerator, utilizing task-generic queries to prompt a Multimodal Large Language\nModel (MLLM) for generating image-specific foreground and background tags; (2)\n\\textbf{Instance Mask Generator}, leveraging Grounding DINO to produce precise\ninstance-level bounding box prompts, alongside the proposed Single-Foreground\nMulti-Background Prompting strategy to sample region-constrained point prompts\nwithin each box, enabling SAM to yield a candidate instance mask; (3)\nSelf-consistency Instance Mask Voting, which selects the final COS prediction\nby identifying the candidate mask most consistent across multiple candidate\ninstance masks. Extensive evaluations on standard COS benchmarks demonstrate\nthat the proposed IAPF significantly surpasses existing state-of-the-art\ntraining-free COS methods."}
{"id": "2508.06905", "pdf": "https://arxiv.org/pdf/2508.06905", "abs": "https://arxiv.org/abs/2508.06905", "authors": ["Ruoxi Chen", "Dongping Chen", "Siyuan Wu", "Sinan Wang", "Shiyun Lang", "Petr Sushko", "Gaoyang Jiang", "Yao Wan", "Ranjay Krishna"], "title": "MultiRef: Controllable Image Generation with Multiple Visual References", "categories": ["cs.CV"], "comment": "Accepted to ACM MM 2025 Datasets", "summary": "Visual designers naturally draw inspiration from multiple visual references,\ncombining diverse elements and aesthetic principles to create artwork. However,\ncurrent image generative frameworks predominantly rely on single-source inputs\n-- either text prompts or individual reference images. In this paper, we focus\non the task of controllable image generation using multiple visual references.\nWe introduce MultiRef-bench, a rigorous evaluation framework comprising 990\nsynthetic and 1,000 real-world samples that require incorporating visual\ncontent from multiple reference images. The synthetic samples are synthetically\ngenerated through our data engine RefBlend, with 10 reference types and 33\nreference combinations. Based on RefBlend, we further construct a dataset\nMultiRef containing 38k high-quality images to facilitate further research. Our\nexperiments across three interleaved image-text models (i.e., OmniGen, ACE, and\nShow-o) and six agentic frameworks (e.g., ChatDiT and LLM + SD) reveal that\neven state-of-the-art systems struggle with multi-reference conditioning, with\nthe best model OmniGen achieving only 66.6% in synthetic samples and 79.0% in\nreal-world cases on average compared to the golden answer. These findings\nprovide valuable directions for developing more flexible and human-like\ncreative tools that can effectively integrate multiple sources of visual\ninspiration. The dataset is publicly available at: https://multiref.github.io/."}
{"id": "2508.06908", "pdf": "https://arxiv.org/pdf/2508.06908", "abs": "https://arxiv.org/abs/2508.06908", "authors": ["Jinhao Li", "Zijian Chen", "Lirong Deng", "Changbo Wang", "Guangtao Zhai"], "title": "MMReID-Bench: Unleashing the Power of MLLMs for Effective and Versatile Person Re-identification", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Person re-identification (ReID) aims to retrieve the images of an interested\nperson in the gallery images, with wide applications in medical rehabilitation,\nabnormal behavior detection, and public security. However, traditional person\nReID models suffer from uni-modal capability, leading to poor generalization\nability in multi-modal data, such as RGB, thermal, infrared, sketch images,\ntextual descriptions, etc. Recently, the emergence of multi-modal large\nlanguage models (MLLMs) shows a promising avenue for addressing this problem.\nDespite this potential, existing methods merely regard MLLMs as feature\nextractors or caption generators, which do not fully unleash their reasoning,\ninstruction-following, and cross-modal understanding capabilities. To bridge\nthis gap, we introduce MMReID-Bench, the first multi-task multi-modal benchmark\nspecifically designed for person ReID. The MMReID-Bench includes 20,710\nmulti-modal queries and gallery images covering 10 different person ReID tasks.\nComprehensive experiments demonstrate the remarkable capabilities of MLLMs in\ndelivering effective and versatile person ReID. Nevertheless, they also have\nlimitations in handling a few modalities, particularly thermal and infrared\ndata. We hope MMReID-Bench can facilitate the community to develop more robust\nand generalizable multimodal foundation models for person ReID."}
{"id": "2508.06916", "pdf": "https://arxiv.org/pdf/2508.06916", "abs": "https://arxiv.org/abs/2508.06916", "authors": ["Shichao Ma", "Yunhe Guo", "Jiahao Su", "Qihe Huang", "Zhengyang Zhou", "Yang Wang"], "title": "Talk2Image: A Multi-Agent System for Multi-Turn Image Generation and Editing", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-image generation tasks have driven remarkable advances in diverse\nmedia applications, yet most focus on single-turn scenarios and struggle with\niterative, multi-turn creative tasks. Recent dialogue-based systems attempt to\nbridge this gap, but their single-agent, sequential paradigm often causes\nintention drift and incoherent edits. To address these limitations, we present\nTalk2Image, a novel multi-agent system for interactive image generation and\nediting in multi-turn dialogue scenarios. Our approach integrates three key\ncomponents: intention parsing from dialogue history, task decomposition and\ncollaborative execution across specialized agents, and feedback-driven\nrefinement based on a multi-view evaluation mechanism. Talk2Image enables\nstep-by-step alignment with user intention and consistent image editing.\nExperiments demonstrate that Talk2Image outperforms existing baselines in\ncontrollability, coherence, and user satisfaction across iterative image\ngeneration and editing tasks."}
{"id": "2508.06924", "pdf": "https://arxiv.org/pdf/2508.06924", "abs": "https://arxiv.org/abs/2508.06924", "authors": ["Shihao Yuan", "Yahui Liu", "Yang Yue", "Jingyuan Zhang", "Wangmeng Zuo", "Qi Wang", "Fuzheng Zhang", "Guorui Zhou"], "title": "AR-GRPO: Training Autoregressive Image Generation Models via Reinforcement Learning", "categories": ["cs.CV"], "comment": "27 pages, 15 figures", "summary": "Inspired by the success of reinforcement learning (RL) in refining large\nlanguage models (LLMs), we propose AR-GRPO, an approach to integrate online RL\ntraining into autoregressive (AR) image generation models. We adapt the Group\nRelative Policy Optimization (GRPO) algorithm to refine the vanilla\nautoregressive models' outputs by carefully designed reward functions that\nevaluate generated images across multiple quality dimensions, including\nperceptual quality, realism, and semantic fidelity. We conduct comprehensive\nexperiments on both class-conditional (i.e., class-to-image) and\ntext-conditional (i.e., text-to-image) image generation tasks, demonstrating\nthat our RL-enhanced framework significantly improves both the image quality\nand human preference of generated images compared to the standard AR baselines.\nOur results show consistent improvements across various evaluation metrics,\nestablishing the viability of RL-based optimization for AR image generation and\nopening new avenues for controllable and high-quality image synthesis. The\nsource codes and models are available at:\nhttps://github.com/Kwai-Klear/AR-GRPO."}
{"id": "2508.06937", "pdf": "https://arxiv.org/pdf/2508.06937", "abs": "https://arxiv.org/abs/2508.06937", "authors": ["Weiyan Xie", "Han Gao", "Didan Deng", "Kaican Li", "April Hua Liu", "Yongxiang Huang", "Nevin L. Zhang"], "title": "CannyEdit: Selective Canny Control and Dual-Prompt Guidance for Training-Free Image Editing", "categories": ["cs.CV", "cs.AI"], "comment": "Project Page: vaynexie.github.io/CannyEdit/", "summary": "Recent advances in text-to-image (T2I) models have enabled training-free\nregional image editing by leveraging the generative priors of foundation\nmodels. However, existing methods struggle to balance text adherence in edited\nregions, context fidelity in unedited areas, and seamless integration of edits.\nWe introduce CannyEdit, a novel training-free framework that addresses these\nchallenges through two key innovations: (1) Selective Canny Control, which\nmasks the structural guidance of Canny ControlNet in user-specified editable\nregions while strictly preserving details of the source images in unedited\nareas via inversion-phase ControlNet information retention. This enables\nprecise, text-driven edits without compromising contextual integrity. (2)\nDual-Prompt Guidance, which combines local prompts for object-specific edits\nwith a global target prompt to maintain coherent scene interactions. On\nreal-world image editing tasks (addition, replacement, removal), CannyEdit\noutperforms prior methods like KV-Edit, achieving a 2.93 to 10.49 percent\nimprovement in the balance of text adherence and context fidelity. In terms of\nediting seamlessness, user studies reveal only 49.2 percent of general users\nand 42.0 percent of AIGC experts identified CannyEdit's results as AI-edited\nwhen paired with real images without edits, versus 76.08 to 89.09 percent for\ncompetitor methods."}
{"id": "2508.06951", "pdf": "https://arxiv.org/pdf/2508.06951", "abs": "https://arxiv.org/abs/2508.06951", "authors": ["Harry Walsh", "Ed Fish", "Ozge Mercanoglu Sincan", "Mohamed Ilyes Lakhal", "Richard Bowden", "Neil Fox", "Bencie Woll", "Kepeng Wu", "Zecheng Li", "Weichao Zhao", "Haodong Wang", "Wengang Zhou", "Houqiang Li", "Shengeng Tang", "Jiayi He", "Xu Wang", "Ruobei Zhang", "Yaxiong Wang", "Lechao Cheng", "Meryem Tasyurek", "Tugce Kiziltepe", "Hacer Yalim Keles"], "title": "SLRTP2025 Sign Language Production Challenge: Methodology, Results, and Future Work", "categories": ["cs.CV", "eess.IV", "eess.SP"], "comment": "11 pages, 6 Figures, CVPR conference", "summary": "Sign Language Production (SLP) is the task of generating sign language video\nfrom spoken language inputs. The field has seen a range of innovations over the\nlast few years, with the introduction of deep learning-based approaches\nproviding significant improvements in the realism and naturalness of generated\noutputs. However, the lack of standardized evaluation metrics for SLP\napproaches hampers meaningful comparisons across different systems. To address\nthis, we introduce the first Sign Language Production Challenge, held as part\nof the third SLRTP Workshop at CVPR 2025. The competition's aims are to\nevaluate architectures that translate from spoken language sentences to a\nsequence of skeleton poses, known as Text-to-Pose (T2P) translation, over a\nrange of metrics. For our evaluation data, we use the\nRWTH-PHOENIX-Weather-2014T dataset, a German Sign Language - Deutsche\nGebardensprache (DGS) weather broadcast dataset. In addition, we curate a\ncustom hidden test set from a similar domain of discourse. This paper presents\nthe challenge design and the winning methodologies. The challenge attracted 33\nparticipants who submitted 231 solutions, with the top-performing team\nachieving BLEU-1 scores of 31.40 and DTW-MJE of 0.0574. The winning approach\nutilized a retrieval-based framework and a pre-trained language model. As part\nof the workshop, we release a standardized evaluation network, including\nhigh-quality skeleton extraction-based keypoints establishing a consistent\nbaseline for the SLP field, which will enable future researchers to compare\ntheir work against a broader range of methods."}
{"id": "2508.06959", "pdf": "https://arxiv.org/pdf/2508.06959", "abs": "https://arxiv.org/abs/2508.06959", "authors": ["Qin Xu", "Lili Zhu", "Xiaoxia Cheng", "Bo Jiang"], "title": "Beyond Frequency: Seeing Subtle Cues Through the Lens of Spatial Decomposition for Fine-Grained Visual Classification", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The crux of resolving fine-grained visual classification (FGVC) lies in\ncapturing discriminative and class-specific cues that correspond to subtle\nvisual characteristics. Recently, frequency decomposition/transform based\napproaches have attracted considerable interests since its appearing\ndiscriminative cue mining ability. However, the frequency-domain methods are\nbased on fixed basis functions, lacking adaptability to image content and\nunable to dynamically adjust feature extraction according to the discriminative\nrequirements of different images. To address this, we propose a novel method\nfor FGVC, named Subtle-Cue Oriented Perception Engine (SCOPE), which adaptively\nenhances the representational capability of low-level details and high-level\nsemantics in the spatial domain, breaking through the limitations of fixed\nscales in the frequency domain and improving the flexibility of multi-scale\nfusion. The core of SCOPE lies in two modules: the Subtle Detail Extractor\n(SDE), which dynamically enhances subtle details such as edges and textures\nfrom shallow features, and the Salient Semantic Refiner (SSR), which learns\nsemantically coherent and structure-aware refinement features from the\nhigh-level features guided by the enhanced shallow features. The SDE and SSR\nare cascaded stage-by-stage to progressively combine local details with global\nsemantics. Extensive experiments demonstrate that our method achieves new\nstate-of-the-art on four popular fine-grained image classification benchmarks."}
{"id": "2508.06964", "pdf": "https://arxiv.org/pdf/2508.06964", "abs": "https://arxiv.org/abs/2508.06964", "authors": ["Qiwei Tian", "Chenhao Lin", "Zhengyu Zhao", "Qian Li", "Shuai Liu", "Chao Shen"], "title": "Adversarial Video Promotion Against Text-to-Video Retrieval", "categories": ["cs.CV"], "comment": null, "summary": "Thanks to the development of cross-modal models, text-to-video retrieval\n(T2VR) is advancing rapidly, but its robustness remains largely unexamined.\nExisting attacks against T2VR are designed to push videos away from queries,\ni.e., suppressing the ranks of videos, while the attacks that pull videos\ntowards selected queries, i.e., promoting the ranks of videos, remain largely\nunexplored. These attacks can be more impactful as attackers may gain more\nviews/clicks for financial benefits and widespread (mis)information. To this\nend, we pioneer the first attack against T2VR to promote videos adversarially,\ndubbed the Video Promotion attack (ViPro). We further propose Modal Refinement\n(MoRe) to capture the finer-grained, intricate interaction between visual and\ntextual modalities to enhance black-box transferability. Comprehensive\nexperiments cover 2 existing baselines, 3 leading T2VR models, 3 prevailing\ndatasets with over 10k videos, evaluated under 3 scenarios. All experiments are\nconducted in a multi-target setting to reflect realistic scenarios where\nattackers seek to promote the video regarding multiple queries simultaneously.\nWe also evaluated our attacks for defences and imperceptibility. Overall, ViPro\nsurpasses other baselines by over $30/10/4\\%$ for white/grey/black-box settings\non average. Our work highlights an overlooked vulnerability, provides a\nqualitative analysis on the upper/lower bound of our attacks, and offers\ninsights into potential counterplays. Code will be publicly available at\nhttps://github.com/michaeltian108/ViPro."}
{"id": "2508.06968", "pdf": "https://arxiv.org/pdf/2508.06968", "abs": "https://arxiv.org/abs/2508.06968", "authors": ["Ulas Gunes", "Matias Turkulainen", "Juho Kannala", "Esa Rahtu"], "title": "Evaluating Fisheye-Compatible 3D Gaussian Splatting Methods on Real Images Beyond 180 Degree Field of View", "categories": ["cs.CV", "cs.GR"], "comment": null, "summary": "We present the first evaluation of fisheye-based 3D Gaussian Splatting\nmethods, Fisheye-GS and 3DGUT, on real images with fields of view exceeding 180\ndegree. Our study covers both indoor and outdoor scenes captured with 200\ndegree fisheye cameras and analyzes how each method handles extreme distortion\nin real world settings. We evaluate performance under varying fields of view\n(200 degree, 160 degree, and 120 degree) to study the tradeoff between\nperipheral distortion and spatial coverage. Fisheye-GS benefits from field of\nview (FoV) reduction, particularly at 160 degree, while 3DGUT remains stable\nacross all settings and maintains high perceptual quality at the full 200\ndegree view. To address the limitations of SfM-based initialization, which\noften fails under strong distortion, we also propose a depth-based strategy\nusing UniK3D predictions from only 2-3 fisheye images per scene. Although\nUniK3D is not trained on real fisheye data, it produces dense point clouds that\nenable reconstruction quality on par with SfM, even in difficult scenes with\nfog, glare, or sky. Our results highlight the practical viability of\nfisheye-based 3DGS methods for wide-angle 3D reconstruction from sparse and\ndistortion-heavy image inputs."}
{"id": "2508.06982", "pdf": "https://arxiv.org/pdf/2508.06982", "abs": "https://arxiv.org/abs/2508.06982", "authors": ["Yixin Zhu", "Zuoliang Zhu", "Miloš Hašan", "Jian Yang", "Jin Xie", "Beibei Wang"], "title": "WeatherDiffusion: Weather-Guided Diffusion Model for Forward and Inverse Rendering", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Forward and inverse rendering have emerged as key techniques for enabling\nunderstanding and reconstruction in the context of autonomous driving (AD).\nHowever, complex weather and illumination pose great challenges to this task.\nThe emergence of large diffusion models has shown promise in achieving\nreasonable results through learning from 2D priors, but these models are\ndifficult to control and lack robustness. In this paper, we introduce\nWeatherDiffusion, a diffusion-based framework for forward and inverse rendering\non AD scenes with various weather and lighting conditions. Our method enables\nauthentic estimation of material properties, scene geometry, and lighting, and\nfurther supports controllable weather and illumination editing through the use\nof predicted intrinsic maps guided by text descriptions. We observe that\ndifferent intrinsic maps should correspond to different regions of the original\nimage. Based on this observation, we propose Intrinsic map-aware attention\n(MAA) to enable high-quality inverse rendering. Additionally, we introduce a\nsynthetic dataset (\\ie WeatherSynthetic) and a real-world dataset (\\ie\nWeatherReal) for forward and inverse rendering on AD scenes with diverse\nweather and lighting. Extensive experiments show that our WeatherDiffusion\noutperforms state-of-the-art methods on several benchmarks. Moreover, our\nmethod demonstrates significant value in downstream tasks for AD, enhancing the\nrobustness of object detection and image segmentation in challenging weather\nscenarios."}
{"id": "2508.06988", "pdf": "https://arxiv.org/pdf/2508.06988", "abs": "https://arxiv.org/abs/2508.06988", "authors": ["Fangmin Zhao", "Weichao Zeng", "Zhenhang Li", "Dongbao Yang", "Yu Zhou"], "title": "TADoc: Robust Time-Aware Document Image Dewarping", "categories": ["cs.CV"], "comment": "8 pages, 8 figures", "summary": "Flattening curved, wrinkled, and rotated document images captured by portable\nphotographing devices, termed document image dewarping, has become an\nincreasingly important task with the rise of digital economy and online\nworking. Although many methods have been proposed recently, they often struggle\nto achieve satisfactory results when confronted with intricate document\nstructures and higher degrees of deformation in real-world scenarios. Our main\ninsight is that, unlike other document restoration tasks (e.g., deblurring),\ndewarping in real physical scenes is a progressive motion rather than a\none-step transformation. Based on this, we have undertaken two key initiatives.\nFirstly, we reformulate this task, modeling it for the first time as a dynamic\nprocess that encompasses a series of intermediate states. Secondly, we design a\nlightweight framework called TADoc (Time-Aware Document Dewarping Network) to\naddress the geometric distortion of document images. In addition, due to the\ninadequacy of OCR metrics for document images containing sparse text, the\ncomprehensiveness of evaluation is insufficient. To address this shortcoming,\nwe propose a new metric -- DLS (Document Layout Similarity) -- to evaluate the\neffectiveness of document dewarping in downstream tasks. Extensive experiments\nand in-depth evaluations have been conducted and the results indicate that our\nmodel possesses strong robustness, achieving superiority on several benchmarks\nwith different document types and degrees of distortion."}
{"id": "2508.06993", "pdf": "https://arxiv.org/pdf/2508.06993", "abs": "https://arxiv.org/abs/2508.06993", "authors": ["Nick Lemke", "John Kalkhof", "Niklas Babendererde", "Anirban Mukhopadhyay"], "title": "OctreeNCA: Single-Pass 184 MP Segmentation on Consumer Hardware", "categories": ["cs.CV"], "comment": null, "summary": "Medical applications demand segmentation of large inputs, like prostate MRIs,\npathology slices, or videos of surgery. These inputs should ideally be inferred\nat once to provide the model with proper spatial or temporal context. When\nsegmenting large inputs, the VRAM consumption of the GPU becomes the\nbottleneck. Architectures like UNets or Vision Transformers scale very poorly\nin VRAM consumption, resulting in patch- or frame-wise approaches that\ncompromise global consistency and inference speed. The lightweight Neural\nCellular Automaton (NCA) is a bio-inspired model that is by construction\nsize-invariant. However, due to its local-only communication rules, it lacks\nglobal knowledge. We propose OctreeNCA by generalizing the neighborhood\ndefinition using an octree data structure. Our generalized neighborhood\ndefinition enables the efficient traversal of global knowledge. Since deep\nlearning frameworks are mainly developed for large multi-layer networks, their\nimplementation does not fully leverage the advantages of NCAs. We implement an\nNCA inference function in CUDA that further reduces VRAM demands and increases\ninference speed. Our OctreeNCA segments high-resolution images and videos\nquickly while occupying 90% less VRAM than a UNet during evaluation. This\nallows us to segment 184 Megapixel pathology slices or 1-minute surgical videos\nat once."}
{"id": "2508.06995", "pdf": "https://arxiv.org/pdf/2508.06995", "abs": "https://arxiv.org/abs/2508.06995", "authors": ["Huihui Xu", "Jin Ye", "Hongqiu Wang", "Changkai Ji", "Jiashi Lin", "Ming Hu", "Ziyan Huang", "Ying Chen", "Chenglong Ma", "Tianbin Li", "Lihao Liu", "Junjun He", "Lei Zhu"], "title": "S2-UniSeg: Fast Universal Agglomerative Pooling for Scalable Segment Anything without Supervision", "categories": ["cs.CV"], "comment": null, "summary": "Recent self-supervised image segmentation models have achieved promising\nperformance on semantic segmentation and class-agnostic instance segmentation.\nHowever, their pretraining schedule is multi-stage, requiring a time-consuming\npseudo-masks generation process between each training epoch. This\ntime-consuming offline process not only makes it difficult to scale with\ntraining dataset size, but also leads to sub-optimal solutions due to its\ndiscontinuous optimization routine. To solve these, we first present a novel\npseudo-mask algorithm, Fast Universal Agglomerative Pooling (UniAP). Each layer\nof UniAP can identify groups of similar nodes in parallel, allowing to generate\nboth semantic-level and instance-level and multi-granular pseudo-masks within\nens of milliseconds for one image. Based on the fast UniAP, we propose the\nScalable Self-Supervised Universal Segmentation (S2-UniSeg), which employs a\nstudent and a momentum teacher for continuous pretraining. A novel\nsegmentation-oriented pretext task, Query-wise Self-Distillation (QuerySD), is\nproposed to pretrain S2-UniSeg to learn the local-to-global correspondences.\nUnder the same setting, S2-UniSeg outperforms the SOTA UnSAM model, achieving\nnotable improvements of AP+6.9 on COCO, AR+11.1 on UVO, PixelAcc+4.5 on\nCOCOStuff-27, RQ+8.0 on Cityscapes. After scaling up to a larger 2M-image\nsubset of SA-1B, S2-UniSeg further achieves performance gains on all four\nbenchmarks. Our code and pretrained models are available at\nhttps://github.com/bio-mlhui/S2-UniSeg"}
{"id": "2508.07006", "pdf": "https://arxiv.org/pdf/2508.07006", "abs": "https://arxiv.org/abs/2508.07006", "authors": ["Gian Mario Favero", "Ge Ya Luo", "Nima Fathi", "Justin Szeto", "Douglas L. Arnold", "Brennan Nichyporuk", "Chris Pal", "Tal Arbel"], "title": "Spatio-Temporal Conditional Diffusion Models for Forecasting Future Multiple Sclerosis Lesion Masks Conditioned on Treatments", "categories": ["cs.CV"], "comment": "Accepted to MICCAI 2025 (LMID Workshop)", "summary": "Image-based personalized medicine has the potential to transform healthcare,\nparticularly for diseases that exhibit heterogeneous progression such as\nMultiple Sclerosis (MS). In this work, we introduce the first treatment-aware\nspatio-temporal diffusion model that is able to generate future masks\ndemonstrating lesion evolution in MS. Our voxel-space approach incorporates\nmulti-modal patient data, including MRI and treatment information, to forecast\nnew and enlarging T2 (NET2) lesion masks at a future time point. Extensive\nexperiments on a multi-centre dataset of 2131 patient 3D MRIs from randomized\nclinical trials for relapsing-remitting MS demonstrate that our generative\nmodel is able to accurately predict NET2 lesion masks for patients across six\ndifferent treatments. Moreover, we demonstrate our model has the potential for\nreal-world clinical applications through downstream tasks such as future lesion\ncount and location estimation, binary lesion activity classification, and\ngenerating counterfactual future NET2 masks for several treatments with\ndifferent efficacies. This work highlights the potential of causal, image-based\ngenerative models as powerful tools for advancing data-driven prognostics in\nMS."}
{"id": "2508.07011", "pdf": "https://arxiv.org/pdf/2508.07011", "abs": "https://arxiv.org/abs/2508.07011", "authors": ["Zixiong Wang", "Jian Yang", "Yiwei Hu", "Milos Hasan", "Beibei Wang"], "title": "HiMat: DiT-based Ultra-High Resolution SVBRDF Generation", "categories": ["cs.CV", "cs.GR"], "comment": null, "summary": "Creating highly detailed SVBRDFs is essential for 3D content creation. The\nrise of high-resolution text-to-image generative models, based on diffusion\ntransformers (DiT), suggests an opportunity to finetune them for this task.\nHowever, retargeting the models to produce multiple aligned SVBRDF maps instead\nof just RGB images, while achieving high efficiency and ensuring consistency\nacross different maps, remains a challenge. In this paper, we introduce HiMat:\na memory- and computation-efficient diffusion-based framework capable of\ngenerating native 4K-resolution SVBRDFs. A key challenge we address is\nmaintaining consistency across different maps in a lightweight manner, without\nrelying on training new VAEs or significantly altering the DiT backbone (which\nwould damage its prior capabilities). To tackle this, we introduce the\nCrossStitch module, a lightweight convolutional module that captures inter-map\ndependencies through localized operations. Its weights are initialized such\nthat the DiT backbone operation is unchanged before finetuning starts. HiMat\nenables generation with strong structural coherence and high-frequency details.\nResults with a large set of text prompts demonstrate the effectiveness of our\napproach for 4K SVBRDF generation. Further experiments suggest generalization\nto tasks such as intrinsic decomposition."}
{"id": "2508.07020", "pdf": "https://arxiv.org/pdf/2508.07020", "abs": "https://arxiv.org/abs/2508.07020", "authors": ["Tanjim Bin Faruk", "Abdul Matin", "Shrideep Pallickara", "Sangmi Lee Pallickara"], "title": "TerraMAE: Learning Spatial-Spectral Representations from Hyperspectral Earth Observation Data via Adaptive Masked Autoencoders", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Hyperspectral satellite imagery offers sub-30 m views of Earth in hundreds of\ncontiguous spectral bands, enabling fine-grained mapping of soils, crops, and\nland cover. While self-supervised Masked Autoencoders excel on RGB and low-band\nmultispectral data, they struggle to exploit the intricate spatial-spectral\ncorrelations in 200+ band hyperspectral images. We introduce TerraMAE, a novel\nHSI encoding framework specifically designed to learn highly representative\nspatial-spectral embeddings for diverse geospatial analyses. TerraMAE features\nan adaptive channel grouping strategy, based on statistical reflectance\nproperties to capture spectral similarities, and an enhanced reconstruction\nloss function that incorporates spatial and spectral quality metrics. We\ndemonstrate TerraMAE's effectiveness through superior spatial-spectral\ninformation preservation in high-fidelity image reconstruction. Furthermore, we\nvalidate its practical utility and the quality of its learned representations\nthrough strong performance on three key downstream geospatial tasks: crop\nidentification, land cover classification, and soil texture prediction."}
{"id": "2508.07021", "pdf": "https://arxiv.org/pdf/2508.07021", "abs": "https://arxiv.org/abs/2508.07021", "authors": ["Kun Qian", "Wenjie Li", "Tianyu Sun", "Wenhong Wang", "Wenhan Luo"], "title": "DocRefine: An Intelligent Framework for Scientific Document Understanding and Content Optimization based on Multimodal Large Model Agents", "categories": ["cs.CV"], "comment": null, "summary": "The exponential growth of scientific literature in PDF format necessitates\nadvanced tools for efficient and accurate document understanding,\nsummarization, and content optimization. Traditional methods fall short in\nhandling complex layouts and multimodal content, while direct application of\nLarge Language Models (LLMs) and Vision-Language Large Models (LVLMs) lacks\nprecision and control for intricate editing tasks. This paper introduces\nDocRefine, an innovative framework designed for intelligent understanding,\ncontent refinement, and automated summarization of scientific PDF documents,\ndriven by natural language instructions. DocRefine leverages the power of\nadvanced LVLMs (e.g., GPT-4o) by orchestrating a sophisticated multi-agent\nsystem comprising six specialized and collaborative agents: Layout & Structure\nAnalysis, Multimodal Content Understanding, Instruction Decomposition, Content\nRefinement, Summarization & Generation, and Fidelity & Consistency\nVerification. This closed-loop feedback architecture ensures high semantic\naccuracy and visual fidelity. Evaluated on the comprehensive DocEditBench\ndataset, DocRefine consistently outperforms state-of-the-art baselines across\nvarious tasks, achieving overall scores of 86.7% for Semantic Consistency Score\n(SCS), 93.9% for Layout Fidelity Index (LFI), and 85.0% for Instruction\nAdherence Rate (IAR). These results demonstrate DocRefine's superior capability\nin handling complex multimodal document editing, preserving semantic integrity,\nand maintaining visual consistency, marking a significant advancement in\nautomated scientific document processing."}
{"id": "2508.07023", "pdf": "https://arxiv.org/pdf/2508.07023", "abs": "https://arxiv.org/abs/2508.07023", "authors": ["Jingwei Peng", "Jiehao Chen", "Mateo Alejandro Rojas", "Meilin Zhang"], "title": "MV-CoRe: Multimodal Visual-Conceptual Reasoning for Complex Visual Question Answering", "categories": ["cs.CV"], "comment": null, "summary": "Complex Visual Question Answering (Complex VQA) tasks, which demand\nsophisticated multi-modal reasoning and external knowledge integration, present\nsignificant challenges for existing large vision-language models (LVLMs) often\nlimited by their reliance on high-level global features. To address this, we\npropose MV-CoRe (Multimodal Visual-Conceptual Reasoning), a novel model\ndesigned to enhance Complex VQA performance through the deep fusion of diverse\nvisual and linguistic information. MV-CoRe meticulously integrates global\nembeddings from pre-trained Vision Large Models (VLMs) and Language Large\nModels (LLMs) with fine-grained semantic-aware visual features, including\nobject detection characteristics and scene graph representations. An innovative\nMultimodal Fusion Transformer then processes and deeply integrates these\ndiverse feature sets, enabling rich cross-modal attention and facilitating\ncomplex reasoning. We evaluate MV-CoRe on challenging Complex VQA benchmarks,\nincluding GQA, A-OKVQA, and OKVQA, after training on VQAv2. Our experimental\nresults demonstrate that MV-CoRe consistently outperforms established LVLM\nbaselines, achieving an overall accuracy of 77.5% on GQA. Ablation studies\nconfirm the critical contribution of both object and scene graph features, and\nhuman evaluations further validate MV-CoRe's superior factual correctness and\nreasoning depth, underscoring its robust capabilities for deep visual and\nconceptual understanding."}
{"id": "2508.07028", "pdf": "https://arxiv.org/pdf/2508.07028", "abs": "https://arxiv.org/abs/2508.07028", "authors": ["Juntong Fan", "Shuyi Fan", "Debesh Jha", "Changsheng Fang", "Tieyong Zeng", "Hengyong Yu", "Dayang Wang"], "title": "Large Language Model Evaluated Stand-alone Attention-Assisted Graph Neural Network with Spatial and Structural Information Interaction for Precise Endoscopic Image Segmentation", "categories": ["cs.CV"], "comment": "Manuscript under review", "summary": "Accurate endoscopic image segmentation on the polyps is critical for early\ncolorectal cancer detection. However, this task remains challenging due to low\ncontrast with surrounding mucosa, specular highlights, and indistinct\nboundaries. To address these challenges, we propose FOCUS-Med, which stands for\nFusion of spatial and structural graph with attentional context-aware polyp\nsegmentation in endoscopic medical imaging. FOCUS-Med integrates a Dual Graph\nConvolutional Network (Dual-GCN) module to capture contextual spatial and\ntopological structural dependencies. This graph-based representation enables\nthe model to better distinguish polyps from background tissues by leveraging\ntopological cues and spatial connectivity, which are often obscured in raw\nimage intensities. It enhances the model's ability to preserve boundaries and\ndelineate complex shapes typical of polyps. In addition, a location-fused\nstand-alone self-attention is employed to strengthen global context\nintegration. To bridge the semantic gap between encoder-decoder layers, we\nincorporate a trainable weighted fast normalized fusion strategy for efficient\nmulti-scale aggregation. Notably, we are the first to introduce the use of a\nLarge Language Model (LLM) to provide detailed qualitative evaluations of\nsegmentation quality. Extensive experiments on public benchmarks demonstrate\nthat FOCUS-Med achieves state-of-the-art performance across five key metrics,\nunderscoring its effectiveness and clinical potential for AI-assisted\ncolonoscopy."}
{"id": "2508.07031", "pdf": "https://arxiv.org/pdf/2508.07031", "abs": "https://arxiv.org/abs/2508.07031", "authors": ["Anindya Bijoy Das", "Shahnewaz Karim Sakib", "Shibbir Ahmed"], "title": "Trustworthy Medical Imaging with Large Language Models: A Study of Hallucinations Across Modalities", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly applied to medical imaging\ntasks, including image interpretation and synthetic image generation. However,\nthese models often produce hallucinations, which are confident but incorrect\noutputs that can mislead clinical decisions. This study examines hallucinations\nin two directions: image to text, where LLMs generate reports from X-ray, CT,\nor MRI scans, and text to image, where models create medical images from\nclinical prompts. We analyze errors such as factual inconsistencies and\nanatomical inaccuracies, evaluating outputs using expert informed criteria\nacross imaging modalities. Our findings reveal common patterns of hallucination\nin both interpretive and generative tasks, with implications for clinical\nreliability. We also discuss factors contributing to these failures, including\nmodel architecture and training data. By systematically studying both image\nunderstanding and generation, this work provides insights into improving the\nsafety and trustworthiness of LLM driven medical imaging systems."}
{"id": "2508.07038", "pdf": "https://arxiv.org/pdf/2508.07038", "abs": "https://arxiv.org/abs/2508.07038", "authors": ["Yuke Xing", "William Gordon", "Qi Yang", "Kaifa Yang", "Jiarui Wang", "Yiling Xu"], "title": "3DGS-VBench: A Comprehensive Video Quality Evaluation Benchmark for 3DGS Compression", "categories": ["cs.CV"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) enables real-time novel view synthesis with high\nvisual fidelity, but its substantial storage requirements hinder practical\ndeployment, prompting state-of-the-art (SOTA) 3DGS methods to incorporate\ncompression modules. However, these 3DGS generative compression techniques\nintroduce unique distortions lacking systematic quality assessment research. To\nthis end, we establish 3DGS-VBench, a large-scale Video Quality Assessment\n(VQA) Dataset and Benchmark with 660 compressed 3DGS models and video sequences\ngenerated from 11 scenes across 6 SOTA 3DGS compression algorithms with\nsystematically designed parameter levels. With annotations from 50\nparticipants, we obtained MOS scores with outlier removal and validated dataset\nreliability. We benchmark 6 3DGS compression algorithms on storage efficiency\nand visual quality, and evaluate 15 quality assessment metrics across multiple\nparadigms. Our work enables specialized VQA model training for 3DGS, serving as\na catalyst for compression and quality assessment research. The dataset is\navailable at https://github.com/YukeXing/3DGS-VBench."}
{"id": "2508.07041", "pdf": "https://arxiv.org/pdf/2508.07041", "abs": "https://arxiv.org/abs/2508.07041", "authors": ["Junkai Liu", "Nay Aung", "Theodoros N. Arvanitis", "Stefan K. Piechnik", "Joao A C Lima", "Steffen E. Petersen", "Le Zhang"], "title": "SAGCNet: Spatial-Aware Graph Completion Network for Missing Slice Imputation in Population CMR Imaging", "categories": ["cs.CV"], "comment": "Accepted by MICCAI 2025", "summary": "Magnetic resonance imaging (MRI) provides detailed soft-tissue\ncharacteristics that assist in disease diagnosis and screening. However, the\naccuracy of clinical practice is often hindered by missing or unusable slices\ndue to various factors. Volumetric MRI synthesis methods have been developed to\naddress this issue by imputing missing slices from available ones. The inherent\n3D nature of volumetric MRI data, such as cardiac magnetic resonance (CMR),\nposes significant challenges for missing slice imputation approaches, including\n(1) the difficulty of modeling local inter-slice correlations and dependencies\nof volumetric slices, and (2) the limited exploration of crucial 3D spatial\ninformation and global context. In this study, to mitigate these issues, we\npresent Spatial-Aware Graph Completion Network (SAGCNet) to overcome the\ndependency on complete volumetric data, featuring two main innovations: (1) a\nvolumetric slice graph completion module that incorporates the inter-slice\nrelationships into a graph structure, and (2) a volumetric spatial adapter\ncomponent that enables our model to effectively capture and utilize various\nforms of 3D spatial context. Extensive experiments on cardiac MRI datasets\ndemonstrate that SAGCNet is capable of synthesizing absent CMR slices,\noutperforming competitive state-of-the-art MRI synthesis methods both\nquantitatively and qualitatively. Notably, our model maintains superior\nperformance even with limited slice data."}
{"id": "2508.07083", "pdf": "https://arxiv.org/pdf/2508.07083", "abs": "https://arxiv.org/abs/2508.07083", "authors": ["Yueyu Hu", "Ran Gong", "Tingyu Fan", "Yao Wang"], "title": "TeSO: Representing and Compressing 3D Point Cloud Scenes with Textured Surfel Octree", "categories": ["cs.CV"], "comment": null, "summary": "3D visual content streaming is a key technology for emerging 3D telepresence\nand AR/VR applications. One fundamental element underlying the technology is a\nversatile 3D representation that is capable of producing high-quality renders\nand can be efficiently compressed at the same time. Existing 3D representations\nlike point clouds, meshes and 3D Gaussians each have limitations in terms of\nrendering quality, surface definition, and compressibility. In this paper, we\npresent the Textured Surfel Octree (TeSO), a novel 3D representation that is\nbuilt from point clouds but addresses the aforementioned limitations. It\nrepresents a 3D scene as cube-bounded surfels organized on an octree, where\neach surfel is further associated with a texture patch. By approximating a\nsmooth surface with a large surfel at a coarser level of the octree, it reduces\nthe number of primitives required to represent the 3D scene, and yet retains\nthe high-frequency texture details through the texture map attached to each\nsurfel. We further propose a compression scheme to encode the geometry and\ntexture efficiently, leveraging the octree structure. The proposed textured\nsurfel octree combined with the compression scheme achieves higher rendering\nquality at lower bit-rates compared to multiple point cloud and 3D\nGaussian-based baselines."}
{"id": "2508.07089", "pdf": "https://arxiv.org/pdf/2508.07089", "abs": "https://arxiv.org/abs/2508.07089", "authors": ["Sandro Papais", "Letian Wang", "Brian Cheong", "Steven L. Waslander"], "title": "ForeSight: Multi-View Streaming Joint Object Detection and Trajectory Forecasting", "categories": ["cs.CV", "cs.RO"], "comment": "Accepted to ICCV 2025", "summary": "We introduce ForeSight, a novel joint detection and forecasting framework for\nvision-based 3D perception in autonomous vehicles. Traditional approaches treat\ndetection and forecasting as separate sequential tasks, limiting their ability\nto leverage temporal cues. ForeSight addresses this limitation with a\nmulti-task streaming and bidirectional learning approach, allowing detection\nand forecasting to share query memory and propagate information seamlessly. The\nforecast-aware detection transformer enhances spatial reasoning by integrating\ntrajectory predictions from a multiple hypothesis forecast memory queue, while\nthe streaming forecast transformer improves temporal consistency using past\nforecasts and refined detections. Unlike tracking-based methods, ForeSight\neliminates the need for explicit object association, reducing error propagation\nwith a tracking-free model that efficiently scales across multi-frame\nsequences. Experiments on the nuScenes dataset show that ForeSight achieves\nstate-of-the-art performance, achieving an EPA of 54.9%, surpassing previous\nmethods by 9.3%, while also attaining the best mAP and minADE among multi-view\ndetection and forecasting models."}
{"id": "2508.07092", "pdf": "https://arxiv.org/pdf/2508.07092", "abs": "https://arxiv.org/abs/2508.07092", "authors": ["Yue Hu", "Juntong Peng", "Yunqiao Yang", "Siheng Chen"], "title": "Communication-Efficient Multi-Agent 3D Detection via Hybrid Collaboration", "categories": ["cs.CV"], "comment": null, "summary": "Collaborative 3D detection can substantially boost detection performance by\nallowing agents to exchange complementary information. It inherently results in\na fundamental trade-off between detection performance and communication\nbandwidth. To tackle this bottleneck issue, we propose a novel hybrid\ncollaboration that adaptively integrates two types of communication messages:\nperceptual outputs, which are compact, and raw observations, which offer richer\ninformation. This approach focuses on two key aspects: i) integrating\ncomplementary information from two message types and ii) prioritizing the most\ncritical data within each type. By adaptively selecting the most critical set\nof messages, it ensures optimal perceptual information and adaptability,\neffectively meeting the demands of diverse communication scenarios.Building on\nthis hybrid collaboration, we present \\texttt{HyComm}, a\ncommunication-efficient LiDAR-based collaborative 3D detection system.\n\\texttt{HyComm} boasts two main benefits: i) it facilitates adaptable\ncompression rates for messages, addressing various communication requirements,\nand ii) it uses standardized data formats for messages. This ensures they are\nindependent of specific detection models, fostering adaptability across\ndifferent agent configurations. To evaluate HyComm, we conduct experiments on\nboth real-world and simulation datasets: DAIR-V2X and OPV2V. HyComm\nconsistently outperforms previous methods and achieves a superior\nperformance-bandwidth trade-off regardless of whether agents use the same or\nvaried detection models. It achieves a lower communication volume of more than\n2,006$\\times$ and still outperforms Where2comm on DAIR-V2X in terms of AP50.\nThe related code will be released."}
{"id": "2508.07112", "pdf": "https://arxiv.org/pdf/2508.07112", "abs": "https://arxiv.org/abs/2508.07112", "authors": ["Nikolai Warner", "Wenjin Zhang", "Irfan Essa", "Apaar Sadhwani"], "title": "AugLift: Boosting Generalization in Lifting-based 3D Human Pose Estimation", "categories": ["cs.CV", "cs.LG"], "comment": "Preprint. Under review", "summary": "Lifting-based methods for 3D Human Pose Estimation (HPE), which predict 3D\nposes from detected 2D keypoints, often generalize poorly to new datasets and\nreal-world settings. To address this, we propose \\emph{AugLift}, a simple yet\neffective reformulation of the standard lifting pipeline that significantly\nimproves generalization performance without requiring additional data\ncollection or sensors. AugLift sparsely enriches the standard input -- the 2D\nkeypoint coordinates $(x, y)$ -- by augmenting it with a keypoint detection\nconfidence score $c$ and a corresponding depth estimate $d$. These additional\nsignals are computed from the image using off-the-shelf, pre-trained models\n(e.g., for monocular depth estimation), thereby inheriting their strong\ngeneralization capabilities. Importantly, AugLift serves as a modular add-on\nand can be readily integrated into existing lifting architectures.\n  Our extensive experiments across four datasets demonstrate that AugLift\nboosts cross-dataset performance on unseen datasets by an average of $10.1\\%$,\nwhile also improving in-distribution performance by $4.0\\%$. These gains are\nconsistent across various lifting architectures, highlighting the robustness of\nour method. Our analysis suggests that these sparse, keypoint-aligned cues\nprovide robust frame-level context, offering a practical way to significantly\nimprove the generalization of any lifting-based pose estimation model. Code\nwill be made publicly available."}
{"id": "2508.07128", "pdf": "https://arxiv.org/pdf/2508.07128", "abs": "https://arxiv.org/abs/2508.07128", "authors": ["Gregory Schuit", "Denis Parra", "Cecilia Besa"], "title": "Perceptual Evaluation of GANs and Diffusion Models for Generating X-rays", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to the Workshop on Human-AI Collaboration at MICCAI 2025", "summary": "Generative image models have achieved remarkable progress in both natural and\nmedical imaging. In the medical context, these techniques offer a potential\nsolution to data scarcity-especially for low-prevalence anomalies that impair\nthe performance of AI-driven diagnostic and segmentation tools. However,\nquestions remain regarding the fidelity and clinical utility of synthetic\nimages, since poor generation quality can undermine model generalizability and\ntrust. In this study, we evaluate the effectiveness of state-of-the-art\ngenerative models-Generative Adversarial Networks (GANs) and Diffusion Models\n(DMs)-for synthesizing chest X-rays conditioned on four abnormalities:\nAtelectasis (AT), Lung Opacity (LO), Pleural Effusion (PE), and Enlarged\nCardiac Silhouette (ECS). Using a benchmark composed of real images from the\nMIMIC-CXR dataset and synthetic images from both GANs and DMs, we conducted a\nreader study with three radiologists of varied experience. Participants were\nasked to distinguish real from synthetic images and assess the consistency\nbetween visual features and the target abnormality. Our results show that while\nDMs generate more visually realistic images overall, GANs can report better\naccuracy for specific conditions, such as absence of ECS. We further identify\nvisual cues radiologists use to detect synthetic images, offering insights into\nthe perceptual gaps in current models. These findings underscore the\ncomplementary strengths of GANs and DMs and point to the need for further\nrefinement to ensure generative models can reliably augment training datasets\nfor AI diagnostic systems."}
{"id": "2508.07140", "pdf": "https://arxiv.org/pdf/2508.07140", "abs": "https://arxiv.org/abs/2508.07140", "authors": ["Yingtie Lei", "Fanghai Yi", "Yihang Dong", "Weihuang Liu", "Xiaofeng Zhang", "Zimeng Li", "Chi-Man Pun", "Xuhang Chen"], "title": "CMAMRNet: A Contextual Mask-Aware Network Enhancing Mural Restoration Through Comprehensive Mask Guidance", "categories": ["cs.CV"], "comment": "Accepted by BMVC 2025", "summary": "Murals, as invaluable cultural artifacts, face continuous deterioration from\nenvironmental factors and human activities. Digital restoration of murals faces\nunique challenges due to their complex degradation patterns and the critical\nneed to preserve artistic authenticity. Existing learning-based methods\nstruggle with maintaining consistent mask guidance throughout their networks,\nleading to insufficient focus on damaged regions and compromised restoration\nquality. We propose CMAMRNet, a Contextual Mask-Aware Mural Restoration Network\nthat addresses these limitations through comprehensive mask guidance and\nmulti-scale feature extraction. Our framework introduces two key components:\n(1) the Mask-Aware Up/Down-Sampler (MAUDS), which ensures consistent mask\nsensitivity across resolution scales through dedicated channel-wise feature\nselection and mask-guided feature fusion; and (2) the Co-Feature Aggregator\n(CFA), operating at both the highest and lowest resolutions to extract\ncomplementary features for capturing fine textures and global structures in\ndegraded regions. Experimental results on benchmark datasets demonstrate that\nCMAMRNet outperforms state-of-the-art methods, effectively preserving both\nstructural integrity and artistic details in restored murals. The code is\navailable\nat~\\href{https://github.com/CXH-Research/CMAMRNet}{https://github.com/CXH-Research/CMAMRNet}."}
{"id": "2508.07144", "pdf": "https://arxiv.org/pdf/2508.07144", "abs": "https://arxiv.org/abs/2508.07144", "authors": ["Xuanhan Wang", "Huimin Deng", "Ke Liu", "Jun Wang", "Lianli Gao", "Jingkuan Song"], "title": "Dynamic Pattern Alignment Learning for Pretraining Lightweight Human-Centric Vision Models", "categories": ["cs.CV"], "comment": null, "summary": "Human-centric vision models (HVMs) have achieved remarkable generalization\ndue to large-scale pretraining on massive person images. However, their\ndependence on large neural architectures and the restricted accessibility of\npretraining data significantly limits their practicality in real-world\napplications. To address this limitation, we propose Dynamic Pattern Alignment\nLearning (DPAL), a novel distillation-based pretraining framework that\nefficiently trains lightweight HVMs to acquire strong generalization from large\nHVMs. In particular, human-centric visual perception are highly dependent on\nthree typical visual patterns, including global identity pattern, local shape\npattern and multi-person interaction pattern. To achieve generalizable\nlightweight HVMs, we firstly design a dynamic pattern decoder (D-PaDe), acting\nas a dynamic Mixture of Expert (MoE) model. It incorporates three specialized\nexperts dedicated to adaptively extract typical visual patterns, conditioned on\nboth input image and pattern queries. And then, we present three levels of\nalignment objectives, which aims to minimize generalization gap between\nlightweight HVMs and large HVMs at global image level, local pixel level, and\ninstance relation level. With these two deliberate designs, the DPAL\neffectively guides lightweight model to learn all typical human visual patterns\nfrom large HVMs, which can generalize to various human-centric vision tasks.\nExtensive experiments conducted on 15 challenging datasets demonstrate the\neffectiveness of the DPAL. Remarkably, when employing PATH-B as the teacher,\nDPAL-ViT/Ti (5M parameters) achieves surprising generalizability similar to\nexisting large HVMs such as PATH-B (84M) and Sapiens-L (307M), and outperforms\nprevious distillation-based pretraining methods including Proteus-ViT/Ti (5M)\nand TinyMiM-ViT/Ti (5M) by a large margin."}
{"id": "2508.07146", "pdf": "https://arxiv.org/pdf/2508.07146", "abs": "https://arxiv.org/abs/2508.07146", "authors": ["Yu Liu", "Zhijie Liu", "Xiao Ren", "You-Fu Li", "He Kong"], "title": "Intention-Aware Diffusion Model for Pedestrian Trajectory Prediction", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Predicting pedestrian motion trajectories is critical for the path planning\nand motion control of autonomous vehicles. Recent diffusion-based models have\nshown promising results in capturing the inherent stochasticity of pedestrian\nbehavior for trajectory prediction. However, the absence of explicit semantic\nmodelling of pedestrian intent in many diffusion-based methods may result in\nmisinterpreted behaviors and reduced prediction accuracy. To address the above\nchallenges, we propose a diffusion-based pedestrian trajectory prediction\nframework that incorporates both short-term and long-term motion intentions.\nShort-term intent is modelled using a residual polar representation, which\ndecouples direction and magnitude to capture fine-grained local motion\npatterns. Long-term intent is estimated through a learnable, token-based\nendpoint predictor that generates multiple candidate goals with associated\nprobabilities, enabling multimodal and context-aware intention modelling.\nFurthermore, we enhance the diffusion process by incorporating adaptive\nguidance and a residual noise predictor that dynamically refines denoising\naccuracy. The proposed framework is evaluated on the widely used ETH, UCY, and\nSDD benchmarks, demonstrating competitive results against state-of-the-art\nmethods."}
{"id": "2508.07149", "pdf": "https://arxiv.org/pdf/2508.07149", "abs": "https://arxiv.org/abs/2508.07149", "authors": ["Ruolin Yang", "Da Li", "Honggang Zhang", "Yi-Zhe Song"], "title": "SketchAnimator: Animate Sketch via Motion Customization of Text-to-Video Diffusion Models", "categories": ["cs.CV"], "comment": "2024 IEEE International Conference on Visual Communications and Image\n  Processing (VCIP); Oral", "summary": "Sketching is a uniquely human tool for expressing ideas and creativity. The\nanimation of sketches infuses life into these static drawings, opening a new\ndimension for designers. Animating sketches is a time-consuming process that\ndemands professional skills and extensive experience, often proving daunting\nfor amateurs. In this paper, we propose a novel sketch animation model\nSketchAnimator, which enables adding creative motion to a given sketch, like \"a\njumping car''. Namely, given an input sketch and a reference video, we divide\nthe sketch animation into three stages: Appearance Learning, Motion Learning\nand Video Prior Distillation. In stages 1 and 2, we utilize LoRA to integrate\nsketch appearance information and motion dynamics from the reference video into\nthe pre-trained T2V model. In the third stage, we utilize Score Distillation\nSampling (SDS) to update the parameters of the Bezier curves in each sketch\nframe according to the acquired motion information. Consequently, our model\nproduces a sketch video that not only retains the original appearance of the\nsketch but also mirrors the dynamic movements of the reference video. We\ncompare our method with alternative approaches and demonstrate that it\ngenerates the desired sketch video under the challenge of one-shot motion\ncustomization."}
{"id": "2508.07162", "pdf": "https://arxiv.org/pdf/2508.07162", "abs": "https://arxiv.org/abs/2508.07162", "authors": ["Xiaotong Lin", "Tianming Liang", "Jian-Fang Hu", "Kun-Yu Lin", "Yulei Kang", "Chunwei Tian", "Jianhuang Lai", "Wei-Shi Zheng"], "title": "CoopDiff: Anticipating 3D Human-object Interactions via Contact-consistent Decoupled Diffusion", "categories": ["cs.CV"], "comment": null, "summary": "3D human-object interaction (HOI) anticipation aims to predict the future\nmotion of humans and their manipulated objects, conditioned on the historical\ncontext. Generally, the articulated humans and rigid objects exhibit different\nmotion patterns, due to their distinct intrinsic physical properties. However,\nthis distinction is ignored by most of the existing works, which intend to\ncapture the dynamics of both humans and objects within a single prediction\nmodel. In this work, we propose a novel contact-consistent decoupled diffusion\nframework CoopDiff, which employs two distinct branches to decouple human and\nobject motion modeling, with the human-object contact points as shared anchors\nto bridge the motion generation across branches. The human dynamics branch is\naimed to predict highly structured human motion, while the object dynamics\nbranch focuses on the object motion with rigid translations and rotations.\nThese two branches are bridged by a series of shared contact points with\nconsistency constraint for coherent human-object motion prediction. To further\nenhance human-object consistency and prediction reliability, we propose a\nhuman-driven interaction module to guide object motion modeling. Extensive\nexperiments on the BEHAVE and Human-object Interaction datasets demonstrate\nthat our CoopDiff outperforms state-of-the-art methods."}
{"id": "2508.07165", "pdf": "https://arxiv.org/pdf/2508.07165", "abs": "https://arxiv.org/abs/2508.07165", "authors": ["Zelin Qiu", "Xi Wang", "Zhuoyao Xie", "Juan Zhou", "Yu Wang", "Lingjie Yang", "Xinrui Jiang", "Juyoung Bae", "Moo Hyun Son", "Qiang Ye", "Dexuan Chen", "Rui Zhang", "Tao Li", "Neeraj Ramesh Mahboobani", "Varut Vardhanabhuti", "Xiaohui Duan", "Yinghua Zhao", "Hao Chen"], "title": "Large-scale Multi-sequence Pretraining for Generalizable MRI Analysis in Versatile Clinical Applications", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Multi-sequence Magnetic Resonance Imaging (MRI) offers remarkable\nversatility, enabling the distinct visualization of different tissue types.\nNevertheless, the inherent heterogeneity among MRI sequences poses significant\nchallenges to the generalization capability of deep learning models. These\nchallenges undermine model performance when faced with varying acquisition\nparameters, thereby severely restricting their clinical utility. In this study,\nwe present PRISM, a foundation model PRe-trained with large-scale\nmultI-Sequence MRI. We collected a total of 64 datasets from both public and\nprivate sources, encompassing a wide range of whole-body anatomical structures,\nwith scans spanning diverse MRI sequences. Among them, 336,476 volumetric MRI\nscans from 34 datasets (8 public and 26 private) were curated to construct the\nlargest multi-organ multi-sequence MRI pretraining corpus to date. We propose a\nnovel pretraining paradigm that disentangles anatomically invariant features\nfrom sequence-specific variations in MRI, while preserving high-level semantic\nrepresentations. We established a benchmark comprising 44 downstream tasks,\nincluding disease diagnosis, image segmentation, registration, progression\nprediction, and report generation. These tasks were evaluated on 32 public\ndatasets and 5 private cohorts. PRISM consistently outperformed both\nnon-pretrained models and existing foundation models, achieving first-rank\nresults in 39 out of 44 downstream benchmarks with statistical significance\nimprovements. These results underscore its ability to learn robust and\ngeneralizable representations across unseen data acquired under diverse MRI\nprotocols. PRISM provides a scalable framework for multi-sequence MRI analysis,\nthereby enhancing the translational potential of AI in radiology. It delivers\nconsistent performance across diverse imaging protocols, reinforcing its\nclinical applicability."}
{"id": "2508.07170", "pdf": "https://arxiv.org/pdf/2508.07170", "abs": "https://arxiv.org/abs/2508.07170", "authors": ["Yunpeng Shi", "Lei Chen", "Xiaolu Shen", "Yanju Guo"], "title": "Lightweight Multi-Scale Feature Extraction with Fully Connected LMF Layer for Salient Object Detection", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "In the domain of computer vision, multi-scale feature extraction is vital for\ntasks such as salient object detection. However, achieving this capability in\nlightweight networks remains challenging due to the trade-off between\nefficiency and performance. This paper proposes a novel lightweight multi-scale\nfeature extraction layer, termed the LMF layer, which employs depthwise\nseparable dilated convolutions in a fully connected structure. By integrating\nmultiple LMF layers, we develop LMFNet, a lightweight network tailored for\nsalient object detection. Our approach significantly reduces the number of\nparameters while maintaining competitive performance. Here, we show that LMFNet\nachieves state-of-the-art or comparable results on five benchmark datasets with\nonly 0.81M parameters, outperforming several traditional and lightweight models\nin terms of both efficiency and accuracy. Our work not only addresses the\nchallenge of multi-scale learning in lightweight networks but also demonstrates\nthe potential for broader applications in image processing tasks. The related\ncode files are available at https://github.com/Shi-Yun-peng/LMFNet"}
{"id": "2508.07171", "pdf": "https://arxiv.org/pdf/2508.07171", "abs": "https://arxiv.org/abs/2508.07171", "authors": ["Huihui Xu", "Jiashi Lin", "Haoyu Chen", "Junjun He", "Lei Zhu"], "title": "EventRR: Event Referential Reasoning for Referring Video Object Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Referring Video Object Segmentation (RVOS) aims to segment out the object in\na video referred by an expression. Current RVOS methods view referring\nexpressions as unstructured sequences, neglecting their crucial semantic\nstructure essential for referent reasoning. Besides, in contrast to\nimage-referring expressions whose semantics focus only on object attributes and\nobject-object relations, video-referring expressions also encompass event\nattributes and event-event temporal relations. This complexity challenges\ntraditional structured reasoning image approaches. In this paper, we propose\nthe Event Referential Reasoning (EventRR) framework. EventRR decouples RVOS\ninto object summarization part and referent reasoning part. The summarization\nphase begins by summarizing each frame into a set of bottleneck tokens, which\nare then efficiently aggregated in the video-level summarization step to\nexchange the global cross-modal temporal context. For reasoning part, EventRR\nextracts semantic eventful structure of a video-referring expression into\nhighly expressive Referential Event Graph (REG), which is a single-rooted\ndirected acyclic graph. Guided by topological traversal of REG, we propose\nTemporal Concept-Role Reasoning (TCRR) to accumulate the referring score of\neach temporal query from REG leaf nodes to root node. Each reasoning step can\nbe interpreted as a question-answer pair derived from the concept-role\nrelations in REG. Extensive experiments across four widely recognized benchmark\ndatasets, show that EventRR quantitatively and qualitatively outperforms\nstate-of-the-art RVOS methods. Code is available at\nhttps://github.com/bio-mlhui/EventRR"}
{"id": "2508.07211", "pdf": "https://arxiv.org/pdf/2508.07211", "abs": "https://arxiv.org/abs/2508.07211", "authors": ["Junyi He", "Liuling Chen", "Hongyang Zhou", "Zhang xiaoxing", "Xiaobin Zhu", "Shengxiang Yu", "Jingyan Qin", "Xu-Cheng Yin"], "title": "Similarity Matters: A Novel Depth-guided Network for Image Restoration and A New Dataset", "categories": ["cs.CV"], "comment": "12 pages, 10 figures", "summary": "Image restoration has seen substantial progress in recent years. However,\nexisting methods often neglect depth information, which hurts similarity\nmatching, results in attention distractions in shallow depth-of-field (DoF)\nscenarios, and excessive enhancement of background content in deep DoF\nsettings. To overcome these limitations, we propose a novel Depth-Guided\nNetwork (DGN) for image restoration, together with a novel large-scale\nhigh-resolution dataset. Specifically, the network consists of two interactive\nbranches: a depth estimation branch that provides structural guidance, and an\nimage restoration branch that performs the core restoration task. In addition,\nthe image restoration branch exploits intra-object similarity through\nprogressive window-based self-attention and captures inter-object similarity\nvia sparse non-local attention. Through joint training, depth features\ncontribute to improved restoration quality, while the enhanced visual features\nfrom the restoration branch in turn help refine depth estimation. Notably, we\nalso introduce a new dataset for training and evaluation, consisting of 9,205\nhigh-resolution images from 403 plant species, with diverse depth and texture\nvariations. Extensive experiments show that our method achieves\nstate-of-the-art performance on several standard benchmarks and generalizes\nwell to unseen plant images, demonstrating its effectiveness and robustness."}
{"id": "2508.07214", "pdf": "https://arxiv.org/pdf/2508.07214", "abs": "https://arxiv.org/abs/2508.07214", "authors": ["Hongyang Zhou", "Xiaobin Zhu", "Liuling Chen", "Junyi He", "Jingyan Qin", "Xu-Cheng Yin", "Zhang xiaoxing"], "title": "Unsupervised Real-World Super-Resolution via Rectified Flow Degradation Modelling", "categories": ["cs.CV"], "comment": "10 pages, 9 figures", "summary": "Unsupervised real-world super-resolution (SR) faces critical challenges due\nto the complex, unknown degradation distributions in practical scenarios.\nExisting methods struggle to generalize from synthetic low-resolution (LR) and\nhigh-resolution (HR) image pairs to real-world data due to a significant domain\ngap. In this paper, we propose an unsupervised real-world SR method based on\nrectified flow to effectively capture and model real-world degradation,\nsynthesizing LR-HR training pairs with realistic degradation. Specifically,\ngiven unpaired LR and HR images, we propose a novel Rectified Flow Degradation\nModule (RFDM) that introduces degradation-transformed LR (DT-LR) images as\nintermediaries. By modeling the degradation trajectory in a continuous and\ninvertible manner, RFDM better captures real-world degradation and enhances the\nrealism of generated LR images. Additionally, we propose a Fourier Prior Guided\nDegradation Module (FGDM) that leverages structural information embedded in\nFourier phase components to ensure more precise modeling of real-world\ndegradation. Finally, the LR images are processed by both FGDM and RFDM,\nproducing final synthetic LR images with real-world degradation. The synthetic\nLR images are paired with the given HR images to train the off-the-shelf SR\nnetworks. Extensive experiments on real-world datasets demonstrate that our\nmethod significantly enhances the performance of existing SR approaches in\nreal-world scenarios."}
{"id": "2508.07216", "pdf": "https://arxiv.org/pdf/2508.07216", "abs": "https://arxiv.org/abs/2508.07216", "authors": ["Songlin Li", "Zhiqing Guo", "Yuanman Li", "Zeyu Li", "Yunfeng Diao", "Gaobo Yang", "Liejun Wang"], "title": "Bridging Semantic Logic Gaps: A Cognition-Inspired Multimodal Boundary-Preserving Network for Image Manipulation Localization", "categories": ["cs.CV"], "comment": null, "summary": "The existing image manipulation localization (IML) models mainly relies on\nvisual cues, but ignores the semantic logical relationships between content\nfeatures. In fact, the content semantics conveyed by real images often conform\nto human cognitive laws. However, image manipulation technology usually\ndestroys the internal relationship between content features, thus leaving\nsemantic clues for IML. In this paper, we propose a cognition-inspired\nmultimodal boundary-preserving network (CMB-Net). Specifically, CMB-Net\nutilizes large language models (LLMs) to analyze manipulated regions within\nimages and generate prompt-based textual information to compensate for the lack\nof semantic relationships in the visual information. Considering that the\nerroneous texts induced by hallucination from LLMs will damage the accuracy of\nIML, we propose an image-text central ambiguity module (ITCAM). It assigns\nweights to the text features by quantifying the ambiguity between text and\nimage features, thereby ensuring the beneficial impact of textual information.\nWe also propose an image-text interaction module (ITIM) that aligns visual and\ntext features using a correlation matrix for fine-grained interaction. Finally,\ninspired by invertible neural networks, we propose a restoration edge decoder\n(RED) that mutually generates input and output features to preserve boundary\ninformation in manipulated regions without loss. Extensive experiments show\nthat CMB-Net outperforms most existing IML models."}
{"id": "2508.07217", "pdf": "https://arxiv.org/pdf/2508.07217", "abs": "https://arxiv.org/abs/2508.07217", "authors": ["Yuqi Han", "Qi Cai", "Yuanxin Wu"], "title": "Generic Calibration: Pose Ambiguity/Linear Solution and Parametric-hybrid Pipeline", "categories": ["cs.CV"], "comment": null, "summary": "Offline camera calibration techniques typically employ parametric or generic\ncamera models. Selecting parametric models relies heavily on user experience,\nand an inappropriate camera model can significantly affect calibration\naccuracy. Meanwhile, generic calibration methods involve complex procedures and\ncannot provide traditional intrinsic parameters. This paper reveals a pose\nambiguity in the pose solutions of generic calibration methods that\nirreversibly impacts subsequent pose estimation. A linear solver and a\nnonlinear optimization are proposed to address this ambiguity issue. Then a\nglobal optimization hybrid calibration method is introduced to integrate\ngeneric and parametric models together, which improves extrinsic parameter\naccuracy of generic calibration and mitigates overfitting and numerical\ninstability in parametric calibration. Simulation and real-world experimental\nresults demonstrate that the generic-parametric hybrid calibration method\nconsistently excels across various lens types and noise contamination,\nhopefully serving as a reliable and accurate solution for camera calibration in\ncomplex scenarios."}
{"id": "2508.07225", "pdf": "https://arxiv.org/pdf/2508.07225", "abs": "https://arxiv.org/abs/2508.07225", "authors": ["Xuepeng Liu", "Zheng Jiang", "Pinan Zhu", "Hanyu Liu", "Chao Li"], "title": "HaDM-ST: Histology-Assisted Differential Modeling for Spatial Transcriptomics Generation", "categories": ["cs.CV", "92C40, 68T07", "I.2.10; I.4.8"], "comment": "10 pages, 5 figures, includes comparisons with TESLA, HiStoGene, and\n  iStar; submitted to arXiv 2025", "summary": "Spatial transcriptomics (ST) reveals spatial heterogeneity of gene\nexpression, yet its resolution is limited by current platforms. Recent methods\nenhance resolution via H&E-stained histology, but three major challenges\npersist: (1) isolating expression-relevant features from visually complex H&E\nimages; (2) achieving spatially precise multimodal alignment in diffusion-based\nframeworks; and (3) modeling gene-specific variation across expression\nchannels. We propose HaDM-ST (Histology-assisted Differential Modeling for ST\nGeneration), a high-resolution ST generation framework conditioned on H&E\nimages and low-resolution ST. HaDM-ST includes: (i) a semantic distillation\nnetwork to extract predictive cues from H&E; (ii) a spatial alignment module\nenforcing pixel-wise correspondence with low-resolution ST; and (iii) a\nchannel-aware adversarial learner for fine-grained gene-level modeling.\nExperiments on 200 genes across diverse tissues and species show HaDM-ST\nconsistently outperforms prior methods, enhancing spatial fidelity and\ngene-level coherence in high-resolution ST predictions."}
{"id": "2508.07233", "pdf": "https://arxiv.org/pdf/2508.07233", "abs": "https://arxiv.org/abs/2508.07233", "authors": ["Lei Yang", "Junshan Jin", "Mingyuan Zhang", "Yi He", "Bofan Chen", "Shilin Wang"], "title": "Landmark Guided Visual Feature Extractor for Visual Speech Recognition with Limited Resource", "categories": ["cs.CV"], "comment": null, "summary": "Visual speech recognition is a technique to identify spoken content in silent\nspeech videos, which has raised significant attention in recent years.\nAdvancements in data-driven deep learning methods have significantly improved\nboth the speed and accuracy of recognition. However, these deep learning\nmethods can be effected by visual disturbances, such as lightning conditions,\nskin texture and other user-specific features. Data-driven approaches could\nreduce the performance degradation caused by these visual disturbances using\nmodels pretrained on large-scale datasets. But these methods often require\nlarge amounts of training data and computational resources, making them costly.\nTo reduce the influence of user-specific features and enhance performance with\nlimited data, this paper proposed a landmark guided visual feature extractor.\nFacial landmarks are used as auxiliary information to aid in training the\nvisual feature extractor. A spatio-temporal multi-graph convolutional network\nis designed to fully exploit the spatial locations and spatio-temporal features\nof facial landmarks. Additionally, a multi-level lip dynamic fusion framework\nis introduced to combine the spatio-temporal features of the landmarks with the\nvisual features extracted from the raw video frames. Experimental results show\nthat this approach performs well with limited data and also improves the\nmodel's accuracy on unseen speakers."}
{"id": "2508.07237", "pdf": "https://arxiv.org/pdf/2508.07237", "abs": "https://arxiv.org/abs/2508.07237", "authors": ["Bo Wang", "Mengyuan Xu", "Yue Yan", "Yuqun Yang", "Kechen Shu", "Wei Ping", "Xu Tang", "Wei Jiang", "Zheng You"], "title": "ASM-UNet: Adaptive Scan Mamba Integrating Group Commonalities and Individual Variations for Fine-Grained Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Precise lesion resection depends on accurately identifying fine-grained\nanatomical structures. While many coarse-grained segmentation (CGS) methods\nhave been successful in large-scale segmentation (e.g., organs), they fall\nshort in clinical scenarios requiring fine-grained segmentation (FGS), which\nremains challenging due to frequent individual variations in small-scale\nanatomical structures. Although recent Mamba-based models have advanced medical\nimage segmentation, they often rely on fixed manually-defined scanning orders,\nwhich limit their adaptability to individual variations in FGS. To address\nthis, we propose ASM-UNet, a novel Mamba-based architecture for FGS. It\nintroduces adaptive scan scores to dynamically guide the scanning order,\ngenerated by combining group-level commonalities and individual-level\nvariations. Experiments on two public datasets (ACDC and Synapse) and a newly\nproposed challenging biliary tract FGS dataset, namely BTMS, demonstrate that\nASM-UNet achieves superior performance in both CGS and FGS tasks. Our code and\ndataset are available at https://github.com/YqunYang/ASM-UNet."}
{"id": "2508.07246", "pdf": "https://arxiv.org/pdf/2508.07246", "abs": "https://arxiv.org/abs/2508.07246", "authors": ["Xin Ma", "Yaohui Wang", "Genyun Jia", "Xinyuan Chen", "Tien-Tsin Wong", "Cunjian Chen"], "title": "Consistent and Controllable Image Animation with Motion Linear Diffusion Transformers", "categories": ["cs.CV"], "comment": "Project Page: https://maxin-cn.github.io/miramo_project", "summary": "Image animation has seen significant progress, driven by the powerful\ngenerative capabilities of diffusion models. However, maintaining appearance\nconsistency with static input images and mitigating abrupt motion transitions\nin generated animations remain persistent challenges. While text-to-video (T2V)\ngeneration has demonstrated impressive performance with diffusion transformer\nmodels, the image animation field still largely relies on U-Net-based diffusion\nmodels, which lag behind the latest T2V approaches. Moreover, the quadratic\ncomplexity of vanilla self-attention mechanisms in Transformers imposes heavy\ncomputational demands, making image animation particularly resource-intensive.\nTo address these issues, we propose MiraMo, a framework designed to enhance\nefficiency, appearance consistency, and motion smoothness in image animation.\nSpecifically, MiraMo introduces three key elements: (1) A foundational\ntext-to-video architecture replacing vanilla self-attention with efficient\nlinear attention to reduce computational overhead while preserving generation\nquality; (2) A novel motion residual learning paradigm that focuses on modeling\nmotion dynamics rather than directly predicting frames, improving temporal\nconsistency; and (3) A DCT-based noise refinement strategy during inference to\nsuppress sudden motion artifacts, complemented by a dynamics control module to\nbalance motion smoothness and expressiveness. Extensive experiments against\nstate-of-the-art methods validate the superiority of MiraMo in generating\nconsistent, smooth, and controllable animations with accelerated inference\nspeed. Additionally, we demonstrate the versatility of MiraMo through\napplications in motion transfer and video editing tasks."}
{"id": "2508.07250", "pdf": "https://arxiv.org/pdf/2508.07250", "abs": "https://arxiv.org/abs/2508.07250", "authors": ["Fengchao Xiong", "Zhenxing Wu", "Sen Jia", "Yuntao Qian"], "title": "SUIT: Spatial-Spectral Union-Intersection Interaction Network for Hyperspectral Object Tracking", "categories": ["cs.CV"], "comment": null, "summary": "Hyperspectral videos (HSVs), with their inherent spatial-spectral-temporal\nstructure, offer distinct advantages in challenging tracking scenarios such as\ncluttered backgrounds and small objects. However, existing methods primarily\nfocus on spatial interactions between the template and search regions, often\noverlooking spectral interactions, leading to suboptimal performance. To\naddress this issue, this paper investigates spectral interactions from both the\narchitectural and training perspectives. At the architectural level, we first\nestablish band-wise long-range spatial relationships between the template and\nsearch regions using Transformers. We then model spectral interactions using\nthe inclusion-exclusion principle from set theory, treating them as the union\nof spatial interactions across all bands. This enables the effective\nintegration of both shared and band-specific spatial cues. At the training\nlevel, we introduce a spectral loss to enforce material distribution alignment\nbetween the template and predicted regions, enhancing robustness to shape\ndeformation and appearance variations. Extensive experiments demonstrate that\nour tracker achieves state-of-the-art tracking performance. The source code,\ntrained models and results will be publicly available via\nhttps://github.com/bearshng/suit to support reproducibility."}
{"id": "2508.07251", "pdf": "https://arxiv.org/pdf/2508.07251", "abs": "https://arxiv.org/abs/2508.07251", "authors": ["Junsheng Huang", "Shengyu Hao", "Bocheng Hu", "Gaoang Wang"], "title": "Understanding Dynamic Scenes in Ego Centric 4D Point Clouds", "categories": ["cs.CV"], "comment": null, "summary": "Understanding dynamic 4D scenes from an egocentric perspective-modeling\nchanges in 3D spatial structure over time-is crucial for human-machine\ninteraction, autonomous navigation, and embodied intelligence. While existing\negocentric datasets contain dynamic scenes, they lack unified 4D annotations\nand task-driven evaluation protocols for fine-grained spatio-temporal\nreasoning, especially on motion of objects and human, together with their\ninteractions. To address this gap, we introduce EgoDynamic4D, a novel QA\nbenchmark on highly dynamic scenes, comprising RGB-D video, camera poses,\nglobally unique instance masks, and 4D bounding boxes. We construct 927K QA\npairs accompanied by explicit Chain-of-Thought (CoT), enabling verifiable,\nstep-by-step spatio-temporal reasoning. We design 12 dynamic QA tasks covering\nagent motion, human-object interaction, trajectory prediction, relation\nunderstanding, and temporal-causal reasoning, with fine-grained,\nmultidimensional metrics. To tackle these tasks, we propose an end-to-end\nspatio-temporal reasoning framework that unifies dynamic and static scene\ninformation, using instance-aware feature encoding, time and camera encoding,\nand spatially adaptive down-sampling to compress large 4D scenes into token\nsequences manageable by LLMs. Experiments on EgoDynamic4D show that our method\nconsistently outperforms baselines, validating the effectiveness of multimodal\ntemporal modeling for egocentric dynamic scene understanding."}
{"id": "2508.07260", "pdf": "https://arxiv.org/pdf/2508.07260", "abs": "https://arxiv.org/abs/2508.07260", "authors": ["Sihan Yang", "Huitong Ji", "Shaolin Lu", "Jiayi Chen", "Binxiao Xu", "Ming Lu", "Yuanxing Zhang", "Wenhui Dong", "Wentao Zhang"], "title": "Small-Large Collaboration: Training-efficient Concept Personalization for Large VLM using a Meta Personalized Small VLM", "categories": ["cs.CV"], "comment": null, "summary": "Personalizing Vision-Language Models (VLMs) to transform them into daily\nassistants has emerged as a trending research direction. However, leading\ncompanies like OpenAI continue to increase model size and develop complex\ndesigns such as the chain of thought (CoT). While large VLMs are proficient in\ncomplex multi-modal understanding, their high training costs and limited access\nvia paid APIs restrict direct personalization. Conversely, small VLMs are\neasily personalized and freely available, but they lack sufficient reasoning\ncapabilities. Inspired by this, we propose a novel collaborative framework\nnamed Small-Large Collaboration (SLC) for large VLM personalization, where the\nsmall VLM is responsible for generating personalized information, while the\nlarge model integrates this personalized information to deliver accurate\nresponses. To effectively incorporate personalized information, we develop a\ntest-time reflection strategy, preventing the potential hallucination of the\nsmall VLM. Since SLC only needs to train a meta personalized small VLM for the\nlarge VLMs, the overall process is training-efficient. To the best of our\nknowledge, this is the first training-efficient framework that supports both\nopen-source and closed-source large VLMs, enabling broader real-world\npersonalized applications. We conduct thorough experiments across various\nbenchmarks and large VLMs to demonstrate the effectiveness of the proposed SLC\nframework. The code will be released at https://github.com/Hhankyangg/SLC."}
{"id": "2508.07270", "pdf": "https://arxiv.org/pdf/2508.07270", "abs": "https://arxiv.org/abs/2508.07270", "authors": ["Xiang Xiang", "Qinhao Zhou", "Zhuo Xu", "Jing Ma", "Jiaxin Dai", "Yifan Liang", "Hanlin Li"], "title": "OpenHAIV: A Framework Towards Practical Open-World Learning", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV", "stat.ML"], "comment": "Codes, results, and OpenHAIV documentation available at\n  https://haiv-lab.github.io/openhaiv", "summary": "Substantial progress has been made in various techniques for open-world\nrecognition. Out-of-distribution (OOD) detection methods can effectively\ndistinguish between known and unknown classes in the data, while incremental\nlearning enables continuous model knowledge updates. However, in open-world\nscenarios, these approaches still face limitations. Relying solely on OOD\ndetection does not facilitate knowledge updates in the model, and incremental\nfine-tuning typically requires supervised conditions, which significantly\ndeviate from open-world settings. To address these challenges, this paper\nproposes OpenHAIV, a novel framework that integrates OOD detection, new class\ndiscovery, and incremental continual fine-tuning into a unified pipeline. This\nframework allows models to autonomously acquire and update knowledge in\nopen-world environments. The proposed framework is available at\nhttps://haiv-lab.github.io/openhaiv ."}
{"id": "2508.07281", "pdf": "https://arxiv.org/pdf/2508.07281", "abs": "https://arxiv.org/abs/2508.07281", "authors": ["Hongbo Zhu", "Angelo Cangelosi"], "title": "Representation Understanding via Activation Maximization", "categories": ["cs.CV", "cs.AI"], "comment": "7 pages,12 figures", "summary": "Understanding internal feature representations of deep neural networks (DNNs)\nis a fundamental step toward model interpretability. Inspired by neuroscience\nmethods that probe biological neurons using visual stimuli, recent deep\nlearning studies have employed Activation Maximization (AM) to synthesize\ninputs that elicit strong responses from artificial neurons. In this work, we\npropose a unified feature visualization framework applicable to both\nConvolutional Neural Networks (CNNs) and Vision Transformers (ViTs). Unlike\nprior efforts that predominantly focus on the last output-layer neurons in\nCNNs, we extend feature visualization to intermediate layers as well, offering\ndeeper insights into the hierarchical structure of learned feature\nrepresentations. Furthermore, we investigate how activation maximization can be\nleveraged to generate adversarial examples, revealing potential vulnerabilities\nand decision boundaries of DNNs. Our experiments demonstrate the effectiveness\nof our approach in both traditional CNNs and modern ViT, highlighting its\ngeneralizability and interpretive value."}
{"id": "2508.07298", "pdf": "https://arxiv.org/pdf/2508.07298", "abs": "https://arxiv.org/abs/2508.07298", "authors": ["Zhiqiang Shen", "Peng Cao", "Xiaoli Liu", "Jinzhu Yang", "Osmar R. Zaiane"], "title": "SynMatch: Rethinking Consistency in Medical Image Segmentation with Sparse Annotations", "categories": ["cs.CV"], "comment": null, "summary": "Label scarcity remains a major challenge in deep learning-based medical image\nsegmentation. Recent studies use strong-weak pseudo supervision to leverage\nunlabeled data. However, performance is often hindered by inconsistencies\nbetween pseudo labels and their corresponding unlabeled images. In this work,\nwe propose \\textbf{SynMatch}, a novel framework that sidesteps the need for\nimproving pseudo labels by synthesizing images to match them instead.\nSpecifically, SynMatch synthesizes images using texture and shape features\nextracted from the same segmentation model that generates the corresponding\npseudo labels for unlabeled images. This design enables the generation of\nhighly consistent synthesized-image-pseudo-label pairs without requiring any\ntraining parameters for image synthesis. We extensively evaluate SynMatch\nacross diverse medical image segmentation tasks under semi-supervised learning\n(SSL), weakly-supervised learning (WSL), and barely-supervised learning (BSL)\nsettings with increasingly limited annotations. The results demonstrate that\nSynMatch achieves superior performance, especially in the most challenging BSL\nsetting. For example, it outperforms the recent strong-weak pseudo\nsupervision-based method by 29.71\\% and 10.05\\% on the polyp segmentation task\nwith 5\\% and 10\\% scribble annotations, respectively. The code will be released\nat https://github.com/Senyh/SynMatch."}
{"id": "2508.07300", "pdf": "https://arxiv.org/pdf/2508.07300", "abs": "https://arxiv.org/abs/2508.07300", "authors": ["Ping-Mao Huang", "I-Tien Chao", "Ping-Chia Huang", "Jia-Wei Liao", "Yung-Yu Chuang"], "title": "BEVANet: Bilateral Efficient Visual Attention Network for Real-Time Semantic Segmentation", "categories": ["cs.CV"], "comment": "Copyright 2025 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "Real-time semantic segmentation presents the dual challenge of designing\nefficient architectures that capture large receptive fields for semantic\nunderstanding while also refining detailed contours. Vision transformers model\nlong-range dependencies effectively but incur high computational cost. To\naddress these challenges, we introduce the Large Kernel Attention (LKA)\nmechanism. Our proposed Bilateral Efficient Visual Attention Network (BEVANet)\nexpands the receptive field to capture contextual information and extracts\nvisual and structural features using Sparse Decomposed Large Separable Kernel\nAttentions (SDLSKA). The Comprehensive Kernel Selection (CKS) mechanism\ndynamically adapts the receptive field to further enhance performance.\nFurthermore, the Deep Large Kernel Pyramid Pooling Module (DLKPPM) enriches\ncontextual features by synergistically combining dilated convolutions and large\nkernel attention. The bilateral architecture facilitates frequent branch\ncommunication, and the Boundary Guided Adaptive Fusion (BGAF) module enhances\nboundary delineation by integrating spatial and semantic features under\nboundary guidance. BEVANet achieves real-time segmentation at 33 FPS, yielding\n79.3% mIoU without pretraining and 81.0% mIoU on Cityscapes after ImageNet\npretraining, demonstrating state-of-the-art performance. The code and model is\navailable at https://github.com/maomao0819/BEVANet."}
{"id": "2508.07306", "pdf": "https://arxiv.org/pdf/2508.07306", "abs": "https://arxiv.org/abs/2508.07306", "authors": ["Md Zahurul Haquea", "Yeahyea Sarker", "Muhammed Farhan Sadique Mahi", "Syed Jubayer Jaman", "Md Robiul Islam"], "title": "DragonFruitQualityNet: A Lightweight Convolutional Neural Network for Real-Time Dragon Fruit Quality Inspection on Mobile Devices", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Dragon fruit, renowned for its nutritional benefits and economic value, has\nexperienced rising global demand due to its affordability and local\navailability. As dragon fruit cultivation expands, efficient pre- and\npost-harvest quality inspection has become essential for improving agricultural\nproductivity and minimizing post-harvest losses. This study presents\nDragonFruitQualityNet, a lightweight Convolutional Neural Network (CNN)\noptimized for real-time quality assessment of dragon fruits on mobile devices.\nWe curated a diverse dataset of 13,789 images, integrating self-collected\nsamples with public datasets (dataset from Mendeley Data), and classified them\ninto four categories: fresh, immature, mature, and defective fruits to ensure\nrobust model training. The proposed model achieves an impressive 93.98%\naccuracy, outperforming existing methods in fruit quality classification. To\nfacilitate practical adoption, we embedded the model into an intuitive mobile\napplication, enabling farmers and agricultural stakeholders to conduct\non-device, real-time quality inspections. This research provides an accurate,\nefficient, and scalable AI-driven solution for dragon fruit quality control,\nsupporting digital agriculture and empowering smallholder farmers with\naccessible technology. By bridging the gap between research and real-world\napplication, our work advances post-harvest management and promotes sustainable\nfarming practices."}
{"id": "2508.07307", "pdf": "https://arxiv.org/pdf/2508.07307", "abs": "https://arxiv.org/abs/2508.07307", "authors": ["Haiyang Guo", "Fei Zhu", "Hongbo Zhao", "Fanhu Zeng", "Wenzhuo Liu", "Shijie Ma", "Da-Han Wang", "Xu-Yao Zhang"], "title": "MCITlib: Multimodal Continual Instruction Tuning Library and Benchmark", "categories": ["cs.CV", "cs.AI"], "comment": "Preprint", "summary": "Continual learning aims to equip AI systems with the ability to continuously\nacquire and adapt to new knowledge without forgetting previously learned\ninformation, similar to human learning. While traditional continual learning\nmethods focusing on unimodal tasks have achieved notable success, the emergence\nof Multimodal Large Language Models has brought increasing attention to\nMultimodal Continual Learning tasks involving multiple modalities, such as\nvision and language. In this setting, models are expected to not only mitigate\ncatastrophic forgetting but also handle the challenges posed by cross-modal\ninteractions and coordination. To facilitate research in this direction, we\nintroduce MCITlib, a comprehensive and constantly evolving code library for\ncontinual instruction tuning of Multimodal Large Language Models. In MCITlib,\nwe have currently implemented 8 representative algorithms for Multimodal\nContinual Instruction Tuning and systematically evaluated them on 2 carefully\nselected benchmarks. MCITlib will be continuously updated to reflect advances\nin the Multimodal Continual Learning field. The codebase is released at\nhttps://github.com/Ghy0501/MCITlib."}
{"id": "2508.07312", "pdf": "https://arxiv.org/pdf/2508.07312", "abs": "https://arxiv.org/abs/2508.07312", "authors": ["Min Yang", "Zihan Jia", "Zhilin Dai", "Sheng Guo", "Limin Wang"], "title": "MobileViCLIP: An Efficient Video-Text Model for Mobile Devices", "categories": ["cs.CV"], "comment": "Accepted by ICCV2025", "summary": "Efficient lightweight neural networks are with increasing attention due to\ntheir faster reasoning speed and easier deployment on mobile devices. However,\nexisting video pre-trained models still focus on the common ViT architecture\nwith high latency, and few works attempt to build efficient architecture on\nmobile devices. This paper bridges this gap by introducing temporal structural\nreparameterization into an efficient image-text model and training it on a\nlarge-scale high-quality video-text dataset, resulting in an efficient\nvideo-text model that can run on mobile devices with strong zero-shot\nclassification and retrieval capabilities, termed as MobileViCLIP. In\nparticular, in terms of inference speed on mobile devices, our\nMobileViCLIP-Small is 55.4x times faster than InternVideo2-L14 and 6.7x faster\nthan InternVideo2-S14. In terms of zero-shot retrieval performance, our\nMobileViCLIP-Small obtains similar performance as InternVideo2-L14 and obtains\n6.9\\% better than InternVideo2-S14 on MSR-VTT. The code is available at\nhttps://github.com/MCG-NJU/MobileViCLIP."}
{"id": "2508.07313", "pdf": "https://arxiv.org/pdf/2508.07313", "abs": "https://arxiv.org/abs/2508.07313", "authors": ["Junyu Xiong", "Yonghui Wang", "Weichao Zhao", "Chenyu Liu", "Bing Yin", "Wengang Zhou", "Houqiang Li"], "title": "DocR1: Evidence Page-Guided GRPO for Multi-Page Document Understanding", "categories": ["cs.CV"], "comment": null, "summary": "Understanding multi-page documents poses a significant challenge for\nmultimodal large language models (MLLMs), as it requires fine-grained visual\ncomprehension and multi-hop reasoning across pages. While prior work has\nexplored reinforcement learning (RL) for enhancing advanced reasoning in MLLMs,\nits application to multi-page document understanding remains underexplored. In\nthis paper, we introduce DocR1, an MLLM trained with a novel RL framework,\nEvidence Page-Guided GRPO (EviGRPO). EviGRPO incorporates an evidence-aware\nreward mechanism that promotes a coarse-to-fine reasoning strategy, guiding the\nmodel to first retrieve relevant pages before generating answers. This training\nparadigm enables us to build high-quality models with limited supervision. To\nsupport this, we design a two-stage annotation pipeline and a curriculum\nlearning strategy, based on which we construct two datasets: EviBench, a\nhigh-quality training set with 4.8k examples, and ArxivFullQA, an evaluation\nbenchmark with 8.6k QA pairs based on scientific papers. Extensive experiments\nacross a wide range of benchmarks demonstrate that DocR1 achieves\nstate-of-the-art performance on multi-page tasks, while consistently\nmaintaining strong results on single-page benchmarks."}
{"id": "2508.07318", "pdf": "https://arxiv.org/pdf/2508.07318", "abs": "https://arxiv.org/abs/2508.07318", "authors": ["Jinjing Gu", "Tianbao Qin", "Yuanyuan Pu", "Zhengpeng Zhao"], "title": "RORPCap: Retrieval-based Objects and Relations Prompt for Image Captioning", "categories": ["cs.CV"], "comment": null, "summary": "Image captioning aims to generate natural language descriptions for input\nimages in an open-form manner. To accurately generate descriptions related to\nthe image, a critical step in image captioning is to identify objects and\nunderstand their relations within the image. Modern approaches typically\ncapitalize on object detectors or combine detectors with Graph Convolutional\nNetwork (GCN). However, these models suffer from redundant detection\ninformation, difficulty in GCN construction, and high training costs. To\naddress these issues, a Retrieval-based Objects and Relations Prompt for Image\nCaptioning (RORPCap) is proposed, inspired by the fact that image-text\nretrieval can provide rich semantic information for input images. RORPCap\nemploys an Objects and relations Extraction Model to extract object and\nrelation words from the image. These words are then incorporate into predefined\nprompt templates and encoded as prompt embeddings. Next, a Mamba-based mapping\nnetwork is designed to quickly map image embeddings extracted by CLIP to\nvisual-text embeddings. Finally, the resulting prompt embeddings and\nvisual-text embeddings are concatenated to form textual-enriched feature\nembeddings, which are fed into a GPT-2 model for caption generation. Extensive\nexperiments conducted on the widely used MS-COCO dataset show that the RORPCap\nrequires only 2.6 hours under cross-entropy loss training, achieving 120.5%\nCIDEr score and 22.0% SPICE score on the \"Karpathy\" test split. RORPCap\nachieves comparable performance metrics to detector-based and GCN-based models\nwith the shortest training time and demonstrates its potential as an\nalternative for image captioning."}
{"id": "2508.07330", "pdf": "https://arxiv.org/pdf/2508.07330", "abs": "https://arxiv.org/abs/2508.07330", "authors": ["Tuyen Tran", "Thao Minh Le", "Quang-Hung Le", "Truyen Tran"], "title": "Planner-Refiner: Dynamic Space-Time Refinement for Vision-Language Alignment in Videos", "categories": ["cs.CV"], "comment": "Accepted for publication at ECAI 2025", "summary": "Vision-language alignment in video must address the complexity of language,\nevolving interacting entities, their action chains, and semantic gaps between\nlanguage and vision. This work introduces Planner-Refiner, a framework to\novercome these challenges. Planner-Refiner bridges the semantic gap by\niteratively refining visual elements' space-time representation, guided by\nlanguage until semantic gaps are minimal. A Planner module schedules language\nguidance by decomposing complex linguistic prompts into short sentence chains.\nThe Refiner processes each short sentence, a noun-phrase and verb-phrase pair,\nto direct visual tokens' self-attention across space then time, achieving\nefficient single-step refinement. A recurrent system chains these steps,\nmaintaining refined visual token representations. The final representation\nfeeds into task-specific heads for alignment generation. We demonstrate\nPlanner-Refiner's effectiveness on two video-language alignment tasks:\nReferring Video Object Segmentation and Temporal Grounding with varying\nlanguage complexity. We further introduce a new MeViS-X benchmark to assess\nmodels' capability with long queries. Superior performance versus\nstate-of-the-art methods on these benchmarks shows the approach's potential,\nespecially for complex prompts."}
{"id": "2508.07341", "pdf": "https://arxiv.org/pdf/2508.07341", "abs": "https://arxiv.org/abs/2508.07341", "authors": ["Fangtai Wu", "Mushui Liu", "Weijie He", "Wanggui He", "Hao Jiang", "Zhao Wang", "Yunlong Yu"], "title": "CoAR: Concept Injection into Autoregressive Models for Personalized Text-to-Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "The unified autoregressive (AR) model excels at multimodal understanding and\ngeneration, but its potential for customized image generation remains\nunderexplored. Existing customized generation methods rely on full fine-tuning\nor adapters, making them costly and prone to overfitting or catastrophic\nforgetting. In this paper, we propose \\textbf{CoAR}, a novel framework for\ninjecting subject concepts into the unified AR models while keeping all\npre-trained parameters completely frozen. CoAR learns effective, specific\nsubject representations with only a minimal number of parameters using a\nLayerwise Multimodal Context Learning strategy. To address overfitting and\nlanguage drift, we further introduce regularization that preserves the\npre-trained distribution and anchors context tokens to improve subject fidelity\nand re-contextualization. Additionally, CoAR supports training-free subject\ncustomization in a user-provided style. Experiments demonstrate that CoAR\nachieves superior performance on both subject-driven personalization and style\npersonalization, while delivering significant gains in computational and memory\nefficiency. Notably, CoAR tunes less than \\textbf{0.05\\%} of the parameters\nwhile achieving competitive performance compared to recent Proxy-Tuning. Code:\nhttps://github.com/KZF-kzf/CoAR"}
{"id": "2508.07346", "pdf": "https://arxiv.org/pdf/2508.07346", "abs": "https://arxiv.org/abs/2508.07346", "authors": ["Tingyu Yang", "Jue Gong", "Jinpei Guo", "Wenbo Li", "Yong Guo", "Yulun Zhang"], "title": "SODiff: Semantic-Oriented Diffusion Model for JPEG Compression Artifacts Removal", "categories": ["cs.CV"], "comment": "7 pages, 5 figures. The code will be available at\n  \\url{https://github.com/frakenation/SODiff}", "summary": "JPEG, as a widely used image compression standard, often introduces severe\nvisual artifacts when achieving high compression ratios. Although existing deep\nlearning-based restoration methods have made considerable progress, they often\nstruggle to recover complex texture details, resulting in over-smoothed\noutputs. To overcome these limitations, we propose SODiff, a novel and\nefficient semantic-oriented one-step diffusion model for JPEG artifacts\nremoval. Our core idea is that effective restoration hinges on providing\nsemantic-oriented guidance to the pre-trained diffusion model, thereby fully\nleveraging its powerful generative prior. To this end, SODiff incorporates a\nsemantic-aligned image prompt extractor (SAIPE). SAIPE extracts rich features\nfrom low-quality (LQ) images and projects them into an embedding space\nsemantically aligned with that of the text encoder. Simultaneously, it\npreserves crucial information for faithful reconstruction. Furthermore, we\npropose a quality factor-aware time predictor that implicitly learns the\ncompression quality factor (QF) of the LQ image and adaptively selects the\noptimal denoising start timestep for the diffusion process. Extensive\nexperimental results show that our SODiff outperforms recent leading methods in\nboth visual quality and quantitative metrics. Code is available at:\nhttps://github.com/frakenation/SODiff"}
{"id": "2508.07355", "pdf": "https://arxiv.org/pdf/2508.07355", "abs": "https://arxiv.org/abs/2508.07355", "authors": ["Qilin Zhang", "Olaf Wysocki", "Boris Jutzi"], "title": "GS4Buildings: Prior-Guided Gaussian Splatting for 3D Building Reconstruction", "categories": ["cs.CV"], "comment": "Accepted for presentation at ISPRS 3D GeoInfo & Smart Data, Smart\n  Cities 2025, Kashiwa, Japan. To appear in the ISPRS Annals of the\n  Photogrammetry, Remote Sensing and Spatial Information Sciences", "summary": "Recent advances in Gaussian Splatting (GS) have demonstrated its\neffectiveness in photo-realistic rendering and 3D reconstruction. Among these,\n2D Gaussian Splatting (2DGS) is particularly suitable for surface\nreconstruction due to its flattened Gaussian representation and integrated\nnormal regularization. However, its performance often degrades in large-scale\nand complex urban scenes with frequent occlusions, leading to incomplete\nbuilding reconstructions. We propose GS4Buildings, a novel prior-guided\nGaussian Splatting method leveraging the ubiquity of semantic 3D building\nmodels for robust and scalable building surface reconstruction. Instead of\nrelying on traditional Structure-from-Motion (SfM) pipelines, GS4Buildings\ninitializes Gaussians directly from low-level Level of Detail (LoD)2 semantic\n3D building models. Moreover, we generate prior depth and normal maps from the\nplanar building geometry and incorporate them into the optimization process,\nproviding strong geometric guidance for surface consistency and structural\naccuracy. We also introduce an optional building-focused mode that limits\nreconstruction to building regions, achieving a 71.8% reduction in Gaussian\nprimitives and enabling a more efficient and compact representation.\nExperiments on urban datasets demonstrate that GS4Buildings improves\nreconstruction completeness by 20.5% and geometric accuracy by 32.8%. These\nresults highlight the potential of semantic building model integration to\nadvance GS-based reconstruction toward real-world urban applications such as\nsmart cities and digital twins. Our project is available:\nhttps://github.com/zqlin0521/GS4Buildings."}
{"id": "2508.07369", "pdf": "https://arxiv.org/pdf/2508.07369", "abs": "https://arxiv.org/abs/2508.07369", "authors": ["Tianyu Xin", "Jin-Liang Xiao", "Zeyu Xia", "Shan Yin", "Liang-Jian Deng"], "title": "Training and Inference within 1 Second -- Tackle Cross-Sensor Degradation of Real-World Pansharpening with Efficient Residual Feature Tailoring", "categories": ["cs.CV"], "comment": null, "summary": "Deep learning methods for pansharpening have advanced rapidly, yet models\npretrained on data from a specific sensor often generalize poorly to data from\nother sensors. Existing methods to tackle such cross-sensor degradation include\nretraining model or zero-shot methods, but they are highly time-consuming or\neven need extra training data. To address these challenges, our method first\nperforms modular decomposition on deep learning-based pansharpening models,\nrevealing a general yet critical interface where high-dimensional fused\nfeatures begin mapping to the channel space of the final image. % may need\nrevisement A Feature Tailor is then integrated at this interface to address\ncross-sensor degradation at the feature level, and is trained efficiently with\nphysics-aware unsupervised losses. Moreover, our method operates in a\npatch-wise manner, training on partial patches and performing parallel\ninference on all patches to boost efficiency. Our method offers two key\nadvantages: (1) $\\textit{Improved Generalization Ability}$: it significantly\nenhance performance in cross-sensor cases. (2) $\\textit{Low Generalization\nCost}$: it achieves sub-second training and inference, requiring only partial\ntest inputs and no external data, whereas prior methods often take minutes or\neven hours. Experiments on the real-world data from multiple datasets\ndemonstrate that our method achieves state-of-the-art quality and efficiency in\ntackling cross-sensor degradation. For example, training and inference of\n$512\\times512\\times8$ image within $\\textit{0.2 seconds}$ and\n$4000\\times4000\\times8$ image within $\\textit{3 seconds}$ at the fastest\nsetting on a commonly used RTX 3090 GPU, which is over 100 times faster than\nzero-shot methods."}
{"id": "2508.07372", "pdf": "https://arxiv.org/pdf/2508.07372", "abs": "https://arxiv.org/abs/2508.07372", "authors": ["Rajaei Khatib", "Raja Giryes"], "title": "DIP-GS: Deep Image Prior For Gaussian Splatting Sparse View Recovery", "categories": ["cs.CV"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) is a leading 3D scene reconstruction method,\nobtaining high-quality reconstruction with real-time rendering runtime\nperformance. The main idea behind 3DGS is to represent the scene as a\ncollection of 3D gaussians, while learning their parameters to fit the given\nviews of the scene. While achieving superior performance in the presence of\nmany views, 3DGS struggles with sparse view reconstruction, where the input\nviews are sparse and do not fully cover the scene and have low overlaps. In\nthis paper, we propose DIP-GS, a Deep Image Prior (DIP) 3DGS representation. By\nusing the DIP prior, which utilizes internal structure and patterns, with\ncoarse-to-fine manner, DIP-based 3DGS can operate in scenarios where vanilla\n3DGS fails, such as sparse view recovery. Note that our approach does not use\nany pre-trained models such as generative models and depth estimation, but\nrather relies only on the input frames. Among such methods, DIP-GS obtains\nstate-of-the-art (SOTA) competitive results on various sparse-view\nreconstruction tasks, demonstrating its capabilities."}
{"id": "2508.07401", "pdf": "https://arxiv.org/pdf/2508.07401", "abs": "https://arxiv.org/abs/2508.07401", "authors": ["Rui Chen", "Xingyu Chen", "Shaoan Wang", "Shihan Kong", "Junzhi Yu"], "title": "LET-US: Long Event-Text Understanding of Scenes", "categories": ["cs.CV"], "comment": null, "summary": "Event cameras output event streams as sparse, asynchronous data with\nmicrosecond-level temporal resolution, enabling visual perception with low\nlatency and a high dynamic range. While existing Multimodal Large Language\nModels (MLLMs) have achieved significant success in understanding and analyzing\nRGB video content, they either fail to interpret event streams effectively or\nremain constrained to very short sequences. In this paper, we introduce LET-US,\na framework for long event-stream--text comprehension that employs an adaptive\ncompression mechanism to reduce the volume of input events while preserving\ncritical visual details. LET-US thus establishes a new frontier in cross-modal\ninferential understanding over extended event sequences. To bridge the\nsubstantial modality gap between event streams and textual representations, we\nadopt a two-stage optimization paradigm that progressively equips our model\nwith the capacity to interpret event-based scenes. To handle the voluminous\ntemporal information inherent in long event streams, we leverage text-guided\ncross-modal queries for feature reduction, augmented by hierarchical clustering\nand similarity computation to distill the most representative event features.\nMoreover, we curate and construct a large-scale event-text aligned dataset to\ntrain our model, achieving tighter alignment of event features within the LLM\nembedding space. We also develop a comprehensive benchmark covering a diverse\nset of tasks -- reasoning, captioning, classification, temporal localization\nand moment retrieval. Experimental results demonstrate that LET-US outperforms\nprior state-of-the-art MLLMs in both descriptive accuracy and semantic\ncomprehension on long-duration event streams. All datasets, codes, and models\nwill be publicly available."}
{"id": "2508.07402", "pdf": "https://arxiv.org/pdf/2508.07402", "abs": "https://arxiv.org/abs/2508.07402", "authors": ["Rongxuan Peng", "Shunquan Tan", "Chenqi Kong", "Anwei Luo", "Alex C. Kot", "Jiwu Huang"], "title": "ForensicsSAM: Toward Robust and Unified Image Forgery Detection and Localization Resisting to Adversarial Attack", "categories": ["cs.CV"], "comment": null, "summary": "Parameter-efficient fine-tuning (PEFT) has emerged as a popular strategy for\nadapting large vision foundation models, such as the Segment Anything Model\n(SAM) and LLaVA, to downstream tasks like image forgery detection and\nlocalization (IFDL). However, existing PEFT-based approaches overlook their\nvulnerability to adversarial attacks. In this paper, we show that highly\ntransferable adversarial images can be crafted solely via the upstream model,\nwithout accessing the downstream model or training data, significantly\ndegrading the IFDL performance. To address this, we propose ForensicsSAM, a\nunified IFDL framework with built-in adversarial robustness. Our design is\nguided by three key ideas: (1) To compensate for the lack of forgery-relevant\nknowledge in the frozen image encoder, we inject forgery experts into each\ntransformer block to enhance its ability to capture forgery artifacts. These\nforgery experts are always activated and shared across any input images. (2) To\ndetect adversarial images, we design an light-weight adversary detector that\nlearns to capture structured, task-specific artifact in RGB domain, enabling\nreliable discrimination across various attack methods. (3) To resist\nadversarial attacks, we inject adversary experts into the global attention\nlayers and MLP modules to progressively correct feature shifts induced by\nadversarial noise. These adversary experts are adaptively activated by the\nadversary detector, thereby avoiding unnecessary interference with clean\nimages. Extensive experiments across multiple benchmarks demonstrate that\nForensicsSAM achieves superior resistance to various adversarial attack\nmethods, while also delivering state-of-the-art performance in image-level\nforgery detection and pixel-level forgery localization. The resource is\navailable at https://github.com/siriusPRX/ForensicsSAM."}
{"id": "2508.07409", "pdf": "https://arxiv.org/pdf/2508.07409", "abs": "https://arxiv.org/abs/2508.07409", "authors": ["Junyao Gao", "Jiaxing Li", "Wenran Liu", "Yanhong Zeng", "Fei Shen", "Kai Chen", "Yanan Sun", "Cairong Zhao"], "title": "CharacterShot: Controllable and Consistent 4D Character Animation", "categories": ["cs.CV"], "comment": "13 pages, 10 figures. Code at https://github.com/Jeoyal/CharacterShot", "summary": "In this paper, we propose \\textbf{CharacterShot}, a controllable and\nconsistent 4D character animation framework that enables any individual\ndesigner to create dynamic 3D characters (i.e., 4D character animation) from a\nsingle reference character image and a 2D pose sequence. We begin by\npretraining a powerful 2D character animation model based on a cutting-edge\nDiT-based image-to-video model, which allows for any 2D pose sequnce as\ncontrollable signal. We then lift the animation model from 2D to 3D through\nintroducing dual-attention module together with camera prior to generate\nmulti-view videos with spatial-temporal and spatial-view consistency. Finally,\nwe employ a novel neighbor-constrained 4D gaussian splatting optimization on\nthese multi-view videos, resulting in continuous and stable 4D character\nrepresentations. Moreover, to improve character-centric performance, we\nconstruct a large-scale dataset Character4D, containing 13,115 unique\ncharacters with diverse appearances and motions, rendered from multiple\nviewpoints. Extensive experiments on our newly constructed benchmark,\nCharacterBench, demonstrate that our approach outperforms current\nstate-of-the-art methods. Code, models, and datasets will be publicly available\nat https://github.com/Jeoyal/CharacterShot."}
{"id": "2508.07413", "pdf": "https://arxiv.org/pdf/2508.07413", "abs": "https://arxiv.org/abs/2508.07413", "authors": ["Youqi Wang", "Shunquan Tan", "Rongxuan Peng", "Bin Li", "Jiwu Huang"], "title": "CLUE: Leveraging Low-Rank Adaptation to Capture Latent Uncovered Evidence for Image Forgery Localization", "categories": ["cs.CV"], "comment": null, "summary": "The increasing accessibility of image editing tools and generative AI has led\nto a proliferation of visually convincing forgeries, compromising the\nauthenticity of digital media. In this paper, in addition to leveraging\ndistortions from conventional forgeries, we repurpose the mechanism of a\nstate-of-the-art (SOTA) text-to-image synthesis model by exploiting its\ninternal generative process, turning it into a high-fidelity forgery\nlocalization tool. To this end, we propose CLUE (Capture Latent Uncovered\nEvidence), a framework that employs Low- Rank Adaptation (LoRA) to\nparameter-efficiently reconfigure Stable Diffusion 3 (SD3) as a forensic\nfeature extractor. Our approach begins with the strategic use of SD3's\nRectified Flow (RF) mechanism to inject noise at varying intensities into the\nlatent representation, thereby steering the LoRAtuned denoising process to\namplify subtle statistical inconsistencies indicative of a forgery. To\ncomplement the latent analysis with high-level semantic context and precise\nspatial details, our method incorporates contextual features from the image\nencoder of the Segment Anything Model (SAM), which is parameter-efficiently\nadapted to better trace the boundaries of forged regions. Extensive evaluations\ndemonstrate CLUE's SOTA generalization performance, significantly outperforming\nprior methods. Furthermore, CLUE shows superior robustness against common\npost-processing attacks and Online Social Networks (OSNs). Code is publicly\navailable at https://github.com/SZAISEC/CLUE."}
{"id": "2508.07432", "pdf": "https://arxiv.org/pdf/2508.07432", "abs": "https://arxiv.org/abs/2508.07432", "authors": ["Vivek Hruday Kavuri", "Vysishtya Karanam", "Venkata Jahnavi Venkamsetty", "Kriti Madumadukala", "Lakshmipathi Balaji Darur", "Ponnurangam Kumaraguru"], "title": "Freeze and Reveal: Exposing Modality Bias in Vision-Language Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Vision Language Models achieve impressive multi-modal performance but often\ninherit gender biases from their training data. This bias might be coming from\nboth the vision and text modalities. In this work, we dissect the contributions\nof vision and text backbones to these biases by applying targeted debiasing\nusing Counterfactual Data Augmentation and Task Vector methods. Inspired by\ndata-efficient approaches in hate-speech classification, we introduce a novel\nmetric, Degree of Stereotypicality and a corresponding debiasing method, Data\nAugmentation Using Degree of Stereotypicality - DAUDoS, to reduce bias with\nminimal computational cost. We curate a gender annotated dataset and evaluate\nall methods on VisoGender benchmark to quantify improvements and identify\ndominant source of bias. Our results show that CDA reduces the gender gap by 6%\nand DAUDoS by 3% but using only one-third of the data. Both methods also\nimprove the model's ability to correctly identify gender in images by 3%, with\nDAUDoS achieving this improvement using only almost one-third of training data.\nFrom our experiment's, we observed that CLIP's vision encoder is more biased\nwhereas PaliGemma2's text encoder is more biased. By identifying whether bias\nstems more from vision or text encoders, our work enables more targeted and\neffective bias mitigation strategies in future multi-modal systems."}
{"id": "2508.07441", "pdf": "https://arxiv.org/pdf/2508.07441", "abs": "https://arxiv.org/abs/2508.07441", "authors": ["Yuxin Zhang", "Yunkang Cao", "Yuqi Cheng", "Yihan Sun", "Weiming Shen"], "title": "Levarging Learning Bias for Noisy Anomaly Detection", "categories": ["cs.CV"], "comment": null, "summary": "This paper addresses the challenge of fully unsupervised image anomaly\ndetection (FUIAD), where training data may contain unlabeled anomalies.\nConventional methods assume anomaly-free training data, but real-world\ncontamination leads models to absorb anomalies as normal, degrading detection\nperformance. To mitigate this, we propose a two-stage framework that\nsystematically exploits inherent learning bias in models. The learning bias\nstems from: (1) the statistical dominance of normal samples, driving models to\nprioritize learning stable normal patterns over sparse anomalies, and (2)\nfeature-space divergence, where normal data exhibit high intra-class\nconsistency while anomalies display high diversity, leading to unstable model\nresponses. Leveraging the learning bias, stage 1 partitions the training set\ninto subsets, trains sub-models, and aggregates cross-model anomaly scores to\nfilter a purified dataset. Stage 2 trains the final detector on this dataset.\nExperiments on the Real-IAD benchmark demonstrate superior anomaly detection\nand localization performance under different noise conditions. Ablation studies\nfurther validate the framework's contamination resilience, emphasizing the\ncritical role of learning bias exploitation. The model-agnostic design ensures\ncompatibility with diverse unsupervised backbones, offering a practical\nsolution for real-world scenarios with imperfect training data. Code is\navailable at https://github.com/hustzhangyuxin/LLBNAD."}
{"id": "2508.07450", "pdf": "https://arxiv.org/pdf/2508.07450", "abs": "https://arxiv.org/abs/2508.07450", "authors": ["Suman Kunwar", "Prabesh Rai"], "title": "Health Care Waste Classification Using Deep Learning Aligned with Nepal's Bin Color Guidelines", "categories": ["cs.CV"], "comment": "7 pages, 5 figures", "summary": "The increasing number of Health Care facilities in Nepal has also added up\nthe challenges on managing health care waste (HCW). Improper segregation and\ndisposal of HCW leads to the contamination, spreading of infectious diseases\nand puts a risk of waste handlers. This study benchmarks the state of the art\nwaste classification models: ResNeXt-50, EfficientNet-B0, MobileNetV3-S,\nYOLOv8-n and YOLOv5-s using Stratified K-fold techniques where we use 5 folds\non combined HCW data, and found that the YOLOv5-s achieved higher of 95.06%\naccuracy but fell short few milliseconds in inference speed with YOLOv8-n\nmodel. The EfficientNet-B0 showed promising results of 93.22% accuracy but took\nthe highest inference time. A repetitive ANOVA was performed to see statistical\nsignificance and the best performing model (YOLOv5-s) was deployed to the web\nwith mapped bin color using Nepal's HCW management standards for public usage.\nFurther work on the data was suggested along with localized context."}
{"id": "2508.07470", "pdf": "https://arxiv.org/pdf/2508.07470", "abs": "https://arxiv.org/abs/2508.07470", "authors": ["Siminfar Samakoush Galougah", "Rishie Raj", "Sanjoy Chowdhury", "Sayan Nag", "Ramani Duraiswami"], "title": "AURA: A Fine-Grained Benchmark and Decomposed Metric for Audio-Visual Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "Current audio-visual (AV) benchmarks focus on final answer accuracy,\noverlooking the underlying reasoning process. This makes it difficult to\ndistinguish genuine comprehension from correct answers derived through flawed\nreasoning or hallucinations. To address this, we introduce AURA (Audio-visual\nUnderstanding and Reasoning Assessment), a benchmark for evaluating the\ncross-modal reasoning capabilities of Audio-Visual Large Language Models\n(AV-LLMs) and Omni-modal Language Models (OLMs). AURA includes questions across\nsix challenging cognitive domains, such as causality, timbre and pitch, tempo\nand AV synchronization, unanswerability, implicit distractions, and skill\nprofiling, explicitly designed to be unanswerable from a single modality. This\nforces models to construct a valid logical path grounded in both audio and\nvideo, setting AURA apart from AV datasets that allow uni-modal shortcuts. To\nassess reasoning traces, we propose a novel metric, AuraScore, which addresses\nthe lack of robust tools for evaluating reasoning fidelity. It decomposes\nreasoning into two aspects: (i) Factual Consistency - whether reasoning is\ngrounded in perceptual evidence, and (ii) Core Inference - the logical validity\nof each reasoning step. Evaluations of SOTA models on AURA reveal a critical\nreasoning gap: although models achieve high accuracy (up to 92% on some tasks),\ntheir Factual Consistency and Core Inference scores fall below 45%. This\ndiscrepancy highlights that models often arrive at correct answers through\nflawed logic, underscoring the need for our benchmark and paving the way for\nmore robust multimodal evaluation."}
{"id": "2508.07483", "pdf": "https://arxiv.org/pdf/2508.07483", "abs": "https://arxiv.org/abs/2508.07483", "authors": ["Pranav Chougule"], "title": "Novel View Synthesis with Gaussian Splatting: Impact on Photogrammetry Model Accuracy and Resolution", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "In this paper, I present a comprehensive study comparing Photogrammetry and\nGaussian Splatting techniques for 3D model reconstruction and view synthesis. I\ncreated a dataset of images from a real-world scene and constructed 3D models\nusing both methods. To evaluate the performance, I compared the models using\nstructural similarity index (SSIM), peak signal-to-noise ratio (PSNR), learned\nperceptual image patch similarity (LPIPS), and lp/mm resolution based on the\nUSAF resolution chart. A significant contribution of this work is the\ndevelopment of a modified Gaussian Splatting repository, which I forked and\nenhanced to enable rendering images from novel camera poses generated in the\nBlender environment. This innovation allows for the synthesis of high-quality\nnovel views, showcasing the flexibility and potential of Gaussian Splatting. My\ninvestigation extends to an augmented dataset that includes both original\nground images and novel views synthesized via Gaussian Splatting. This\naugmented dataset was employed to generate a new photogrammetry model, which\nwas then compared against the original photogrammetry model created using only\nthe original images. The results demonstrate the efficacy of using Gaussian\nSplatting to generate novel high-quality views and its potential to improve\nphotogrammetry-based 3D reconstructions. The comparative analysis highlights\nthe strengths and limitations of both approaches, providing valuable\ninformation for applications in extended reality (XR), photogrammetry, and\nautonomous vehicle simulations. Code is available at\nhttps://github.com/pranavc2255/gaussian-splatting-novel-view-render.git."}
{"id": "2508.07493", "pdf": "https://arxiv.org/pdf/2508.07493", "abs": "https://arxiv.org/abs/2508.07493", "authors": ["Jian Chen", "Ming Li", "Jihyung Kil", "Chenguang Wang", "Tong Yu", "Ryan Rossi", "Tianyi Zhou", "Changyou Chen", "Ruiyi Zhang"], "title": "VisR-Bench: An Empirical Study on Visual Retrieval-Augmented Generation for Multilingual Long Document Understanding", "categories": ["cs.CV"], "comment": "Under Review", "summary": "Most organizational data in this world are stored as documents, and visual\nretrieval plays a crucial role in unlocking the collective intelligence from\nall these documents. However, existing benchmarks focus on English-only\ndocument retrieval or only consider multilingual question-answering on a\nsingle-page image. To bridge this gap, we introduce VisR-Bench, a multilingual\nbenchmark designed for question-driven multimodal retrieval in long documents.\nOur benchmark comprises over 35K high-quality QA pairs across 1.2K documents,\nenabling fine-grained evaluation of multimodal retrieval. VisR-Bench spans\nsixteen languages with three question types (figures, text, and tables),\noffering diverse linguistic and question coverage. Unlike prior datasets, we\ninclude queries without explicit answers, preventing models from relying on\nsuperficial keyword matching. We evaluate various retrieval models, including\ntext-based methods, multimodal encoders, and MLLMs, providing insights into\ntheir strengths and limitations. Our results show that while MLLMs\nsignificantly outperform text-based and multimodal encoder models, they still\nstruggle with structured tables and low-resource languages, highlighting key\nchallenges in multilingual visual retrieval."}
{"id": "2508.07501", "pdf": "https://arxiv.org/pdf/2508.07501", "abs": "https://arxiv.org/abs/2508.07501", "authors": ["Xiaoye Zuo", "Nikos Athanasiou", "Ginger Delmas", "Yiming Huang", "Xingyu Fu", "Lingjie Liu"], "title": "FormCoach: Lift Smarter, Not Harder", "categories": ["cs.CV", "cs.HC"], "comment": null, "summary": "Good form is the difference between strength and strain, yet for the\nfast-growing community of at-home fitness enthusiasts, expert feedback is often\nout of reach. FormCoach transforms a simple camera into an always-on,\ninteractive AI training partner, capable of spotting subtle form errors and\ndelivering tailored corrections in real time, leveraging vision-language models\n(VLMs). We showcase this capability through a web interface and benchmark\nstate-of-the-art VLMs on a dataset of 1,700 expert-annotated user-reference\nvideo pairs spanning 22 strength and mobility exercises. To accelerate research\nin AI-driven coaching, we release both the dataset and an automated,\nrubric-based evaluation pipeline, enabling standardized comparison across\nmodels. Our benchmarks reveal substantial gaps compared to human-level\ncoaching, underscoring both the challenges and opportunities in integrating\nnuanced, context-aware movement analysis into interactive AI systems. By\nframing form correction as a collaborative and creative process between humans\nand machines, FormCoach opens a new frontier in embodied AI."}
{"id": "2508.07514", "pdf": "https://arxiv.org/pdf/2508.07514", "abs": "https://arxiv.org/abs/2508.07514", "authors": ["Artzai Picon", "Itziar Eguskiza", "Daniel Mugica", "Javier Romero", "Carlos Javier Jimenez", "Eric White", "Gabriel Do-Lago-Junqueira", "Christian Klukas", "Ramon Navarra-Mestre"], "title": "From Field to Drone: Domain Drift Tolerant Automated Multi-Species and Damage Plant Semantic Segmentation for Herbicide Trials", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Field trials are vital in herbicide research and development to assess\neffects on crops and weeds under varied conditions. Traditionally, evaluations\nrely on manual visual assessments, which are time-consuming, labor-intensive,\nand subjective. Automating species and damage identification is challenging due\nto subtle visual differences, but it can greatly enhance efficiency and\nconsistency.\n  We present an improved segmentation model combining a general-purpose\nself-supervised visual model with hierarchical inference based on botanical\ntaxonomy. Trained on a multi-year dataset (2018-2020) from Germany and Spain\nusing digital and mobile cameras, the model was tested on digital camera data\n(year 2023) and drone imagery from the United States, Germany, and Spain (year\n2024) to evaluate robustness under domain shift. This cross-device evaluation\nmarks a key step in assessing generalization across platforms of the model.\n  Our model significantly improved species identification (F1-score: 0.52 to\n0.85, R-squared: 0.75 to 0.98) and damage classification (F1-score: 0.28 to\n0.44, R-squared: 0.71 to 0.87) over prior methods. Under domain shift (drone\nimages), it maintained strong performance with moderate degradation (species:\nF1-score 0.60, R-squared 0.80; damage: F1-score 0.41, R-squared 0.62), where\nearlier models failed.\n  These results confirm the model's robustness and real-world applicability. It\nis now deployed in BASF's phenotyping pipeline, enabling large-scale, automated\ncrop and weed monitoring across diverse geographies."}
{"id": "2508.07519", "pdf": "https://arxiv.org/pdf/2508.07519", "abs": "https://arxiv.org/abs/2508.07519", "authors": ["Joonghyuk Shin", "Alchan Hwang", "Yujin Kim", "Daneul Kim", "Jaesik Park"], "title": "Exploring Multimodal Diffusion Transformers for Enhanced Prompt-based Image Editing", "categories": ["cs.CV"], "comment": "ICCV 2025. Project webpage:\n  https://joonghyuk.com/exploring-mmdit-web/", "summary": "Transformer-based diffusion models have recently superseded traditional U-Net\narchitectures, with multimodal diffusion transformers (MM-DiT) emerging as the\ndominant approach in state-of-the-art models like Stable Diffusion 3 and\nFlux.1. Previous approaches have relied on unidirectional cross-attention\nmechanisms, with information flowing from text embeddings to image latents. In\ncontrast, MMDiT introduces a unified attention mechanism that concatenates\ninput projections from both modalities and performs a single full attention\noperation, allowing bidirectional information flow between text and image\nbranches. This architectural shift presents significant challenges for existing\nediting techniques. In this paper, we systematically analyze MM-DiT's attention\nmechanism by decomposing attention matrices into four distinct blocks,\nrevealing their inherent characteristics. Through these analyses, we propose a\nrobust, prompt-based image editing method for MM-DiT that supports global to\nlocal edits across various MM-DiT variants, including few-step models. We\nbelieve our findings bridge the gap between existing U-Net-based methods and\nemerging architectures, offering deeper insights into MMDiT's behavioral\npatterns."}
{"id": "2508.07528", "pdf": "https://arxiv.org/pdf/2508.07528", "abs": "https://arxiv.org/abs/2508.07528", "authors": ["Xiaotong Ji", "Ryoma Bise", "Seiichi Uchida"], "title": "Enhancing Reliability of Medical Image Diagnosis through Top-rank Learning with Rejection Module", "categories": ["cs.CV"], "comment": null, "summary": "In medical image processing, accurate diagnosis is of paramount importance.\nLeveraging machine learning techniques, particularly top-rank learning, shows\nsignificant promise by focusing on the most crucial instances. However,\nchallenges arise from noisy labels and class-ambiguous instances, which can\nseverely hinder the top-rank objective, as they may be erroneously placed among\nthe top-ranked instances. To address these, we propose a novel approach that\nenhances toprank learning by integrating a rejection module. Cooptimized with\nthe top-rank loss, this module identifies and mitigates the impact of outliers\nthat hinder training effectiveness. The rejection module functions as an\nadditional branch, assessing instances based on a rejection function that\nmeasures their deviation from the norm. Through experimental validation on a\nmedical dataset, our methodology demonstrates its efficacy in detecting and\nmitigating outliers, improving the reliability and accuracy of medical image\ndiagnoses."}
{"id": "2508.07537", "pdf": "https://arxiv.org/pdf/2508.07537", "abs": "https://arxiv.org/abs/2508.07537", "authors": ["Xiaoming Li", "Wangmeng Zuo", "Chen Change Loy"], "title": "Enhanced Generative Structure Prior for Chinese Text Image Super-resolution", "categories": ["cs.CV"], "comment": "TPAMI", "summary": "Faithful text image super-resolution (SR) is challenging because each\ncharacter has a unique structure and usually exhibits diverse font styles and\nlayouts. While existing methods primarily focus on English text, less attention\nhas been paid to more complex scripts like Chinese. In this paper, we introduce\na high-quality text image SR framework designed to restore the precise strokes\nof low-resolution (LR) Chinese characters. Unlike methods that rely on\ncharacter recognition priors to regularize the SR task, we propose a novel\nstructure prior that offers structure-level guidance to enhance visual quality.\nOur framework incorporates this structure prior within a StyleGAN model,\nleveraging its generative capabilities for restoration. To maintain the\nintegrity of character structures while accommodating various font styles and\nlayouts, we implement a codebook-based mechanism that restricts the generative\nspace of StyleGAN. Each code in the codebook represents the structure of a\nspecific character, while the vector $w$ in StyleGAN controls the character's\nstyle, including typeface, orientation, and location. Through the collaborative\ninteraction between the codebook and style, we generate a high-resolution\nstructure prior that aligns with LR characters both spatially and structurally.\nExperiments demonstrate that this structure prior provides robust,\ncharacter-specific guidance, enabling the accurate restoration of clear strokes\nin degraded characters, even for real-world LR Chinese text with irregular\nlayouts. Our code and pre-trained models will be available at\nhttps://github.com/csxmli2016/MARCONetPlusPlus"}
{"id": "2508.07538", "pdf": "https://arxiv.org/pdf/2508.07538", "abs": "https://arxiv.org/abs/2508.07538", "authors": ["Hongzhu Jiang", "Sihan Xie", "Zhiyu Wan"], "title": "A DICOM Image De-identification Algorithm in the MIDI-B Challenge", "categories": ["cs.CV", "cs.AI"], "comment": "8 pages, 5 figures", "summary": "Image de-identification is essential for the public sharing of medical\nimages, particularly in the widely used Digital Imaging and Communications in\nMedicine (DICOM) format as required by various regulations and standards,\nincluding Health Insurance Portability and Accountability Act (HIPAA) privacy\nrules, the DICOM PS3.15 standard, and best practices recommended by the Cancer\nImaging Archive (TCIA). The Medical Image De-Identification Benchmark (MIDI-B)\nChallenge at the 27th International Conference on Medical Image Computing and\nComputer Assisted Intervention (MICCAI 2024) was organized to evaluate\nrule-based DICOM image de-identification algorithms with a large dataset of\nclinical DICOM images. In this report, we explore the critical challenges of\nde-identifying DICOM images, emphasize the importance of removing personally\nidentifiable information (PII) to protect patient privacy while ensuring the\ncontinued utility of medical data for research, diagnostics, and treatment, and\nprovide a comprehensive overview of the standards and regulations that govern\nthis process. Additionally, we detail the de-identification methods we applied\n- such as pixel masking, date shifting, date hashing, text recognition, text\nreplacement, and text removal - to process datasets during the test phase in\nstrict compliance with these standards. According to the final leaderboard of\nthe MIDI-B challenge, the latest version of our solution algorithm correctly\nexecuted 99.92% of the required actions and ranked 2nd out of 10 teams that\ncompleted the challenge (from a total of 22 registered teams). Finally, we\nconducted a thorough analysis of the resulting statistics and discussed the\nlimitations of current approaches and potential avenues for future improvement."}
{"id": "2508.07539", "pdf": "https://arxiv.org/pdf/2508.07539", "abs": "https://arxiv.org/abs/2508.07539", "authors": ["Yuki Shigeyasu", "Shota Harada", "Akihiko Yoshizawa", "Kazuhiro Terada", "Naoki Nakazima", "Mariyo Kurata", "Hiroyuki Abe", "Tetsuo Ushiku", "Ryoma Bise"], "title": "Domain Generalization of Pathological Image Segmentation by Patch-Level and WSI-Level Contrastive Learning", "categories": ["cs.CV"], "comment": null, "summary": "In this paper, we address domain shifts in pathological images by focusing on\nshifts within whole slide images~(WSIs), such as patient characteristics and\ntissue thickness, rather than shifts between hospitals. Traditional approaches\nrely on multi-hospital data, but data collection challenges often make this\nimpractical. Therefore, the proposed domain generalization method captures and\nleverages intra-hospital domain shifts by clustering WSI-level features from\nnon-tumor regions and treating these clusters as domains. To mitigate domain\nshift, we apply contrastive learning to reduce feature gaps between WSI pairs\nfrom different clusters. The proposed method introduces a two-stage contrastive\nlearning approach WSI-level and patch-level contrastive learning to minimize\nthese gaps effectively."}
{"id": "2508.07540", "pdf": "https://arxiv.org/pdf/2508.07540", "abs": "https://arxiv.org/abs/2508.07540", "authors": ["Junuk Cha", "Jihyeon Kim"], "title": "CoT-Pose: Chain-of-Thought Reasoning for 3D Pose Generation from Abstract Prompts", "categories": ["cs.CV"], "comment": "ICCVW'25", "summary": "Recent advances in multi-modal large language models (MLLMs) and\nchain-of-thought (CoT) reasoning have led to significant progress in image and\ntext generation tasks. However, the field of 3D human pose generation still\nfaces critical limitations. Most existing text-to-pose models rely heavily on\ndetailed (low-level) prompts that explicitly describe joint configurations. In\ncontrast, humans tend to communicate actions and intentions using abstract\n(high-level) language. This mismatch results in a practical challenge for\ndeploying pose generation systems in real-world scenarios. To bridge this gap,\nwe introduce a novel framework that incorporates CoT reasoning into the pose\ngeneration process, enabling the interpretation of abstract prompts into\naccurate 3D human poses. We further propose a data synthesis pipeline that\nautomatically generates triplets of abstract prompts, detailed prompts, and\ncorresponding 3D poses for training process. Experimental results demonstrate\nthat our reasoning-enhanced model, CoT-Pose, can effectively generate plausible\nand semantically aligned poses from abstract textual inputs. This work\nhighlights the importance of high-level understanding in pose generation and\nopens new directions for reasoning-enhanced approach for human pose generation."}
{"id": "2508.07543", "pdf": "https://arxiv.org/pdf/2508.07543", "abs": "https://arxiv.org/abs/2508.07543", "authors": ["Chidaksh Ravuru"], "title": "Commentary Generation for Soccer Highlights", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Automated soccer commentary generation has evolved from template-based\nsystems to advanced neural architectures, aiming to produce real-time\ndescriptions of sports events. While frameworks like SoccerNet-Caption laid\nfoundational work, their inability to achieve fine-grained alignment between\nvideo content and commentary remains a significant challenge. Recent efforts\nsuch as MatchTime, with its MatchVoice model, address this issue through coarse\nand fine-grained alignment techniques, achieving improved temporal\nsynchronization. In this paper, we extend MatchVoice to commentary generation\nfor soccer highlights using the GOAL dataset, which emphasizes short clips over\nentire games. We conduct extensive experiments to reproduce the original\nMatchTime results and evaluate our setup, highlighting the impact of different\ntraining configurations and hardware limitations. Furthermore, we explore the\neffect of varying window sizes on zero-shot performance. While MatchVoice\nexhibits promising generalization capabilities, our findings suggest the need\nfor integrating techniques from broader video-language domains to further\nenhance performance. Our code is available at\nhttps://github.com/chidaksh/SoccerCommentary."}
{"id": "2508.07548", "pdf": "https://arxiv.org/pdf/2508.07548", "abs": "https://arxiv.org/abs/2508.07548", "authors": ["Takehiro Yamane", "Itaru Tsuge", "Susumu Saito", "Ryoma Bise"], "title": "Adaptive Pseudo Label Selection for Individual Unlabeled Data by Positive and Unlabeled Learning", "categories": ["cs.CV"], "comment": null, "summary": "This paper proposes a novel pseudo-labeling method for medical image\nsegmentation that can perform learning on ``individual images'' to select\neffective pseudo-labels. We introduce Positive and Unlabeled Learning (PU\nlearning), which uses only positive and unlabeled data for binary\nclassification problems, to obtain the appropriate metric for discriminating\nforeground and background regions on each unlabeled image. Our PU learning\nmakes us easy to select pseudo-labels for various background regions. The\nexperimental results show the effectiveness of our method."}
{"id": "2508.07552", "pdf": "https://arxiv.org/pdf/2508.07552", "abs": "https://arxiv.org/abs/2508.07552", "authors": ["Ludan Zhang", "Sihan Wang", "Yuqi Dai", "Shuofei Qiao", "Lei He"], "title": "Decoupled Functional Evaluation of Autonomous Driving Models via Feature Map Quality Scoring", "categories": ["cs.CV"], "comment": null, "summary": "End-to-end models are emerging as the mainstream in autonomous driving\nperception and planning. However, the lack of explicit supervision signals for\nintermediate functional modules leads to opaque operational mechanisms and\nlimited interpretability, making it challenging for traditional methods to\nindependently evaluate and train these modules. Pioneering in the issue, this\nstudy builds upon the feature map-truth representation similarity-based\nevaluation framework and proposes an independent evaluation method based on\nFeature Map Convergence Score (FMCS). A Dual-Granularity Dynamic Weighted\nScoring System (DG-DWSS) is constructed, formulating a unified quantitative\nmetric - Feature Map Quality Score - to enable comprehensive evaluation of the\nquality of feature maps generated by functional modules. A CLIP-based Feature\nMap Quality Evaluation Network (CLIP-FMQE-Net) is further developed, combining\nfeature-truth encoders and quality score prediction heads to enable real-time\nquality analysis of feature maps generated by functional modules. Experimental\nresults on the NuScenes dataset demonstrate that integrating our evaluation\nmodule into the training improves 3D object detection performance, achieving a\n3.89 percent gain in NDS. These results verify the effectiveness of our method\nin enhancing feature representation quality and overall model performance."}
{"id": "2508.07557", "pdf": "https://arxiv.org/pdf/2508.07557", "abs": "https://arxiv.org/abs/2508.07557", "authors": ["Minghao Yin", "Yukang Cao", "Songyou Peng", "Kai Han"], "title": "Splat4D: Diffusion-Enhanced 4D Gaussian Splatting for Temporally and Spatially Consistent Content Creation", "categories": ["cs.CV"], "comment": null, "summary": "Generating high-quality 4D content from monocular videos for applications\nsuch as digital humans and AR/VR poses challenges in ensuring temporal and\nspatial consistency, preserving intricate details, and incorporating user\nguidance effectively. To overcome these challenges, we introduce Splat4D, a\nnovel framework enabling high-fidelity 4D content generation from a monocular\nvideo. Splat4D achieves superior performance while maintaining faithful\nspatial-temporal coherence by leveraging multi-view rendering, inconsistency\nidentification, a video diffusion model, and an asymmetric U-Net for\nrefinement. Through extensive evaluations on public benchmarks, Splat4D\nconsistently demonstrates state-of-the-art performance across various metrics,\nunderscoring the efficacy of our approach. Additionally, the versatility of\nSplat4D is validated in various applications such as text/image conditioned 4D\ngeneration, 4D human generation, and text-guided content editing, producing\ncoherent outcomes following user instructions."}
{"id": "2508.07570", "pdf": "https://arxiv.org/pdf/2508.07570", "abs": "https://arxiv.org/abs/2508.07570", "authors": ["Khanh-Binh Nguyen", "Phuoc-Nguyen Bui", "Hyunseung Choo", "Duc Thanh Nguyen"], "title": "Adaptive Cache Enhancement for Test-Time Adaptation of Vision-Language Models", "categories": ["cs.CV"], "comment": "12 pages, Under review", "summary": "Vision-language models (VLMs) exhibit remarkable zero-shot generalization but\nsuffer performance degradation under distribution shifts in downstream tasks,\nparticularly in the absence of labeled data. Test-Time Adaptation (TTA)\naddresses this challenge by enabling online optimization of VLMs during\ninference, eliminating the need for annotated data. Cache-based TTA methods\nexploit historical knowledge by maintaining a dynamic memory cache of\nlow-entropy or high-confidence samples, promoting efficient adaptation to\nout-of-distribution data. Nevertheless, these methods face two critical\nchallenges: (1) unreliable confidence metrics under significant distribution\nshifts, resulting in error accumulation within the cache and degraded\nadaptation performance; and (2) rigid decision boundaries that fail to\naccommodate substantial distributional variations, leading to suboptimal\npredictions. To overcome these limitations, we introduce the Adaptive Cache\nEnhancement (ACE) framework, which constructs a robust cache by selectively\nstoring high-confidence or low-entropy image embeddings per class, guided by\ndynamic, class-specific thresholds initialized from zero-shot statistics and\niteratively refined using an exponential moving average and\nexploration-augmented updates. This approach enables adaptive, class-wise\ndecision boundaries, ensuring robust and accurate predictions across diverse\nvisual distributions. Extensive experiments on 15 diverse benchmark datasets\ndemonstrate that ACE achieves state-of-the-art performance, delivering superior\nrobustness and generalization compared to existing TTA methods in challenging\nout-of-distribution scenarios."}
{"id": "2508.07577", "pdf": "https://arxiv.org/pdf/2508.07577", "abs": "https://arxiv.org/abs/2508.07577", "authors": ["Zhaorui Tan", "Tan Pan", "Kaizhu Huang", "Weimiao Yu", "Kai Yao", "Chen Jiang", "Qiufeng Wang", "Anh Nguyen", "Xin Guo", "Yuan Cheng", "Xi Yang"], "title": "Exploiting Layer Normalization Fine-tuning in Visual Transformer Foundation Models for Classification", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "LayerNorm is pivotal in Vision Transformers (ViTs), yet its fine-tuning\ndynamics under data scarcity and domain shifts remain underexplored. This paper\nshows that shifts in LayerNorm parameters after fine-tuning (LayerNorm shifts)\nare indicative of the transitions between source and target domains; its\nefficacy is contingent upon the degree to which the target training samples\naccurately represent the target domain, as quantified by our proposed\nFine-tuning Shift Ratio ($FSR$). Building on this, we propose a simple yet\neffective rescaling mechanism using a scalar $\\lambda$ that is negatively\ncorrelated to $FSR$ to align learned LayerNorm shifts with those ideal shifts\nachieved under fully representative data, combined with a cyclic framework that\nfurther enhances the LayerNorm fine-tuning. Extensive experiments across\nnatural and pathological images, in both in-distribution (ID) and\nout-of-distribution (OOD) settings, and various target training sample regimes\nvalidate our framework. Notably, OOD tasks tend to yield lower $FSR$ and higher\n$\\lambda$ in comparison to ID cases, especially with scarce data, indicating\nunder-represented target training samples. Moreover, ViTFs fine-tuned on\npathological data behave more like ID settings, favoring conservative LayerNorm\nupdates. Our findings illuminate the underexplored dynamics of LayerNorm in\ntransfer learning and provide practical strategies for LayerNorm fine-tuning."}
{"id": "2508.07585", "pdf": "https://arxiv.org/pdf/2508.07585", "abs": "https://arxiv.org/abs/2508.07585", "authors": ["Yu-Huan Wu", "Wei Liu", "Zi-Xuan Zhu", "Zizhou Wang", "Yong Liu", "Liangli Zhen"], "title": "GAPNet: A Lightweight Framework for Image and Video Salient Object Detection via Granularity-Aware Paradigm", "categories": ["cs.CV"], "comment": "21 pages, 7 figures, 6 tables", "summary": "Recent salient object detection (SOD) models predominantly rely on\nheavyweight backbones, incurring substantial computational cost and hindering\ntheir practical application in various real-world settings, particularly on\nedge devices. This paper presents GAPNet, a lightweight network built on the\ngranularity-aware paradigm for both image and video SOD. We assign saliency\nmaps of different granularities to supervise the multi-scale decoder\nside-outputs: coarse object locations for high-level outputs and fine-grained\nobject boundaries for low-level outputs. Specifically, our decoder is built\nwith granularity-aware connections which fuse high-level features of low\ngranularity and low-level features of high granularity, respectively. To\nsupport these connections, we design granular pyramid convolution (GPC) and\ncross-scale attention (CSA) modules for efficient fusion of low-scale and\nhigh-scale features, respectively. On top of the encoder, a self-attention\nmodule is built to learn global information, enabling accurate object\nlocalization with negligible computational cost. Unlike traditional U-Net-based\napproaches, our proposed method optimizes feature utilization and semantic\ninterpretation while applying appropriate supervision at each processing stage.\nExtensive experiments show that the proposed method achieves a new\nstate-of-the-art performance among lightweight image and video SOD models. Code\nis available at https://github.com/yuhuan-wu/GAPNet."}
{"id": "2508.07587", "pdf": "https://arxiv.org/pdf/2508.07587", "abs": "https://arxiv.org/abs/2508.07587", "authors": ["Sri Raksha Siva", "Nived Suthahar", "Prakash Boominathan", "Uma Ranjan"], "title": "Voice Pathology Detection Using Phonation", "categories": ["cs.CV", "cs.SD"], "comment": "17 Pages, 11 Figures", "summary": "Voice disorders significantly affect communication and quality of life,\nrequiring an early and accurate diagnosis. Traditional methods like\nlaryngoscopy are invasive, subjective, and often inaccessible. This research\nproposes a noninvasive, machine learning-based framework for detecting voice\npathologies using phonation data.\n  Phonation data from the Saarbr\\\"ucken Voice Database are analyzed using\nacoustic features such as Mel Frequency Cepstral Coefficients (MFCCs), chroma\nfeatures, and Mel spectrograms. Recurrent Neural Networks (RNNs), including\nLSTM and attention mechanisms, classify samples into normal and pathological\ncategories. Data augmentation techniques, including pitch shifting and Gaussian\nnoise addition, enhance model generalizability, while preprocessing ensures\nsignal quality. Scale-based features, such as H\\\"older and Hurst exponents,\nfurther capture signal irregularities and long-term dependencies.\n  The proposed framework offers a noninvasive, automated diagnostic tool for\nearly detection of voice pathologies, supporting AI-driven healthcare, and\nimproving patient outcomes."}
{"id": "2508.07596", "pdf": "https://arxiv.org/pdf/2508.07596", "abs": "https://arxiv.org/abs/2508.07596", "authors": ["Shahroz Tariq", "Simon S. Woo", "Priyanka Singh", "Irena Irmalasari", "Saakshi Gupta", "Dev Gupta"], "title": "From Prediction to Explanation: Multimodal, Explainable, and Interactive Deepfake Detection Framework for Non-Expert Users", "categories": ["cs.CV"], "comment": "11 pages, 3 tables, 5 figures, accepted for publicaiton in the 33rd\n  ACM International Conference on Multimedia (MM '25), October 27-31, 2025,\n  Dublin, Ireland", "summary": "The proliferation of deepfake technologies poses urgent challenges and\nserious risks to digital integrity, particularly within critical sectors such\nas forensics, journalism, and the legal system. While existing detection\nsystems have made significant progress in classification accuracy, they\ntypically function as black-box models, offering limited transparency and\nminimal support for human reasoning. This lack of interpretability hinders\ntheir usability in real-world decision-making contexts, especially for\nnon-expert users. In this paper, we present DF-P2E (Deepfake: Prediction to\nExplanation), a novel multimodal framework that integrates visual, semantic,\nand narrative layers of explanation to make deepfake detection interpretable\nand accessible. The framework consists of three modular components: (1) a\ndeepfake classifier with Grad-CAM-based saliency visualisation, (2) a visual\ncaptioning module that generates natural language summaries of manipulated\nregions, and (3) a narrative refinement module that uses a fine-tuned Large\nLanguage Model (LLM) to produce context-aware, user-sensitive explanations. We\ninstantiate and evaluate the framework on the DF40 benchmark, the most diverse\ndeepfake dataset to date. Experiments demonstrate that our system achieves\ncompetitive detection performance while providing high-quality explanations\naligned with Grad-CAM activations. By unifying prediction and explanation in a\ncoherent, human-aligned pipeline, this work offers a scalable approach to\ninterpretable deepfake detection, advancing the broader vision of trustworthy\nand transparent AI systems in adversarial media environments."}
{"id": "2508.07597", "pdf": "https://arxiv.org/pdf/2508.07597", "abs": "https://arxiv.org/abs/2508.07597", "authors": ["Yuang Zhang", "Junqi Cheng", "Haoyu Zhao", "Jiaxi Gu", "Fangyuan Zou", "Zenghui Lu", "Peng Shu"], "title": "ShoulderShot: Generating Over-the-Shoulder Dialogue Videos", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Over-the-shoulder dialogue videos are essential in films, short dramas, and\nadvertisements, providing visual variety and enhancing viewers' emotional\nconnection. Despite their importance, such dialogue scenes remain largely\nunderexplored in video generation research. The main challenges include\nmaintaining character consistency across different shots, creating a sense of\nspatial continuity, and generating long, multi-turn dialogues within limited\ncomputational budgets. Here, we present ShoulderShot, a framework that combines\ndual-shot generation with looping video, enabling extended dialogues while\npreserving character consistency. Our results demonstrate capabilities that\nsurpass existing methods in terms of shot-reverse-shot layout, spatial\ncontinuity, and flexibility in dialogue length, thereby opening up new\npossibilities for practical dialogue video generation. Videos and comparisons\nare available at https://shouldershot.github.io."}
{"id": "2508.07603", "pdf": "https://arxiv.org/pdf/2508.07603", "abs": "https://arxiv.org/abs/2508.07603", "authors": ["Wenhui Song", "Hanhui Li", "Jiehui Huang", "Panwen Hu", "Yuhao Cheng", "Long Chen", "Yiqiang Yan", "Xiaodan Liang"], "title": "LaVieID: Local Autoregressive Diffusion Transformers for Identity-Preserving Video Creation", "categories": ["cs.CV"], "comment": "Accepted to ACM MM 2025", "summary": "In this paper, we present LaVieID, a novel \\underline{l}ocal\n\\underline{a}utoregressive \\underline{vi}d\\underline{e}o diffusion framework\ndesigned to tackle the challenging \\underline{id}entity-preserving\ntext-to-video task. The key idea of LaVieID is to mitigate the loss of identity\ninformation inherent in the stochastic global generation process of diffusion\ntransformers (DiTs) from both spatial and temporal perspectives. Specifically,\nunlike the global and unstructured modeling of facial latent states in existing\nDiTs, LaVieID introduces a local router to explicitly represent latent states\nby weighted combinations of fine-grained local facial structures. This\nalleviates undesirable feature interference and encourages DiTs to capture\ndistinctive facial characteristics. Furthermore, a temporal autoregressive\nmodule is integrated into LaVieID to refine denoised latent tokens before video\ndecoding. This module divides latent tokens temporally into chunks, exploiting\ntheir long-range temporal dependencies to predict biases for rectifying tokens,\nthereby significantly enhancing inter-frame identity consistency. Consequently,\nLaVieID can generate high-fidelity personalized videos and achieve\nstate-of-the-art performance. Our code and models are available at\nhttps://github.com/ssugarwh/LaVieID."}
{"id": "2508.07607", "pdf": "https://arxiv.org/pdf/2508.07607", "abs": "https://arxiv.org/abs/2508.07607", "authors": ["Jian Ma", "Xujie Zhu", "Zihao Pan", "Qirong Peng", "Xu Guo", "Chen Chen", "Haonan Lu"], "title": "X2Edit: Revisiting Arbitrary-Instruction Image Editing through Self-Constructed Data and Task-Aware Representation Learning", "categories": ["cs.CV"], "comment": "https://github.com/OPPO-Mente-Lab/X2Edit", "summary": "Existing open-source datasets for arbitrary-instruction image editing remain\nsuboptimal, while a plug-and-play editing module compatible with\ncommunity-prevalent generative models is notably absent. In this paper, we\nfirst introduce the X2Edit Dataset, a comprehensive dataset covering 14 diverse\nediting tasks, including subject-driven generation. We utilize the\nindustry-leading unified image generation models and expert models to construct\nthe data. Meanwhile, we design reasonable editing instructions with the VLM and\nimplement various scoring mechanisms to filter the data. As a result, we\nconstruct 3.7 million high-quality data with balanced categories. Second, to\nbetter integrate seamlessly with community image generation models, we design\ntask-aware MoE-LoRA training based on FLUX.1, with only 8\\% of the parameters\nof the full model. To further improve the final performance, we utilize the\ninternal representations of the diffusion model and define positive/negative\nsamples based on image editing types to introduce contrastive learning.\nExtensive experiments demonstrate that the model's editing performance is\ncompetitive among many excellent models. Additionally, the constructed dataset\nexhibits substantial advantages over existing open-source datasets. The\nopen-source code, checkpoints, and datasets for X2Edit can be found at the\nfollowing link: https://github.com/OPPO-Mente-Lab/X2Edit."}
{"id": "2508.07618", "pdf": "https://arxiv.org/pdf/2508.07618", "abs": "https://arxiv.org/abs/2508.07618", "authors": ["Hyoung Suk Park", "Kiwan Jeon"], "title": "An Iterative Reconstruction Method for Dental Cone-Beam Computed Tomography with a Truncated Field of View", "categories": ["cs.CV", "68Wxx"], "comment": "8 pages, 2 figures, 2 tables", "summary": "In dental cone-beam computed tomography (CBCT), compact and cost-effective\nsystem designs often use small detectors, resulting in a truncated field of\nview (FOV) that does not fully encompass the patient's head. In iterative\nreconstruction approaches, the discrepancy between the actual projection and\nthe forward projection within the truncated FOV accumulates over iterations,\nleading to significant degradation in the reconstructed image quality. In this\nstudy, we propose a two-stage approach to mitigate truncation artifacts in\ndental CBCT. In the first stage, we employ Implicit Neural Representation\n(INR), leveraging its superior representation power, to generate a prior image\nover an extended region so that its forward projection fully covers the\npatient's head. To reduce computational and memory burdens, INR reconstruction\nis performed with a coarse voxel size. The forward projection of this prior\nimage is then used to estimate the discrepancy due to truncated FOV in the\nmeasured projection data. In the second stage, the discrepancy-corrected\nprojection data is utilized in a conventional iterative reconstruction process\nwithin the truncated region. Our numerical results demonstrate that the\nproposed two-grid approach effectively suppresses truncation artifacts, leading\nto improved CBCT image quality."}
{"id": "2508.07621", "pdf": "https://arxiv.org/pdf/2508.07621", "abs": "https://arxiv.org/abs/2508.07621", "authors": ["Yunsung Chung", "Chanho Lim", "Ghassan Bidaoui", "Christian Massad", "Nassir Marrouche", "Jihun Hamm"], "title": "SOFA: Deep Learning Framework for Simulating and Optimizing Atrial Fibrillation Ablation", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted at MICCAI 2025. This is the author's original preprint", "summary": "Atrial fibrillation (AF) is a prevalent cardiac arrhythmia often treated with\ncatheter ablation procedures, but procedural outcomes are highly variable.\nEvaluating and improving ablation efficacy is challenging due to the complex\ninteraction between patient-specific tissue and procedural factors. This paper\nasks two questions: Can AF recurrence be predicted by simulating the effects of\nprocedural parameters? How should we ablate to reduce AF recurrence? We propose\nSOFA (Simulating and Optimizing Atrial Fibrillation Ablation), a novel\ndeep-learning framework that addresses these questions. SOFA first simulates\nthe outcome of an ablation strategy by generating a post-ablation image\ndepicting scar formation, conditioned on a patient's pre-ablation LGE-MRI and\nthe specific procedural parameters used (e.g., ablation locations, duration,\ntemperature, power, and force). During this simulation, it predicts AF\nrecurrence risk. Critically, SOFA then introduces an optimization scheme that\nrefines these procedural parameters to minimize the predicted risk. Our method\nleverages a multi-modal, multi-view generator that processes 2.5D\nrepresentations of the atrium. Quantitative evaluations show that SOFA\naccurately synthesizes post-ablation images and that our optimization scheme\nleads to a 22.18\\% reduction in the model-predicted recurrence risk. To the\nbest of our knowledge, SOFA is the first framework to integrate the simulation\nof procedural effects, recurrence prediction, and parameter optimization,\noffering a novel tool for personalizing AF ablation."}
{"id": "2508.07624", "pdf": "https://arxiv.org/pdf/2508.07624", "abs": "https://arxiv.org/abs/2508.07624", "authors": ["Vishakha Lall", "Yisi Liu"], "title": "Enhancing Egocentric Object Detection in Static Environments using Graph-based Spatial Anomaly Detection and Correction", "categories": ["cs.CV"], "comment": null, "summary": "In many real-world applications involving static environments, the spatial\nlayout of objects remains consistent across instances. However,\nstate-of-the-art object detection models often fail to leverage this spatial\nprior, resulting in inconsistent predictions, missed detections, or\nmisclassifications, particularly in cluttered or occluded scenes. In this work,\nwe propose a graph-based post-processing pipeline that explicitly models the\nspatial relationships between objects to correct detection anomalies in\negocentric frames. Using a graph neural network (GNN) trained on manually\nannotated data, our model identifies invalid object class labels and predicts\ncorrected class labels based on their neighbourhood context. We evaluate our\napproach both as a standalone anomaly detection and correction framework and as\na post-processing module for standard object detectors such as YOLOv7 and\nRT-DETR. Experiments demonstrate that incorporating this spatial reasoning\nsignificantly improves detection performance, with mAP@50 gains of up to 4%.\nThis method highlights the potential of leveraging the environment's spatial\nstructure to improve reliability in object detection systems."}
{"id": "2508.07625", "pdf": "https://arxiv.org/pdf/2508.07625", "abs": "https://arxiv.org/abs/2508.07625", "authors": ["Junxiao Xue", "Xiaozhen Liu", "Jie Wang", "Xuecheng Wu", "Bin Wu"], "title": "A Trustworthy Method for Multimodal Emotion Recognition", "categories": ["cs.CV"], "comment": "Accepted for publication in Big Data Mining and Analytics (BDMA),\n  2025", "summary": "Existing emotion recognition methods mainly focus on enhancing performance by\nemploying complex deep models, typically resulting in significantly higher\nmodel complexity. Although effective, it is also crucial to ensure the\nreliability of the final decision, especially for noisy, corrupted and\nout-of-distribution data. To this end, we propose a novel emotion recognition\nmethod called trusted emotion recognition (TER), which utilizes uncertainty\nestimation to calculate the confidence value of predictions. TER combines the\nresults from multiple modalities based on their confidence values to output the\ntrusted predictions. We also provide a new evaluation criterion to assess the\nreliability of predictions. Specifically, we incorporate trusted precision and\ntrusted recall to determine the trusted threshold and formulate the trusted\nAcc. and trusted F1 score to evaluate the model's trusted performance. The\nproposed framework combines the confidence module that accordingly endows the\nmodel with reliability and robustness against possible noise or corruption. The\nextensive experimental results validate the effectiveness of our proposed\nmodel. The TER achieves state-of-the-art performance on the Music-video,\nachieving 82.40% Acc. In terms of trusted performance, TER outperforms other\nmethods on the IEMOCAP and Music-video, achieving trusted F1 scores of 0.7511\nand 0.9035, respectively."}
{"id": "2508.07626", "pdf": "https://arxiv.org/pdf/2508.07626", "abs": "https://arxiv.org/abs/2508.07626", "authors": ["Dejie Yang", "Zijing Zhao", "Yang Liu"], "title": "AR-VRM: Imitating Human Motions for Visual Robot Manipulation with Analogical Reasoning", "categories": ["cs.CV", "cs.RO"], "comment": "Accepted by ICCV2025", "summary": "Visual Robot Manipulation (VRM) aims to enable a robot to follow natural\nlanguage instructions based on robot states and visual observations, and\ntherefore requires costly multi-modal data. To compensate for the deficiency of\nrobot data, existing approaches have employed vision-language pretraining with\nlarge-scale data. However, they either utilize web data that differs from\nrobotic tasks, or train the model in an implicit way (e.g., predicting future\nframes at the pixel level), thus showing limited generalization ability under\ninsufficient robot data. In this paper, we propose to learn from large-scale\nhuman action video datasets in an explicit way (i.e., imitating human actions\nfrom hand keypoints), introducing Visual Robot Manipulation with Analogical\nReasoning (AR-VRM). To acquire action knowledge explicitly from human action\nvideos, we propose a keypoint Vision-Language Model (VLM) pretraining scheme,\nenabling the VLM to learn human action knowledge and directly predict human\nhand keypoints. During fine-tuning on robot data, to facilitate the robotic arm\nin imitating the action patterns of human motions, we first retrieve human\naction videos that perform similar manipulation tasks and have similar\nhistorical observations , and then learn the Analogical Reasoning (AR) map\nbetween human hand keypoints and robot components. Taking advantage of focusing\non action keypoints instead of irrelevant visual cues, our method achieves\nleading performance on the CALVIN benchmark {and real-world experiments}. In\nfew-shot scenarios, our AR-VRM outperforms previous methods by large margins ,\nunderscoring the effectiveness of explicitly imitating human actions under data\nscarcity."}
{"id": "2508.07647", "pdf": "https://arxiv.org/pdf/2508.07647", "abs": "https://arxiv.org/abs/2508.07647", "authors": ["Xiaohang Zhan", "Dingming Liu"], "title": "LaRender: Training-Free Occlusion Control in Image Generation via Latent Rendering", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025 (oral). Project page:\n  https://xiaohangzhan.github.io/projects/larender/", "summary": "We propose a novel training-free image generation algorithm that precisely\ncontrols the occlusion relationships between objects in an image. Existing\nimage generation methods typically rely on prompts to influence occlusion,\nwhich often lack precision. While layout-to-image methods provide control over\nobject locations, they fail to address occlusion relationships explicitly.\nGiven a pre-trained image diffusion model, our method leverages volume\nrendering principles to \"render\" the scene in latent space, guided by occlusion\nrelationships and the estimated transmittance of objects. This approach does\nnot require retraining or fine-tuning the image diffusion model, yet it enables\naccurate occlusion control due to its physics-grounded foundation. In extensive\nexperiments, our method significantly outperforms existing approaches in terms\nof occlusion accuracy. Furthermore, we demonstrate that by adjusting the\nopacities of objects or concepts during rendering, our method can achieve a\nvariety of effects, such as altering the transparency of objects, the density\nof mass (e.g., forests), the concentration of particles (e.g., rain, fog), the\nintensity of light, and the strength of lens effects, etc."}
{"id": "2508.07656", "pdf": "https://arxiv.org/pdf/2508.07656", "abs": "https://arxiv.org/abs/2508.07656", "authors": ["Yimin Fu", "Zhunga Liu", "Dongxiu Guo", "Longfei Wang"], "title": "Collaborative Learning of Scattering and Deep Features for SAR Target Recognition with Noisy Labels", "categories": ["cs.CV"], "comment": "The code will be released at https://github.com/fuyimin96/CLSDF upon\n  acceptance", "summary": "The acquisition of high-quality labeled synthetic aperture radar (SAR) data\nis challenging due to the demanding requirement for expert knowledge.\nConsequently, the presence of unreliable noisy labels is unavoidable, which\nresults in performance degradation of SAR automatic target recognition (ATR).\nExisting research on learning with noisy labels mainly focuses on image data.\nHowever, the non-intuitive visual characteristics of SAR data are insufficient\nto achieve noise-robust learning. To address this problem, we propose\ncollaborative learning of scattering and deep features (CLSDF) for SAR ATR with\nnoisy labels. Specifically, a multi-model feature fusion framework is designed\nto integrate scattering and deep features. The attributed scattering centers\n(ASCs) are treated as dynamic graph structure data, and the extracted physical\ncharacteristics effectively enrich the representation of deep image features.\nThen, the samples with clean and noisy labels are divided by modeling the loss\ndistribution with multiple class-wise Gaussian Mixture Models (GMMs).\nAfterward, the semi-supervised learning of two divergent branches is conducted\nbased on the data divided by each other. Moreover, a joint distribution\nalignment strategy is introduced to enhance the reliability of co-guessed\nlabels. Extensive experiments have been done on the Moving and Stationary\nTarget Acquisition and Recognition (MSTAR) dataset, and the results show that\nthe proposed method can achieve state-of-the-art performance under different\noperating conditions with various label noises."}
{"id": "2508.07680", "pdf": "https://arxiv.org/pdf/2508.07680", "abs": "https://arxiv.org/abs/2508.07680", "authors": ["Zhiying Li", "Junhao Wu", "Yeying Jin", "Daiheng Gao", "Yun Ji", "Kaichuan Kong", "Lei Yu", "Hao Xu", "Kai Chen", "Bruce Gu", "Nana Wang", "Zhaoxin Fan"], "title": "Undress to Redress: A Training-Free Framework for Virtual Try-On", "categories": ["cs.CV"], "comment": "13 pages, 8 figures", "summary": "Virtual try-on (VTON) is a crucial task for enhancing user experience in\nonline shopping by generating realistic garment previews on personal photos.\nAlthough existing methods have achieved impressive results, they struggle with\nlong-sleeve-to-short-sleeve conversions-a common and practical scenario-often\nproducing unrealistic outputs when exposed skin is underrepresented in the\noriginal image. We argue that this challenge arises from the ''majority''\ncompletion rule in current VTON models, which leads to inaccurate skin\nrestoration in such cases. To address this, we propose UR-VTON (Undress-Redress\nVirtual Try-ON), a novel, training-free framework that can be seamlessly\nintegrated with any existing VTON method. UR-VTON introduces an\n''undress-to-redress'' mechanism: it first reveals the user's torso by\nvirtually ''undressing,'' then applies the target short-sleeve garment,\neffectively decomposing the conversion into two more manageable steps.\nAdditionally, we incorporate Dynamic Classifier-Free Guidance scheduling to\nbalance diversity and image quality during DDPM sampling, and employ Structural\nRefiner to enhance detail fidelity using high-frequency cues. Finally, we\npresent LS-TON, a new benchmark for long-sleeve-to-short-sleeve try-on.\nExtensive experiments demonstrate that UR-VTON outperforms state-of-the-art\nmethods in both detail preservation and image quality. Code will be released\nupon acceptance."}
{"id": "2508.07682", "pdf": "https://arxiv.org/pdf/2508.07682", "abs": "https://arxiv.org/abs/2508.07682", "authors": ["Wenzhuo Ma", "Zhenzhong Chen"], "title": "DiffVC-OSD: One-Step Diffusion-based Perceptual Neural Video Compression Framework", "categories": ["cs.CV"], "comment": null, "summary": "In this work, we first propose DiffVC-OSD, a One-Step Diffusion-based\nPerceptual Neural Video Compression framework. Unlike conventional multi-step\ndiffusion-based methods, DiffVC-OSD feeds the reconstructed latent\nrepresentation directly into a One-Step Diffusion Model, enhancing perceptual\nquality through a single diffusion step guided by both temporal context and the\nlatent itself. To better leverage temporal dependencies, we design a Temporal\nContext Adapter that encodes conditional inputs into multi-level features,\noffering more fine-grained guidance for the Denoising Unet. Additionally, we\nemploy an End-to-End Finetuning strategy to improve overall compression\nperformance. Extensive experiments demonstrate that DiffVC-OSD achieves\nstate-of-the-art perceptual compression performance, offers about 20$\\times$\nfaster decoding and a 86.92\\% bitrate reduction compared to the corresponding\nmulti-step diffusion-based variant."}
{"id": "2508.07683", "pdf": "https://arxiv.org/pdf/2508.07683", "abs": "https://arxiv.org/abs/2508.07683", "authors": ["Chaohong Guo", "Xun Mo", "Yongwei Nie", "Xuemiao Xu", "Chao Xu", "Fei Yu", "Chengjiang Long"], "title": "TAR-TVG: Enhancing VLMs with Timestamp Anchor-Constrained Reasoning for Temporal Video Grounding", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Temporal Video Grounding (TVG) aims to precisely localize video segments\ncorresponding to natural language queries, which is a critical capability for\nlong-form video understanding. Although existing reinforcement learning\napproaches encourage models to generate reasoning chains before predictions,\nthey fail to explicitly constrain the reasoning process to ensure the quality\nof the final temporal predictions. To address this limitation, we propose\nTimestamp Anchor-constrained Reasoning for Temporal Video Grounding (TAR-TVG),\na novel framework that introduces timestamp anchors within the reasoning\nprocess to enforce explicit supervision to the thought content. These anchors\nserve as intermediate verification points. More importantly, we require each\nreasoning step to produce increasingly accurate temporal estimations, thereby\nensuring that the reasoning process contributes meaningfully to the final\nprediction. To address the challenge of low-probability anchor generation in\nmodels (e.g., Qwen2.5-VL-3B), we develop an efficient self-distillation\ntraining strategy: (1) initial GRPO training to collect 30K high-quality\nreasoning traces containing multiple timestamp anchors, (2) supervised\nfine-tuning (SFT) on distilled data, and (3) final GRPO optimization on the\nSFT-enhanced model. This three-stage training strategy enables robust anchor\ngeneration while maintaining reasoning quality. Experiments show that our model\nachieves state-of-the-art performance while producing interpretable, verifiable\nreasoning chains with progressively refined temporal estimations."}
{"id": "2508.07700", "pdf": "https://arxiv.org/pdf/2508.07700", "abs": "https://arxiv.org/abs/2508.07700", "authors": ["Weitao Wang", "Haoran Xu", "Jun Meng", "Haoqian Wang"], "title": "Make Your MoVe: Make Your 3D Contents by Adapting Multi-View Diffusion Models to External Editing", "categories": ["cs.CV"], "comment": null, "summary": "As 3D generation techniques continue to flourish, the demand for generating\npersonalized content is rapidly rising. Users increasingly seek to apply\nvarious editing methods to polish generated 3D content, aiming to enhance its\ncolor, style, and lighting without compromising the underlying geometry.\nHowever, most existing editing tools focus on the 2D domain, and directly\nfeeding their results into 3D generation methods (like multi-view diffusion\nmodels) will introduce information loss, degrading the quality of the final 3D\nassets. In this paper, we propose a tuning-free, plug-and-play scheme that\naligns edited assets with their original geometry in a single inference run.\nCentral to our approach is a geometry preservation module that guides the\nedited multi-view generation with original input normal latents. Besides, an\ninjection switcher is proposed to deliberately control the supervision extent\nof the original normals, ensuring the alignment between the edited color and\nnormal views. Extensive experiments show that our method consistently improves\nboth the multi-view consistency and mesh quality of edited 3D assets, across\nmultiple combinations of multi-view diffusion models and editing methods."}
{"id": "2508.07701", "pdf": "https://arxiv.org/pdf/2508.07701", "abs": "https://arxiv.org/abs/2508.07701", "authors": ["Bo Jia", "Yanan Guo", "Ying Chang", "Benkui Zhang", "Ying Xie", "Kangning Du", "Lin Cao"], "title": "Multi-view Normal and Distance Guidance Gaussian Splatting for Surface Reconstruction", "categories": ["cs.CV", "cs.RO"], "comment": "This paper has been accepted by IROS 2025", "summary": "3D Gaussian Splatting (3DGS) achieves remarkable results in the field of\nsurface reconstruction. However, when Gaussian normal vectors are aligned\nwithin the single-view projection plane, while the geometry appears reasonable\nin the current view, biases may emerge upon switching to nearby views. To\naddress the distance and global matching challenges in multi-view scenes, we\ndesign multi-view normal and distance-guided Gaussian splatting. This method\nachieves geometric depth unification and high-accuracy reconstruction by\nconstraining nearby depth maps and aligning 3D normals. Specifically, for the\nreconstruction of small indoor and outdoor scenes, we propose a multi-view\ndistance reprojection regularization module that achieves multi-view Gaussian\nalignment by computing the distance loss between two nearby views and the same\nGaussian surface. Additionally, we develop a multi-view normal enhancement\nmodule, which ensures consistency across views by matching the normals of pixel\npoints in nearby views and calculating the loss. Extensive experimental results\ndemonstrate that our method outperforms the baseline in both quantitative and\nqualitative evaluations, significantly enhancing the surface reconstruction\ncapability of 3DGS."}
{"id": "2508.07714", "pdf": "https://arxiv.org/pdf/2508.07714", "abs": "https://arxiv.org/abs/2508.07714", "authors": ["Licheng Zhang", "Bach Le", "Naveed Akhtar", "Tuan Ngo"], "title": "DoorDet: Semi-Automated Multi-Class Door Detection Dataset via Object Detection and Large Language Models", "categories": ["cs.CV", "cs.AI", "cs.ET"], "comment": null, "summary": "Accurate detection and classification of diverse door types in floor plans\ndrawings is critical for multiple applications, such as building compliance\nchecking, and indoor scene understanding. Despite their importance, publicly\navailable datasets specifically designed for fine-grained multi-class door\ndetection remain scarce. In this work, we present a semi-automated pipeline\nthat leverages a state-of-the-art object detector and a large language model\n(LLM) to construct a multi-class door detection dataset with minimal manual\neffort. Doors are first detected as a unified category using a deep object\ndetection model. Next, an LLM classifies each detected instance based on its\nvisual and contextual features. Finally, a human-in-the-loop stage ensures\nhigh-quality labels and bounding boxes. Our method significantly reduces\nannotation cost while producing a dataset suitable for benchmarking neural\nmodels in floor plan analysis. This work demonstrates the potential of\ncombining deep learning and multimodal reasoning for efficient dataset\nconstruction in complex real-world domains."}
{"id": "2508.07721", "pdf": "https://arxiv.org/pdf/2508.07721", "abs": "https://arxiv.org/abs/2508.07721", "authors": ["Daoping Zhang", "Xue-Cheng Tai", "Lok Ming Lui"], "title": "A Registration-Based Star-Shape Segmentation Model and Fast Algorithms", "categories": ["cs.CV", "cs.NA", "math.NA", "65D18, 68U10, 94A08"], "comment": null, "summary": "Image segmentation plays a crucial role in extracting objects of interest and\nidentifying their boundaries within an image. However, accurate segmentation\nbecomes challenging when dealing with occlusions, obscurities, or noise in\ncorrupted images. To tackle this challenge, prior information is often\nutilized, with recent attention on star-shape priors. In this paper, we propose\na star-shape segmentation model based on the registration framework. By\ncombining the level set representation with the registration framework and\nimposing constraints on the deformed level set function, our model enables both\nfull and partial star-shape segmentation, accommodating single or multiple\ncenters. Additionally, our approach allows for the enforcement of identified\nboundaries to pass through specified landmark locations. We tackle the proposed\nmodels using the alternating direction method of multipliers. Through numerical\nexperiments conducted on synthetic and real images, we demonstrate the efficacy\nof our approach in achieving accurate star-shape segmentation."}
{"id": "2508.07723", "pdf": "https://arxiv.org/pdf/2508.07723", "abs": "https://arxiv.org/abs/2508.07723", "authors": ["Ting Xiang", "Changjian Chen", "Zhuo Tang", "Qifeng Zhang", "Fei Lyu", "Li Yang", "Jiapeng Zhang", "Kenli Li"], "title": "Enhancing Small-Scale Dataset Expansion with Triplet-Connection-based Sample Re-Weighting", "categories": ["cs.CV"], "comment": "15 pages, 8 figures, published to ACM MM2025", "summary": "The performance of computer vision models in certain real-world applications,\nsuch as medical diagnosis, is often limited by the scarcity of available\nimages. Expanding datasets using pre-trained generative models is an effective\nsolution. However, due to the uncontrollable generation process and the\nambiguity of natural language, noisy images may be generated. Re-weighting is\nan effective way to address this issue by assigning low weights to such noisy\nimages. We first theoretically analyze three types of supervision for the\ngenerated images. Based on the theoretical analysis, we develop TriReWeight, a\ntriplet-connection-based sample re-weighting method to enhance generative data\naugmentation. Theoretically, TriReWeight can be integrated with any generative\ndata augmentation methods and never downgrade their performance. Moreover, its\ngeneralization approaches the optimal in the order $O(\\sqrt{d\\ln (n)/n})$. Our\nexperiments validate the correctness of the theoretical analysis and\ndemonstrate that our method outperforms the existing SOTA methods by $7.9\\%$ on\naverage over six natural image datasets and by $3.4\\%$ on average over three\nmedical datasets. We also experimentally validate that our method can enhance\nthe performance of different generative data augmentation methods."}
{"id": "2508.07747", "pdf": "https://arxiv.org/pdf/2508.07747", "abs": "https://arxiv.org/abs/2508.07747", "authors": ["Junhyuk So", "Juncheol Shin", "Hyunho Kook", "Eunhyeok Park"], "title": "Grouped Speculative Decoding for Autoregressive Image Generation", "categories": ["cs.CV"], "comment": "Accepted to the ICCV 2025", "summary": "Recently, autoregressive (AR) image models have demonstrated remarkable\ngenerative capabilities, positioning themselves as a compelling alternative to\ndiffusion models. However, their sequential nature leads to long inference\ntimes, limiting their practical scalability. In this work, we introduce Grouped\nSpeculative Decoding (GSD), a novel, training-free acceleration method for AR\nimage models. While recent studies have explored Speculative Decoding (SD) as a\nmeans to speed up AR image generation, existing approaches either provide only\nmodest acceleration or require additional training. Our in-depth analysis\nreveals a fundamental difference between language and image tokens: image\ntokens exhibit inherent redundancy and diversity, meaning multiple tokens can\nconvey valid semantics. However, traditional SD methods are designed to accept\nonly a single most-likely token, which fails to leverage this difference,\nleading to excessive false-negative rejections. To address this, we propose a\nnew SD strategy that evaluates clusters of visually valid tokens rather than\nrelying on a single target token. Additionally, we observe that static\nclustering based on embedding distance is ineffective, which motivates our\ndynamic GSD approach. Extensive experiments show that GSD accelerates AR image\nmodels by an average of 3.7x while preserving image quality-all without\nrequiring any additional training. The source code is available at\nhttps://github.com/junhyukso/GSD"}
{"id": "2508.07755", "pdf": "https://arxiv.org/pdf/2508.07755", "abs": "https://arxiv.org/abs/2508.07755", "authors": ["Minseo Kim", "Minchan Kwon", "Dongyeun Lee", "Yunho Jeon", "Junmo Kim"], "title": "Comparison Reveals Commonality: Customized Image Generation through Contrastive Inversion", "categories": ["cs.CV"], "comment": "Accepted at CVPR 2025 workshop (AI4CC)", "summary": "The recent demand for customized image generation raises a need for\ntechniques that effectively extract the common concept from small sets of\nimages. Existing methods typically rely on additional guidance, such as text\nprompts or spatial masks, to capture the common target concept. Unfortunately,\nrelying on manually provided guidance can lead to incomplete separation of\nauxiliary features, which degrades generation quality.In this paper, we propose\nContrastive Inversion, a novel approach that identifies the common concept by\ncomparing the input images without relying on additional information. We train\nthe target token along with the image-wise auxiliary text tokens via\ncontrastive learning, which extracts the well-disentangled true semantics of\nthe target. Then we apply disentangled cross-attention fine-tuning to improve\nconcept fidelity without overfitting. Experimental results and analysis\ndemonstrate that our method achieves a balanced, high-level performance in both\nconcept representation and editing, outperforming existing techniques."}
{"id": "2508.07759", "pdf": "https://arxiv.org/pdf/2508.07759", "abs": "https://arxiv.org/abs/2508.07759", "authors": ["Haoran Wang", "Zekun Li", "Jian Zhang", "Lei Qi", "Yinghuan Shi"], "title": "Correspondence as Video: Test-Time Adaption on SAM2 for Reference Segmentation in the Wild", "categories": ["cs.CV"], "comment": null, "summary": "Large vision models like the Segment Anything Model (SAM) exhibit significant\nlimitations when applied to downstream tasks in the wild. Consequently,\nreference segmentation, which leverages reference images and their\ncorresponding masks to impart novel knowledge to the model, emerges as a\npromising new direction for adapting vision models. However, existing reference\nsegmentation approaches predominantly rely on meta-learning, which still\nnecessitates an extensive meta-training process and brings massive data and\ncomputational cost. In this study, we propose a novel approach by representing\nthe inherent correspondence between reference-target image pairs as a pseudo\nvideo. This perspective allows the latest version of SAM, known as SAM2, which\nis equipped with interactive video object segmentation (iVOS) capabilities, to\nbe adapted to downstream tasks in a lightweight manner. We term this approach\nCorrespondence As Video for SAM (CAV-SAM). CAV-SAM comprises two key modules:\nthe Diffusion-Based Semantic Transition (DBST) module employs a diffusion model\nto construct a semantic transformation sequence, while the Test-Time Geometric\nAlignment (TTGA) module aligns the geometric changes within this sequence\nthrough test-time fine-tuning. We evaluated CAVSAM on widely-used datasets,\nachieving segmentation performance improvements exceeding 5% over SOTA methods.\nImplementation is provided in the supplementary materials."}
{"id": "2508.07766", "pdf": "https://arxiv.org/pdf/2508.07766", "abs": "https://arxiv.org/abs/2508.07766", "authors": ["Jinke Li", "Jiarui Yu", "Chenxing Wei", "Hande Dong", "Qiang Lin", "Liangjing Yang", "Zhicai Wang", "Yanbin Hao"], "title": "UniSVG: A Unified Dataset for Vector Graphic Understanding and Generation with Multimodal Large Language Models", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted at ACM MM 2025 Dataset Track", "summary": "Unlike bitmap images, scalable vector graphics (SVG) maintain quality when\nscaled, frequently employed in computer vision and artistic design in the\nrepresentation of SVG code. In this era of proliferating AI-powered systems,\nenabling AI to understand and generate SVG has become increasingly urgent.\nHowever, AI-driven SVG understanding and generation (U&G) remain significant\nchallenges. SVG code, equivalent to a set of curves and lines controlled by\nfloating-point parameters, demands high precision in SVG U&G. Besides, SVG\ngeneration operates under diverse conditional constraints, including textual\nprompts and visual references, which requires powerful multi-modal processing\nfor condition-to-SVG transformation. Recently, the rapid growth of Multi-modal\nLarge Language Models (MLLMs) have demonstrated capabilities to process\nmulti-modal inputs and generate complex vector controlling parameters,\nsuggesting the potential to address SVG U&G tasks within a unified model. To\nunlock MLLM's capabilities in the SVG area, we propose an SVG-centric dataset\ncalled UniSVG, comprising 525k data items, tailored for MLLM training and\nevaluation. To our best knowledge, it is the first comprehensive dataset\ndesigned for unified SVG generation (from textual prompts and images) and SVG\nunderstanding (color, category, usage, etc.). As expected, learning on the\nproposed dataset boosts open-source MLLMs' performance on various SVG U&G\ntasks, surpassing SOTA close-source MLLMs like GPT-4V. We release dataset,\nbenchmark, weights, codes and experiment details on\nhttps://ryanlijinke.github.io/."}
{"id": "2508.07769", "pdf": "https://arxiv.org/pdf/2508.07769", "abs": "https://arxiv.org/abs/2508.07769", "authors": ["Xiaoyan Liu", "Kangrui Li", "Jiaxin Liu"], "title": "Dream4D: Lifting Camera-Controlled I2V towards Spatiotemporally Consistent 4D Generation", "categories": ["cs.CV"], "comment": "Project Page: https://wanderer7-sk.github.io/Dream4D.github.io/", "summary": "The synthesis of spatiotemporally coherent 4D content presents fundamental\nchallenges in computer vision, requiring simultaneous modeling of high-fidelity\nspatial representations and physically plausible temporal dynamics. Current\napproaches often struggle to maintain view consistency while handling complex\nscene dynamics, particularly in large-scale environments with multiple\ninteracting elements. This work introduces Dream4D, a novel framework that\nbridges this gap through a synergy of controllable video generation and neural\n4D reconstruction. Our approach seamlessly combines a two-stage architecture:\nit first predicts optimal camera trajectories from a single image using\nfew-shot learning, then generates geometrically consistent multi-view sequences\nvia a specialized pose-conditioned diffusion process, which are finally\nconverted into a persistent 4D representation. This framework is the first to\nleverage both rich temporal priors from video diffusion models and geometric\nawareness of the reconstruction models, which significantly facilitates 4D\ngeneration and shows higher quality (e.g., mPSNR, mSSIM) over existing methods."}
{"id": "2508.07771", "pdf": "https://arxiv.org/pdf/2508.07771", "abs": "https://arxiv.org/abs/2508.07771", "authors": ["Lei Wang", "Shiming Chen", "Guo-Sen Xie", "Ziming Hong", "Chaojian Yu", "Qinmu Peng", "Xinge You"], "title": "Prototype-Guided Curriculum Learning for Zero-Shot Learning", "categories": ["cs.CV"], "comment": "12 pages, 7 figures", "summary": "In Zero-Shot Learning (ZSL), embedding-based methods enable knowledge\ntransfer from seen to unseen classes by learning a visual-semantic mapping from\nseen-class images to class-level semantic prototypes (e.g., attributes).\nHowever, these semantic prototypes are manually defined and may introduce noisy\nsupervision for two main reasons: (i) instance-level mismatch: variations in\nperspective, occlusion, and annotation bias will cause discrepancies between\nindividual sample and the class-level semantic prototypes; and (ii) class-level\nimprecision: the manually defined semantic prototypes may not accurately\nreflect the true semantics of the class. Consequently, the visual-semantic\nmapping will be misled, reducing the effectiveness of knowledge transfer to\nunseen classes. In this work, we propose a prototype-guided curriculum learning\nframework (dubbed as CLZSL), which mitigates instance-level mismatches through\na Prototype-Guided Curriculum Learning (PCL) module and addresses class-level\nimprecision via a Prototype Update (PUP) module. Specifically, the PCL module\nprioritizes samples with high cosine similarity between their visual mappings\nand the class-level semantic prototypes, and progressively advances to\nless-aligned samples, thereby reducing the interference of instance-level\nmismatches to achieve accurate visual-semantic mapping. Besides, the PUP module\ndynamically updates the class-level semantic prototypes by leveraging the\nvisual mappings learned from instances, thereby reducing class-level\nimprecision and further improving the visual-semantic mapping. Experiments were\nconducted on standard benchmark datasets-AWA2, SUN, and CUB-to verify the\neffectiveness of our method."}
{"id": "2508.07775", "pdf": "https://arxiv.org/pdf/2508.07775", "abs": "https://arxiv.org/abs/2508.07775", "authors": ["Lennart Bastian", "Mohammad Rashed", "Nassir Navab", "Tolga Birdal"], "title": "Forecasting Continuous Non-Conservative Dynamical Systems in SO(3)", "categories": ["cs.CV"], "comment": "ICCV 2025 Oral", "summary": "Modeling the rotation of moving objects is a fundamental task in computer\nvision, yet $SO(3)$ extrapolation still presents numerous challenges: (1)\nunknown quantities such as the moment of inertia complicate dynamics, (2) the\npresence of external forces and torques can lead to non-conservative\nkinematics, and (3) estimating evolving state trajectories under sparse, noisy\nobservations requires robustness. We propose modeling trajectories of noisy\npose estimates on the manifold of 3D rotations in a physically and\ngeometrically meaningful way by leveraging Neural Controlled Differential\nEquations guided with $SO(3)$ Savitzky-Golay paths. Existing extrapolation\nmethods often rely on energy conservation or constant velocity assumptions,\nlimiting their applicability in real-world scenarios involving non-conservative\nforces. In contrast, our approach is agnostic to energy and momentum\nconservation while being robust to input noise, making it applicable to\ncomplex, non-inertial systems. Our approach is easily integrated as a module in\nexisting pipelines and generalizes well to trajectories with unknown physical\nparameters. By learning to approximate object dynamics from noisy states during\ntraining, our model attains robust extrapolation capabilities in simulation and\nvarious real-world settings. Code is available at\nhttps://github.com/bastianlb/forecasting-rotational-dynamics"}
{"id": "2508.07782", "pdf": "https://arxiv.org/pdf/2508.07782", "abs": "https://arxiv.org/abs/2508.07782", "authors": ["Saihui Hou", "Chenye Wang", "Wenpeng Lang", "Zhengxiang Lan", "Yongzhen Huang"], "title": "GaitSnippet: Gait Recognition Beyond Unordered Sets and Ordered Sequences", "categories": ["cs.CV"], "comment": "13 pages, 5 figures", "summary": "Recent advancements in gait recognition have significantly enhanced\nperformance by treating silhouettes as either an unordered set or an ordered\nsequence. However, both set-based and sequence-based approaches exhibit notable\nlimitations. Specifically, set-based methods tend to overlook short-range\ntemporal context for individual frames, while sequence-based methods struggle\nto capture long-range temporal dependencies effectively. To address these\nchallenges, we draw inspiration from human identification and propose a new\nperspective that conceptualizes human gait as a composition of individualized\nactions. Each action is represented by a series of frames, randomly selected\nfrom a continuous segment of the sequence, which we term a snippet.\nFundamentally, the collection of snippets for a given sequence enables the\nincorporation of multi-scale temporal context, facilitating more comprehensive\ngait feature learning. Moreover, we introduce a non-trivial solution for\nsnippet-based gait recognition, focusing on Snippet Sampling and Snippet\nModeling as key components. Extensive experiments on four widely-used gait\ndatasets validate the effectiveness of our proposed approach and, more\nimportantly, highlight the potential of gait snippets. For instance, our method\nachieves the rank-1 accuracy of 77.5% on Gait3D and 81.7% on GREW using a 2D\nconvolution-based backbone."}
{"id": "2508.07788", "pdf": "https://arxiv.org/pdf/2508.07788", "abs": "https://arxiv.org/abs/2508.07788", "authors": ["Runze Wang", "Zeli Chen", "Zhiyun Song", "Wei Fang", "Jiajin Zhang", "Danyang Tu", "Yuxing Tang", "Minfeng Xu", "Xianghua Ye", "Le Lu", "Dakai Jin"], "title": "Anatomy-Aware Low-Dose CT Denoising via Pretrained Vision Models and Semantic-Guided Contrastive Learning", "categories": ["cs.CV"], "comment": null, "summary": "To reduce radiation exposure and improve the diagnostic efficacy of low-dose\ncomputed tomography (LDCT), numerous deep learning-based denoising methods have\nbeen developed to mitigate noise and artifacts. However, most of these\napproaches ignore the anatomical semantics of human tissues, which may\npotentially result in suboptimal denoising outcomes. To address this problem,\nwe propose ALDEN, an anatomy-aware LDCT denoising method that integrates\nsemantic features of pretrained vision models (PVMs) with adversarial and\ncontrastive learning. Specifically, we introduce an anatomy-aware discriminator\nthat dynamically fuses hierarchical semantic features from reference\nnormal-dose CT (NDCT) via cross-attention mechanisms, enabling tissue-specific\nrealism evaluation in the discriminator. In addition, we propose a\nsemantic-guided contrastive learning module that enforces anatomical\nconsistency by contrasting PVM-derived features from LDCT, denoised CT and\nNDCT, preserving tissue-specific patterns through positive pairs and\nsuppressing artifacts via dual negative pairs. Extensive experiments conducted\non two LDCT denoising datasets reveal that ALDEN achieves the state-of-the-art\nperformance, offering superior anatomy preservation and substantially reducing\nover-smoothing issue of previous work. Further validation on a downstream\nmulti-organ segmentation task (encompassing 117 anatomical structures) affirms\nthe model's ability to maintain anatomical awareness."}
{"id": "2508.07795", "pdf": "https://arxiv.org/pdf/2508.07795", "abs": "https://arxiv.org/abs/2508.07795", "authors": ["Hongrui Zheng", "Yuezun Li", "Liejun Wang", "Yunfeng Diao", "Zhiqing Guo"], "title": "Boosting Active Defense Persistence: A Two-Stage Defense Framework Combining Interruption and Poisoning Against Deepfake", "categories": ["cs.CV"], "comment": null, "summary": "Active defense strategies have been developed to counter the threat of\ndeepfake technology. However, a primary challenge is their lack of persistence,\nas their effectiveness is often short-lived. Attackers can bypass these\ndefenses by simply collecting protected samples and retraining their models.\nThis means that static defenses inevitably fail when attackers retrain their\nmodels, which severely limits practical use. We argue that an effective defense\nnot only distorts forged content but also blocks the model's ability to adapt,\nwhich occurs when attackers retrain their models on protected images. To\nachieve this, we propose an innovative Two-Stage Defense Framework (TSDF).\nBenefiting from the intensity separation mechanism designed in this paper, the\nframework uses dual-function adversarial perturbations to perform two roles.\nFirst, it can directly distort the forged results. Second, it acts as a\npoisoning vehicle that disrupts the data preparation process essential for an\nattacker's retraining pipeline. By poisoning the data source, TSDF aims to\nprevent the attacker's model from adapting to the defensive perturbations, thus\nensuring the defense remains effective long-term. Comprehensive experiments\nshow that the performance of traditional interruption methods degrades sharply\nwhen it is subjected to adversarial retraining. However, our framework shows a\nstrong dual defense capability, which can improve the persistence of active\ndefense. Our code will be available at https://github.com/vpsg-research/TSDF."}
{"id": "2508.07797", "pdf": "https://arxiv.org/pdf/2508.07797", "abs": "https://arxiv.org/abs/2508.07797", "authors": ["Xiaoqi Zhao", "Peiqian Cao", "Lihe Zhang", "Zonglei Feng", "Hanqi Liu", "Jiaming Zuo", "Youwei Pang", "Weisi Lin", "Georges El Fakhri", "Huchuan Lu", "Xiaofeng Liu"], "title": "Power Battery Detection", "categories": ["cs.CV"], "comment": "Under submission to IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (T-PAMI)", "summary": "Power batteries are essential components in electric vehicles, where internal\nstructural defects can pose serious safety risks. We conduct a comprehensive\nstudy on a new task, power battery detection (PBD), which aims to localize the\ndense endpoints of cathode and anode plates from industrial X-ray images for\nquality inspection. Manual inspection is inefficient and error-prone, while\ntraditional vision algorithms struggle with densely packed plates, low\ncontrast, scale variation, and imaging artifacts. To address this issue and\ndrive more attention into this meaningful task, we present PBD5K, the first\nlarge-scale benchmark for this task, consisting of 5,000 X-ray images from nine\nbattery types with fine-grained annotations and eight types of real-world\nvisual interference. To support scalable and consistent labeling, we develop an\nintelligent annotation pipeline that combines image filtering, model-assisted\npre-labeling, cross-verification, and layered quality evaluation. We formulate\nPBD as a point-level segmentation problem and propose MDCNeXt, a model designed\nto extract and integrate multi-dimensional structure clues including point,\nline, and count information from the plate itself. To improve discrimination\nbetween plates and suppress visual interference, MDCNeXt incorporates two state\nspace modules. The first is a prompt-filtered module that learns contrastive\nrelationships guided by task-specific prompts. The second is a density-aware\nreordering module that refines segmentation in regions with high plate density.\nIn addition, we propose a distance-adaptive mask generation strategy to provide\nrobust supervision under varying spatial distributions of anode and cathode\npositions. The source code and datasets will be publicly available at\n\\href{https://github.com/Xiaoqi-Zhao-DLUT/X-ray-PBD}{PBD5K}."}
{"id": "2508.07803", "pdf": "https://arxiv.org/pdf/2508.07803", "abs": "https://arxiv.org/abs/2508.07803", "authors": ["Yushen Xu", "Xiaosong Li", "Zhenyu Kuang", "Xiaoqi Cheng", "Haishu Tan", "Huafeng Li"], "title": "MambaTrans: Multimodal Fusion Image Translation via Large Language Model Priors for Downstream Visual Tasks", "categories": ["cs.CV"], "comment": null, "summary": "The goal of multimodal image fusion is to integrate complementary information\nfrom infrared and visible images, generating multimodal fused images for\ndownstream tasks. Existing downstream pre-training models are typically trained\non visible images. However, the significant pixel distribution differences\nbetween visible and multimodal fusion images can degrade downstream task\nperformance, sometimes even below that of using only visible images. This paper\nexplores adapting multimodal fused images with significant modality differences\nto object detection and semantic segmentation models trained on visible images.\nTo address this, we propose MambaTrans, a novel multimodal fusion image\nmodality translator. MambaTrans uses descriptions from a multimodal large\nlanguage model and masks from semantic segmentation models as input. Its core\ncomponent, the Multi-Model State Space Block, combines mask-image-text\ncross-attention and a 3D-Selective Scan Module, enhancing pure visual\ncapabilities. By leveraging object detection prior knowledge, MambaTrans\nminimizes detection loss during training and captures long-term dependencies\namong text, masks, and images. This enables favorable results in pre-trained\nmodels without adjusting their parameters. Experiments on public datasets show\nthat MambaTrans effectively improves multimodal image performance in downstream\ntasks."}
{"id": "2508.07804", "pdf": "https://arxiv.org/pdf/2508.07804", "abs": "https://arxiv.org/abs/2508.07804", "authors": ["Bao Li", "Xiaomei Zhang", "Miao Xu", "Zhaoxin Fan", "Xiangyu Zhu", "Zhen Lei"], "title": "Pose-RFT: Enhancing MLLMs for 3D Pose Generation via Hybrid Action Reinforcement Fine-Tuning", "categories": ["cs.CV"], "comment": null, "summary": "Generating 3D human poses from multimodal inputs such as images or text\nrequires models to capture both rich spatial and semantic correspondences.\nWhile pose-specific multimodal large language models (MLLMs) have shown promise\nin this task, they are typically trained with supervised objectives such as\nSMPL parameter regression or token-level prediction, which struggle to model\nthe inherent ambiguity and achieve task-specific alignment required for\naccurate 3D pose generation. To address these limitations, we propose Pose-RFT,\na reinforcement fine-tuning framework tailored for 3D human pose generation in\nMLLMs. We formulate the task as a hybrid action reinforcement learning problem\nthat jointly optimizes discrete language prediction and continuous pose\ngeneration. To this end, we introduce HyGRPO, a hybrid reinforcement learning\nalgorithm that performs group-wise reward normalization over sampled responses\nto guide joint optimization of discrete and continuous actions. Pose-RFT\nfurther incorporates task-specific reward functions to guide optimization\ntowards spatial alignment in image-to-pose generation and semantic consistency\nin text-to-pose generation. Extensive experiments on multiple pose generation\nbenchmarks demonstrate that Pose-RFT significantly improves performance over\nexisting pose-specific MLLMs, validating the effectiveness of hybrid action\nreinforcement fine-tuning for 3D pose generation."}
{"id": "2508.07811", "pdf": "https://arxiv.org/pdf/2508.07811", "abs": "https://arxiv.org/abs/2508.07811", "authors": ["Sicheng Gao", "Nancy Mehta", "Zongwei Wu", "Radu Timofte"], "title": "DiTVR: Zero-Shot Diffusion Transformer for Video Restoration", "categories": ["cs.CV"], "comment": "7 pages, 6 figures", "summary": "Video restoration aims to reconstruct high quality video sequences from low\nquality inputs, addressing tasks such as super resolution, denoising, and\ndeblurring. Traditional regression based methods often produce unrealistic\ndetails and require extensive paired datasets, while recent generative\ndiffusion models face challenges in ensuring temporal consistency. We introduce\nDiTVR, a zero shot video restoration framework that couples a diffusion\ntransformer with trajectory aware attention and a wavelet guided, flow\nconsistent sampler. Unlike prior 3D convolutional or frame wise diffusion\napproaches, our attention mechanism aligns tokens along optical flow\ntrajectories, with particular emphasis on vital layers that exhibit the highest\nsensitivity to temporal dynamics. A spatiotemporal neighbour cache dynamically\nselects relevant tokens based on motion correspondences across frames. The flow\nguided sampler injects data consistency only into low-frequency bands,\npreserving high frequency priors while accelerating convergence. DiTVR\nestablishes a new zero shot state of the art on video restoration benchmarks,\ndemonstrating superior temporal consistency and detail preservation while\nremaining robust to flow noise and occlusions."}
{"id": "2508.07812", "pdf": "https://arxiv.org/pdf/2508.07812", "abs": "https://arxiv.org/abs/2508.07812", "authors": ["Jingze Gai", "Changchun Li"], "title": "Semi-supervised Multiscale Matching for SAR-Optical Image", "categories": ["cs.CV"], "comment": "15 pages, 9 figures", "summary": "Driven by the complementary nature of optical and synthetic aperture radar\n(SAR) images, SAR-optical image matching has garnered significant interest.\nMost existing SAR-optical image matching methods aim to capture effective\nmatching features by employing the supervision of pixel-level matched\ncorrespondences within SAR-optical image pairs, which, however, suffers from\ntime-consuming and complex manual annotation, making it difficult to collect\nsufficient labeled SAR-optical image pairs. To handle this, we design a\nsemi-supervised SAR-optical image matching pipeline that leverages both scarce\nlabeled and abundant unlabeled image pairs and propose a semi-supervised\nmultiscale matching for SAR-optical image matching (S2M2-SAR). Specifically, we\npseudo-label those unlabeled SAR-optical image pairs with pseudo ground-truth\nsimilarity heatmaps by combining both deep and shallow level matching results,\nand train the matching model by employing labeled and pseudo-labeled similarity\nheatmaps. In addition, we introduce a cross-modal feature enhancement module\ntrained using a cross-modality mutual independence loss, which requires no\nground-truth labels. This unsupervised objective promotes the separation of\nmodality-shared and modality-specific features by encouraging statistical\nindependence between them, enabling effective feature disentanglement across\noptical and SAR modalities. To evaluate the effectiveness of S2M2-SAR, we\ncompare it with existing competitors on benchmark datasets. Experimental\nresults demonstrate that S2M2-SAR not only surpasses existing semi-supervised\nmethods but also achieves performance competitive with fully supervised SOTA\nmethods, demonstrating its efficiency and practical potential."}
{"id": "2508.07818", "pdf": "https://arxiv.org/pdf/2508.07818", "abs": "https://arxiv.org/abs/2508.07818", "authors": ["Chenyue Song", "Chen Hui", "Haiqi Zhu", "Feng Jiang", "Yachun Mi", "Wei Zhang", "Shaohui Liu"], "title": "Segmenting and Understanding: Region-aware Semantic Attention for Fine-grained Image Quality Assessment with Large Language Models", "categories": ["cs.CV"], "comment": null, "summary": "No-reference image quality assessment (NR-IQA) aims to simulate the process\nof perceiving image quality aligned with subjective human perception. However,\nexisting NR-IQA methods either focus on global representations that leads to\nlimited insights into the semantically salient regions or employ a uniform\nweighting for region features that weakens the sensitivity to local quality\nvariations. In this paper, we propose a fine-grained image quality assessment\nmodel, named RSFIQA, which integrates region-level distortion information to\nperceive multi-dimensional quality discrepancies. To enhance regional quality\nawareness, we first utilize the Segment Anything Model (SAM) to dynamically\npartition the input image into non-overlapping semantic regions. For each\nregion, we teach a powerful Multi-modal Large Language Model (MLLM) to extract\ndescriptive content and perceive multi-dimensional distortions, enabling a\ncomprehensive understanding of both local semantics and quality degradations.\nTo effectively leverage this information, we introduce Region-Aware Semantic\nAttention (RSA) mechanism, which generates a global attention map by\naggregating fine-grained representations from local regions. In addition,\nRSFIQA is backbone-agnostic and can be seamlessly integrated into various deep\nneural network architectures. Extensive experiments demonstrate the robustness\nand effectiveness of the proposed method, which achieves competitive quality\nprediction performance across multiple benchmark datasets."}
{"id": "2508.07819", "pdf": "https://arxiv.org/pdf/2508.07819", "abs": "https://arxiv.org/abs/2508.07819", "authors": ["Ke Ma", "Jun Long", "Hongxiao Fei", "Liujie Hua", "Yueyi Luo"], "title": "Architectural Co-Design for Zero-Shot Anomaly Detection: Decoupling Representation and Dynamically Fusing Features in CLIP", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "4 pages, 1 reference, 3 figures, icassp 2026", "summary": "Pre-trained Vision-Language Models (VLMs) face a significant adaptation gap\nwhen applied to Zero-Shot Anomaly Detection (ZSAD), stemming from their lack of\nlocal inductive biases for dense prediction and their reliance on inflexible\nfeature fusion paradigms. We address these limitations through an Architectural\nCo-Design framework that jointly refines feature representation and cross-modal\nfusion. Our method integrates a parameter-efficient Convolutional Low-Rank\nAdaptation (Conv-LoRA) adapter to inject local inductive biases for\nfine-grained representation, and introduces a Dynamic Fusion Gateway (DFG) that\nleverages visual context to adaptively modulate text prompts, enabling a\npowerful bidirectional fusion. Extensive experiments on diverse industrial and\nmedical benchmarks demonstrate superior accuracy and robustness, validating\nthat this synergistic co-design is critical for robustly adapting foundation\nmodels to dense perception tasks."}
{"id": "2508.07833", "pdf": "https://arxiv.org/pdf/2508.07833", "abs": "https://arxiv.org/abs/2508.07833", "authors": ["Animesh Jain", "Alexandros Stergiou"], "title": "MIMIC: Multimodal Inversion for Model Interpretation and Conceptualization", "categories": ["cs.CV"], "comment": "Project page: https://anaekin.github.io/MIMIC", "summary": "Vision Language Models (VLMs) encode multimodal inputs over large, complex,\nand difficult-to-interpret architectures, which limit transparency and trust.\nWe propose a Multimodal Inversion for Model Interpretation and\nConceptualization (MIMIC) framework to visualize the internal representations\nof VLMs by synthesizing visual concepts corresponding to internal encodings.\nMIMIC uses a joint VLM-based inversion and a feature alignment objective to\naccount for VLM's autoregressive processing. It additionally includes a triplet\nof regularizers for spatial alignment, natural image smoothness, and semantic\nrealism. We quantitatively and qualitatively evaluate MIMIC by inverting visual\nconcepts over a range of varying-length free-form VLM output texts. Reported\nresults include both standard visual quality metrics as well as semantic\ntext-based metrics. To the best of our knowledge, this is the first model\ninversion approach addressing visual interpretations of VLM concepts."}
{"id": "2508.07835", "pdf": "https://arxiv.org/pdf/2508.07835", "abs": "https://arxiv.org/abs/2508.07835", "authors": ["Jingna Qiu", "Nishanth Jain", "Jonas Ammeling", "Marc Aubreville", "Katharina Breininger"], "title": "Effortless Vision-Language Model Specialization in Histopathology without Annotation", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in Vision-Language Models (VLMs) in histopathology, such as\nCONCH and QuiltNet, have demonstrated impressive zero-shot classification\ncapabilities across various tasks. However, their general-purpose design may\nlead to suboptimal performance in specific downstream applications. While\nsupervised fine-tuning methods address this issue, they require manually\nlabeled samples for adaptation. This paper investigates annotation-free\nadaptation of VLMs through continued pretraining on domain- and task-relevant\nimage-caption pairs extracted from existing databases. Our experiments on two\nVLMs, CONCH and QuiltNet, across three downstream tasks reveal that these pairs\nsubstantially enhance both zero-shot and few-shot performance. Notably, with\nlarger training sizes, continued pretraining matches the performance of\nfew-shot methods while eliminating manual labeling. Its effectiveness,\ntask-agnostic design, and annotation-free workflow make it a promising pathway\nfor adapting VLMs to new histopathology tasks. Code is available at\nhttps://github.com/DeepMicroscopy/Annotation-free-VLM-specialization."}
{"id": "2508.07838", "pdf": "https://arxiv.org/pdf/2508.07838", "abs": "https://arxiv.org/abs/2508.07838", "authors": ["Qi Xiang", "Kunsong Shi", "Zhigui Lin", "Lei He"], "title": "CBDES MoE: Hierarchically Decoupled Mixture-of-Experts for Functional Modules in Autonomous Driving", "categories": ["cs.CV"], "comment": null, "summary": "Bird's Eye View (BEV) perception systems based on multi-sensor feature fusion\nhave become a fundamental cornerstone for end-to-end autonomous driving.\nHowever, existing multi-modal BEV methods commonly suffer from limited input\nadaptability, constrained modeling capacity, and suboptimal generalization. To\naddress these challenges, we propose a hierarchically decoupled\nMixture-of-Experts architecture at the functional module level, termed\nComputing Brain DEvelopment System Mixture-of-Experts (CBDES MoE). CBDES MoE\nintegrates multiple structurally heterogeneous expert networks with a\nlightweight Self-Attention Router (SAR) gating mechanism, enabling dynamic\nexpert path selection and sparse, input-aware efficient inference. To the best\nof our knowledge, this is the first modular Mixture-of-Experts framework\nconstructed at the functional module granularity within the autonomous driving\ndomain. Extensive evaluations on the real-world nuScenes dataset demonstrate\nthat CBDES MoE consistently outperforms fixed single-expert baselines in 3D\nobject detection. Compared to the strongest single-expert model, CBDES MoE\nachieves a 1.6-point increase in mAP and a 4.1-point improvement in NDS,\ndemonstrating the effectiveness and practical advantages of the proposed\napproach."}
{"id": "2508.07847", "pdf": "https://arxiv.org/pdf/2508.07847", "abs": "https://arxiv.org/abs/2508.07847", "authors": ["Shunya Nagashima", "Komei Sugiura"], "title": "Deep Space Weather Model: Long-Range Solar Flare Prediction from Multi-Wavelength Images", "categories": ["cs.CV", "cs.AI"], "comment": "ICCV 2025", "summary": "Accurate, reliable solar flare prediction is crucial for mitigating potential\ndisruptions to critical infrastructure, while predicting solar flares remains a\nsignificant challenge. Existing methods based on heuristic physical features\noften lack representation learning from solar images. On the other hand,\nend-to-end learning approaches struggle to model long-range temporal\ndependencies in solar images. In this study, we propose Deep Space Weather\nModel (Deep SWM), which is based on multiple deep state space models for\nhandling both ten-channel solar images and long-range spatio-temporal\ndependencies. Deep SWM also features a sparse masked autoencoder, a novel\npretraining strategy that employs a two-phase masking approach to preserve\ncrucial regions such as sunspots while compressing spatial information.\nFurthermore, we built FlareBench, a new public benchmark for solar flare\nprediction covering a full 11-year solar activity cycle, to validate our\nmethod. Our method outperformed baseline methods and even human expert\nperformance on standard metrics in terms of performance and reliability. The\nproject page can be found at https://keio-smilab25.github.io/DeepSWM."}
{"id": "2508.07850", "pdf": "https://arxiv.org/pdf/2508.07850", "abs": "https://arxiv.org/abs/2508.07850", "authors": ["Noriko Nitta", "Rei Miyata", "Naoto Oishi"], "title": "Morphological Analysis of Semiconductor Microstructures using Skeleton Graphs", "categories": ["cs.CV"], "comment": "CV4MS: Computer Vision for Materials Science, Workshop in conjunction\n  with the IEEE/CVF ICCV 2025", "summary": "In this paper, electron microscopy images of microstructures formed on Ge\nsurfaces by ion beam irradiation were processed to extract topological features\nas skeleton graphs, which were then embedded using a graph convolutional\nnetwork. The resulting embeddings were analyzed using principal component\nanalysis, and cluster separability in the resulting PCA space was evaluated\nusing the Davies-Bouldin index. The results indicate that variations in\nirradiation angle have a more significant impact on the morphological\nproperties of Ge surfaces than variations in irradiation fluence."}
{"id": "2508.07851", "pdf": "https://arxiv.org/pdf/2508.07851", "abs": "https://arxiv.org/abs/2508.07851", "authors": ["Konrad Reuter", "Suresh Guttikonda", "Sarah Latus", "Lennart Maack", "Christian Betz", "Tobias Maurer", "Alexander Schlaefer"], "title": "Tracking Any Point Methods for Markerless 3D Tissue Tracking in Endoscopic Stereo Images", "categories": ["cs.CV"], "comment": "Accecpted to CURAC conference 2025", "summary": "Minimally invasive surgery presents challenges such as dynamic tissue motion\nand a limited field of view. Accurate tissue tracking has the potential to\nsupport surgical guidance, improve safety by helping avoid damage to sensitive\nstructures, and enable context-aware robotic assistance during complex\nprocedures. In this work, we propose a novel method for markerless 3D tissue\ntracking by leveraging 2D Tracking Any Point (TAP) networks. Our method\ncombines two CoTracker models, one for temporal tracking and one for stereo\nmatching, to estimate 3D motion from stereo endoscopic images. We evaluate the\nsystem using a clinical laparoscopic setup and a robotic arm simulating tissue\nmotion, with experiments conducted on a synthetic 3D-printed phantom and a\nchicken tissue phantom. Tracking on the chicken tissue phantom yielded more\nreliable results, with Euclidean distance errors as low as 1.1 mm at a velocity\nof 10 mm/s. These findings highlight the potential of TAP-based models for\naccurate, markerless 3D tracking in challenging surgical scenarios."}
{"id": "2508.07863", "pdf": "https://arxiv.org/pdf/2508.07863", "abs": "https://arxiv.org/abs/2508.07863", "authors": ["Bin Cao", "Sipeng Zheng", "Ye Wang", "Lujie Xia", "Qianshan Wei", "Qin Jin", "Jing Liu", "Zongqing Lu"], "title": "Being-M0.5: A Real-Time Controllable Vision-Language-Motion Model", "categories": ["cs.CV", "cs.LG"], "comment": "16 pages", "summary": "Human motion generation has emerged as a critical technology with\ntransformative potential for real-world applications. However, existing\nvision-language-motion models (VLMMs) face significant limitations that hinder\ntheir practical deployment. We identify controllability as a main bottleneck,\nmanifesting in five key aspects: inadequate response to diverse human commands,\nlimited pose initialization capabilities, poor performance on long-term\nsequences, insufficient handling of unseen scenarios, and lack of fine-grained\ncontrol over individual body parts. To overcome these limitations, we present\nBeing-M0.5, the first real-time, controllable VLMM that achieves\nstate-of-the-art performance across multiple motion generation tasks. Our\napproach is built upon HuMo100M, the largest and most comprehensive human\nmotion dataset to date, comprising over 5 million self-collected motion\nsequences, 100 million multi-task instructional instances, and detailed\npart-level annotations that address a critical gap in existing datasets. We\nintroduce a novel part-aware residual quantization technique for motion\ntokenization that enables precise, granular control over individual body parts\nduring generation. Extensive experimental validation demonstrates Being-M0.5's\nsuperior performance across diverse motion benchmarks, while comprehensive\nefficiency analysis confirms its real-time capabilities. Our contributions\ninclude design insights and detailed computational analysis to guide future\ndevelopment of practical motion generators. We believe that HuMo100M and\nBeing-M0.5 represent significant advances that will accelerate the adoption of\nmotion generation technologies in real-world applications. The project page is\navailable at https://beingbeyond.github.io/Being-M0.5."}
{"id": "2508.07871", "pdf": "https://arxiv.org/pdf/2508.07871", "abs": "https://arxiv.org/abs/2508.07871", "authors": ["Yanshu Li", "Jianjiang Yang", "Zhennan Shen", "Ligong Han", "Haoyan Xu", "Ruixiang Tang"], "title": "CATP: Contextually Adaptive Token Pruning for Efficient and Enhanced Multimodal In-Context Learning", "categories": ["cs.CV"], "comment": "13 pages, 12 figures, 6 tables", "summary": "Modern large vision-language models (LVLMs) convert each input image into a\nlarge set of tokens, far outnumbering the text tokens. Although this improves\nvisual perception, it introduces severe image token redundancy. Because image\ntokens carry sparse information, many add little to reasoning, yet greatly\nincrease inference cost. The emerging image token pruning methods tackle this\nissue by identifying the most important tokens and discarding the rest. These\nmethods can raise efficiency with only modest performance loss. However, most\nof them only consider single-image tasks and overlook multimodal in-context\nlearning (ICL), where redundancy is greater and efficiency is more critical.\nRedundant tokens weaken the advantage of multimodal ICL for rapid domain\nadaptation and cause unstable performance. Applying existing pruning methods in\nthis setting leads to large accuracy drops, exposing a clear gap and the need\nfor new techniques. Thus, we propose Contextually Adaptive Token Pruning\n(CATP), a training-free pruning method targeted at multimodal ICL. CATP\nconsists of two stages that perform progressive pruning to fully account for\nthe complex cross-modal interactions in the input sequence. After removing\n77.8\\% of the image tokens, CATP produces an average performance gain of 0.6\\%\nover the vanilla model on four LVLMs and eight benchmarks, exceeding all\nbaselines remarkably. Meanwhile, it effectively improves efficiency by\nachieving an average reduction of 10.78\\% in inference latency. CATP enhances\nthe practical value of multimodal ICL and lays the groundwork for future\nprogress in interleaved image-text scenarios."}
{"id": "2508.07875", "pdf": "https://arxiv.org/pdf/2508.07875", "abs": "https://arxiv.org/abs/2508.07875", "authors": ["Shuo Han", "Ahmed Karam Eldaly", "Solomon Sunday Oyelere"], "title": "Towards Human-AI Collaboration System for the Detection of Invasive Ductal Carcinoma in Histopathology Images", "categories": ["cs.CV", "cs.AI", "cs.HC", "cs.LG"], "comment": null, "summary": "Invasive ductal carcinoma (IDC) is the most prevalent form of breast cancer,\nand early, accurate diagnosis is critical to improving patient survival rates\nby guiding treatment decisions. Combining medical expertise with artificial\nintelligence (AI) holds significant promise for enhancing the precision and\nefficiency of IDC detection. In this work, we propose a human-in-the-loop\n(HITL) deep learning system designed to detect IDC in histopathology images.\nThe system begins with an initial diagnosis provided by a high-performance\nEfficientNetV2S model, offering feedback from AI to the human expert. Medical\nprofessionals then review the AI-generated results, correct any misclassified\nimages, and integrate the revised labels into the training dataset, forming a\nfeedback loop from the human back to the AI. This iterative process refines the\nmodel's performance over time. The EfficientNetV2S model itself achieves\nstate-of-the-art performance compared to existing methods in the literature,\nwith an overall accuracy of 93.65\\%. Incorporating the human-in-the-loop system\nfurther improves the model's accuracy using four experimental groups with\nmisclassified images. These results demonstrate the potential of this\ncollaborative approach to enhance AI performance in diagnostic systems. This\nwork contributes to advancing automated, efficient, and highly accurate methods\nfor IDC detection through human-AI collaboration, offering a promising\ndirection for future AI-assisted medical diagnostics."}
{"id": "2508.07877", "pdf": "https://arxiv.org/pdf/2508.07877", "abs": "https://arxiv.org/abs/2508.07877", "authors": ["WonJun Moon", "Hyun Seok Seong", "Jae-Pil Heo"], "title": "Selective Contrastive Learning for Weakly Supervised Affordance Grounding", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to ICCV 2025", "summary": "Facilitating an entity's interaction with objects requires accurately\nidentifying parts that afford specific actions. Weakly supervised affordance\ngrounding (WSAG) seeks to imitate human learning from third-person\ndemonstrations, where humans intuitively grasp functional parts without needing\npixel-level annotations. To achieve this, grounding is typically learned using\na shared classifier across images from different perspectives, along with\ndistillation strategies incorporating part discovery process. However, since\naffordance-relevant parts are not always easily distinguishable, models\nprimarily rely on classification, often focusing on common class-specific\npatterns that are unrelated to affordance. To address this limitation, we move\nbeyond isolated part-level learning by introducing selective prototypical and\npixel contrastive objectives that adaptively learn affordance-relevant cues at\nboth the part and object levels, depending on the granularity of the available\ninformation. Initially, we find the action-associated objects in both\negocentric (object-focused) and exocentric (third-person example) images by\nleveraging CLIP. Then, by cross-referencing the discovered objects of\ncomplementary views, we excavate the precise part-level affordance clues in\neach perspective. By consistently learning to distinguish affordance-relevant\nregions from affordance-irrelevant background context, our approach effectively\nshifts activation from irrelevant areas toward meaningful affordance cues.\nExperimental results demonstrate the effectiveness of our method. Codes are\navailable at github.com/hynnsk/SelectiveCL."}
{"id": "2508.07878", "pdf": "https://arxiv.org/pdf/2508.07878", "abs": "https://arxiv.org/abs/2508.07878", "authors": ["Hanting Wang", "Shengpeng Ji", "Shulei Wang", "Hai Huang", "Xiao Jin", "Qifei Zhang", "Tao Jin"], "title": "TAP: Parameter-efficient Task-Aware Prompting for Adverse Weather Removal", "categories": ["cs.CV"], "comment": null, "summary": "Image restoration under adverse weather conditions has been extensively\nexplored, leading to numerous high-performance methods. In particular, recent\nadvances in All-in-One approaches have shown impressive results by training on\nmulti-task image restoration datasets. However, most of these methods rely on\ndedicated network modules or parameters for each specific degradation type,\nresulting in a significant parameter overhead. Moreover, the relatedness across\ndifferent restoration tasks is often overlooked. In light of these issues, we\npropose a parameter-efficient All-in-One image restoration framework that\nleverages task-aware enhanced prompts to tackle various adverse weather\ndegradations.Specifically, we adopt a two-stage training paradigm consisting of\na pretraining phase and a prompt-tuning phase to mitigate parameter conflicts\nacross tasks. We first employ supervised learning to acquire general\nrestoration knowledge, and then adapt the model to handle specific degradation\nvia trainable soft prompts. Crucially, we enhance these task-specific prompts\nin a task-aware manner. We apply low-rank decomposition to these prompts to\ncapture both task-general and task-specific characteristics, and impose\ncontrastive constraints to better align them with the actual inter-task\nrelatedness. These enhanced prompts not only improve the parameter efficiency\nof the restoration model but also enable more accurate task modeling, as\nevidenced by t-SNE analysis. Experimental results on different restoration\ntasks demonstrate that the proposed method achieves superior performance with\nonly 2.75M parameters."}
{"id": "2508.07897", "pdf": "https://arxiv.org/pdf/2508.07897", "abs": "https://arxiv.org/abs/2508.07897", "authors": ["Tianle Zeng", "Junlei Hu", "Gerardo Loza Galindo", "Sharib Ali", "Duygu Sarikaya", "Pietro Valdastri", "Dominic Jones"], "title": "NeeCo: Image Synthesis of Novel Instrument States Based on Dynamic and Deformable 3D Gaussian Reconstruction", "categories": ["cs.CV", "cs.AI", "I.3.3"], "comment": "13 pages, 9 figures", "summary": "Computer vision-based technologies significantly enhance surgical automation\nby advancing tool tracking, detection, and localization. However, Current\ndata-driven approaches are data-voracious, requiring large, high-quality\nlabeled image datasets, which limits their application in surgical data\nscience. Our Work introduces a novel dynamic Gaussian Splatting technique to\naddress the data scarcity in surgical image datasets. We propose a dynamic\nGaussian model to represent dynamic surgical scenes, enabling the rendering of\nsurgical instruments from unseen viewpoints and deformations with real tissue\nbackgrounds. We utilize a dynamic training adjustment strategy to address\nchallenges posed by poorly calibrated camera poses from real-world scenarios.\nAdditionally, we propose a method based on dynamic Gaussians for automatically\ngenerating annotations for our synthetic data. For evaluation, we constructed a\nnew dataset featuring seven scenes with 14,000 frames of tool and camera motion\nand tool jaw articulation, with a background of an ex-vivo porcine model. Using\nthis dataset, we synthetically replicate the scene deformation from the ground\ntruth data, allowing direct comparisons of synthetic image quality.\nExperimental results illustrate that our method generates photo-realistic\nlabeled image datasets with the highest values in Peak-Signal-to-Noise Ratio\n(29.87). We further evaluate the performance of medical-specific neural\nnetworks trained on real and synthetic images using an unseen real-world image\ndataset. Our results show that the performance of models trained on synthetic\nimages generated by the proposed method outperforms those trained with\nstate-of-the-art standard data augmentation by 10%, leading to an overall\nimprovement in model performances by nearly 15%."}
{"id": "2508.07901", "pdf": "https://arxiv.org/pdf/2508.07901", "abs": "https://arxiv.org/abs/2508.07901", "authors": ["Bowen Xue", "Qixin Yan", "Wenjing Wang", "Hao Liu", "Chen Li"], "title": "Stand-In: A Lightweight and Plug-and-Play Identity Control for Video Generation", "categories": ["cs.CV"], "comment": null, "summary": "Generating high-fidelity human videos that match user-specified identities is\nimportant yet challenging in the field of generative AI. Existing methods often\nrely on an excessive number of training parameters and lack compatibility with\nother AIGC tools. In this paper, we propose Stand-In, a lightweight and\nplug-and-play framework for identity preservation in video generation.\nSpecifically, we introduce a conditional image branch into the pre-trained\nvideo generation model. Identity control is achieved through restricted\nself-attentions with conditional position mapping, and can be learned quickly\nwith only 2000 pairs. Despite incorporating and training just $\\sim$1\\%\nadditional parameters, our framework achieves excellent results in video\nquality and identity preservation, outperforming other full-parameter training\nmethods. Moreover, our framework can be seamlessly integrated for other tasks,\nsuch as subject-driven video generation, pose-referenced video generation,\nstylization, and face swapping."}
{"id": "2508.07903", "pdf": "https://arxiv.org/pdf/2508.07903", "abs": "https://arxiv.org/abs/2508.07903", "authors": ["Johanna P. Müller", "Anika Knupfer", "Pedro Blöss", "Edoardo Berardi Vittur", "Bernhard Kainz", "Jana Hutter"], "title": "Diffusing the Blind Spot: Uterine MRI Synthesis with Diffusion Models", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted at MICCAI CAPI 2025", "summary": "Despite significant progress in generative modelling, existing diffusion\nmodels often struggle to produce anatomically precise female pelvic images,\nlimiting their application in gynaecological imaging, where data scarcity and\npatient privacy concerns are critical. To overcome these barriers, we introduce\na novel diffusion-based framework for uterine MRI synthesis, integrating both\nunconditional and conditioned Denoising Diffusion Probabilistic Models (DDPMs)\nand Latent Diffusion Models (LDMs) in 2D and 3D. Our approach generates\nanatomically coherent, high fidelity synthetic images that closely mimic real\nscans and provide valuable resources for training robust diagnostic models. We\nevaluate generative quality using advanced perceptual and distributional\nmetrics, benchmarking against standard reconstruction methods, and demonstrate\nsubstantial gains in diagnostic accuracy on a key classification task. A\nblinded expert evaluation further validates the clinical realism of our\nsynthetic images. We release our models with privacy safeguards and a\ncomprehensive synthetic uterine MRI dataset to support reproducible research\nand advance equitable AI in gynaecology."}
{"id": "2508.07904", "pdf": "https://arxiv.org/pdf/2508.07904", "abs": "https://arxiv.org/abs/2508.07904", "authors": ["Marco Peer", "Anna Scius-Bertrand", "Andreas Fischer"], "title": "CTC Transcription Alignment of the Bullinger Letters: Automatic Improvement of Annotation Quality", "categories": ["cs.CV"], "comment": "10 pages, 2 pages supplementary material. Accepted for\n  VisionDocs@ICCV2025", "summary": "Handwritten text recognition for historical documents remains challenging due\nto handwriting variability, degraded sources, and limited layout-aware\nannotations. In this work, we address annotation errors - particularly\nhyphenation issues - in the Bullinger correspondence, a large 16th-century\nletter collection. We introduce a self-training method based on a CTC alignment\nalgorithm that matches full transcriptions to text line images using dynamic\nprogramming and model output probabilities trained with the CTC loss. Our\napproach improves performance (e.g., by 1.1 percentage points CER with PyLaia)\nand increases alignment accuracy. Interestingly, we find that weaker models\nyield more accurate alignments, enabling an iterative training strategy. We\nrelease a new manually corrected subset of 100 pages from the Bullinger\ndataset, along with our code and benchmarks. Our approach can be applied\niteratively to further improve the CER as well as the alignment quality for\ntext recognition pipelines. Code and data are available via\nhttps://github.com/andreas-fischer-unifr/nntp."}
{"id": "2508.07905", "pdf": "https://arxiv.org/pdf/2508.07905", "abs": "https://arxiv.org/abs/2508.07905", "authors": ["Yongtao Ge", "Kangyang Xie", "Guangkai Xu", "Mingyu Liu", "Li Ke", "Longtao Huang", "Hui Xue", "Hao Chen", "Chunhua Shen"], "title": "Generative Video Matting", "categories": ["cs.CV"], "comment": null, "summary": "Video matting has traditionally been limited by the lack of high-quality\nground-truth data. Most existing video matting datasets provide only\nhuman-annotated imperfect alpha and foreground annotations, which must be\ncomposited to background images or videos during the training stage. Thus, the\ngeneralization capability of previous methods in real-world scenarios is\ntypically poor. In this work, we propose to solve the problem from two\nperspectives. First, we emphasize the importance of large-scale pre-training by\npursuing diverse synthetic and pseudo-labeled segmentation datasets. We also\ndevelop a scalable synthetic data generation pipeline that can render diverse\nhuman bodies and fine-grained hairs, yielding around 200 video clips with a\n3-second duration for fine-tuning. Second, we introduce a novel video matting\napproach that can effectively leverage the rich priors from pre-trained video\ndiffusion models. This architecture offers two key advantages. First, strong\npriors play a critical role in bridging the domain gap between synthetic and\nreal-world scenes. Second, unlike most existing methods that process video\nmatting frame-by-frame and use an independent decoder to aggregate temporal\ninformation, our model is inherently designed for video, ensuring strong\ntemporal consistency. We provide a comprehensive quantitative evaluation across\nthree benchmark datasets, demonstrating our approach's superior performance,\nand present comprehensive qualitative results in diverse real-world scenes,\nillustrating the strong generalization capability of our method. The code is\navailable at https://github.com/aim-uofa/GVM."}
{"id": "2508.07908", "pdf": "https://arxiv.org/pdf/2508.07908", "abs": "https://arxiv.org/abs/2508.07908", "authors": ["Xudong Cai", "Shuo Wang", "Peng Wang", "Yongcai Wang", "Zhaoxin Fan", "Wanting Li", "Tianbao Zhang", "Jianrong Tao", "Yeying Jin", "Deying Li"], "title": "Mem4D: Decoupling Static and Dynamic Memory for Dynamic Scene Reconstruction", "categories": ["cs.CV"], "comment": null, "summary": "Reconstructing dense geometry for dynamic scenes from a monocular video is a\ncritical yet challenging task. Recent memory-based methods enable efficient\nonline reconstruction, but they fundamentally suffer from a Memory Demand\nDilemma: The memory representation faces an inherent conflict between the\nlong-term stability required for static structures and the rapid, high-fidelity\ndetail retention needed for dynamic motion. This conflict forces existing\nmethods into a compromise, leading to either geometric drift in static\nstructures or blurred, inaccurate reconstructions of dynamic objects. To\naddress this dilemma, we propose Mem4D, a novel framework that decouples the\nmodeling of static geometry and dynamic motion. Guided by this insight, we\ndesign a dual-memory architecture: 1) The Transient Dynamics Memory (TDM)\nfocuses on capturing high-frequency motion details from recent frames, enabling\naccurate and fine-grained modeling of dynamic content; 2) The Persistent\nStructure Memory (PSM) compresses and preserves long-term spatial information,\nensuring global consistency and drift-free reconstruction for static elements.\nBy alternating queries to these specialized memories, Mem4D simultaneously\nmaintains static geometry with global consistency and reconstructs dynamic\nelements with high fidelity. Experiments on challenging benchmarks demonstrate\nthat our method achieves state-of-the-art or competitive performance while\nmaintaining high efficiency. Codes will be publicly available."}
{"id": "2508.07918", "pdf": "https://arxiv.org/pdf/2508.07918", "abs": "https://arxiv.org/abs/2508.07918", "authors": ["Xing Zi", "Jinghao Xiao", "Yunxiao Shi", "Xian Tao", "Jun Li", "Ali Braytee", "Mukesh Prasad"], "title": "RSVLM-QA: A Benchmark Dataset for Remote Sensing Vision Language Model-based Question Answering", "categories": ["cs.CV"], "comment": "This paper has been accepted to the proceedings of the 33rd ACM\n  International Multimedia Conference (ACM Multimedia 2025)", "summary": "Visual Question Answering (VQA) in remote sensing (RS) is pivotal for\ninterpreting Earth observation data. However, existing RS VQA datasets are\nconstrained by limitations in annotation richness, question diversity, and the\nassessment of specific reasoning capabilities. This paper introduces RSVLM-QA\ndataset, a new large-scale, content-rich VQA dataset for the RS domain.\nRSVLM-QA is constructed by integrating data from several prominent RS\nsegmentation and detection datasets: WHU, LoveDA, INRIA, and iSAID. We employ\nan innovative dual-track annotation generation pipeline. Firstly, we leverage\nLarge Language Models (LLMs), specifically GPT-4.1, with meticulously designed\nprompts to automatically generate a suite of detailed annotations including\nimage captions, spatial relations, and semantic tags, alongside complex\ncaption-based VQA pairs. Secondly, to address the challenging task of object\ncounting in RS imagery, we have developed a specialized automated process that\nextracts object counts directly from the original segmentation data; GPT-4.1\nthen formulates natural language answers from these counts, which are paired\nwith preset question templates to create counting QA pairs. RSVLM-QA comprises\n13,820 images and 162,373 VQA pairs, featuring extensive annotations and\ndiverse question types. We provide a detailed statistical analysis of the\ndataset and a comparison with existing RS VQA benchmarks, highlighting the\nsuperior depth and breadth of RSVLM-QA's annotations. Furthermore, we conduct\nbenchmark experiments on Six mainstream Vision Language Models (VLMs),\ndemonstrating that RSVLM-QA effectively evaluates and challenges the\nunderstanding and reasoning abilities of current VLMs in the RS domain. We\nbelieve RSVLM-QA will serve as a pivotal resource for the RS VQA and VLM\nresearch communities, poised to catalyze advancements in the field."}
{"id": "2508.07923", "pdf": "https://arxiv.org/pdf/2508.07923", "abs": "https://arxiv.org/abs/2508.07923", "authors": ["Jakub Binda", "Valentina Paneta", "Vasileios Eleftheriadis", "Hongkyou Chung", "Panagiotis Papadimitroulas", "Neo Christopher Chung"], "title": "Safeguarding Generative AI Applications in Preclinical Imaging through Hybrid Anomaly Detection", "categories": ["cs.CV", "cs.HC", "cs.LG"], "comment": null, "summary": "Generative AI holds great potentials to automate and enhance data synthesis\nin nuclear medicine. However, the high-stakes nature of biomedical imaging\nnecessitates robust mechanisms to detect and manage unexpected or erroneous\nmodel behavior. We introduce development and implementation of a hybrid anomaly\ndetection framework to safeguard GenAI models in BIOEMTECH's eyes(TM) systems.\nTwo applications are demonstrated: Pose2Xray, which generates synthetic X-rays\nfrom photographic mouse images, and DosimetrEYE, which estimates 3D radiation\ndose maps from 2D SPECT/CT scans. In both cases, our outlier detection (OD)\nenhances reliability, reduces manual oversight, and supports real-time quality\ncontrol. This approach strengthens the industrial viability of GenAI in\npreclinical settings by increasing robustness, scalability, and regulatory\ncompliance."}
{"id": "2508.07925", "pdf": "https://arxiv.org/pdf/2508.07925", "abs": "https://arxiv.org/abs/2508.07925", "authors": ["Jin-Seop Lee", "SungJoon Lee", "Jaehan Ahn", "YunSeok Choi", "Jee-Hyong Lee"], "title": "TAG: A Simple Yet Effective Temporal-Aware Approach for Zero-Shot Video Temporal Grounding", "categories": ["cs.CV"], "comment": null, "summary": "Video Temporal Grounding (VTG) aims to extract relevant video segments based\non a given natural language query. Recently, zero-shot VTG methods have gained\nattention by leveraging pretrained vision-language models (VLMs) to localize\ntarget moments without additional training. However, existing approaches suffer\nfrom semantic fragmentation, where temporally continuous frames sharing the\nsame semantics are split across multiple segments. When segments are\nfragmented, it becomes difficult to predict an accurate target moment that\naligns with the text query. Also, they rely on skewed similarity distributions\nfor localization, making it difficult to select the optimal segment.\nFurthermore, they heavily depend on the use of LLMs which require expensive\ninferences. To address these limitations, we propose a \\textit{TAG}, a simple\nyet effective Temporal-Aware approach for zero-shot video temporal Grounding,\nwhich incorporates temporal pooling, temporal coherence clustering, and\nsimilarity adjustment. Our proposed method effectively captures the temporal\ncontext of videos and addresses distorted similarity distributions without\ntraining. Our approach achieves state-of-the-art results on Charades-STA and\nActivityNet Captions benchmark datasets without rely on LLMs. Our code is\navailable at https://github.com/Nuetee/TAG"}
{"id": "2508.07960", "pdf": "https://arxiv.org/pdf/2508.07960", "abs": "https://arxiv.org/abs/2508.07960", "authors": ["Ajnas Muhammed", "Iurri Medvedev", "Nuno Gonçalves"], "title": "VOIDFace: A Privacy-Preserving Multi-Network Face Recognition With Enhanced Security", "categories": ["cs.CV"], "comment": "Accepted at IEEE International Joint Conference on Biometrics (IJCB)\n  2025", "summary": "Advancement of machine learning techniques, combined with the availability of\nlarge-scale datasets, has significantly improved the accuracy and efficiency of\nfacial recognition. Modern facial recognition systems are trained using large\nface datasets collected from diverse individuals or public repositories.\nHowever, for training, these datasets are often replicated and stored in\nmultiple workstations, resulting in data replication, which complicates\ndatabase management and oversight. Currently, once a user submits their face\nfor dataset preparation, they lose control over how their data is used, raising\nsignificant privacy and ethical concerns. This paper introduces VOIDFace, a\nnovel framework for facial recognition systems that addresses two major issues.\nFirst, it eliminates the need of data replication and improves data control to\nsecurely store training face data by using visual secret sharing. Second, it\nproposes a patch-based multi-training network that uses this novel training\ndata storage mechanism to develop a robust, privacy-preserving facial\nrecognition system. By integrating these advancements, VOIDFace aims to improve\nthe privacy, security, and efficiency of facial recognition training, while\nensuring greater control over sensitive personal face data. VOIDFace also\nenables users to exercise their Right-To-Be-Forgotten property to control their\npersonal data. Experimental evaluations on the VGGFace2 dataset show that\nVOIDFace provides Right-To-Be-Forgotten, improved data control, security, and\nprivacy while maintaining competitive facial recognition performance. Code is\navailable at: https://github.com/ajnasmuhammed89/VOIDFace"}
{"id": "2508.07968", "pdf": "https://arxiv.org/pdf/2508.07968", "abs": "https://arxiv.org/abs/2508.07968", "authors": ["Tony Danjun Wang", "Christian Heiliger", "Nassir Navab", "Lennart Bastian"], "title": "TrackOR: Towards Personalized Intelligent Operating Rooms Through Robust Tracking", "categories": ["cs.CV"], "comment": "Full Research Paper, presented at MICCAI'25 Workshop on Collaborative\n  Intelligence and Autonomy in Image-guided Surgery", "summary": "Providing intelligent support to surgical teams is a key frontier in\nautomated surgical scene understanding, with the long-term goal of improving\npatient outcomes. Developing personalized intelligence for all staff members\nrequires maintaining a consistent state of who is located where for long\nsurgical procedures, which still poses numerous computational challenges. We\npropose TrackOR, a framework for tackling long-term multi-person tracking and\nre-identification in the operating room. TrackOR uses 3D geometric signatures\nto achieve state-of-the-art online tracking performance (+11% Association\nAccuracy over the strongest baseline), while also enabling an effective offline\nrecovery process to create analysis-ready trajectories. Our work shows that by\nleveraging 3D geometric information, persistent identity tracking becomes\nattainable, enabling a critical shift towards the more granular, staff-centric\nanalyses required for personalized intelligent systems in the operating room.\nThis new capability opens up various applications, including our proposed\ntemporal pathway imprints that translate raw tracking data into actionable\ninsights for improving team efficiency and safety and ultimately providing\npersonalized support."}
{"id": "2508.07981", "pdf": "https://arxiv.org/pdf/2508.07981", "abs": "https://arxiv.org/abs/2508.07981", "authors": ["Fangyuan Mao", "Aiming Hao", "Jintao Chen", "Dongxia Liu", "Xiaokun Feng", "Jiashu Zhu", "Meiqi Wu", "Chubin Chen", "Jiahong Wu", "Xiangxiang Chu"], "title": "Omni-Effects: Unified and Spatially-Controllable Visual Effects Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Visual effects (VFX) are essential visual enhancements fundamental to modern\ncinematic production. Although video generation models offer cost-efficient\nsolutions for VFX production, current methods are constrained by per-effect\nLoRA training, which limits generation to single effects. This fundamental\nlimitation impedes applications that require spatially controllable composite\neffects, i.e., the concurrent generation of multiple effects at designated\nlocations. However, integrating diverse effects into a unified framework faces\nmajor challenges: interference from effect variations and spatial\nuncontrollability during multi-VFX joint training. To tackle these challenges,\nwe propose Omni-Effects, a first unified framework capable of generating\nprompt-guided effects and spatially controllable composite effects. The core of\nour framework comprises two key innovations: (1) LoRA-based Mixture of Experts\n(LoRA-MoE), which employs a group of expert LoRAs, integrating diverse effects\nwithin a unified model while effectively mitigating cross-task interference.\n(2) Spatial-Aware Prompt (SAP) incorporates spatial mask information into the\ntext token, enabling precise spatial control. Furthermore, we introduce an\nIndependent-Information Flow (IIF) module integrated within the SAP, isolating\nthe control signals corresponding to individual effects to prevent any unwanted\nblending. To facilitate this research, we construct a comprehensive VFX dataset\nOmni-VFX via a novel data collection pipeline combining image editing and\nFirst-Last Frame-to-Video (FLF2V) synthesis, and introduce a dedicated VFX\nevaluation framework for validating model performance. Extensive experiments\ndemonstrate that Omni-Effects achieves precise spatial control and diverse\neffect generation, enabling users to specify both the category and location of\ndesired effects."}
{"id": "2508.07989", "pdf": "https://arxiv.org/pdf/2508.07989", "abs": "https://arxiv.org/abs/2508.07989", "authors": ["Xiantao Zhang"], "title": "The Escalator Problem: Identifying Implicit Motion Blindness in AI for Accessibility", "categories": ["cs.CV", "cs.HC"], "comment": "9 pages, 3 figures, 2 tables. Accepted at CV4A11y, ICCV 2025", "summary": "Multimodal Large Language Models (MLLMs) hold immense promise as assistive\ntechnologies for the blind and visually impaired (BVI) community. However, we\nidentify a critical failure mode that undermines their trustworthiness in\nreal-world applications. We introduce the Escalator Problem -- the inability of\nstate-of-the-art models to perceive an escalator's direction of travel -- as a\ncanonical example of a deeper limitation we term Implicit Motion Blindness.\nThis blindness stems from the dominant frame-sampling paradigm in video\nunderstanding, which, by treating videos as discrete sequences of static\nimages, fundamentally struggles to perceive continuous, low-signal motion. As a\nposition paper, our contribution is not a new model but rather to: (I) formally\narticulate this blind spot, (II) analyze its implications for user trust, and\n(III) issue a call to action. We advocate for a paradigm shift from purely\nsemantic recognition towards robust physical perception and urge the\ndevelopment of new, human-centered benchmarks that prioritize safety,\nreliability, and the genuine needs of users in dynamic environments."}
{"id": "2508.07996", "pdf": "https://arxiv.org/pdf/2508.07996", "abs": "https://arxiv.org/abs/2508.07996", "authors": ["Thinesh Thiyakesan Ponbagavathi", "Chengzheng Yang", "Alina Roitberg"], "title": "Prompt-Guided Relational Reasoning for Social Behavior Understanding with Vision Foundation Models", "categories": ["cs.CV"], "comment": null, "summary": "Group Activity Detection (GAD) involves recognizing social groups and their\ncollective behaviors in videos. Vision Foundation Models (VFMs), like DinoV2,\noffer excellent features, but are pretrained primarily on object-centric data\nand remain underexplored for modeling group dynamics. While they are a\npromising alternative to highly task-specific GAD architectures that require\nfull fine-tuning, our initial investigation reveals that simply swapping CNN\nbackbones used in these methods with VFMs brings little gain, underscoring the\nneed for structured, group-aware reasoning on top.\n  We introduce Prompt-driven Group Activity Detection (ProGraD) -- a method\nthat bridges this gap through 1) learnable group prompts to guide the VFM\nattention toward social configurations, and 2) a lightweight two-layer\nGroupContext Transformer that infers actor-group associations and collective\nbehavior. We evaluate our approach on two recent GAD benchmarks: Cafe, which\nfeatures multiple concurrent social groups, and Social-CAD, which focuses on\nsingle-group interactions. While we surpass state-of-the-art in both settings,\nour method is especially effective in complex multi-group scenarios, where we\nyield a gain of 6.5\\% (Group mAP\\@1.0) and 8.2\\% (Group mAP\\@0.5) using only\n10M trainable parameters. Furthermore, our experiments reveal that ProGraD\nproduces interpretable attention maps, offering insights into actor-group\nreasoning. Code and models will be released."}
{"id": "2508.08004", "pdf": "https://arxiv.org/pdf/2508.08004", "abs": "https://arxiv.org/abs/2508.08004", "authors": ["Anqi Xiao", "Weichen Yu", "Hongyuan Yu"], "title": "Sample-aware RandAugment: Search-free Automatic Data Augmentation for Effective Image Recognition", "categories": ["cs.CV"], "comment": "International Journal of Computer Vision, 2025", "summary": "Automatic data augmentation (AutoDA) plays an important role in enhancing the\ngeneralization of neural networks. However, mainstream AutoDA methods often\nencounter two challenges: either the search process is excessively\ntime-consuming, hindering practical application, or the performance is\nsuboptimal due to insufficient policy adaptation during training. To address\nthese issues, we propose Sample-aware RandAugment (SRA), an asymmetric,\nsearch-free AutoDA method that dynamically adjusts augmentation policies while\nmaintaining straightforward implementation. SRA incorporates a heuristic\nscoring module that evaluates the complexity of the original training data,\nenabling the application of tailored augmentations for each sample.\nAdditionally, an asymmetric augmentation strategy is employed to maximize the\npotential of this scoring module. In multiple experimental settings, SRA\nnarrows the performance gap between search-based and search-free AutoDA\nmethods, achieving a state-of-the-art Top-1 accuracy of 78.31\\% on ImageNet\nwith ResNet-50. Notably, SRA demonstrates good compatibility with existing\naugmentation pipelines and solid generalization across new tasks, without\nrequiring hyperparameter tuning. The pretrained models leveraging SRA also\nenhance recognition in downstream object detection tasks. SRA represents a\npromising step towards simpler, more effective, and practical AutoDA designs\napplicable to a variety of future tasks. Our code is available at\n\\href{https://github.com/ainieli/Sample-awareRandAugment}{https://github.com/ainieli/Sample-awareRandAugment"}
{"id": "2508.08028", "pdf": "https://arxiv.org/pdf/2508.08028", "abs": "https://arxiv.org/abs/2508.08028", "authors": ["Tony Danjun Wang", "Tobias Czempiel", "Nassir Navab", "Lennart Bastian"], "title": "Mitigating Biases in Surgical Operating Rooms with Geometry", "categories": ["cs.CV"], "comment": "Extended Abstract, presented at the MICCAI'25 workshop on\n  Collaborative Intelligence and Autonomy in Image-guided Surgery", "summary": "Deep neural networks are prone to learning spurious correlations, exploiting\ndataset-specific artifacts rather than meaningful features for prediction. In\nsurgical operating rooms (OR), these manifest through the standardization of\nsmocks and gowns that obscure robust identifying landmarks, introducing model\nbias for tasks related to modeling OR personnel. Through gradient-based\nsaliency analysis on two public OR datasets, we reveal that CNN models succumb\nto such shortcuts, fixating on incidental visual cues such as footwear beneath\nsurgical gowns, distinctive eyewear, or other role-specific identifiers.\nAvoiding such biases is essential for the next generation of intelligent\nassistance systems in the OR, which should accurately recognize personalized\nworkflow traits, such as surgical skill level or coordination with other staff\nmembers. We address this problem by encoding personnel as 3D point cloud\nsequences, disentangling identity-relevant shape and motion patterns from\nappearance-based confounders. Our experiments demonstrate that while RGB and\ngeometric methods achieve comparable performance on datasets with apparent\nsimulation artifacts, RGB models suffer a 12% accuracy drop in realistic\nclinical settings with decreased visual diversity due to standardizations. This\nperformance gap confirms that geometric representations capture more meaningful\nbiometric features, providing an avenue to developing robust methods of\nmodeling humans in the OR."}
{"id": "2508.08038", "pdf": "https://arxiv.org/pdf/2508.08038", "abs": "https://arxiv.org/abs/2508.08038", "authors": ["Huawei Sun", "Zixu Wang", "Hao Feng", "Julius Ott", "Lorenzo Servadei", "Robert Wille"], "title": "TRIDE: A Text-assisted Radar-Image weather-aware fusion network for Depth Estimation", "categories": ["cs.CV"], "comment": "Accepted by TMLR (2025.08)", "summary": "Depth estimation, essential for autonomous driving, seeks to interpret the 3D\nenvironment surrounding vehicles. The development of radar sensors, known for\ntheir cost-efficiency and robustness, has spurred interest in radar-camera\nfusion-based solutions. However, existing algorithms fuse features from these\nmodalities without accounting for weather conditions, despite radars being\nknown to be more robust than cameras under adverse weather. Additionally, while\nVision-Language models have seen rapid advancement, utilizing language\ndescriptions alongside other modalities for depth estimation remains an open\nchallenge. This paper first introduces a text-generation strategy along with\nfeature extraction and fusion techniques that can assist monocular depth\nestimation pipelines, leading to improved accuracy across different algorithms\non the KITTI dataset. Building on this, we propose TRIDE, a radar-camera fusion\nalgorithm that enhances text feature extraction by incorporating radar point\ninformation. To address the impact of weather on sensor performance, we\nintroduce a weather-aware fusion block that adaptively adjusts radar weighting\nbased on current weather conditions. Our method, benchmarked on the nuScenes\ndataset, demonstrates performance gains over the state-of-the-art, achieving a\n12.87% improvement in MAE and a 9.08% improvement in RMSE. Code:\nhttps://github.com/harborsarah/TRIDE"}
{"id": "2508.08048", "pdf": "https://arxiv.org/pdf/2508.08048", "abs": "https://arxiv.org/abs/2508.08048", "authors": ["Peng Dai", "Feitong Tan", "Qiangeng Xu", "Yihua Huang", "David Futschik", "Ruofei Du", "Sean Fanello", "Yinda Zhang", "Xiaojuan Qi"], "title": "S^2VG: 3D Stereoscopic and Spatial Video Generation via Denoising Frame Matrix", "categories": ["cs.CV"], "comment": "immsersive video generation", "summary": "While video generation models excel at producing high-quality monocular\nvideos, generating 3D stereoscopic and spatial videos for immersive\napplications remains an underexplored challenge. We present a pose-free and\ntraining-free method that leverages an off-the-shelf monocular video generation\nmodel to produce immersive 3D videos. Our approach first warps the generated\nmonocular video into pre-defined camera viewpoints using estimated depth\ninformation, then applies a novel \\textit{frame matrix} inpainting framework.\nThis framework utilizes the original video generation model to synthesize\nmissing content across different viewpoints and timestamps, ensuring spatial\nand temporal consistency without requiring additional model fine-tuning.\nMoreover, we develop a \\dualupdate~scheme that further improves the quality of\nvideo inpainting by alleviating the negative effects propagated from\ndisoccluded areas in the latent space. The resulting multi-view videos are then\nadapted into stereoscopic pairs or optimized into 4D Gaussians for spatial\nvideo synthesis. We validate the efficacy of our proposed method by conducting\nexperiments on videos from various generative models, such as Sora, Lumiere,\nWALT, and Zeroscope. The experiments demonstrate that our method has a\nsignificant improvement over previous methods. Project page at:\nhttps://daipengwa.github.io/S-2VG_ProjectPage/"}
{"id": "2508.08058", "pdf": "https://arxiv.org/pdf/2508.08058", "abs": "https://arxiv.org/abs/2508.08058", "authors": ["Ziad Al-Haj Hemidi", "Eytan Kats", "Mattias P. Heinrich"], "title": "PrIINeR: Towards Prior-Informed Implicit Neural Representations for Accelerated MRI", "categories": ["cs.CV", "cs.LG"], "comment": "Submitted to the British Machine Vision Conference (BMVC) 2025\n  (Before peer review version)", "summary": "Accelerating Magnetic Resonance Imaging (MRI) reduces scan time but often\ndegrades image quality. While Implicit Neural Representations (INRs) show\npromise for MRI reconstruction, they struggle at high acceleration factors due\nto weak prior constraints, leading to structural loss and aliasing artefacts.\nTo address this, we propose PrIINeR, an INR-based MRI reconstruction method\nthat integrates prior knowledge from pre-trained deep learning models into the\nINR framework. By combining population-level knowledge with instance-based\noptimization and enforcing dual data consistency, PrIINeR aligns both with the\nacquired k-space data and the prior-informed reconstruction. Evaluated on the\nNYU fastMRI dataset, our method not only outperforms state-of-the-art INR-based\napproaches but also improves upon several learning-based state-of-the-art\nmethods, significantly improving structural preservation and fidelity while\neffectively removing aliasing artefacts.PrIINeR bridges deep learning and\nINR-based techniques, offering a more reliable solution for high-quality,\naccelerated MRI reconstruction. The code is publicly available on\nhttps://github.com/multimodallearning/PrIINeR."}
{"id": "2508.08066", "pdf": "https://arxiv.org/pdf/2508.08066", "abs": "https://arxiv.org/abs/2508.08066", "authors": ["Weitai Kang", "Weiming Zhuang", "Zhizhong Li", "Yan Yan", "Lingjuan Lyu"], "title": "Investigating the Design Space of Visual Grounding in Multimodal Large Language Model", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "8 pages for the main paper", "summary": "Fine-grained multimodal capability in Multimodal Large Language Models\n(MLLMs) has emerged as a critical research direction, particularly for tackling\nthe visual grounding (VG) problem. Despite the strong performance achieved by\nexisting approaches, they often employ disparate design choices when\nfine-tuning MLLMs for VG, lacking systematic verification to support these\ndesigns. To bridge this gap, this paper presents a comprehensive study of\nvarious design choices that impact the VG performance of MLLMs. We conduct our\nanalysis using LLaVA-1.5, which has been widely adopted in prior empirical\nstudies of MLLMs. While more recent models exist, we follow this convention to\nensure our findings remain broadly applicable and extendable to other\narchitectures. We cover two key aspects: (1) exploring different visual\ngrounding paradigms in MLLMs, identifying the most effective design, and\nproviding our insights; and (2) conducting ablation studies on the design of\ngrounding data to optimize MLLMs' fine-tuning for the VG task. Finally, our\nfindings contribute to a stronger MLLM for VG, achieving improvements of +5.6%\n/ +6.9% / +7.0% on RefCOCO/+/g over the LLaVA-1.5."}
{"id": "2508.08069", "pdf": "https://arxiv.org/pdf/2508.08069", "abs": "https://arxiv.org/abs/2508.08069", "authors": ["Xiaoxiao Cui", "Yiran Li", "Kai He", "Shanzhi Jiang", "Mengli Xue", "Wentao Li", "Junhong Leng", "Zhi Liu", "Lizhen Cui", "Shuo Li"], "title": "Information Bottleneck-based Causal Attention for Multi-label Medical Image Recognition", "categories": ["cs.CV"], "comment": "Early accepted by MICCAI 2025", "summary": "Multi-label classification (MLC) of medical images aims to identify multiple\ndiseases and holds significant clinical potential. A critical step is to learn\nclass-specific features for accurate diagnosis and improved interpretability\neffectively. However, current works focus primarily on causal attention to\nlearn class-specific features, yet they struggle to interpret the true cause\ndue to the inadvertent attention to class-irrelevant features. To address this\nchallenge, we propose a new structural causal model (SCM) that treats\nclass-specific attention as a mixture of causal, spurious, and noisy factors,\nand a novel Information Bottleneck-based Causal Attention (IBCA) that is\ncapable of learning the discriminative class-specific attention for MLC of\nmedical images. Specifically, we propose learning Gaussian mixture multi-label\nspatial attention to filter out class-irrelevant information and capture each\nclass-specific attention pattern. Then a contrastive enhancement-based causal\nintervention is proposed to gradually mitigate the spurious attention and\nreduce noise information by aligning multi-head attention with the Gaussian\nmixture multi-label spatial. Quantitative and ablation results on Endo and\nMuReD show that IBCA outperforms all methods. Compared to the second-best\nresults for each metric, IBCA achieves improvements of 6.35\\% in CR, 7.72\\% in\nOR, and 5.02\\% in mAP for MuReD, 1.47\\% in CR, and 1.65\\% in CF1, and 1.42\\% in\nmAP for Endo."}
{"id": "2508.08082", "pdf": "https://arxiv.org/pdf/2508.08082", "abs": "https://arxiv.org/abs/2508.08082", "authors": ["Zizheng Guo", "Bochao Zou", "Junbao Zhuo", "Huimin Ma"], "title": "ME-TST+: Micro-expression Analysis via Temporal State Transition with ROI Relationship Awareness", "categories": ["cs.CV"], "comment": null, "summary": "Micro-expressions (MEs) are regarded as important indicators of an\nindividual's intrinsic emotions, preferences, and tendencies. ME analysis\nrequires spotting of ME intervals within long video sequences and recognition\nof their corresponding emotional categories. Previous deep learning approaches\ncommonly employ sliding-window classification networks. However, the use of\nfixed window lengths and hard classification presents notable limitations in\npractice. Furthermore, these methods typically treat ME spotting and\nrecognition as two separate tasks, overlooking the essential relationship\nbetween them. To address these challenges, this paper proposes two state space\nmodel-based architectures, namely ME-TST and ME-TST+, which utilize temporal\nstate transition mechanisms to replace conventional window-level classification\nwith video-level regression. This enables a more precise characterization of\nthe temporal dynamics of MEs and supports the modeling of MEs with varying\ndurations. In ME-TST+, we further introduce multi-granularity ROI modeling and\nthe slowfast Mamba framework to alleviate information loss associated with\ntreating ME analysis as a time-series task. Additionally, we propose a synergy\nstrategy for spotting and recognition at both the feature and result levels,\nleveraging their intrinsic connection to enhance overall analysis performance.\nExtensive experiments demonstrate that the proposed methods achieve\nstate-of-the-art performance. The codes are available at\nhttps://github.com/zizheng-guo/ME-TST."}
{"id": "2508.08086", "pdf": "https://arxiv.org/pdf/2508.08086", "abs": "https://arxiv.org/abs/2508.08086", "authors": ["Zhongqi Yang", "Wenhang Ge", "Yuqi Li", "Jiaqi Chen", "Haoyuan Li", "Mengyin An", "Fei Kang", "Hua Xue", "Baixin Xu", "Yuyang Yin", "Eric Li", "Yang Liu", "Yikai Wang", "Hao-Xiang Guo", "Yahui Zhou"], "title": "Matrix-3D: Omnidirectional Explorable 3D World Generation", "categories": ["cs.CV", "cs.GR"], "comment": "Technical Report", "summary": "Explorable 3D world generation from a single image or text prompt forms a\ncornerstone of spatial intelligence. Recent works utilize video model to\nachieve wide-scope and generalizable 3D world generation. However, existing\napproaches often suffer from a limited scope in the generated scenes. In this\nwork, we propose Matrix-3D, a framework that utilize panoramic representation\nfor wide-coverage omnidirectional explorable 3D world generation that combines\nconditional video generation and panoramic 3D reconstruction. We first train a\ntrajectory-guided panoramic video diffusion model that employs scene mesh\nrenders as condition, to enable high-quality and geometrically consistent scene\nvideo generation. To lift the panorama scene video to 3D world, we propose two\nseparate methods: (1) a feed-forward large panorama reconstruction model for\nrapid 3D scene reconstruction and (2) an optimization-based pipeline for\naccurate and detailed 3D scene reconstruction. To facilitate effective\ntraining, we also introduce the Matrix-Pano dataset, the first large-scale\nsynthetic collection comprising 116K high-quality static panoramic video\nsequences with depth and trajectory annotations. Extensive experiments\ndemonstrate that our proposed framework achieves state-of-the-art performance\nin panoramic video generation and 3D world generation. See more in\nhttps://matrix-3d.github.io."}
{"id": "2508.08093", "pdf": "https://arxiv.org/pdf/2508.08093", "abs": "https://arxiv.org/abs/2508.08093", "authors": ["Md Rezwanul Haque", "Md. Milon Islam", "S M Taslim Uddin Raju", "Hamdi Altaheri", "Lobna Nassar", "Fakhri Karray"], "title": "MDD-Net: Multimodal Depression Detection through Mutual Transformer", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted for the 2025 IEEE International Conference on Systems, Man,\n  and Cybernetics (SMC), Vienna, Austria", "summary": "Depression is a major mental health condition that severely impacts the\nemotional and physical well-being of individuals. The simple nature of data\ncollection from social media platforms has attracted significant interest in\nproperly utilizing this information for mental health research. A Multimodal\nDepression Detection Network (MDD-Net), utilizing acoustic and visual data\nobtained from social media networks, is proposed in this work where mutual\ntransformers are exploited to efficiently extract and fuse multimodal features\nfor efficient depression detection. The MDD-Net consists of four core modules:\nan acoustic feature extraction module for retrieving relevant acoustic\nattributes, a visual feature extraction module for extracting significant\nhigh-level patterns, a mutual transformer for computing the correlations among\nthe generated features and fusing these features from multiple modalities, and\na detection layer for detecting depression using the fused feature\nrepresentations. The extensive experiments are performed using the multimodal\nD-Vlog dataset, and the findings reveal that the developed multimodal\ndepression detection network surpasses the state-of-the-art by up to 17.37% for\nF1-Score, demonstrating the greater performance of the proposed system. The\nsource code is accessible at\nhttps://github.com/rezwanh001/Multimodal-Depression-Detection."}
{"id": "2508.08094", "pdf": "https://arxiv.org/pdf/2508.08094", "abs": "https://arxiv.org/abs/2508.08094", "authors": ["Jiakai Lin", "Jinchang Zhang", "Ge Jin", "Wenzhan Song", "Tianming Liu", "Guoyu Lu"], "title": "3D Plant Root Skeleton Detection and Extraction", "categories": ["cs.CV"], "comment": null, "summary": "Plant roots typically exhibit a highly complex and dense architecture,\nincorporating numerous slender lateral roots and branches, which significantly\nhinders the precise capture and modeling of the entire root system.\nAdditionally, roots often lack sufficient texture and color information, making\nit difficult to identify and track root traits using visual methods. Previous\nresearch on roots has been largely confined to 2D studies; however, exploring\nthe 3D architecture of roots is crucial in botany. Since roots grow in real 3D\nspace, 3D phenotypic information is more critical for studying genetic traits\nand their impact on root development. We have introduced a 3D root skeleton\nextraction method that efficiently derives the 3D architecture of plant roots\nfrom a few images. This method includes the detection and matching of lateral\nroots, triangulation to extract the skeletal structure of lateral roots, and\nthe integration of lateral and primary roots. We developed a highly complex\nroot dataset and tested our method on it. The extracted 3D root skeletons\nshowed considerable similarity to the ground truth, validating the\neffectiveness of the model. This method can play a significant role in\nautomated breeding robots. Through precise 3D root structure analysis, breeding\nrobots can better identify plant phenotypic traits, especially root structure\nand growth patterns, helping practitioners select seeds with superior root\nsystems. This automated approach not only improves breeding efficiency but also\nreduces manual intervention, making the breeding process more intelligent and\nefficient, thus advancing modern agriculture."}
{"id": "2508.08098", "pdf": "https://arxiv.org/pdf/2508.08098", "abs": "https://arxiv.org/abs/2508.08098", "authors": ["Junzhe Xu", "Yuyang Yin", "Xi Chen"], "title": "TBAC-UniImage: Unified Understanding and Generation by Ladder-Side Diffusion Tuning", "categories": ["cs.CV"], "comment": null, "summary": "This paper introduces TBAC-UniImage, a novel unified model for multimodal\nunderstanding and generation. We achieve this by deeply integrating a\npre-trained Diffusion Model, acting as a generative ladder, with a Multimodal\nLarge Language Model (MLLM). Previous diffusion-based unified models face two\nprimary limitations. One approach uses only the MLLM's final hidden state as\nthe generative condition. This creates a shallow connection, as the generator\nis isolated from the rich, hierarchical representations within the MLLM's\nintermediate layers. The other approach, pretraining a unified generative\narchitecture from scratch, is computationally expensive and prohibitive for\nmany researchers. To overcome these issues, our work explores a new paradigm.\nInstead of relying on a single output, we use representations from multiple,\ndiverse layers of the MLLM as generative conditions for the diffusion model.\nThis method treats the pre-trained generator as a ladder, receiving guidance\nfrom various depths of the MLLM's understanding process. Consequently,\nTBAC-UniImage achieves a much deeper and more fine-grained unification of\nunderstanding and generation."}
{"id": "2508.08107", "pdf": "https://arxiv.org/pdf/2508.08107", "abs": "https://arxiv.org/abs/2508.08107", "authors": ["Danfeng Hong", "Chenyu Li", "Naoto Yokoya", "Bing Zhang", "Xiuping Jia", "Antonio Plaza", "Paolo Gamba", "Jon Atli Benediktsson", "Jocelyn Chanussot"], "title": "Hyperspectral Imaging", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Hyperspectral imaging (HSI) is an advanced sensing modality that\nsimultaneously captures spatial and spectral information, enabling\nnon-invasive, label-free analysis of material, chemical, and biological\nproperties. This Primer presents a comprehensive overview of HSI, from the\nunderlying physical principles and sensor architectures to key steps in data\nacquisition, calibration, and correction. We summarize common data structures\nand highlight classical and modern analysis methods, including dimensionality\nreduction, classification, spectral unmixing, and AI-driven techniques such as\ndeep learning. Representative applications across Earth observation, precision\nagriculture, biomedicine, industrial inspection, cultural heritage, and\nsecurity are also discussed, emphasizing HSI's ability to uncover sub-visual\nfeatures for advanced monitoring, diagnostics, and decision-making. Persistent\nchallenges, such as hardware trade-offs, acquisition variability, and the\ncomplexity of high-dimensional data, are examined alongside emerging solutions,\nincluding computational imaging, physics-informed modeling, cross-modal fusion,\nand self-supervised learning. Best practices for dataset sharing,\nreproducibility, and metadata documentation are further highlighted to support\ntransparency and reuse. Looking ahead, we explore future directions toward\nscalable, real-time, and embedded HSI systems, driven by sensor\nminiaturization, self-supervised learning, and foundation models. As HSI\nevolves into a general-purpose, cross-disciplinary platform, it holds promise\nfor transformative applications in science, technology, and society."}
{"id": "2508.08117", "pdf": "https://arxiv.org/pdf/2508.08117", "abs": "https://arxiv.org/abs/2508.08117", "authors": ["Xudong Han", "Pengcheng Fang", "Yueying Tian", "Jianhui Yu", "Xiaohao Cai", "Daniel Roggen", "Philip Birch"], "title": "GRASPTrack: Geometry-Reasoned Association via Segmentation and Projection for Multi-Object Tracking", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Multi-object tracking (MOT) in monocular videos is fundamentally challenged\nby occlusions and depth ambiguity, issues that conventional\ntracking-by-detection (TBD) methods struggle to resolve owing to a lack of\ngeometric awareness. To address these limitations, we introduce GRASPTrack, a\nnovel depth-aware MOT framework that integrates monocular depth estimation and\ninstance segmentation into a standard TBD pipeline to generate high-fidelity 3D\npoint clouds from 2D detections, thereby enabling explicit 3D geometric\nreasoning. These 3D point clouds are then voxelized to enable a precise and\nrobust Voxel-Based 3D Intersection-over-Union (IoU) for spatial association. To\nfurther enhance tracking robustness, our approach incorporates Depth-aware\nAdaptive Noise Compensation, which dynamically adjusts the Kalman filter\nprocess noise based on occlusion severity for more reliable state estimation.\nAdditionally, we propose a Depth-enhanced Observation-Centric Momentum, which\nextends the motion direction consistency from the image plane into 3D space to\nimprove motion-based association cues, particularly for objects with complex\ntrajectories. Extensive experiments on the MOT17, MOT20, and DanceTrack\nbenchmarks demonstrate that our method achieves competitive performance,\nsignificantly improving tracking robustness in complex scenes with frequent\nocclusions and intricate motion patterns."}
{"id": "2508.08123", "pdf": "https://arxiv.org/pdf/2508.08123", "abs": "https://arxiv.org/abs/2508.08123", "authors": ["Lingjing Chen", "Chengxiu Zhang", "Yinqiao Yi", "Yida Wang", "Yang Song", "Xu Yan", "Shengfang Xu", "Dalin Zhu", "Mengqiu Cao", "Yan Zhou", "Chenglong Wang", "Guang Yang"], "title": "A Physics-Driven Neural Network with Parameter Embedding for Generating Quantitative MR Maps from Weighted Images", "categories": ["cs.CV"], "comment": null, "summary": "We propose a deep learning-based approach that integrates MRI sequence\nparameters to improve the accuracy and generalizability of quantitative image\nsynthesis from clinical weighted MRI. Our physics-driven neural network embeds\nMRI sequence parameters -- repetition time (TR), echo time (TE), and inversion\ntime (TI) -- directly into the model via parameter embedding, enabling the\nnetwork to learn the underlying physical principles of MRI signal formation.\nThe model takes conventional T1-weighted, T2-weighted, and T2-FLAIR images as\ninput and synthesizes T1, T2, and proton density (PD) quantitative maps.\nTrained on healthy brain MR images, it was evaluated on both internal and\nexternal test datasets. The proposed method achieved high performance with PSNR\nvalues exceeding 34 dB and SSIM values above 0.92 for all synthesized parameter\nmaps. It outperformed conventional deep learning models in accuracy and\nrobustness, including data with previously unseen brain structures and lesions.\nNotably, our model accurately synthesized quantitative maps for these unseen\npathological regions, highlighting its superior generalization capability.\nIncorporating MRI sequence parameters via parameter embedding allows the neural\nnetwork to better learn the physical characteristics of MR signals,\nsignificantly enhancing the performance and reliability of quantitative MRI\nsynthesis. This method shows great potential for accelerating qMRI and\nimproving its clinical utility."}
{"id": "2508.08134", "pdf": "https://arxiv.org/pdf/2508.08134", "abs": "https://arxiv.org/abs/2508.08134", "authors": ["Zeqian Long", "Mingzhe Zheng", "Kunyu Feng", "Xinhua Zhang", "Hongyu Liu", "Harry Yang", "Linfeng Zhang", "Qifeng Chen", "Yue Ma"], "title": "Follow-Your-Shape: Shape-Aware Image Editing via Trajectory-Guided Region Control", "categories": ["cs.CV"], "comment": "Project webpage is available at https://follow-your-shape.github.io/", "summary": "While recent flow-based image editing models demonstrate general-purpose\ncapabilities across diverse tasks, they often struggle to specialize in\nchallenging scenarios -- particularly those involving large-scale shape\ntransformations. When performing such structural edits, these methods either\nfail to achieve the intended shape change or inadvertently alter non-target\nregions, resulting in degraded background quality. We propose\nFollow-Your-Shape, a training-free and mask-free framework that supports\nprecise and controllable editing of object shapes while strictly preserving\nnon-target content. Motivated by the divergence between inversion and editing\ntrajectories, we compute a Trajectory Divergence Map (TDM) by comparing\ntoken-wise velocity differences between the inversion and denoising paths. The\nTDM enables precise localization of editable regions and guides a Scheduled KV\nInjection mechanism that ensures stable and faithful editing. To facilitate a\nrigorous evaluation, we introduce ReShapeBench, a new benchmark comprising 120\nnew images and enriched prompt pairs specifically curated for shape-aware\nediting. Experiments demonstrate that our method achieves superior editability\nand visual fidelity, particularly in tasks requiring large-scale shape\nreplacement."}
{"id": "2508.08136", "pdf": "https://arxiv.org/pdf/2508.08136", "abs": "https://arxiv.org/abs/2508.08136", "authors": ["Yitong Yang", "Yinglin Wang", "Changshuo Wang", "Huajie Wang", "Shuting He"], "title": "FantasyStyle: Controllable Stylized Distillation for 3D Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "The success of 3DGS in generative and editing applications has sparked\ngrowing interest in 3DGS-based style transfer. However, current methods still\nface two major challenges: (1) multi-view inconsistency often leads to style\nconflicts, resulting in appearance smoothing and distortion; and (2) heavy\nreliance on VGG features, which struggle to disentangle style and content from\nstyle images, often causing content leakage and excessive stylization. To\ntackle these issues, we introduce \\textbf{FantasyStyle}, a 3DGS-based style\ntransfer framework, and the first to rely entirely on diffusion model\ndistillation. It comprises two key components: (1) \\textbf{Multi-View Frequency\nConsistency}. We enhance cross-view consistency by applying a 3D filter to\nmulti-view noisy latent, selectively reducing low-frequency components to\nmitigate stylized prior conflicts. (2) \\textbf{Controllable Stylized\nDistillation}. To suppress content leakage from style images, we introduce\nnegative guidance to exclude undesired content. In addition, we identify the\nlimitations of Score Distillation Sampling and Delta Denoising Score in 3D\nstyle transfer and remove the reconstruction term accordingly. Building on\nthese insights, we propose a controllable stylized distillation that leverages\nnegative guidance to more effectively optimize the 3D Gaussians. Extensive\nexperiments demonstrate that our method consistently outperforms\nstate-of-the-art approaches, achieving higher stylization quality and visual\nrealism across various scenes and styles."}
{"id": "2508.08141", "pdf": "https://arxiv.org/pdf/2508.08141", "abs": "https://arxiv.org/abs/2508.08141", "authors": ["Nicholas Klein", "Hemlata Tak", "James Fullwood", "Krishna Regmi", "Leonidas Spinoulas", "Ganesh Sivaraman", "Tianxiang Chen", "Elie Khoury"], "title": "Pindrop it! Audio and Visual Deepfake Countermeasures for Robust Detection and Fine Grained-Localization", "categories": ["cs.CV", "cs.SD", "eess.AS"], "comment": null, "summary": "The field of visual and audio generation is burgeoning with new\nstate-of-the-art methods. This rapid proliferation of new techniques\nunderscores the need for robust solutions for detecting synthetic content in\nvideos. In particular, when fine-grained alterations via localized\nmanipulations are performed in visual, audio, or both domains, these subtle\nmodifications add challenges to the detection algorithms. This paper presents\nsolutions for the problems of deepfake video classification and localization.\nThe methods were submitted to the ACM 1M Deepfakes Detection Challenge,\nachieving the best performance in the temporal localization task and a top four\nranking in the classification task for the TestA split of the evaluation\ndataset."}
{"id": "2508.08165", "pdf": "https://arxiv.org/pdf/2508.08165", "abs": "https://arxiv.org/abs/2508.08165", "authors": ["Yan Wang", "Da-Wei Zhou", "Han-Jia Ye"], "title": "Integrating Task-Specific and Universal Adapters for Pre-Trained Model-based Class-Incremental Learning", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted to ICCV 2025. Code is available at:\n  https://github.com/LAMDA-CL/ICCV2025-TUNA", "summary": "Class-Incremental Learning (CIL) requires a learning system to continually\nlearn new classes without forgetting. Existing pre-trained model-based CIL\nmethods often freeze the pre-trained network and adapt to incremental tasks\nusing additional lightweight modules such as adapters. However, incorrect\nmodule selection during inference hurts performance, and task-specific modules\noften overlook shared general knowledge, leading to errors on distinguishing\nbetween similar classes across tasks. To address the aforementioned challenges,\nwe propose integrating Task-Specific and Universal Adapters (TUNA) in this\npaper. Specifically, we train task-specific adapters to capture the most\ncrucial features relevant to their respective tasks and introduce an\nentropy-based selection mechanism to choose the most suitable adapter.\nFurthermore, we leverage an adapter fusion strategy to construct a universal\nadapter, which encodes the most discriminative features shared across tasks. We\ncombine task-specific and universal adapter predictions to harness both\nspecialized and general knowledge during inference. Extensive experiments on\nvarious benchmark datasets demonstrate the state-of-the-art performance of our\napproach. Code is available at: https://github.com/LAMDA-CL/ICCV2025-TUNA"}
{"id": "2508.08170", "pdf": "https://arxiv.org/pdf/2508.08170", "abs": "https://arxiv.org/abs/2508.08170", "authors": ["Chaojun Ni", "Guosheng Zhao", "Xiaofeng Wang", "Zheng Zhu", "Wenkang Qin", "Xinze Chen", "Guanghong Jia", "Guan Huang", "Wenjun Mei"], "title": "ReconDreamer-RL: Enhancing Reinforcement Learning via Diffusion-based Scene Reconstruction", "categories": ["cs.CV"], "comment": null, "summary": "Reinforcement learning for training end-to-end autonomous driving models in\nclosed-loop simulations is gaining growing attention. However, most simulation\nenvironments differ significantly from real-world conditions, creating a\nsubstantial simulation-to-reality (sim2real) gap. To bridge this gap, some\napproaches utilize scene reconstruction techniques to create photorealistic\nenvironments as a simulator. While this improves realistic sensor simulation,\nthese methods are inherently constrained by the distribution of the training\ndata, making it difficult to render high-quality sensor data for novel\ntrajectories or corner case scenarios. Therefore, we propose ReconDreamer-RL, a\nframework designed to integrate video diffusion priors into scene\nreconstruction to aid reinforcement learning, thereby enhancing end-to-end\nautonomous driving training. Specifically, in ReconDreamer-RL, we introduce\nReconSimulator, which combines the video diffusion prior for appearance\nmodeling and incorporates a kinematic model for physical modeling, thereby\nreconstructing driving scenarios from real-world data. This narrows the\nsim2real gap for closed-loop evaluation and reinforcement learning. To cover\nmore corner-case scenarios, we introduce the Dynamic Adversary Agent (DAA),\nwhich adjusts the trajectories of surrounding vehicles relative to the ego\nvehicle, autonomously generating corner-case traffic scenarios (e.g., cut-in).\nFinally, the Cousin Trajectory Generator (CTG) is proposed to address the issue\nof training data distribution, which is often biased toward simple\nstraight-line movements. Experiments show that ReconDreamer-RL improves\nend-to-end autonomous driving training, outperforming imitation learning\nmethods with a 5x reduction in the Collision Ratio."}
{"id": "2508.08173", "pdf": "https://arxiv.org/pdf/2508.08173", "abs": "https://arxiv.org/abs/2508.08173", "authors": ["Chongke Bi", "Xin Gao", "Jiangkang Deng", "Guan"], "title": "CD-TVD: Contrastive Diffusion for 3D Super-Resolution with Scarce High-Resolution Time-Varying Data", "categories": ["cs.CV"], "comment": "Time-varying data visualization, deep learning, super-resolution,\n  diffusion model", "summary": "Large-scale scientific simulations require significant resources to generate\nhigh-resolution time-varying data (TVD). While super-resolution is an efficient\npost-processing strategy to reduce costs, existing methods rely on a large\namount of HR training data, limiting their applicability to diverse simulation\nscenarios. To address this constraint, we proposed CD-TVD, a novel framework\nthat combines contrastive learning and an improved diffusion-based\nsuper-resolution model to achieve accurate 3D super-resolution from limited\ntime-step high-resolution data. During pre-training on historical simulation\ndata, the contrastive encoder and diffusion superresolution modules learn\ndegradation patterns and detailed features of high-resolution and\nlow-resolution samples. In the training phase, the improved diffusion model\nwith a local attention mechanism is fine-tuned using only one newly generated\nhigh-resolution timestep, leveraging the degradation knowledge learned by the\nencoder. This design minimizes the reliance on large-scale high-resolution\ndatasets while maintaining the capability to recover fine-grained details.\nExperimental results on fluid and atmospheric simulation datasets confirm that\nCD-TVD delivers accurate and resource-efficient 3D super-resolution, marking a\nsignificant advancement in data augmentation for large-scale scientific\nsimulations. The code is available at\nhttps://github.com/Xin-Gao-private/CD-TVD."}
{"id": "2508.08177", "pdf": "https://arxiv.org/pdf/2508.08177", "abs": "https://arxiv.org/abs/2508.08177", "authors": ["Zhonghao Yan", "Muxi Diao", "Yuxuan Yang", "Jiayuan Xu", "Kaizhou Zhang", "Ruoyan Jing", "Lele Yang", "Yanxi Liu", "Kongming Liang", "Zhanyu Ma"], "title": "MedReasoner: Reinforcement Learning Drives Reasoning Grounding from Clinical Thought to Pixel-Level Precision", "categories": ["cs.CV", "cs.AI"], "comment": "37 pages", "summary": "Accurately grounding regions of interest (ROIs) is critical for diagnosis and\ntreatment planning in medical imaging. While multimodal large language models\n(MLLMs) combine visual perception with natural language, current\nmedical-grounding pipelines still rely on supervised fine-tuning with explicit\nspatial hints, making them ill-equipped to handle the implicit queries common\nin clinical practice. This work makes three core contributions. We first define\nUnified Medical Reasoning Grounding (UMRG), a novel vision-language task that\ndemands clinical reasoning and pixel-level grounding. Second, we release\nU-MRG-14K, a dataset of 14K samples featuring pixel-level masks alongside\nimplicit clinical queries and reasoning traces, spanning 10 modalities, 15\nsuper-categories, and 108 specific categories. Finally, we introduce\nMedReasoner, a modular framework that distinctly separates reasoning from\nsegmentation: an MLLM reasoner is optimized with reinforcement learning, while\na frozen segmentation expert converts spatial prompts into masks, with\nalignment achieved through format and accuracy rewards. MedReasoner achieves\nstate-of-the-art performance on U-MRG-14K and demonstrates strong\ngeneralization to unseen clinical queries, underscoring the significant promise\nof reinforcement learning for interpretable medical grounding."}
{"id": "2508.08178", "pdf": "https://arxiv.org/pdf/2508.08178", "abs": "https://arxiv.org/abs/2508.08178", "authors": ["Ozhan Suat", "Bedirhan Uguz", "Batuhan Karagoz", "Muhammed Can Keles", "Emre Akbas"], "title": "3D Human Mesh Estimation from Single View RGBD", "categories": ["cs.CV"], "comment": null, "summary": "Despite significant progress in 3D human mesh estimation from RGB images;\nRGBD cameras, offering additional depth data, remain underutilized. In this\npaper, we present a method for accurate 3D human mesh estimation from a single\nRGBD view, leveraging the affordability and widespread adoption of RGBD cameras\nfor real-world applications. A fully supervised approach for this problem,\nrequires a dataset with RGBD image and 3D mesh label pairs. However, collecting\nsuch a dataset is costly and challenging, hence, existing datasets are small,\nand limited in pose and shape diversity. To overcome this data scarcity, we\nleverage existing Motion Capture (MoCap) datasets. We first obtain complete 3D\nmeshes from the body models found in MoCap datasets, and create partial,\nsingle-view versions of them by projection to a virtual camera. This simulates\nthe depth data provided by an RGBD camera from a single viewpoint. Then, we\ntrain a masked autoencoder to complete the partial, single-view mesh. During\ninference, our method, which we name as M$^3$ for ``Masked Mesh Modeling'',\nmatches the depth values coming from the sensor to vertices of a template human\nmesh, which creates a partial, single-view mesh. We effectively recover parts\nof the 3D human body mesh model that are not visible, resulting in a full body\nmesh. M$^3$ achieves 16.8 mm and 22.0 mm per-vertex-error (PVE) on the SURREAL\nand CAPE datasets, respectively; outperforming existing methods that use\nfull-body point clouds as input. We obtain a competitive 70.9 PVE on the BEHAVE\ndataset, outperforming a recently published RGB based method by 18.4 mm,\nhighlighting the usefulness of depth data. Code will be released."}
{"id": "2508.08179", "pdf": "https://arxiv.org/pdf/2508.08179", "abs": "https://arxiv.org/abs/2508.08179", "authors": ["Sihan Zhao", "Zixuan Wang", "Tianyu Luan", "Jia Jia", "Wentao Zhu", "Jiebo Luo", "Junsong Yuan", "Nan Xi"], "title": "PP-Motion: Physical-Perceptual Fidelity Evaluation for Human Motion Generation", "categories": ["cs.CV", "cs.MM"], "comment": "Accepted by ACM Multimedia 2025", "summary": "Human motion generation has found widespread applications in AR/VR, film,\nsports, and medical rehabilitation, offering a cost-effective alternative to\ntraditional motion capture systems. However, evaluating the fidelity of such\ngenerated motions is a crucial, multifaceted task. Although previous approaches\nhave attempted at motion fidelity evaluation using human perception or physical\nconstraints, there remains an inherent gap between human-perceived fidelity and\nphysical feasibility. Moreover, the subjective and coarse binary labeling of\nhuman perception further undermines the development of a robust data-driven\nmetric. We address these issues by introducing a physical labeling method. This\nmethod evaluates motion fidelity by calculating the minimum modifications\nneeded for a motion to align with physical laws. With this approach, we are\nable to produce fine-grained, continuous physical alignment annotations that\nserve as objective ground truth. With these annotations, we propose PP-Motion,\na novel data-driven metric to evaluate both physical and perceptual fidelity of\nhuman motion. To effectively capture underlying physical priors, we employ\nPearson's correlation loss for the training of our metric. Additionally, by\nincorporating a human-based perceptual fidelity loss, our metric can capture\nfidelity that simultaneously considers both human perception and physical\nalignment. Experimental results demonstrate that our metric, PP-Motion, not\nonly aligns with physical laws but also aligns better with human perception of\nmotion fidelity than previous work."}
{"id": "2508.08180", "pdf": "https://arxiv.org/pdf/2508.08180", "abs": "https://arxiv.org/abs/2508.08180", "authors": ["Luca Zedda", "Andrea Loddo", "Cecilia Di Ruberto", "Carsten Marr"], "title": "RedDino: A foundation model for red blood cell analysis", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Red blood cells (RBCs) are essential to human health, and their precise\nmorphological analysis is important for diagnosing hematological disorders.\nDespite the promise of foundation models in medical diagnostics, comprehensive\nAI solutions for RBC analysis remain scarce. We present RedDino, a\nself-supervised foundation model designed for RBC image analysis. RedDino uses\nan RBC-specific adaptation of the DINOv2 self-supervised learning framework and\nis trained on a curated dataset of 1.25 million RBC images from diverse\nacquisition modalities and sources. Extensive evaluations show that RedDino\noutperforms existing state-of-the-art models on RBC shape classification.\nThrough assessments including linear probing and nearest neighbor\nclassification, we confirm its strong feature representations and\ngeneralization ability. Our main contributions are: (1) a foundation model\ntailored for RBC analysis, (2) ablation studies exploring DINOv2 configurations\nfor RBC modeling, and (3) a detailed evaluation of generalization performance.\nRedDino addresses key challenges in computational hematology by capturing\nnuanced morphological features, advancing the development of reliable\ndiagnostic tools. The source code and pretrained models for RedDino are\navailable at https://github.com/Snarci/RedDino, and the pretrained models can\nbe downloaded from our Hugging Face collection at\nhttps://huggingface.co/collections/Snarcy/reddino-689a13e29241d2e5690202fc"}
{"id": "2508.08183", "pdf": "https://arxiv.org/pdf/2508.08183", "abs": "https://arxiv.org/abs/2508.08183", "authors": ["Hongkun Jin", "Hongcheng Jiang", "Zejun Zhang", "Yuan Zhang", "Jia Fu", "Tingfeng Li", "Kai Luo"], "title": "THAT: Token-wise High-frequency Augmentation Transformer for Hyperspectral Pansharpening", "categories": ["cs.CV"], "comment": "Accepted to 2025 IEEE International Conference on Systems, Man, and\n  Cybernetics (SMC)", "summary": "Transformer-based methods have demonstrated strong potential in hyperspectral\npansharpening by modeling long-range dependencies. However, their effectiveness\nis often limited by redundant token representations and a lack of multi-scale\nfeature modeling. Hyperspectral images exhibit intrinsic spectral priors (e.g.,\nabundance sparsity) and spatial priors (e.g., non-local similarity), which are\ncritical for accurate reconstruction. From a spectral-spatial perspective,\nVision Transformers (ViTs) face two major limitations: they struggle to\npreserve high-frequency components--such as material edges and texture\ntransitions--and suffer from attention dispersion across redundant tokens.\nThese issues stem from the global self-attention mechanism, which tends to\ndilute high-frequency signals and overlook localized details. To address these\nchallenges, we propose the Token-wise High-frequency Augmentation Transformer\n(THAT), a novel framework designed to enhance hyperspectral pansharpening\nthrough improved high-frequency feature representation and token selection.\nSpecifically, THAT introduces: (1) Pivotal Token Selective Attention (PTSA) to\nprioritize informative tokens and suppress redundancy; (2) a Multi-level\nVariance-aware Feed-forward Network (MVFN) to enhance high-frequency detail\nlearning. Experiments on standard benchmarks show that THAT achieves\nstate-of-the-art performance with improved reconstruction quality and\nefficiency. The source code is available at https://github.com/kailuo93/THAT."}
{"id": "2508.08186", "pdf": "https://arxiv.org/pdf/2508.08186", "abs": "https://arxiv.org/abs/2508.08186", "authors": ["Md Meftahul Ferdaus", "Mahdi Abdelguerfi", "Elias Ioup", "Steven Sloan", "Kendall N. Niles", "Ken Pathak"], "title": "KARMA: Efficient Structural Defect Segmentation via Kolmogorov-Arnold Representation Learning", "categories": ["cs.CV"], "comment": "submitted to IEEE Transactions on Pattern Analysis and Machine\n  Intelligence", "summary": "Semantic segmentation of structural defects in civil infrastructure remains\nchallenging due to variable defect appearances, harsh imaging conditions, and\nsignificant class imbalance. Current deep learning methods, despite their\neffectiveness, typically require millions of parameters, rendering them\nimpractical for real-time inspection systems. We introduce KARMA\n(Kolmogorov-Arnold Representation Mapping Architecture), a highly efficient\nsemantic segmentation framework that models complex defect patterns through\ncompositions of one-dimensional functions rather than conventional\nconvolutions. KARMA features three technical innovations: (1) a\nparameter-efficient Tiny Kolmogorov-Arnold Network (TiKAN) module leveraging\nlow-rank factorization for KAN-based feature transformation; (2) an optimized\nfeature pyramid structure with separable convolutions for multi-scale defect\nanalysis; and (3) a static-dynamic prototype mechanism that enhances feature\nrepresentation for imbalanced classes. Extensive experiments on benchmark\ninfrastructure inspection datasets demonstrate that KARMA achieves competitive\nor superior mean IoU performance compared to state-of-the-art approaches, while\nusing significantly fewer parameters (0.959M vs. 31.04M, a 97% reduction).\nOperating at 0.264 GFLOPS, KARMA maintains inference speeds suitable for\nreal-time deployment, enabling practical automated infrastructure inspection\nsystems without compromising accuracy. The source code can be accessed at the\nfollowing URL: https://github.com/faeyelab/karma."}
{"id": "2508.08189", "pdf": "https://arxiv.org/pdf/2508.08189", "abs": "https://arxiv.org/abs/2508.08189", "authors": ["Weijia Wu", "Chen Gao", "Joya Chen", "Kevin Qinghong Lin", "Qingwei Meng", "Yiming Zhang", "Yuke Qiu", "Hong Zhou", "Mike Zheng Shou"], "title": "Reinforcement Learning in Vision: A Survey", "categories": ["cs.CV"], "comment": "22 pages", "summary": "Recent advances at the intersection of reinforcement learning (RL) and visual\nintelligence have enabled agents that not only perceive complex visual scenes\nbut also reason, generate, and act within them. This survey offers a critical\nand up-to-date synthesis of the field. We first formalize visual RL problems\nand trace the evolution of policy-optimization strategies from RLHF to\nverifiable reward paradigms, and from Proximal Policy Optimization to Group\nRelative Policy Optimization. We then organize more than 200 representative\nworks into four thematic pillars: multi-modal large language models, visual\ngeneration, unified model frameworks, and vision-language-action models. For\neach pillar we examine algorithmic design, reward engineering, benchmark\nprogress, and we distill trends such as curriculum-driven training,\npreference-aligned diffusion, and unified reward modeling. Finally, we review\nevaluation protocols spanning set-level fidelity, sample-level preference, and\nstate-level stability, and we identify open challenges that include sample\nefficiency, generalization, and safe deployment. Our goal is to provide\nresearchers and practitioners with a coherent map of the rapidly expanding\nlandscape of visual RL and to highlight promising directions for future\ninquiry. Resources are available at:\nhttps://github.com/weijiawu/Awesome-Visual-Reinforcement-Learning."}
{"id": "2508.08199", "pdf": "https://arxiv.org/pdf/2508.08199", "abs": "https://arxiv.org/abs/2508.08199", "authors": ["Peiqi He", "Zhenhao Zhang", "Yixiang Zhang", "Xiongjun Zhao", "Shaoliang Peng"], "title": "Spatial-ORMLLM: Improve Spatial Relation Understanding in the Operating Room with Multimodal Large Language Model", "categories": ["cs.CV"], "comment": null, "summary": "Precise spatial modeling in the operating room (OR) is foundational to many\nclinical tasks, supporting intraoperative awareness, hazard avoidance, and\nsurgical decision-making. While existing approaches leverage large-scale\nmultimodal datasets for latent-space alignment to implicitly learn spatial\nrelationships, they overlook the 3D capabilities of MLLMs. However, this\napproach raises two issues: (1) Operating rooms typically lack multiple video\nand audio sensors, making multimodal 3D data difficult to obtain; (2) Training\nsolely on readily available 2D data fails to capture fine-grained details in\ncomplex scenes. To address this gap, we introduce Spatial-ORMLLM, the first\nlarge vision-language model for 3D spatial reasoning in operating rooms using\nonly RGB modality to infer volumetric and semantic cues, enabling downstream\nmedical tasks with detailed and holistic spatial context. Spatial-ORMLLM\nincorporates a Spatial-Enhanced Feature Fusion Block, which integrates 2D\nmodality inputs with rich 3D spatial knowledge extracted by the estimation\nalgorithm and then feeds the combined features into the visual tower. By\nemploying a unified end-to-end MLLM framework, it combines powerful spatial\nfeatures with textual features to deliver robust 3D scene reasoning without any\nadditional expert annotations or sensor inputs. Experiments on multiple\nbenchmark clinical datasets demonstrate that Spatial-ORMLLM achieves\nstate-of-the-art performance and generalizes robustly to previously unseen\nsurgical scenarios and downstream tasks."}
{"id": "2508.08219", "pdf": "https://arxiv.org/pdf/2508.08219", "abs": "https://arxiv.org/abs/2508.08219", "authors": ["Wentao Sun", "Quanyun Wu", "Hanqing Xu", "Kyle Gao", "Zhengsen Xu", "Yiping Chen", "Dedong Zhang", "Lingfei Ma", "John S. Zelek", "Jonathan Li"], "title": "SAGOnline: Segment Any Gaussians Online", "categories": ["cs.CV"], "comment": "19 pages, 10 figures", "summary": "3D Gaussian Splatting (3DGS) has emerged as a powerful paradigm for explicit\n3D scene representation, yet achieving efficient and consistent 3D segmentation\nremains challenging. Current methods suffer from prohibitive computational\ncosts, limited 3D spatial reasoning, and an inability to track multiple objects\nsimultaneously. We present Segment Any Gaussians Online (SAGOnline), a\nlightweight and zero-shot framework for real-time 3D segmentation in Gaussian\nscenes that addresses these limitations through two key innovations: (1) a\ndecoupled strategy that integrates video foundation models (e.g., SAM2) for\nview-consistent 2D mask propagation across synthesized views; and (2) a\nGPU-accelerated 3D mask generation and Gaussian-level instance labeling\nalgorithm that assigns unique identifiers to 3D primitives, enabling lossless\nmulti-object tracking and segmentation across views. SAGOnline achieves\nstate-of-the-art performance on NVOS (92.7% mIoU) and Spin-NeRF (95.2% mIoU)\nbenchmarks, outperforming Feature3DGS, OmniSeg3D-gs, and SA3D by 15--1500 times\nin inference speed (27 ms/frame). Qualitative results demonstrate robust\nmulti-object segmentation and tracking in complex scenes. Our contributions\ninclude: (i) a lightweight and zero-shot framework for 3D segmentation in\nGaussian scenes, (ii) explicit labeling of Gaussian primitives enabling\nsimultaneous segmentation and tracking, and (iii) the effective adaptation of\n2D video foundation models to the 3D domain. This work allows real-time\nrendering and 3D scene understanding, paving the way for practical AR/VR and\nrobotic applications."}
{"id": "2508.08220", "pdf": "https://arxiv.org/pdf/2508.08220", "abs": "https://arxiv.org/abs/2508.08220", "authors": ["Wenyi Mo", "Ying Ba", "Tianyu Zhang", "Yalong Bai", "Biye Li"], "title": "Learning User Preferences for Image Generation Model", "categories": ["cs.CV"], "comment": null, "summary": "User preference prediction requires a comprehensive and accurate\nunderstanding of individual tastes. This includes both surface-level\nattributes, such as color and style, and deeper content-related aspects, such\nas themes and composition. However, existing methods typically rely on general\nhuman preferences or assume static user profiles, often neglecting individual\nvariability and the dynamic, multifaceted nature of personal taste. To address\nthese limitations, we propose an approach built upon Multimodal Large Language\nModels, introducing contrastive preference loss and preference tokens to learn\npersonalized user preferences from historical interactions. The contrastive\npreference loss is designed to effectively distinguish between user ''likes''\nand ''dislikes'', while the learnable preference tokens capture shared interest\nrepresentations among existing users, enabling the model to activate\ngroup-specific preferences and enhance consistency across similar users.\nExtensive experiments demonstrate our model outperforms other methods in\npreference prediction accuracy, effectively identifying users with similar\naesthetic inclinations and providing more precise guidance for generating\nimages that align with individual tastes. The project page is\n\\texttt{https://learn-user-pref.github.io/}."}
{"id": "2508.08227", "pdf": "https://arxiv.org/pdf/2508.08227", "abs": "https://arxiv.org/abs/2508.08227", "authors": ["Zhiqiang Wu", "Zhaomang Sun", "Tong Zhou", "Bingtao Fu", "Ji Cong", "Yitong Dong", "Huaqi Zhang", "Xuan Tang", "Mingsong Chen", "Xian Wei"], "title": "OMGSR: You Only Need One Mid-timestep Guidance for Real-World Image Super-Resolution", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Denoising Diffusion Probabilistic Models (DDPM) and Flow Matching (FM)\ngenerative models show promising potential for one-step Real-World Image\nSuper-Resolution (Real-ISR). Recent one-step Real-ISR models typically inject a\nLow-Quality (LQ) image latent distribution at the initial timestep. However, a\nfundamental gap exists between the LQ image latent distribution and the\nGaussian noisy latent distribution, limiting the effective utilization of\ngenerative priors. We observe that the noisy latent distribution at DDPM/FM\nmid-timesteps aligns more closely with the LQ image latent distribution. Based\non this insight, we present One Mid-timestep Guidance Real-ISR (OMGSR), a\nuniversal framework applicable to DDPM/FM-based generative models. OMGSR\ninjects the LQ image latent distribution at a pre-computed mid-timestep,\nincorporating the proposed Latent Distribution Refinement loss to alleviate the\nlatent distribution gap. We also design the Overlap-Chunked LPIPS/GAN loss to\neliminate checkerboard artifacts in image generation. Within this framework, we\ninstantiate OMGSR for DDPM/FM-based generative models with two variants:\nOMGSR-S (SD-Turbo) and OMGSR-F (FLUX.1-dev). Experimental results demonstrate\nthat OMGSR-S/F achieves balanced/excellent performance across quantitative and\nqualitative metrics at 512-resolution. Notably, OMGSR-F establishes\noverwhelming dominance in all reference metrics. We further train a\n1k-resolution OMGSR-F to match the default resolution of FLUX.1-dev, which\nyields excellent results, especially in the details of the image generation. We\nalso generate 2k-resolution images by the 1k-resolution OMGSR-F using our\ntwo-stage Tiled VAE & Diffusion."}
{"id": "2508.08244", "pdf": "https://arxiv.org/pdf/2508.08244", "abs": "https://arxiv.org/abs/2508.08244", "authors": ["Jingwen He", "Hongbo Liu", "Jiajun Li", "Ziqi Huang", "Yu Qiao", "Wanli Ouyang", "Ziwei Liu"], "title": "Cut2Next: Generating Next Shot via In-Context Tuning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Effective multi-shot generation demands purposeful, film-like transitions and\nstrict cinematic continuity. Current methods, however, often prioritize basic\nvisual consistency, neglecting crucial editing patterns (e.g., shot/reverse\nshot, cutaways) that drive narrative flow for compelling storytelling. This\nyields outputs that may be visually coherent but lack narrative sophistication\nand true cinematic integrity. To bridge this, we introduce Next Shot Generation\n(NSG): synthesizing a subsequent, high-quality shot that critically conforms to\nprofessional editing patterns while upholding rigorous cinematic continuity.\nOur framework, Cut2Next, leverages a Diffusion Transformer (DiT). It employs\nin-context tuning guided by a novel Hierarchical Multi-Prompting strategy. This\nstrategy uses Relational Prompts to define overall context and inter-shot\nediting styles. Individual Prompts then specify per-shot content and\ncinematographic attributes. Together, these guide Cut2Next to generate\ncinematically appropriate next shots. Architectural innovations, Context-Aware\nCondition Injection (CACI) and Hierarchical Attention Mask (HAM), further\nintegrate these diverse signals without introducing new parameters. We\nconstruct RawCuts (large-scale) and CuratedCuts (refined) datasets, both with\nhierarchical prompts, and introduce CutBench for evaluation. Experiments show\nCut2Next excels in visual consistency and text fidelity. Crucially, user\nstudies reveal a strong preference for Cut2Next, particularly for its adherence\nto intended editing patterns and overall cinematic continuity, validating its\nability to generate high-quality, narratively expressive, and cinematically\ncoherent subsequent shots."}
{"id": "2508.08248", "pdf": "https://arxiv.org/pdf/2508.08248", "abs": "https://arxiv.org/abs/2508.08248", "authors": ["Shuyuan Tu", "Yueming Pan", "Yinming Huang", "Xintong Han", "Zhen Xing", "Qi Dai", "Chong Luo", "Zuxuan Wu", "Yu-Gang Jiang"], "title": "StableAvatar: Infinite-Length Audio-Driven Avatar Video Generation", "categories": ["cs.CV"], "comment": null, "summary": "Current diffusion models for audio-driven avatar video generation struggle to\nsynthesize long videos with natural audio synchronization and identity\nconsistency. This paper presents StableAvatar, the first end-to-end video\ndiffusion transformer that synthesizes infinite-length high-quality videos\nwithout post-processing. Conditioned on a reference image and audio,\nStableAvatar integrates tailored training and inference modules to enable\ninfinite-length video generation. We observe that the main reason preventing\nexisting models from generating long videos lies in their audio modeling. They\ntypically rely on third-party off-the-shelf extractors to obtain audio\nembeddings, which are then directly injected into the diffusion model via\ncross-attention. Since current diffusion backbones lack any audio-related\npriors, this approach causes severe latent distribution error accumulation\nacross video clips, leading the latent distribution of subsequent segments to\ndrift away from the optimal distribution gradually. To address this,\nStableAvatar introduces a novel Time-step-aware Audio Adapter that prevents\nerror accumulation via time-step-aware modulation. During inference, we propose\na novel Audio Native Guidance Mechanism to further enhance the audio\nsynchronization by leveraging the diffusion's own evolving joint audio-latent\nprediction as a dynamic guidance signal. To enhance the smoothness of the\ninfinite-length videos, we introduce a Dynamic Weighted Sliding-window Strategy\nthat fuses latent over time. Experiments on benchmarks show the effectiveness\nof StableAvatar both qualitatively and quantitatively."}
{"id": "2508.08252", "pdf": "https://arxiv.org/pdf/2508.08252", "abs": "https://arxiv.org/abs/2508.08252", "authors": ["Shuting He", "Guangquan Jie", "Changshuo Wang", "Yun Zhou", "Shuming Hu", "Guanbin Li", "Henghui Ding"], "title": "ReferSplat: Referring Segmentation in 3D Gaussian Splatting", "categories": ["cs.CV"], "comment": "ICML 2025 Oral, Code: https://github.com/heshuting555/ReferSplat", "summary": "We introduce Referring 3D Gaussian Splatting Segmentation (R3DGS), a new task\nthat aims to segment target objects in a 3D Gaussian scene based on natural\nlanguage descriptions, which often contain spatial relationships or object\nattributes. This task requires the model to identify newly described objects\nthat may be occluded or not directly visible in a novel view, posing a\nsignificant challenge for 3D multi-modal understanding. Developing this\ncapability is crucial for advancing embodied AI. To support research in this\narea, we construct the first R3DGS dataset, Ref-LERF. Our analysis reveals that\n3D multi-modal understanding and spatial relationship modeling are key\nchallenges for R3DGS. To address these challenges, we propose ReferSplat, a\nframework that explicitly models 3D Gaussian points with natural language\nexpressions in a spatially aware paradigm. ReferSplat achieves state-of-the-art\nperformance on both the newly proposed R3DGS task and 3D open-vocabulary\nsegmentation benchmarks. Dataset and code are available at\nhttps://github.com/heshuting555/ReferSplat."}
{"id": "2508.08254", "pdf": "https://arxiv.org/pdf/2508.08254", "abs": "https://arxiv.org/abs/2508.08254", "authors": ["Emily Yue-Ting Jia", "Jiageng Mao", "Zhiyuan Gao", "Yajie Zhao", "Yue Wang"], "title": "Learning an Implicit Physics Model for Image-based Fluid Simulation", "categories": ["cs.CV"], "comment": "Accepted at ICCV 2025", "summary": "Humans possess an exceptional ability to imagine 4D scenes, encompassing both\nmotion and 3D geometry, from a single still image. This ability is rooted in\nour accumulated observations of similar scenes and an intuitive understanding\nof physics. In this paper, we aim to replicate this capacity in neural\nnetworks, specifically focusing on natural fluid imagery. Existing methods for\nthis task typically employ simplistic 2D motion estimators to animate the\nimage, leading to motion predictions that often defy physical principles,\nresulting in unrealistic animations. Our approach introduces a novel method for\ngenerating 4D scenes with physics-consistent animation from a single image. We\npropose the use of a physics-informed neural network that predicts motion for\neach surface point, guided by a loss term derived from fundamental physical\nprinciples, including the Navier-Stokes equations. To capture appearance, we\npredict feature-based 3D Gaussians from the input image and its estimated\ndepth, which are then animated using the predicted motions and rendered from\nany desired camera perspective. Experimental results highlight the\neffectiveness of our method in producing physically plausible animations,\nshowcasing significant performance improvements over existing methods. Our\nproject page is https://physfluid.github.io/ ."}
{"id": "2402.16868", "pdf": "https://arxiv.org/pdf/2402.16868", "abs": "https://arxiv.org/abs/2402.16868", "authors": ["Peigen Ye", "Yaping Sun", "Shumin Yao", "Hao Chen", "Xiaodong Xu", "Shuguang Cui"], "title": "Codebook-enabled Generative End-to-end Semantic Communication Powered by Transformer", "categories": ["cs.IT", "cs.AI", "cs.CV", "math.IT"], "comment": "IEEE INFOCOM PerAI6G 2024(accepted)", "summary": "Codebook-based generative semantic communication attracts increasing\nattention, since only indices are required to be transmitted when the codebook\nis shared between transmitter and receiver. However, due to the fact that the\nsemantic relations among code vectors are not necessarily related to the\ndistance of the corresponding code indices, the performance of the\ncodebook-enabled semantic communication system is susceptible to the channel\nnoise. Thus, how to improve the system robustness against the noise requires\ncareful design. This paper proposes a robust codebook-assisted image semantic\ncommunication system, where semantic codec and codebook are first jointly\nconstructed, and then vector-to-index transformer is designed guided by the\ncodebook to eliminate the effects of channel noise, and achieve image\ngeneration. Thanks to the assistance of the high-quality codebook to the\nTransformer, the generated images at the receiver outperform those of the\ncompared methods in terms of visual perception. In the end, numerical results\nand generated images demonstrate the advantages of the generative semantic\ncommunication method over JPEG+LDPC and traditional joint source channel coding\n(JSCC) methods."}
{"id": "2508.06571", "pdf": "https://arxiv.org/pdf/2508.06571", "abs": "https://arxiv.org/abs/2508.06571", "authors": ["Anqing Jiang", "Yu Gao", "Yiru Wang", "Zhigang Sun", "Shuo Wang", "Yuwen Heng", "Hao Sun", "Shichen Tang", "Lijuan Zhu", "Jinhao Chai", "Jijun Wang", "Zichong Gu", "Hao Jiang", "Li Sun"], "title": "IRL-VLA: Training an Vision-Language-Action Policy via Reward World Model", "categories": ["cs.AI", "cs.CV", "cs.RO"], "comment": "9 pagres, 2 figures", "summary": "Vision-Language-Action (VLA) models have demonstrated potential in autonomous\ndriving. However, two critical challenges hinder their development: (1)\nExisting VLA architectures are typically based on imitation learning in\nopen-loop setup which tends to capture the recorded behaviors in the dataset,\nleading to suboptimal and constrained performance, (2) Close-loop training\nrelies heavily on high-fidelity sensor simulation, where domain gaps and\ncomputational inefficiencies pose significant barriers. In this paper, we\nintroduce IRL-VLA, a novel close-loop Reinforcement Learning via\n\\textbf{I}nverse \\textbf{R}einforcement \\textbf{L}earning reward world model\nwith a self-built VLA approach. Our framework proceeds in a three-stage\nparadigm: In the first stage, we propose a VLA architecture and pretrain the\nVLA policy via imitation learning. In the second stage, we construct a\nlightweight reward world model via inverse reinforcement learning to enable\nefficient close-loop reward computation. To further enhance planning\nperformance, finally, we design specialized reward world model guidence\nreinforcement learning via PPO(Proximal Policy Optimization) to effectively\nbalance the safety incidents, comfortable driving, and traffic efficiency. Our\napproach achieves state-of-the-art performance in NAVSIM v2 end-to-end driving\nbenchmark, 1st runner up in CVPR2025 Autonomous Grand Challenge. We hope that\nour framework will accelerate VLA research in close-loop autonomous driving."}
{"id": "2508.06585", "pdf": "https://arxiv.org/pdf/2508.06585", "abs": "https://arxiv.org/abs/2508.06585", "authors": ["Jayant Sravan Tamarapalli", "Rynaa Grover", "Nilay Pande", "Sahiti Yerramilli"], "title": "CountQA: How Well Do MLLMs Count in the Wild?", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) demonstrate remarkable fluency in\nunderstanding visual scenes, yet they exhibit a critical lack in a fundamental\ncognitive skill: object counting. This blind spot severely limits their\nreliability in real-world applications. To date, this capability has been\nlargely unevaluated in complex scenarios, as existing benchmarks either feature\nsparse object densities or are confined to specific visual domains, failing to\ntest models under realistic conditions. Addressing this gap, we introduce\nCountQA, a challenging new benchmark designed to probe this deficiency.\nComprising over 1,500 question-answer pairs, CountQA features real-world images\nwith high object density, clutter, and occlusion. We investigate this weakness\nby evaluating 15 prominent MLLMs on the CountQA benchmark and reveal that the\ntop-performing model achieves a mere 42.9% accuracy, with performance declining\nas object counts rise. By providing a dedicated benchmark to diagnose and\nrectify this core weakness, CountQA paves the way for a new generation of MLLMs\nthat are not only descriptively fluent but also numerically grounded and\nspatially aware. We will open-source the dataset and code upon paper acceptance\nto foster further research."}
{"id": "2508.06664", "pdf": "https://arxiv.org/pdf/2508.06664", "abs": "https://arxiv.org/abs/2508.06664", "authors": ["Sima Zeinali Danalou", "Hooman Chamani", "Arash Rabbani", "Patrick C. Lee", "Jason Hattrick Simpers", "Jay R Werber"], "title": "Digital generation of the 3-D pore architecture of isotropic membranes using 2-D cross-sectional scanning electron microscopy images", "categories": ["cond-mat.mtrl-sci", "cs.CV", "physics.app-ph"], "comment": null, "summary": "A major limitation of two-dimensional scanning electron microscopy (SEM) in\nimaging porous membranes is its inability to resolve three-dimensional pore\narchitecture and interconnectivity, which are critical factors governing\nmembrane performance. Although conventional tomographic 3-D reconstruction\ntechniques can address this limitation, they are often expensive, technically\nchallenging, and not widely accessible. We previously introduced a\nproof-of-concept method for reconstructing a membrane's 3-D pore network from a\nsingle 2-D SEM image, yielding statistically equivalent results to those\nobtained from 3-D tomography. However, this initial approach struggled to\nreplicate the diverse pore geometries commonly observed in real membranes. In\nthis study, we advance the methodology by developing an enhanced reconstruction\nalgorithm that not only maintains essential statistical properties (e.g., pore\nsize distribution), but also accurately reproduces intricate pore morphologies.\nApplying this technique to a commercial microfiltration membrane, we generated\na high-fidelity 3-D reconstruction and derived key membrane properties.\nValidation with X-ray tomography data revealed excellent agreement in\nstructural metrics, with our SEM-based approach achieving superior resolution\nin resolving fine pore features. The tool can be readily applied to isotropic\nporous membrane structures of any pore size, as long as those pores can be\nvisualized by SEM. Further work is needed for 3-D structure generation of\nanisotropic membranes."}
{"id": "2508.06859", "pdf": "https://arxiv.org/pdf/2508.06859", "abs": "https://arxiv.org/abs/2508.06859", "authors": ["Shuo Tang", "Jian Xu", "Jiadong Zhang", "Yi Chen", "Qizhao Jin", "Lingdong Shen", "Chenglin Liu", "Shiming Xiang"], "title": "MeteorPred: A Meteorological Multimodal Large Model and Dataset for Severe Weather Event Prediction", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "Timely and accurate severe weather warnings are critical for disaster\nmitigation. However, current forecasting systems remain heavily reliant on\nmanual expert interpretation, introducing subjectivity and significant\noperational burdens. With the rapid development of AI technologies, the\nend-to-end \"AI weather station\" is gradually emerging as a new trend in\npredicting severe weather events. Three core challenges impede the development\nof end-to-end AI severe weather system: (1) scarcity of severe weather event\nsamples; (2) imperfect alignment between high-dimensional meteorological data\nand textual warnings; (3) existing multimodal language models are unable to\nhandle high-dimensional meteorological data and struggle to fully capture the\ncomplex dependencies across temporal sequences, vertical pressure levels, and\nspatial dimensions. To address these challenges, we introduce MP-Bench, the\nfirst large-scale temporal multimodal dataset for severe weather events\nprediction, comprising 421,363 pairs of raw multi-year meteorological data and\ncorresponding text caption, covering a wide range of severe weather scenarios\nacross China. On top of this dataset, we develop a meteorology multimodal large\nmodel (MMLM) that directly ingests 4D meteorological inputs. In addition, it is\ndesigned to accommodate the unique characteristics of 4D meteorological data\nflow, incorporating three plug-and-play adaptive fusion modules that enable\ndynamic feature extraction and integration across temporal sequences, vertical\npressure layers, and spatial dimensions. Extensive experiments on MP-Bench\ndemonstrate that MMLM performs exceptionally well across multiple tasks,\nhighlighting its effectiveness in severe weather understanding and marking a\nkey step toward realizing automated, AI-driven weather forecasting systems. Our\nsource code and dataset will be made publicly available."}
{"id": "2508.06921", "pdf": "https://arxiv.org/pdf/2508.06921", "abs": "https://arxiv.org/abs/2508.06921", "authors": ["Zhongyu Chen", "Chenyang Li", "Xuesong Li", "Dianye Huang", "Zhongliang Jiang", "Stefanie Speidel", "Xiangyu Chu", "K. W. Samuel Au"], "title": "Vibration-Based Energy Metric for Restoring Needle Alignment in Autonomous Robotic Ultrasound", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Precise needle alignment is essential for percutaneous needle insertion in\nrobotic ultrasound-guided procedures. However, inherent challenges such as\nspeckle noise, needle-like artifacts, and low image resolution make robust\nneedle detection difficult, particularly when visibility is reduced or lost. In\nthis paper, we propose a method to restore needle alignment when the ultrasound\nimaging plane and the needle insertion plane are misaligned. Unlike many\nexisting approaches that rely heavily on needle visibility in ultrasound\nimages, our method uses a more robust feature by periodically vibrating the\nneedle using a mechanical system. Specifically, we propose a vibration-based\nenergy metric that remains effective even when the needle is fully out of\nplane. Using this metric, we develop a control strategy to reposition the\nultrasound probe in response to misalignments between the imaging plane and the\nneedle insertion plane in both translation and rotation. Experiments conducted\non ex-vivo porcine tissue samples using a dual-arm robotic ultrasound-guided\nneedle insertion system demonstrate the effectiveness of the proposed approach.\nThe experimental results show the translational error of 0.41$\\pm$0.27 mm and\nthe rotational error of 0.51$\\pm$0.19 degrees."}
{"id": "2508.06944", "pdf": "https://arxiv.org/pdf/2508.06944", "abs": "https://arxiv.org/abs/2508.06944", "authors": ["Lixuan He", "Jie Feng", "Yong Li"], "title": "AMFT: Aligning LLM Reasoners by Meta-Learning the Optimal Imitation-Exploration Balance", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Large Language Models (LLMs) are typically fine-tuned for reasoning tasks\nthrough a two-stage pipeline of Supervised Fine-Tuning (SFT) followed by\nReinforcement Learning (RL), a process fraught with catastrophic forgetting and\nsuboptimal trade-offs between imitation and exploration. Recent single-stage\nmethods attempt to unify SFT and RL using heuristics, but lack a principled\nmechanism for dynamically balancing the two paradigms. In this paper, we\nreframe this challenge through the theoretical lens of \\textbf{implicit\nrewards}, viewing SFT and RL not as distinct methods but as complementary\nreward signals. We introduce \\textbf{Adaptive Meta Fine-Tuning (AMFT)}, a novel\nsingle-stage algorithm that learns the optimal balance between SFT's implicit,\npath-level reward and RL's explicit, outcome-based reward. The core of AMFT is\na \\textbf{meta-gradient adaptive weight controller} that treats the SFT-RL\nbalance as a learnable parameter, dynamically optimizing it to maximize\nlong-term task performance. This forward-looking approach, regularized by\npolicy entropy for stability, autonomously discovers an effective training\ncurriculum. We conduct a comprehensive evaluation on challenging benchmarks\nspanning mathematical reasoning, abstract visual reasoning (General Points),\nand vision-language navigation (V-IRL). AMFT consistently establishes a new\nstate-of-the-art and demonstrats superior generalization on out-of-distribution\n(OOD) tasks. Ablation studies and training dynamic analysis confirm that the\nmeta-learning controller is crucial for AMFT's stability, sample efficiency,\nand performance, offering a more principled and effective paradigm for LLM\nalignment.Our codes are open-sourced via https://github.com/hlxtsyj/AMFT."}
{"id": "2508.07066", "pdf": "https://arxiv.org/pdf/2508.07066", "abs": "https://arxiv.org/abs/2508.07066", "authors": ["Chenxu Zhao", "Wei Qian", "Aobo Chen", "Mengdi Huai"], "title": "Membership Inference Attacks with False Discovery Rate Control", "categories": ["stat.ML", "cs.CV", "cs.LG"], "comment": null, "summary": "Recent studies have shown that deep learning models are vulnerable to\nmembership inference attacks (MIAs), which aim to infer whether a data record\nwas used to train a target model or not. To analyze and study these\nvulnerabilities, various MIA methods have been proposed. Despite the\nsignificance and popularity of MIAs, existing works on MIAs are limited in\nproviding guarantees on the false discovery rate (FDR), which refers to the\nexpected proportion of false discoveries among the identified positive\ndiscoveries. However, it is very challenging to ensure the false discovery rate\nguarantees, because the underlying distribution is usually unknown, and the\nestimated non-member probabilities often exhibit interdependence. To tackle the\nabove challenges, in this paper, we design a novel membership inference attack\nmethod, which can provide the guarantees on the false discovery rate.\nAdditionally, we show that our method can also provide the marginal probability\nguarantee on labeling true non-member data as member data. Notably, our method\ncan work as a wrapper that can be seamlessly integrated with existing MIA\nmethods in a post-hoc manner, while also providing the FDR control. We perform\nthe theoretical analysis for our method. Extensive experiments in various\nsettings (e.g., the black-box setting and the lifelong learning setting) are\nalso conducted to verify the desirable performance of our method."}
{"id": "2508.07102", "pdf": "https://arxiv.org/pdf/2508.07102", "abs": "https://arxiv.org/abs/2508.07102", "authors": ["Yang Cao", "Yubin Chen", "Zhao Song", "Jiahao Zhang"], "title": "Towards High-Order Mean Flow Generative Models: Feasibility, Expressivity, and Provably Efficient Criteria", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Generative modelling has seen significant advances through simulation-free\nparadigms such as Flow Matching, and in particular, the MeanFlow framework,\nwhich replaces instantaneous velocity fields with average velocities to enable\nefficient single-step sampling. In this work, we introduce a theoretical study\non Second-Order MeanFlow, a novel extension that incorporates average\nacceleration fields into the MeanFlow objective. We first establish the\nfeasibility of our approach by proving that the average acceleration satisfies\na generalized consistency condition analogous to first-order MeanFlow, thereby\nsupporting stable, one-step sampling and tractable loss functions. We then\ncharacterize its expressivity via circuit complexity analysis, showing that\nunder mild assumptions, the Second-Order MeanFlow sampling process can be\nimplemented by uniform threshold circuits within the $\\mathsf{TC}^0$ class.\nFinally, we derive provably efficient criteria for scalable implementation by\nleveraging fast approximate attention computations: we prove that attention\noperations within the Second-Order MeanFlow architecture can be approximated to\nwithin $1/\\mathrm{poly}(n)$ error in time $n^{2+o(1)}$. Together, these results\nlay the theoretical foundation for high-order flow matching models that combine\nrich dynamics with practical sampling efficiency."}
{"id": "2508.07115", "pdf": "https://arxiv.org/pdf/2508.07115", "abs": "https://arxiv.org/abs/2508.07115", "authors": ["Antonino Greco", "Marco D'Alessandro", "Karl J. Friston", "Giovanni Pezzulo", "Markus Siegel"], "title": "Sensory robustness through top-down feedback and neural stochasticity in recurrent vision models", "categories": ["q-bio.NC", "cs.CV", "cs.LG"], "comment": null, "summary": "Biological systems leverage top-down feedback for visual processing, yet most\nartificial vision models succeed in image classification using purely\nfeedforward or recurrent architectures, calling into question the functional\nsignificance of descending cortical pathways. Here, we trained convolutional\nrecurrent neural networks (ConvRNN) on image classification in the presence or\nabsence of top-down feedback projections to elucidate the specific\ncomputational contributions of those feedback pathways. We found that ConvRNNs\nwith top-down feedback exhibited remarkable speed-accuracy trade-off and\nrobustness to noise perturbations and adversarial attacks, but only when they\nwere trained with stochastic neural variability, simulated by randomly\nsilencing single units via dropout. By performing detailed analyses to identify\nthe reasons for such benefits, we observed that feedback information\nsubstantially shaped the representational geometry of the post-integration\nlayer, combining the bottom-up and top-down streams, and this effect was\namplified by dropout. Moreover, feedback signals coupled with dropout optimally\nconstrained network activity onto a low-dimensional manifold and encoded object\ninformation more efficiently in out-of-distribution regimes, with top-down\ninformation stabilizing the representational dynamics at the population level.\nTogether, these findings uncover a dual mechanism for resilient sensory coding.\nOn the one hand, neural stochasticity prevents unit-level co-adaptation albeit\nat the cost of more chaotic dynamics. On the other hand, top-down feedback\nharnesses high-level information to stabilize network activity on compact\nlow-dimensional manifolds."}
{"id": "2508.07263", "pdf": "https://arxiv.org/pdf/2508.07263", "abs": "https://arxiv.org/abs/2508.07263", "authors": ["Qingyuan Zeng", "Shu Jiang", "Jiajing Lin", "Zhenzhong Wang", "Kay Chen Tan", "Min Jiang"], "title": "Fading the Digital Ink: A Universal Black-Box Attack Framework for 3DGS Watermarking Systems", "categories": ["cs.CR", "cs.CV"], "comment": null, "summary": "With the rise of 3D Gaussian Splatting (3DGS), a variety of digital\nwatermarking techniques, embedding either 1D bitstreams or 2D images, are used\nfor copyright protection. However, the robustness of these watermarking\ntechniques against potential attacks remains underexplored. This paper\nintroduces the first universal black-box attack framework, the Group-based\nMulti-objective Evolutionary Attack (GMEA), designed to challenge these\nwatermarking systems. We formulate the attack as a large-scale multi-objective\noptimization problem, balancing watermark removal with visual quality. In a\nblack-box setting, we introduce an indirect objective function that blinds the\nwatermark detector by minimizing the standard deviation of features extracted\nby a convolutional network, thus rendering the feature maps uninformative. To\nmanage the vast search space of 3DGS models, we employ a group-based\noptimization strategy to partition the model into multiple, independent\nsub-optimization problems. Experiments demonstrate that our framework\neffectively removes both 1D and 2D watermarks from mainstream 3DGS watermarking\nmethods while maintaining high visual fidelity. This work reveals critical\nvulnerabilities in existing 3DGS copyright protection schemes and calls for the\ndevelopment of more robust watermarking systems."}
{"id": "2508.07337", "pdf": "https://arxiv.org/pdf/2508.07337", "abs": "https://arxiv.org/abs/2508.07337", "authors": ["Ivan Kukanov", "Jun Wah Ng"], "title": "KLASSify to Verify: Audio-Visual Deepfake Detection Using SSL-based Audio and Handcrafted Visual Features", "categories": ["eess.AS", "cs.CV"], "comment": "7 pages, accepted to the 33rd ACM International Conference on\n  Multimedia (MM'25)", "summary": "The rapid development of audio-driven talking head generators and advanced\nText-To-Speech (TTS) models has led to more sophisticated temporal deepfakes.\nThese advances highlight the need for robust methods capable of detecting and\nlocalizing deepfakes, even under novel, unseen attack scenarios. Current\nstate-of-the-art deepfake detectors, while accurate, are often computationally\nexpensive and struggle to generalize to novel manipulation techniques. To\naddress these challenges, we propose multimodal approaches for the\nAV-Deepfake1M 2025 challenge. For the visual modality, we leverage handcrafted\nfeatures to improve interpretability and adaptability. For the audio modality,\nwe adapt a self-supervised learning (SSL) backbone coupled with graph attention\nnetworks to capture rich audio representations, improving detection robustness.\nOur approach strikes a balance between performance and real-world deployment,\nfocusing on resilience and potential interpretability. On the AV-Deepfake1M++\ndataset, our multimodal system achieves AUC of 92.78% for deepfake\nclassification task and IoU of 0.3536 for temporal localization using only the\naudio modality."}
{"id": "2508.07406", "pdf": "https://arxiv.org/pdf/2508.07406", "abs": "https://arxiv.org/abs/2508.07406", "authors": ["Xiaobei Zhao", "Xingqi Lyu", "Xiang Li"], "title": "AgriVLN: Vision-and-Language Navigation for Agricultural Robots", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "Agricultural robots have emerged as powerful members in agricultural tasks,\nnevertheless, still heavily rely on manual operation or untransportable railway\nfor movement, resulting in limited mobility and poor adaptability.\nVision-and-Language Navigation (VLN) enables robots to navigate to the target\ndestinations following natural language instructions, demonstrating strong\nperformance on several domains. However, none of the existing benchmarks or\nmethods is specifically designed for agricultural scenes. To bridge this gap,\nwe propose Agriculture to Agriculture (A2A) benchmark, containing 1,560\nepisodes across six diverse agricultural scenes, in which all realistic RGB\nvideos are captured by front-facing camera on a quadruped robot at a height of\n0.38 meters, aligning with the practical deployment conditions. Meanwhile, we\npropose Vision-and-Language Navigation for Agricultural Robots (AgriVLN)\nbaseline based on Vision-Language Model (VLM) prompted with carefully crafted\ntemplates, which can understand both given instructions and agricultural\nenvironments to generate appropriate low-level actions for robot control. When\nevaluated on A2A, AgriVLN performs well on short instructions but struggles\nwith long instructions, because it often fails to track which part of the\ninstruction is currently being executed. To address this, we further propose\nSubtask List (STL) instruction decomposition module and integrate it into\nAgriVLN, improving Success Rate (SR) from 0.33 to 0.47. We additionally compare\nAgriVLN with several existing VLN methods, demonstrating the state-of-the-art\nperformance in the agricultural domain."}
{"id": "2508.07486", "pdf": "https://arxiv.org/pdf/2508.07486", "abs": "https://arxiv.org/abs/2508.07486", "authors": ["Morteza Ziabakhsh", "Kiyan Rezaee", "Sadegh Eskandari", "Seyed Amir Hossein Tabatabaei", "Mohammad M. Ghassemi"], "title": "Extracting Overlapping Microservices from Monolithic Code via Deep Semantic Embeddings and Graph Neural Network-Based Soft Clustering", "categories": ["cs.SE", "cs.AI", "cs.CV"], "comment": null, "summary": "Modern software systems are increasingly shifting from monolithic\narchitectures to microservices to enhance scalability, maintainability, and\ndeployment flexibility. Existing microservice extraction methods typically rely\non hard clustering, assigning each software component to a single microservice.\nThis approach often increases inter-service coupling and reduces intra-service\ncohesion. We propose Mo2oM (Monolithic to Overlapping Microservices), a\nframework that formulates microservice extraction as a soft clustering problem,\nallowing components to belong probabilistically to multiple microservices. This\napproach is inspired by expert-driven decompositions, where practitioners\nintentionally replicate certain software components across services to reduce\ncommunication overhead. Mo2oM combines deep semantic embeddings with structural\ndependencies extracted from methodcall graphs to capture both functional and\narchitectural relationships. A graph neural network-based soft clustering\nalgorithm then generates the final set of microservices. We evaluate Mo2oM on\nfour open-source monolithic benchmarks and compare it against eight\nstate-of-the-art baselines. Our results demonstrate that Mo2oM achieves\nimprovements of up to 40.97% in structural modularity (balancing cohesion and\ncoupling), 58% in inter-service call percentage (communication overhead),\n26.16% in interface number (modularity and decoupling), and 38.96% in\nnon-extreme distribution (service size balance) across all benchmarks."}
{"id": "2508.07560", "pdf": "https://arxiv.org/pdf/2508.07560", "abs": "https://arxiv.org/abs/2508.07560", "authors": ["Yan Gong", "Naibang Wang", "Jianli Lu", "Xinyu Zhang", "Yongsheng Gao", "Jie Zhao", "Zifan Huang", "Haozhi Bai", "Nanxin Zeng", "Nayu Su", "Lei Yang", "Ziying Song", "Xiaoxi Hu", "Xinmin Jiang", "Xiaojuan Zhang", "Susanto Rahardja"], "title": "Progressive Bird's Eye View Perception for Safety-Critical Autonomous Driving: A Comprehensive Survey", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Bird's-Eye-View (BEV) perception has become a foundational paradigm in\nautonomous driving, enabling unified spatial representations that support\nrobust multi-sensor fusion and multi-agent collaboration. As autonomous\nvehicles transition from controlled environments to real-world deployment,\nensuring the safety and reliability of BEV perception in complex scenarios -\nsuch as occlusions, adverse weather, and dynamic traffic - remains a critical\nchallenge. This survey provides the first comprehensive review of BEV\nperception from a safety-critical perspective, systematically analyzing\nstate-of-the-art frameworks and implementation strategies across three\nprogressive stages: single-modality vehicle-side, multimodal vehicle-side, and\nmulti-agent collaborative perception. Furthermore, we examine public datasets\nencompassing vehicle-side, roadside, and collaborative settings, evaluating\ntheir relevance to safety and robustness. We also identify key open-world\nchallenges - including open-set recognition, large-scale unlabeled data, sensor\ndegradation, and inter-agent communication latency - and outline future\nresearch directions, such as integration with end-to-end autonomous driving\nsystems, embodied intelligence, and large language models."}
{"id": "2508.07590", "pdf": "https://arxiv.org/pdf/2508.07590", "abs": "https://arxiv.org/abs/2508.07590", "authors": ["Xiongwei Xiao", "Baoying Chen", "Jishen Zeng", "Jianquan Yang"], "title": "MSPT: A Lightweight Face Image Quality Assessment Method with Multi-stage Progressive Training", "categories": ["cs.MM", "cs.CV"], "comment": null, "summary": "Accurately assessing the perceptual quality of face images is crucial,\nespecially with the rapid progress in face restoration and generation.\nTraditional quality assessment methods often struggle with the unique\ncharacteristics of face images, limiting their generalizability. While\nlearning-based approaches demonstrate superior performance due to their strong\nfitting capabilities, their high complexity typically incurs significant\ncomputational and storage costs, hindering practical deployment. To address\nthis, we propose a lightweight face quality assessment network with Multi-Stage\nProgressive Training (MSPT). Our network employs a three-stage progressive\ntraining strategy that gradually introduces more diverse data samples and\nincreases input image resolution. This novel approach enables lightweight\nnetworks to achieve high performance by effectively learning complex quality\nfeatures while significantly mitigating catastrophic forgetting. Our MSPT\nachieved the second highest score on the VQualA 2025 face image quality\nassessment benchmark dataset, demonstrating that MSPT achieves comparable or\nbetter performance than state-of-the-art methods while maintaining efficient\ninference."}
{"id": "2508.07608", "pdf": "https://arxiv.org/pdf/2508.07608", "abs": "https://arxiv.org/abs/2508.07608", "authors": ["Junxiao Xue", "Xiaozhen Liu", "Xuecheng Wu", "Xinyi Yin", "Danlei Huang", "Fei Yu"], "title": "AD-AVSR: Asymmetric Dual-stream Enhancement for Robust Audio-Visual Speech Recognition", "categories": ["cs.MM", "cs.CV", "cs.SD"], "comment": "Accepted by the ACM MM 2025 Workshop on SVC", "summary": "Audio-visual speech recognition (AVSR) combines audio-visual modalities to\nimprove speech recognition, especially in noisy environments. However, most\nexisting methods deploy the unidirectional enhancement or symmetric fusion\nmanner, which limits their capability to capture heterogeneous and\ncomplementary correlations of audio-visual data-especially under asymmetric\ninformation conditions. To tackle these gaps, we introduce a new AVSR framework\ntermed AD-AVSR based on bidirectional modality enhancement. Specifically, we\nfirst introduce the audio dual-stream encoding strategy to enrich audio\nrepresentations from multiple perspectives and intentionally establish\nasymmetry to support subsequent cross-modal interactions. The enhancement\nprocess involves two key components, Audio-aware Visual Refinement Module for\nenhanced visual representations under audio guidance, and Cross-modal Noise\nSuppression Masking Module which refines audio representations using visual\ncues, collaboratively leading to the closed-loop and bidirectional information\nflow. To further enhance correlation robustness, we adopt a threshold-based\nselection mechanism to filter out irrelevant or weakly correlated audio-visual\npairs. Extensive experimental results on the LRS2 and LRS3 datasets indicate\nthat our AD-AVSR consistently surpasses SOTA methods in both performance and\nnoise robustness, highlighting the effectiveness of our model design."}
{"id": "2508.07630", "pdf": "https://arxiv.org/pdf/2508.07630", "abs": "https://arxiv.org/abs/2508.07630", "authors": ["Anirudh Iyengar Kaniyar Narayana Iyengar", "Srija Mukhopadhyay", "Adnan Qidwai", "Shubhankar Singh", "Dan Roth", "Vivek Gupta"], "title": "InterChart: Benchmarking Visual Reasoning Across Decomposed and Distributed Chart Information", "categories": ["cs.CL", "cs.AI", "cs.CV", "I.2.7; I.2.10; I.4.10; I.7.5"], "comment": "18 pages, 6 figures, 12 tables. Benchmark dataset and evaluation code\n  will be publicly made available", "summary": "We introduce InterChart, a diagnostic benchmark that evaluates how well\nvision-language models (VLMs) reason across multiple related charts, a task\ncentral to real-world applications such as scientific reporting, financial\nanalysis, and public policy dashboards. Unlike prior benchmarks focusing on\nisolated, visually uniform charts, InterChart challenges models with diverse\nquestion types ranging from entity inference and trend correlation to numerical\nestimation and abstract multi-step reasoning grounded in 2-3 thematically or\nstructurally related charts. We organize the benchmark into three tiers of\nincreasing difficulty: (1) factual reasoning over individual charts, (2)\nintegrative analysis across synthetically aligned chart sets, and (3) semantic\ninference over visually complex, real-world chart pairs. Our evaluation of\nstate-of-the-art open and closed-source VLMs reveals consistent and steep\naccuracy declines as chart complexity increases. We find that models perform\nbetter when we decompose multi-entity charts into simpler visual units,\nunderscoring their struggles with cross-chart integration. By exposing these\nsystematic limitations, InterChart provides a rigorous framework for advancing\nmultimodal reasoning in complex, multi-visual environments."}
{"id": "2508.07642", "pdf": "https://arxiv.org/pdf/2508.07642", "abs": "https://arxiv.org/abs/2508.07642", "authors": ["Tianyi Ma", "Yue Zhang", "Zehao Wang", "Parisa Kordjamshidi"], "title": "Breaking Down and Building Up: Mixture of Skill-Based Vision-and-Language Navigation Agents", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": "18 pages, 5 Figures,", "summary": "Vision-and-Language Navigation (VLN) poses significant challenges in enabling\nagents to interpret natural language instructions and navigate complex 3D\nenvironments. While recent progress has been driven by large-scale pre-training\nand data augmentation, current methods still struggle to generalize to unseen\nscenarios, particularly when complex spatial and temporal reasoning is\nrequired. In this work, we propose SkillNav, a modular framework that\nintroduces structured, skill-based reasoning into Transformer-based VLN agents.\nOur method decomposes navigation into a set of interpretable atomic skills\n(e.g., Vertical Movement, Area and Region Identification, Stop and Pause), each\nhandled by a specialized agent. We then introduce a novel zero-shot\nVision-Language Model (VLM)-based router, which dynamically selects the most\nsuitable agent at each time step by aligning sub-goals with visual observations\nand historical actions. SkillNav achieves a new state-of-the-art performance on\nthe R2R benchmark and demonstrates strong generalization to the GSA-R2R\nbenchmark that includes novel instruction styles and unseen environments."}
{"id": "2508.07760", "pdf": "https://arxiv.org/pdf/2508.07760", "abs": "https://arxiv.org/abs/2508.07760", "authors": ["Maximilian Kromer", "Panagiotis Agrafiotis", "Begüm Demir"], "title": "Sea-Undistort: A Dataset for Through-Water Image Restoration in High Resolution Airborne Bathymetric Mapping", "categories": ["eess.IV", "cs.CV", "cs.GR"], "comment": "Under review in IEEE Geoscience and Remote Sensing Letters", "summary": "Accurate image-based bathymetric mapping in shallow waters remains\nchallenging due to the complex optical distortions such as wave induced\npatterns, scattering and sunglint, introduced by the dynamic water surface, the\nwater column properties, and solar illumination. In this work, we introduce\nSea-Undistort, a comprehensive synthetic dataset of 1200 paired 512x512\nthrough-water scenes rendered in Blender. Each pair comprises a distortion-free\nand a distorted view, featuring realistic water effects such as sun glint,\nwaves, and scattering over diverse seabeds. Accompanied by per-image metadata\nsuch as camera parameters, sun position, and average depth, Sea-Undistort\nenables supervised training that is otherwise infeasible in real environments.\nWe use Sea-Undistort to benchmark two state-of-the-art image restoration\nmethods alongside an enhanced lightweight diffusion-based framework with an\nearly-fusion sun-glint mask. When applied to real aerial data, the enhanced\ndiffusion model delivers more complete Digital Surface Models (DSMs) of the\nseabed, especially in deeper areas, reduces bathymetric errors, suppresses\nglint and scattering, and crisply restores fine seabed details. Dataset,\nweights, and code are publicly available at\nhttps://www.magicbathy.eu/Sea-Undistort.html."}
{"id": "2508.07773", "pdf": "https://arxiv.org/pdf/2508.07773", "abs": "https://arxiv.org/abs/2508.07773", "authors": ["Mohammed Salah", "Numan Saeed", "Davor Svetinovic", "Stefano Sfarra", "Mohammed Omar", "Yusra Abdulrahman"], "title": "PCA-Guided Autoencoding for Structured Dimensionality Reduction in Active Infrared Thermography", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": "Infrared thermography, Non-Destructive Testing, Principal Component\n  Analysis, PCA-Guided Autoencoder, PCA Distillation Loss, Dimensionality\n  Reduction", "summary": "Active Infrared thermography (AIRT) is a widely adopted non-destructive\ntesting (NDT) technique for detecting subsurface anomalies in industrial\ncomponents. Due to the high dimensionality of AIRT data, current approaches\nemploy non-linear autoencoders (AEs) for dimensionality reduction. However, the\nlatent space learned by AIRT AEs lacks structure, limiting their effectiveness\nin downstream defect characterization tasks. To address this limitation, this\npaper proposes a principal component analysis guided (PCA-guided) autoencoding\nframework for structured dimensionality reduction to capture intricate,\nnon-linear features in thermographic signals while enforcing a structured\nlatent space. A novel loss function, PCA distillation loss, is introduced to\nguide AIRT AEs to align the latent representation with structured PCA\ncomponents while capturing the intricate, non-linear patterns in thermographic\nsignals. To evaluate the utility of the learned, structured latent space, we\npropose a neural network-based evaluation metric that assesses its suitability\nfor defect characterization. Experimental results show that the proposed\nPCA-guided AE outperforms state-of-the-art dimensionality reduction methods on\nPVC, CFRP, and PLA samples in terms of contrast, signal-to-noise ratio (SNR),\nand neural network-based metrics."}
{"id": "2508.07817", "pdf": "https://arxiv.org/pdf/2508.07817", "abs": "https://arxiv.org/abs/2508.07817", "authors": ["Tao Tang", "Chengxu Yang"], "title": "MIND: A Noise-Adaptive Denoising Framework for Medical Images Integrating Multi-Scale Transformer", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG", "cs.MM"], "comment": "6 pages, 6 figures", "summary": "The core role of medical images in disease diagnosis makes their quality\ndirectly affect the accuracy of clinical judgment. However, due to factors such\nas low-dose scanning, equipment limitations and imaging artifacts, medical\nimages are often accompanied by non-uniform noise interference, which seriously\naffects structure recognition and lesion detection. This paper proposes a\nmedical image adaptive denoising model (MI-ND) that integrates multi-scale\nconvolutional and Transformer architecture, introduces a noise level estimator\n(NLE) and a noise adaptive attention module (NAAB), and realizes\nchannel-spatial attention regulation and cross-modal feature fusion driven by\nnoise perception. Systematic testing is carried out on multimodal public\ndatasets. Experiments show that this method significantly outperforms the\ncomparative methods in image quality indicators such as PSNR, SSIM, and LPIPS,\nand improves the F1 score and ROC-AUC in downstream diagnostic tasks, showing\nstrong prac-tical value and promotional potential. The model has outstanding\nbenefits in structural recovery, diagnostic sensitivity, and cross-modal\nrobustness, and provides an effective solution for medical image enhancement\nand AI-assisted diagnosis and treatment."}
{"id": "2508.07885", "pdf": "https://arxiv.org/pdf/2508.07885", "abs": "https://arxiv.org/abs/2508.07885", "authors": ["Shoaib Ahmmad", "Zubayer Ahmed Aditto", "Md Mehrab Hossain", "Noushin Yeasmin", "Shorower Hossain"], "title": "Autonomous Navigation of Cloud-Controlled Quadcopters in Confined Spaces Using Multi-Modal Perception and LLM-Driven High Semantic Reasoning", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.SY", "eess.SY"], "comment": null, "summary": "This paper introduces an advanced AI-driven perception system for autonomous\nquadcopter navigation in GPS-denied indoor environments. The proposed framework\nleverages cloud computing to offload computationally intensive tasks and\nincorporates a custom-designed printed circuit board (PCB) for efficient sensor\ndata acquisition, enabling robust navigation in confined spaces. The system\nintegrates YOLOv11 for object detection, Depth Anything V2 for monocular depth\nestimation, a PCB equipped with Time-of-Flight (ToF) sensors and an Inertial\nMeasurement Unit (IMU), and a cloud-based Large Language Model (LLM) for\ncontext-aware decision-making. A virtual safety envelope, enforced by\ncalibrated sensor offsets, ensures collision avoidance, while a multithreaded\narchitecture achieves low-latency processing. Enhanced spatial awareness is\nfacilitated by 3D bounding box estimation with Kalman filtering. Experimental\nresults in an indoor testbed demonstrate strong performance, with object\ndetection achieving a mean Average Precision (mAP50) of 0.6, depth estimation\nMean Absolute Error (MAE) of 7.2 cm, only 16 safety envelope breaches across 42\ntrials over approximately 11 minutes, and end-to-end system latency below 1\nsecond. This cloud-supported, high-intelligence framework serves as an\nauxiliary perception and navigation system, complementing state-of-the-art\ndrone autonomy for GPS-denied confined spaces."}
{"id": "2508.07950", "pdf": "https://arxiv.org/pdf/2508.07950", "abs": "https://arxiv.org/abs/2508.07950", "authors": ["Chen Shen", "Wanqing Zhang", "Kehan Li", "Erwen Huang", "Haitao Bi", "Aiying Fan", "Yiwen Shen", "Hongmei Dong", "Ji Zhang", "Yuming Shao", "Zengjia Liu", "Xinshe Liu", "Tao Li", "Chunxia Yan", "Shuanliang Fan", "Di Wu", "Jianhua Ma", "Bin Cong", "Zhenyuan Wang", "Chunfeng Lian"], "title": "FEAT: A Multi-Agent Forensic AI System with Domain-Adapted Large Language Model for Automated Cause-of-Death Analysis", "categories": ["cs.AI", "cs.CV", "cs.LG", "cs.MA"], "comment": "18pages, 6 figures", "summary": "Forensic cause-of-death determination faces systemic challenges, including\nworkforce shortages and diagnostic variability, particularly in high-volume\nsystems like China's medicolegal infrastructure. We introduce FEAT (ForEnsic\nAgenT), a multi-agent AI framework that automates and standardizes death\ninvestigations through a domain-adapted large language model. FEAT's\napplication-oriented architecture integrates: (i) a central Planner for task\ndecomposition, (ii) specialized Local Solvers for evidence analysis, (iii) a\nMemory & Reflection module for iterative refinement, and (iv) a Global Solver\nfor conclusion synthesis. The system employs tool-augmented reasoning,\nhierarchical retrieval-augmented generation, forensic-tuned LLMs, and\nhuman-in-the-loop feedback to ensure legal and medical validity. In evaluations\nacross diverse Chinese case cohorts, FEAT outperformed state-of-the-art AI\nsystems in both long-form autopsy analyses and concise cause-of-death\nconclusions. It demonstrated robust generalization across six geographic\nregions and achieved high expert concordance in blinded validations. Senior\npathologists validated FEAT's outputs as comparable to those of human experts,\nwith improved detection of subtle evidentiary nuances. To our knowledge, FEAT\nis the first LLM-based AI agent system dedicated to forensic medicine, offering\nscalable, consistent death certification while maintaining expert-level rigor.\nBy integrating AI efficiency with human oversight, this work could advance\nequitable access to reliable medicolegal services while addressing critical\ncapacity constraints in forensic systems."}
{"id": "2508.08031", "pdf": "https://arxiv.org/pdf/2508.08031", "abs": "https://arxiv.org/abs/2508.08031", "authors": ["Jiayao Wang", "Yang Song", "Zhendong Zhao", "Jiale Zhang", "Qilin Wu", "Junwu Zhu", "Dongfang Zhao"], "title": "IPBA: Imperceptible Perturbation Backdoor Attack in Federated Self-Supervised Learning", "categories": ["cs.CR", "cs.CV"], "comment": null, "summary": "Federated self-supervised learning (FSSL) combines the advantages of\ndecentralized modeling and unlabeled representation learning, serving as a\ncutting-edge paradigm with strong potential for scalability and privacy\npreservation. Although FSSL has garnered increasing attention, research\nindicates that it remains vulnerable to backdoor attacks. Existing methods\ngenerally rely on visually obvious triggers, which makes it difficult to meet\nthe requirements for stealth and practicality in real-world deployment. In this\npaper, we propose an imperceptible and effective backdoor attack method against\nFSSL, called IPBA. Our empirical study reveals that existing imperceptible\ntriggers face a series of challenges in FSSL, particularly limited\ntransferability, feature entanglement with augmented samples, and\nout-of-distribution properties. These issues collectively undermine the\neffectiveness and stealthiness of traditional backdoor attacks in FSSL. To\novercome these challenges, IPBA decouples the feature distributions of backdoor\nand augmented samples, and introduces Sliced-Wasserstein distance to mitigate\nthe out-of-distribution properties of backdoor samples, thereby optimizing the\ntrigger generation process. Our experimental results on several FSSL scenarios\nand datasets show that IPBA significantly outperforms existing backdoor attack\nmethods in performance and exhibits strong robustness under various defense\nmechanisms."}
{"id": "2508.08114", "pdf": "https://arxiv.org/pdf/2508.08114", "abs": "https://arxiv.org/abs/2508.08114", "authors": ["Bowen Tong", "Hao Chen", "Shaorui Guo", "Dong Liu"], "title": "Learned Regularization for Microwave Tomography", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Microwave Tomography (MWT) aims to reconstruct the dielectric properties of\ntissues from measured scattered electromagnetic fields. This inverse problem is\nhighly nonlinear and ill-posed, posing significant challenges for conventional\noptimization-based methods, which, despite being grounded in physical models,\noften fail to recover fine structural details. Recent deep learning strategies,\nincluding end-to-end and post-processing networks, have improved reconstruction\nquality but typically require large paired training datasets and may struggle\nto generalize. To overcome these limitations, we propose a physics-informed\nhybrid framework that integrates diffusion models as learned regularization\nwithin a data-consistency-driven variational scheme. Specifically, we introduce\nSingle-Step Diffusion Regularization (SSD-Reg), a novel approach that embeds\ndiffusion priors into the iterative reconstruction process, enabling the\nrecovery of complex anatomical structures without the need for paired data.\nSSD-Reg maintains fidelity to both the governing physics and learned structural\ndistributions, improving accuracy, stability, and robustness. Extensive\nexperiments demonstrate that SSD-Reg, implemented as a Plug-and-Play (PnP)\nmodule, provides a flexible and effective solution for tackling the\nill-posedness inherent in functional image reconstruction."}
{"id": "2508.08120", "pdf": "https://arxiv.org/pdf/2508.08120", "abs": "https://arxiv.org/abs/2508.08120", "authors": ["Keyan Rahimi", "Md. Wasiul Haque", "Sagar Dasgupta", "Mizanur Rahman"], "title": "Vision-Based Localization and LLM-based Navigation for Indoor Environments", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "20 pages, 6 figures, 1 table", "summary": "Indoor navigation remains a complex challenge due to the absence of reliable\nGPS signals and the architectural intricacies of large enclosed environments.\nThis study presents an indoor localization and navigation approach that\nintegrates vision-based localization with large language model (LLM)-based\nnavigation. The localization system utilizes a ResNet-50 convolutional neural\nnetwork fine-tuned through a two-stage process to identify the user's position\nusing smartphone camera input. To complement localization, the navigation\nmodule employs an LLM, guided by a carefully crafted system prompt, to\ninterpret preprocessed floor plan images and generate step-by-step directions.\nExperimental evaluation was conducted in a realistic office corridor with\nrepetitive features and limited visibility to test localization robustness. The\nmodel achieved high confidence and an accuracy of 96% across all tested\nwaypoints, even under constrained viewing conditions and short-duration\nqueries. Navigation tests using ChatGPT on real building floor maps yielded an\naverage instruction accuracy of 75%, with observed limitations in zero-shot\nreasoning and inference time. This research demonstrates the potential for\nscalable, infrastructure-free indoor navigation using off-the-shelf cameras and\npublicly available floor plans, particularly in resource-constrained settings\nlike hospitals, airports, and educational institutions."}
{"id": "2508.08240", "pdf": "https://arxiv.org/pdf/2508.08240", "abs": "https://arxiv.org/abs/2508.08240", "authors": ["Kaijun Wang", "Liqin Lu", "Mingyu Liu", "Jianuo Jiang", "Zeju Li", "Bolin Zhang", "Wancai Zheng", "Xinyi Yu", "Hao Chen", "Chunhua Shen"], "title": "ODYSSEY: Open-World Quadrupeds Exploration and Manipulation for Long-Horizon Tasks", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Language-guided long-horizon mobile manipulation has long been a grand\nchallenge in embodied semantic reasoning, generalizable manipulation, and\nadaptive locomotion. Three fundamental limitations hinder progress: First,\nalthough large language models have improved spatial reasoning and task\nplanning through semantic priors, existing implementations remain confined to\ntabletop scenarios, failing to address the constrained perception and limited\nactuation ranges of mobile platforms. Second, current manipulation strategies\nexhibit insufficient generalization when confronted with the diverse object\nconfigurations encountered in open-world environments. Third, while crucial for\npractical deployment, the dual requirement of maintaining high platform\nmaneuverability alongside precise end-effector control in unstructured settings\nremains understudied.\n  In this work, we present ODYSSEY, a unified mobile manipulation framework for\nagile quadruped robots equipped with manipulators, which seamlessly integrates\nhigh-level task planning with low-level whole-body control. To address the\nchallenge of egocentric perception in language-conditioned tasks, we introduce\na hierarchical planner powered by a vision-language model, enabling\nlong-horizon instruction decomposition and precise action execution. At the\ncontrol level, our novel whole-body policy achieves robust coordination across\nchallenging terrains. We further present the first benchmark for long-horizon\nmobile manipulation, evaluating diverse indoor and outdoor scenarios. Through\nsuccessful sim-to-real transfer, we demonstrate the system's generalization and\nrobustness in real-world deployments, underscoring the practicality of legged\nmanipulators in unstructured environments. Our work advances the feasibility of\ngeneralized robotic assistants capable of complex, dynamic tasks. Our project\npage: https://kaijwang.github.io/odyssey.github.io/"}
