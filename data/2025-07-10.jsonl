{"id": "2507.06234", "pdf": "https://arxiv.org/pdf/2507.06234", "abs": "https://arxiv.org/abs/2507.06234", "authors": ["Jiangzhong Cao", "Zekai Zeng", "Xu Zhang", "Huan Zhang", "Chunling Fan", "Gangyi Jiang", "Weisi Lin"], "title": "Unveiling the Underwater World: CLIP Perception Model-Guided Underwater Image Enhancement", "categories": ["cs.CV"], "comment": "10 pages, 7 figures;Accepted to PR 2025;The source code is available\n  at https://github.com/Ave001025/UIE_CLIP", "summary": "High-quality underwater images are essential for both machine vision tasks\nand viewers with their aesthetic appeal.However, the quality of underwater\nimages is severely affected by light absorption and scattering. Deep\nlearning-based methods for Underwater Image Enhancement (UIE) have achieved\ngood performance. However, these methods often overlook considering human\nperception and lack sufficient constraints within the solution space.\nConsequently, the enhanced images often suffer from diminished perceptual\nquality or poor content restoration.To address these issues, we propose a UIE\nmethod with a Contrastive Language-Image Pre-Training (CLIP) perception loss\nmodule and curriculum contrastive regularization. Above all, to develop a\nperception model for underwater images that more aligns with human visual\nperception, the visual semantic feature extraction capability of the CLIP model\nis leveraged to learn an appropriate prompt pair to map and evaluate the\nquality of underwater images. This CLIP perception model is then incorporated\nas a perception loss module into the enhancement network to improve the\nperceptual quality of enhanced images. Furthermore, the CLIP perception model\nis integrated with the curriculum contrastive regularization to enhance the\nconstraints imposed on the enhanced images within the CLIP perceptual space,\nmitigating the risk of both under-enhancement and over-enhancement.\nSpecifically, the CLIP perception model is employed to assess and categorize\nthe learning difficulty level of negatives in the regularization process,\nensuring comprehensive and nuanced utilization of distorted images and\nnegatives with varied quality levels. Extensive experiments demonstrate that\nour method outperforms state-of-the-art methods in terms of visual quality and\ngeneralization ability."}
{"id": "2507.06265", "pdf": "https://arxiv.org/pdf/2507.06265", "abs": "https://arxiv.org/abs/2507.06265", "authors": ["Ali Nasiri-Sarvi", "Hassan Rivaz", "Mahdi S. Hosseini"], "title": "SPARC: Concept-Aligned Sparse Autoencoders for Cross-Model and Cross-Modal Interpretability", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Understanding how different AI models encode the same high-level concepts,\nsuch as objects or attributes, remains challenging because each model typically\nproduces its own isolated representation. Existing interpretability methods\nlike Sparse Autoencoders (SAEs) produce latent concepts individually for each\nmodel, resulting in incompatible concept spaces and limiting cross-model\ninterpretability. To address this, we introduce SPARC (Sparse Autoencoders for\nAligned Representation of Concepts), a new framework that learns a single,\nunified latent space shared across diverse architectures and modalities (e.g.,\nvision models like DINO, and multimodal models like CLIP). SPARC's alignment is\nenforced through two key innovations: (1) a Global TopK sparsity mechanism,\nensuring all input streams activate identical latent dimensions for a given\nconcept; and (2) a Cross-Reconstruction Loss, which explicitly encourages\nsemantic consistency between models. On Open Images, SPARC dramatically\nimproves concept alignment, achieving a Jaccard similarity of 0.80, more than\ntripling the alignment compared to previous methods. SPARC creates a shared\nsparse latent space where individual dimensions often correspond to similar\nhigh-level concepts across models and modalities, enabling direct comparison of\nhow different architectures represent identical concepts without requiring\nmanual alignment or model-specific analysis. As a consequence of this aligned\nrepresentation, SPARC also enables practical applications such as text-guided\nspatial localization in vision-only models and cross-model/cross-modal\nretrieval. Code and models are available at\nhttps://github.com/AtlasAnalyticsLab/SPARC."}
{"id": "2507.06269", "pdf": "https://arxiv.org/pdf/2507.06269", "abs": "https://arxiv.org/abs/2507.06269", "authors": ["Rushil Desai", "Frederik Warburg", "Trevor Darrell", "Marissa Ramirez de Chanlatte"], "title": "A Probabilistic Approach to Uncertainty Quantification Leveraging 3D Geometry", "categories": ["cs.CV", "cs.AI"], "comment": "ICCV 2025 Workshops (8 Pages, 6 Figures, 2 Tables)", "summary": "Quantifying uncertainty in neural implicit 3D representations, particularly\nthose utilizing Signed Distance Functions (SDFs), remains a substantial\nchallenge due to computational inefficiencies, scalability issues, and\ngeometric inconsistencies. Existing methods typically neglect direct geometric\nintegration, leading to poorly calibrated uncertainty maps. We introduce\nBayesSDF, a novel probabilistic framework for uncertainty quantification in\nneural implicit SDF models, motivated by scientific simulation applications\nwith 3D environments (e.g., forests) such as modeling fluid flow through\nforests, where precise surface geometry and awareness of fidelity surface\ngeometric uncertainty are essential. Unlike radiance-based models such as NeRF\nor 3D Gaussian splatting, which lack explicit surface formulations, SDFs define\ncontinuous and differentiable geometry, making them better suited for physical\nmodeling and analysis. BayesSDF leverages a Laplace approximation to quantify\nlocal surface instability via Hessian-based metrics, enabling computationally\nefficient, surface-aware uncertainty estimation. Our method shows that\nuncertainty predictions correspond closely with poorly reconstructed geometry,\nproviding actionable confidence measures for downstream use. Extensive\nevaluations on synthetic and real-world datasets demonstrate that BayesSDF\noutperforms existing methods in both calibration and geometric consistency,\nestablishing a strong foundation for uncertainty-aware 3D scene reconstruction,\nsimulation, and robotic decision-making."}
{"id": "2507.06272", "pdf": "https://arxiv.org/pdf/2507.06272", "abs": "https://arxiv.org/abs/2507.06272", "authors": ["Zhang Li", "Biao Yang", "Qiang Liu", "Shuo Zhang", "Zhiyin Ma", "Shuo Zhang", "Liang Yin", "Linger Deng", "Yabo Sun", "Yuliang Liu", "Xiang Bai"], "title": "LIRA: Inferring Segmentation in Large Multi-modal Models with Local Interleaved Region Assistance", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "While large multi-modal models (LMMs) demonstrate promising capabilities in\nsegmentation and comprehension, they still struggle with two limitations:\ninaccurate segmentation and hallucinated comprehension. These challenges stem\nprimarily from constraints in weak visual comprehension and a lack of\nfine-grained perception. To alleviate these limitations, we propose LIRA, a\nframework that capitalizes on the complementary relationship between visual\ncomprehension and segmentation via two key components: (1) Semantic-Enhanced\nFeature Extractor (SEFE) improves object attribute inference by fusing semantic\nand pixel-level features, leading to more accurate segmentation; (2)\nInterleaved Local Visual Coupling (ILVC) autoregressively generates local\ndescriptions after extracting local features based on segmentation masks,\noffering fine-grained supervision to mitigate hallucinations. Furthermore, we\nfind that the precision of object segmentation is positively correlated with\nthe latent related semantics of the <seg> token. To quantify this relationship\nand the model's potential semantic inferring ability, we introduce the\nAttributes Evaluation (AttrEval) dataset. Our experiments show that LIRA\nachieves state-of-the-art performance in both segmentation and comprehension\ntasks. Code will be available at https://github.com/echo840/LIRA."}
{"id": "2507.06275", "pdf": "https://arxiv.org/pdf/2507.06275", "abs": "https://arxiv.org/abs/2507.06275", "authors": ["Yassin Hussein Rassul", "Aram M. Ahmed", "Polla Fattah", "Bryar A. Hassan", "Arwaa W. Abdulkareem", "Tarik A. Rashid", "Joan Lu"], "title": "Advancing Offline Handwritten Text Recognition: A Systematic Review of Data Augmentation and Generation Techniques", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Offline Handwritten Text Recognition (HTR) systems play a crucial role in\napplications such as historical document digitization, automatic form\nprocessing, and biometric authentication. However, their performance is often\nhindered by the limited availability of annotated training data, particularly\nfor low-resource languages and complex scripts. This paper presents a\ncomprehensive survey of offline handwritten data augmentation and generation\ntechniques designed to improve the accuracy and robustness of HTR systems. We\nsystematically examine traditional augmentation methods alongside recent\nadvances in deep learning, including Generative Adversarial Networks (GANs),\ndiffusion models, and transformer-based approaches. Furthermore, we explore the\nchallenges associated with generating diverse and realistic handwriting\nsamples, particularly in preserving script authenticity and addressing data\nscarcity. This survey follows the PRISMA methodology, ensuring a structured and\nrigorous selection process. Our analysis began with 1,302 primary studies,\nwhich were filtered down to 848 after removing duplicates, drawing from key\nacademic sources such as IEEE Digital Library, Springer Link, Science Direct,\nand ACM Digital Library. By evaluating existing datasets, assessment metrics,\nand state-of-the-art methodologies, this survey identifies key research gaps\nand proposes future directions to advance the field of handwritten text\ngeneration across diverse linguistic and stylistic landscapes."}
{"id": "2507.06321", "pdf": "https://arxiv.org/pdf/2507.06321", "abs": "https://arxiv.org/abs/2507.06321", "authors": ["Joon Tai Kim", "Tianle Chen", "Ziyu Dong", "Nishanth Kunchala", "Alexander Guller", "Daniel Ospina Acero", "Roger Williams", "Mrinal Kumar"], "title": "Centralized Copy-Paste: Enhanced Data Augmentation Strategy for Wildland Fire Semantic Segmentation", "categories": ["cs.CV", "cs.LG"], "comment": "21 pages, 5 figures, and under review for AIAA SciTech 2026", "summary": "Collecting and annotating images for the purpose of training segmentation\nmodels is often cost prohibitive. In the domain of wildland fire science, this\nchallenge is further compounded by the scarcity of reliable public datasets\nwith labeled ground truth. This paper presents the Centralized Copy-Paste Data\nAugmentation (CCPDA) method, for the purpose of assisting with the training of\ndeep-learning multiclass segmentation models, with special focus on improving\nsegmentation outcomes for the fire-class. CCPDA has three main steps: (i)\nidentify fire clusters in the source image, (ii) apply a centralization\ntechnique to focus on the core of the fire area, and (iii) paste the refined\nfire clusters onto a target image. This method increases dataset diversity\nwhile preserving the essential characteristics of the fire class. The\neffectiveness of this augmentation technique is demonstrated via numerical\nanalysis and comparison against various other augmentation methods using a\nweighted sum-based multi-objective optimization approach. This approach helps\nelevate segmentation performance metrics specific to the fire class, which\ncarries significantly more operational significance than other classes (fuel,\nash, or background). Numerical performance assessment validates the efficacy of\nthe presented CCPDA method in alleviating the difficulties associated with\nsmall, manually labeled training datasets. It also illustrates that CCPDA\noutperforms other augmentation strategies in the application scenario\nconsidered, particularly in improving fire-class segmentation performance."}
{"id": "2507.06332", "pdf": "https://arxiv.org/pdf/2507.06332", "abs": "https://arxiv.org/abs/2507.06332", "authors": ["Fuyuan Zhang", "Qichen Wang", "Jianjun Zhao"], "title": "AR2: Attention-Guided Repair for the Robustness of CNNs Against Common Corruptions", "categories": ["cs.CV", "cs.LG", "cs.SE"], "comment": null, "summary": "Deep neural networks suffer from significant performance degradation when\nexposed to common corruptions such as noise, blur, weather, and digital\ndistortions, limiting their reliability in real-world applications. In this\npaper, we propose AR2 (Attention-Guided Repair for Robustness), a simple yet\neffective method to enhance the corruption robustness of pretrained CNNs. AR2\noperates by explicitly aligning the class activation maps (CAMs) between clean\nand corrupted images, encouraging the model to maintain consistent attention\neven under input perturbations. Our approach follows an iterative repair\nstrategy that alternates between CAM-guided refinement and standard\nfine-tuning, without requiring architectural changes. Extensive experiments\nshow that AR2 consistently outperforms existing state-of-the-art methods in\nrestoring robustness on standard corruption benchmarks (CIFAR-10-C, CIFAR-100-C\nand ImageNet-C), achieving a favorable balance between accuracy on clean data\nand corruption robustness. These results demonstrate that AR2 provides a robust\nand scalable solution for enhancing model reliability in real-world\nenvironments with diverse corruptions."}
{"id": "2507.06400", "pdf": "https://arxiv.org/pdf/2507.06400", "abs": "https://arxiv.org/abs/2507.06400", "authors": ["Weiran Li", "Yeqiang Liu", "Qiannan Guo", "Yijie Wei", "Hwa Liang Leo", "Zhenbo Li"], "title": "When Trackers Date Fish: A Benchmark and Framework for Underwater Multiple Fish Tracking", "categories": ["cs.CV"], "comment": null, "summary": "Multiple object tracking (MOT) technology has made significant progress in\nterrestrial applications, but underwater tracking scenarios remain\nunderexplored despite their importance to marine ecology and aquaculture. We\npresent Multiple Fish Tracking Dataset 2025 (MFT25), the first comprehensive\ndataset specifically designed for underwater multiple fish tracking, featuring\n15 diverse video sequences with 408,578 meticulously annotated bounding boxes\nacross 48,066 frames. Our dataset captures various underwater environments,\nfish species, and challenging conditions including occlusions, similar\nappearances, and erratic motion patterns. Additionally, we introduce\nScale-aware and Unscented Tracker (SU-T), a specialized tracking framework\nfeaturing an Unscented Kalman Filter (UKF) optimized for non-linear fish\nswimming patterns and a novel Fish-Intersection-over-Union (FishIoU) matching\nthat accounts for the unique morphological characteristics of aquatic species.\nExtensive experiments demonstrate that our SU-T baseline achieves\nstate-of-the-art performance on MFT25, with 34.1 HOTA and 44.6 IDF1, while\nrevealing fundamental differences between fish tracking and terrestrial object\ntracking scenarios. MFT25 establishes a robust foundation for advancing\nresearch in underwater tracking systems with important applications in marine\nbiology, aquaculture monitoring, and ecological conservation. The dataset and\ncodes are released at https://vranlee.github.io/SU-T/."}
{"id": "2507.06405", "pdf": "https://arxiv.org/pdf/2507.06405", "abs": "https://arxiv.org/abs/2507.06405", "authors": ["Lala Shakti Swarup Ray", "Mengxi Liu", "Deepika Gurung", "Bo Zhou", "Sungho Suh", "Paul Lukowicz"], "title": "SImpHAR: Advancing impedance-based human activity recognition using 3D simulation and text-to-motion models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Human Activity Recognition (HAR) with wearable sensors is essential for\napplications in healthcare, fitness, and human-computer interaction.\nBio-impedance sensing offers unique advantages for fine-grained motion capture\nbut remains underutilized due to the scarcity of labeled data. We introduce\nSImpHAR, a novel framework addressing this limitation through two core\ncontributions. First, we propose a simulation pipeline that generates realistic\nbio-impedance signals from 3D human meshes using shortest-path estimation,\nsoft-body physics, and text-to-motion generation serving as a digital twin for\ndata augmentation. Second, we design a two-stage training strategy with\ndecoupled approach that enables broader activity coverage without requiring\nlabel-aligned synthetic data. We evaluate SImpHAR on our collected ImpAct\ndataset and two public benchmarks, showing consistent improvements over\nstate-of-the-art methods, with gains of up to 22.3% and 21.8%, in terms of\naccuracy and macro F1 score, respectively. Our results highlight the promise of\nsimulation-driven augmentation and modular training for impedance-based HAR."}
{"id": "2507.06411", "pdf": "https://arxiv.org/pdf/2507.06411", "abs": "https://arxiv.org/abs/2507.06411", "authors": ["Hayat Ullah", "Arslan Munir", "Oliver Nina"], "title": "Hierarchical Multi-Stage Transformer Architecture for Context-Aware Temporal Action Localization", "categories": ["cs.CV"], "comment": "17 pages, 6 figures,", "summary": "Inspired by the recent success of transformers and multi-stage architectures\nin video recognition and object detection domains. We thoroughly explore the\nrich spatio-temporal properties of transformers within a multi-stage\narchitecture paradigm for the temporal action localization (TAL) task. This\nexploration led to the development of a hierarchical multi-stage transformer\narchitecture called PCL-Former, where each subtask is handled by a dedicated\ntransformer module with a specialized loss function. Specifically, the\nProposal-Former identifies candidate segments in an untrimmed video that may\ncontain actions, the Classification-Former classifies the action categories\nwithin those segments, and the Localization-Former precisely predicts the\ntemporal boundaries (i.e., start and end) of the action instances. To evaluate\nthe performance of our method, we have conducted extensive experiments on three\nchallenging benchmark datasets: THUMOS-14, ActivityNet-1.3, and HACS Segments.\nWe also conducted detailed ablation experiments to assess the impact of each\nindividual module of our PCL-Former. The obtained quantitative results validate\nthe effectiveness of the proposed PCL-Former, outperforming state-of-the-art\nTAL approaches by 2.8%, 1.2%, and 4.8% on THUMOS14, ActivityNet-1.3, and HACS\ndatasets, respectively."}
{"id": "2507.06442", "pdf": "https://arxiv.org/pdf/2507.06442", "abs": "https://arxiv.org/abs/2507.06442", "authors": ["Soroush Shahi", "Farzad Shahabi", "Rama Nabulsi", "Glenn Fernandes", "Aggelos Katsaggelos", "Nabil Alshurafa"], "title": "THOR: Thermal-guided Hand-Object Reasoning via Adaptive Vision Sampling", "categories": ["cs.CV"], "comment": null, "summary": "Wearable cameras are increasingly used as an observational and interventional\ntool for human behaviors by providing detailed visual data of hand-related\nactivities. This data can be leveraged to facilitate memory recall for logging\nof behavior or timely interventions aimed at improving health. However,\ncontinuous processing of RGB images from these cameras consumes significant\npower impacting battery lifetime, generates a large volume of unnecessary video\ndata for post-processing, raises privacy concerns, and requires substantial\ncomputational resources for real-time analysis. We introduce THOR, a real-time\nadaptive spatio-temporal RGB frame sampling method that leverages thermal\nsensing to capture hand-object patches and classify them in real-time. We use\nlow-resolution thermal camera data to identify moments when a person switches\nfrom one hand-related activity to another, and adjust the RGB frame sampling\nrate by increasing it during activity transitions and reducing it during\nperiods of sustained activity. Additionally, we use the thermal cues from the\nhand to localize the region of interest (i.e., the hand-object interaction) in\neach RGB frame, allowing the system to crop and process only the necessary part\nof the image for activity recognition. We develop a wearable device to validate\nour method through an in-the-wild study with 14 participants and over 30\nactivities, and further evaluate it on Ego4D (923 participants across 9\ncountries, totaling 3,670 hours of video). Our results show that using only 3%\nof the original RGB video data, our method captures all the activity segments,\nand achieves hand-related activity recognition F1-score (95%) comparable to\nusing the entire RGB video (94%). Our work provides a more practical path for\nthe longitudinal use of wearable cameras to monitor hand-related activities and\nhealth-risk behaviors in real time."}
{"id": "2507.06459", "pdf": "https://arxiv.org/pdf/2507.06459", "abs": "https://arxiv.org/abs/2507.06459", "authors": ["Riadul Islam", "Joey Mulé", "Dhandeep Challagundla", "Shahmir Rizvi", "Sean Carson"], "title": "EA: An Event Autoencoder for High-Speed Vision Sensing", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "High-speed vision sensing is essential for real-time perception in\napplications such as robotics, autonomous vehicles, and industrial automation.\nTraditional frame-based vision systems suffer from motion blur, high latency,\nand redundant data processing, limiting their performance in dynamic\nenvironments. Event cameras, which capture asynchronous brightness changes at\nthe pixel level, offer a promising alternative but pose challenges in object\ndetection due to sparse and noisy event streams. To address this, we propose an\nevent autoencoder architecture that efficiently compresses and reconstructs\nevent data while preserving critical spatial and temporal features. The\nproposed model employs convolutional encoding and incorporates adaptive\nthreshold selection and a lightweight classifier to enhance recognition\naccuracy while reducing computational complexity. Experimental results on the\nexisting Smart Event Face Dataset (SEFD) demonstrate that our approach achieves\ncomparable accuracy to the YOLO-v4 model while utilizing up to $35.5\\times$\nfewer parameters. Implementations on embedded platforms, including Raspberry Pi\n4B and NVIDIA Jetson Nano, show high frame rates ranging from 8 FPS up to 44.8\nFPS. The proposed classifier exhibits up to 87.84x better FPS than the\nstate-of-the-art and significantly improves event-based vision performance,\nmaking it ideal for low-power, high-speed applications in real-time edge\ncomputing."}
{"id": "2507.06485", "pdf": "https://arxiv.org/pdf/2507.06485", "abs": "https://arxiv.org/abs/2507.06485", "authors": ["Ziyang Wang", "Jaehong Yoon", "Shoubin Yu", "Md Mohaiminul Islam", "Gedas Bertasius", "Mohit Bansal"], "title": "Video-RTS: Rethinking Reinforcement Learning and Test-Time Scaling for Efficient and Enhanced Video Reasoning", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "The first two authors contributed equally. Project page:\n  https://sites.google.com/cs.unc.edu/videorts2025/", "summary": "Despite advances in reinforcement learning (RL)-based video reasoning with\nlarge language models (LLMs), data collection and finetuning remain significant\nchallenges. These methods often rely on large-scale supervised fine-tuning\n(SFT) with extensive video data and long Chain-of-Thought (CoT) annotations,\nmaking them costly and hard to scale. To address this, we present Video-RTS, a\nnew approach to improve video reasoning capability with drastically improved\ndata efficiency by combining data-efficient RL with a video-adaptive test-time\nscaling (TTS) strategy. Based on observations about the data scaling of RL\nsamples, we skip the resource-intensive SFT step and employ efficient pure-RL\ntraining with output-based rewards, requiring no additional annotations or\nextensive fine-tuning. Furthermore, to utilize computational resources more\nefficiently, we introduce a sparse-to-dense video TTS strategy that improves\ninference by iteratively adding frames based on output consistency. We validate\nour approach on multiple video reasoning benchmarks, showing that Video-RTS\nsurpasses existing video reasoning models by an average of 2.4% in accuracy\nusing only 3.6% training samples. For example, Video-RTS achieves a 4.2%\nimprovement on Video-Holmes, a recent and challenging video reasoning\nbenchmark, and a 2.6% improvement on MMVU. Notably, our pure RL training and\nadaptive video TTS offer complementary strengths, enabling Video-RTS's strong\nreasoning performance."}
{"id": "2507.06486", "pdf": "https://arxiv.org/pdf/2507.06486", "abs": "https://arxiv.org/abs/2507.06486", "authors": ["Yuechen Xie", "Haobo Jiang", "Jin Xie"], "title": "Mask6D: Masked Pose Priors For 6D Object Pose Estimation", "categories": ["cs.CV"], "comment": "Accepted at ICASSP 2024. 4 figures, 3 tables", "summary": "Robust 6D object pose estimation in cluttered or occluded conditions using\nmonocular RGB images remains a challenging task. One reason is that current\npose estimation networks struggle to extract discriminative, pose-aware\nfeatures using 2D feature backbones, especially when the available RGB\ninformation is limited due to target occlusion in cluttered scenes. To mitigate\nthis, we propose a novel pose estimation-specific pre-training strategy named\nMask6D. Our approach incorporates pose-aware 2D-3D correspondence maps and\nvisible mask maps as additional modal information, which is combined with RGB\nimages for the reconstruction-based model pre-training. Essentially, this 2D-3D\ncorrespondence maps a transformed 3D object model to 2D pixels, reflecting the\npose information of the target in camera coordinate system. Meanwhile, the\nintegrated visible mask map can effectively guide our model to disregard\ncluttered background information. In addition, an object-focused pre-training\nloss function is designed to further facilitate our network to remove the\nbackground interference. Finally, we fine-tune our pre-trained pose prior-aware\nnetwork via conventional pose training strategy to realize the reliable pose\nprediction. Extensive experiments verify that our method outperforms previous\nend-to-end pose estimation methods."}
{"id": "2507.06510", "pdf": "https://arxiv.org/pdf/2507.06510", "abs": "https://arxiv.org/abs/2507.06510", "authors": ["Yupeng Hu", "Changxing Ding", "Chang Sun", "Shaoli Huang", "Xiangmin Xu"], "title": "Bilateral Collaboration with Large Vision-Language Models for Open Vocabulary Human-Object Interaction Detection", "categories": ["cs.CV"], "comment": "ICCV 2025", "summary": "Open vocabulary Human-Object Interaction (HOI) detection is a challenging\ntask that detects all <human, verb, object> triplets of interest in an image,\neven those that are not pre-defined in the training set. Existing approaches\ntypically rely on output features generated by large Vision-Language Models\n(VLMs) to enhance the generalization ability of interaction representations.\nHowever, the visual features produced by VLMs are holistic and coarse-grained,\nwhich contradicts the nature of detection tasks. To address this issue, we\npropose a novel Bilateral Collaboration framework for open vocabulary HOI\ndetection (BC-HOI). This framework includes an Attention Bias Guidance (ABG)\ncomponent, which guides the VLM to produce fine-grained instance-level\ninteraction features according to the attention bias provided by the HOI\ndetector. It also includes a Large Language Model (LLM)-based Supervision\nGuidance (LSG) component, which provides fine-grained token-level supervision\nfor the HOI detector by the LLM component of the VLM. LSG enhances the ability\nof ABG to generate high-quality attention bias. We conduct extensive\nexperiments on two popular benchmarks: HICO-DET and V-COCO, consistently\nachieving superior performance in the open vocabulary and closed settings. The\ncode will be released in Github."}
{"id": "2507.06513", "pdf": "https://arxiv.org/pdf/2507.06513", "abs": "https://arxiv.org/abs/2507.06513", "authors": ["Yaoqi Huang", "Julie Stephany Berrio", "Mao Shan", "Stewart Worrall"], "title": "What Demands Attention in Urban Street Scenes? From Scene Understanding towards Road Safety: A Survey of Vision-driven Datasets and Studies", "categories": ["cs.CV"], "comment": "45 pages, 52 figures, 2 large tables (divided into 5), 73 datatsets,\n  35 tasks", "summary": "Advances in vision-based sensors and computer vision algorithms have\nsignificantly improved the analysis and understanding of traffic scenarios. To\nfacilitate the use of these improvements for road safety, this survey\nsystematically categorizes the critical elements that demand attention in\ntraffic scenarios and comprehensively analyzes available vision-driven tasks\nand datasets. Compared to existing surveys that focus on isolated domains, our\ntaxonomy categorizes attention-worthy traffic entities into two main groups\nthat are anomalies and normal but critical entities, integrating ten categories\nand twenty subclasses. It establishes connections between inherently related\nfields and provides a unified analytical framework. Our survey highlights the\nanalysis of 35 vision-driven tasks and comprehensive examinations and\nvisualizations of 73 available datasets based on the proposed taxonomy. The\ncross-domain investigation covers the pros and cons of each benchmark with the\naim of providing information on standards unification and resource\noptimization. Our article concludes with a systematic discussion of the\nexisting weaknesses, underlining the potential effects and promising solutions\nfrom various perspectives. The integrated taxonomy, comprehensive analysis, and\nrecapitulatory tables serve as valuable contributions to this rapidly evolving\nfield by providing researchers with a holistic overview, guiding strategic\nresource selection, and highlighting critical research gaps."}
{"id": "2507.06523", "pdf": "https://arxiv.org/pdf/2507.06523", "abs": "https://arxiv.org/abs/2507.06523", "authors": ["Liqiang Jing", "Viet Lai", "Seunghyun Yoon", "Trung Bui", "Xinya Du"], "title": "FIFA: Unified Faithfulness Evaluation Framework for Text-to-Video and Video-to-Text Generation", "categories": ["cs.CV", "cs.CL", "cs.GR"], "comment": null, "summary": "Video Multimodal Large Language Models (VideoMLLMs) have achieved remarkable\nprogress in both Video-to-Text and Text-to-Video tasks. However, they often\nsuffer fro hallucinations, generating content that contradicts the visual\ninput. Existing evaluation methods are limited to one task (e.g., V2T) and also\nfail to assess hallucinations in open-ended, free-form responses. To address\nthis gap, we propose FIFA, a unified FaIthFulness evAluation framework that\nextracts comprehensive descriptive facts, models their semantic dependencies\nvia a Spatio-Temporal Semantic Dependency Graph, and verifies them using\nVideoQA models. We further introduce Post-Correction, a tool-based correction\nframework that revises hallucinated content. Extensive experiments demonstrate\nthat FIFA aligns more closely with human judgment than existing evaluation\nmethods, and that Post-Correction effectively improves factual consistency in\nboth text and video generation."}
{"id": "2507.06526", "pdf": "https://arxiv.org/pdf/2507.06526", "abs": "https://arxiv.org/abs/2507.06526", "authors": ["Chaoshuo Zhang", "Chenhao Lin", "Zhengyu Zhao", "Le Yang", "Qian Wang", "Chao Shen"], "title": "Concept Unlearning by Modeling Key Steps of Diffusion Process", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-image diffusion models (T2I DMs), represented by Stable Diffusion,\nwhich generate highly realistic images based on textual input, have been widely\nused. However, their misuse poses serious security risks. While existing\nconcept unlearning methods aim to mitigate these risks, they struggle to\nbalance unlearning effectiveness with generative retainability.To overcome this\nlimitation, we innovatively propose the Key Step Concept Unlearning (KSCU)\nmethod, which ingeniously capitalizes on the unique stepwise sampling\ncharacteristic inherent in diffusion models during the image generation\nprocess. Unlike conventional approaches that treat all denoising steps equally,\nKSCU strategically focuses on pivotal steps with the most influence over the\nfinal outcome by dividing key steps for different concept unlearning tasks and\nfine-tuning the model only at those steps. This targeted approach reduces the\nnumber of parameter updates needed for effective unlearning, while maximizing\nthe retention of the model's generative capabilities.Through extensive\nbenchmark experiments, we demonstrate that KSCU effectively prevents T2I DMs\nfrom generating undesirable images while better retaining the model's\ngenerative capabilities.Our code will be released."}
{"id": "2507.06530", "pdf": "https://arxiv.org/pdf/2507.06530", "abs": "https://arxiv.org/abs/2507.06530", "authors": ["Kazi Mahathir Rahman", "Naveed Imtiaz Nafis", "Md. Farhan Sadik", "Mohammad Al Rafi", "Mehedi Hasan Shahed"], "title": "Speak2Sign3D: A Multi-modal Pipeline for English Speech to American Sign Language Animation", "categories": ["cs.CV"], "comment": "11 pages, 12 figures", "summary": "Helping deaf and hard-of-hearing people communicate more easily is the main\ngoal of Automatic Sign Language Translation. Although most past research has\nfocused on turning sign language into text, doing the reverse, turning spoken\nEnglish into sign language animations, has been largely overlooked. That's\nbecause it involves multiple steps, such as understanding speech, translating\nit into sign-friendly grammar, and generating natural human motion. In this\nwork, we introduce a complete pipeline that converts English speech into\nsmooth, realistic 3D sign language animations. Our system starts with Whisper\nto translate spoken English into text. Then, we use a MarianMT machine\ntranslation model to translate that text into American Sign Language (ASL)\ngloss, a simplified version of sign language that captures meaning without\ngrammar. This model performs well, reaching BLEU scores of 0.7714 and 0.8923.\nTo make the gloss translation more accurate, we also use word embeddings such\nas Word2Vec and FastText to understand word meanings. Finally, we animate the\ntranslated gloss using a 3D keypoint-based motion system trained on\nSign3D-WLASL, a dataset we created by extracting body, hand, and face key\npoints from real ASL videos in the WLASL dataset. To support the gloss\ntranslation stage, we also built a new dataset called BookGlossCorpus-CG, which\nturns everyday English sentences from the BookCorpus dataset into ASL gloss\nusing grammar rules. Our system stitches everything together by smoothly\ninterpolating between signs to create natural, continuous animations. Unlike\nprevious works like How2Sign and Phoenix-2014T that focus on recognition or use\nonly one type of data, our pipeline brings together audio, text, and motion in\na single framework that goes all the way from spoken English to lifelike 3D\nsign language animation."}
{"id": "2507.06531", "pdf": "https://arxiv.org/pdf/2507.06531", "abs": "https://arxiv.org/abs/2507.06531", "authors": ["Mingjin Zeng", "Nan Ouyang", "Wenkang Wan", "Lei Ao", "Qing Cai", "Kai Sheng"], "title": "ILNet: Trajectory Prediction with Inverse Learning Attention for Enhancing Intention Capture", "categories": ["cs.CV"], "comment": null, "summary": "Trajectory prediction for multi-agent interaction scenarios is a crucial\nchallenge. Most advanced methods model agent interactions by efficiently\nfactorized attention based on the temporal and agent axes. However, this static\nand foward modeling lacks explicit interactive spatio-temporal coordination,\ncapturing only obvious and immediate behavioral intentions. Alternatively, the\nmodern trajectory prediction framework refines the successive predictions by a\nfixed-anchor selection strategy, which is difficult to adapt in different\nfuture environments. It is acknowledged that human drivers dynamically adjust\ninitial driving decisions based on further assumptions about the intentions of\nsurrounding vehicles. Motivated by human driving behaviors, this paper proposes\nILNet, a multi-agent trajectory prediction method with Inverse Learning (IL)\nattention and Dynamic Anchor Selection (DAS) module. IL Attention employs an\ninverse learning paradigm to model interactions at neighboring moments,\nintroducing proposed intentions to dynamically encode the spatio-temporal\ncoordination of interactions, thereby enhancing the model's ability to capture\ncomplex interaction patterns. Then, the learnable DAS module is proposed to\nextract multiple trajectory change keypoints as anchors in parallel with almost\nno increase in parameters. Experimental results show that the ILNet achieves\nstate-of-the-art performance on the INTERACTION and Argoverse motion\nforecasting datasets. Particularly, in challenged interaction scenarios, ILNet\nachieves higher accuracy and more multimodal distributions of trajectories over\nfewer parameters. Our codes are available at https://github.com/mjZeng11/ILNet."}
{"id": "2507.06537", "pdf": "https://arxiv.org/pdf/2507.06537", "abs": "https://arxiv.org/abs/2507.06537", "authors": ["Thi Thu Thuy Nguyen", "Duc Thanh Nguyen"], "title": "A model-agnostic active learning approach for animal detection from camera traps", "categories": ["cs.CV"], "comment": null, "summary": "Smart data selection is becoming increasingly important in data-driven\nmachine learning. Active learning offers a promising solution by allowing\nmachine learning models to be effectively trained with optimal data including\nthe most informative samples from large datasets. Wildlife data captured by\ncamera traps are excessive in volume, requiring tremendous effort in data\nlabelling and animal detection models training. Therefore, applying active\nlearning to optimise the amount of labelled data would be a great aid in\nenabling automated wildlife monitoring and conservation. However, existing\nactive learning techniques require that a machine learning model (i.e., an\nobject detector) be fully accessible, limiting the applicability of the\ntechniques. In this paper, we propose a model-agnostic active learning approach\nfor detection of animals captured by camera traps. Our approach integrates\nuncertainty and diversity quantities of samples at both the object-based and\nimage-based levels into the active learning sample selection process. We\nvalidate our approach in a benchmark animal dataset. Experimental results\ndemonstrate that, using only 30% of the training data selected by our approach,\na state-of-the-art animal detector can achieve a performance of equal or\ngreater than that with the use of the complete training dataset."}
{"id": "2507.06543", "pdf": "https://arxiv.org/pdf/2507.06543", "abs": "https://arxiv.org/abs/2507.06543", "authors": ["Taekyung Kim", "Dongyoon Han", "Byeongho Heo", "Jeongeun Park", "Sangdoo Yun"], "title": "Token Bottleneck: One Token to Remember Dynamics", "categories": ["cs.CV"], "comment": "17 pages, 9 figures, 8 tables, project page:\n  https://token-bottleneck.github.io, code: https://github.com/naver-ai/tobo", "summary": "Deriving compact and temporally aware visual representations from dynamic\nscenes is essential for successful execution of sequential scene understanding\ntasks such as visual tracking and robotic manipulation. In this paper, we\nintroduce Token Bottleneck (ToBo), a simple yet intuitive self-supervised\nlearning pipeline that squeezes a scene into a bottleneck token and predicts\nthe subsequent scene using minimal patches as hints. The ToBo pipeline\nfacilitates the learning of sequential scene representations by conservatively\nencoding the reference scene into a compact bottleneck token during the squeeze\nstep. In the expansion step, we guide the model to capture temporal dynamics by\npredicting the target scene using the bottleneck token along with few target\npatches as hints. This design encourages the vision backbone to embed temporal\ndependencies, thereby enabling understanding of dynamic transitions across\nscenes. Extensive experiments in diverse sequential tasks, including video\nlabel propagation and robot manipulation in simulated environments demonstrate\nthe superiority of ToBo over baselines. Moreover, deploying our pre-trained\nmodel on physical robots confirms its robustness and effectiveness in\nreal-world environments. We further validate the scalability of ToBo across\ndifferent model scales."}
{"id": "2507.06547", "pdf": "https://arxiv.org/pdf/2507.06547", "abs": "https://arxiv.org/abs/2507.06547", "authors": ["Yonghyun Park", "Chieh-Hsin Lai", "Satoshi Hayakawa", "Yuhta Takida", "Naoki Murata", "Wei-Hsiang Liao", "Woosung Choi", "Kin Wai Cheuk", "Junghyun Koo", "Yuki Mitsufuji"], "title": "Concept-TRAK: Understanding how diffusion models learn concepts through concept-level attribution", "categories": ["cs.CV", "cs.LG"], "comment": "Preprint", "summary": "While diffusion models excel at image generation, their growing adoption\nraises critical concerns around copyright issues and model transparency.\nExisting attribution methods identify training examples influencing an entire\nimage, but fall short in isolating contributions to specific elements, such as\nstyles or objects, that matter most to stakeholders. To bridge this gap, we\nintroduce \\emph{concept-level attribution} via a novel method called\n\\emph{Concept-TRAK}. Concept-TRAK extends influence functions with two key\ninnovations: (1) a reformulated diffusion training loss based on diffusion\nposterior sampling, enabling robust, sample-specific attribution; and (2) a\nconcept-aware reward function that emphasizes semantic relevance. We evaluate\nConcept-TRAK on the AbC benchmark, showing substantial improvements over prior\nmethods. Through diverse case studies--ranging from identifying IP-protected\nand unsafe content to analyzing prompt engineering and compositional\nlearning--we demonstrate how concept-level attribution yields actionable\ninsights for responsible generative AI development and governance."}
{"id": "2507.06560", "pdf": "https://arxiv.org/pdf/2507.06560", "abs": "https://arxiv.org/abs/2507.06560", "authors": ["Jae Hyoung Jeon", "Cheolsu Lim", "Myungjoo Kang"], "title": "Divergence-Based Similarity Function for Multi-View Contrastive Learning", "categories": ["cs.CV", "cs.LG", "68T07, 62H12", "I.2.6; I.4.8; I.5.1"], "comment": "9 pages, 5 figures", "summary": "Recent success in contrastive learning has sparked growing interest in more\neffectively leveraging multiple augmented views of an instance. While prior\nmethods incorporate multiple views at the loss or feature level, they primarily\ncapture pairwise relationships and fail to model the joint structure across all\nviews. In this work, we propose a divergence-based similarity function (DSF)\nthat explicitly captures the joint structure by representing each set of\naugmented views as a distribution and measuring similarity as the divergence\nbetween distributions. Extensive experiments demonstrate that DSF consistently\nimproves performance across various tasks, including kNN classification and\nlinear evaluation, while also offering greater efficiency compared to other\nmulti-view methods. Furthermore, we establish a theoretical connection between\nDSF and cosine similarity, and show that, unlike cosine similarity, DSF\noperates effectively without requiring a temperature hyperparameter."}
{"id": "2507.06569", "pdf": "https://arxiv.org/pdf/2507.06569", "abs": "https://arxiv.org/abs/2507.06569", "authors": ["Hao Shu"], "title": "Edge-Boundary-Texture Loss: A Tri-Class Generalization of Weighted Binary Cross-Entropy for Enhanced Edge Detection", "categories": ["cs.CV"], "comment": "10 pages", "summary": "Edge detection (ED) remains a fundamental task in computer vision, yet its\nperformance is often hindered by the ambiguous nature of non-edge pixels near\nobject boundaries. The widely adopted Weighted Binary Cross-Entropy (WBCE) loss\ntreats all non-edge pixels uniformly, overlooking the structural nuances around\nedges and often resulting in blurred predictions. In this paper, we propose the\nEdge-Boundary-Texture (EBT) loss, a novel objective that explicitly divides\npixels into three categories, edge, boundary, and texture, and assigns each a\ndistinct supervisory weight. This tri-class formulation enables more structured\nlearning by guiding the model to focus on both edge precision and contextual\nboundary localization. We theoretically show that the EBT loss generalizes the\nWBCE loss, with the latter becoming a limit case. Extensive experiments across\nmultiple benchmarks demonstrate the superiority of the EBT loss both\nquantitatively and perceptually. Furthermore, the consistent use of unified\nhyperparameters across all models and datasets, along with robustness to their\nmoderate variations, indicates that the EBT loss requires minimal fine-tuning\nand is easily deployable in practice."}
{"id": "2507.06590", "pdf": "https://arxiv.org/pdf/2507.06590", "abs": "https://arxiv.org/abs/2507.06590", "authors": ["Yin Wang", "Mu li", "Zhiying Leng", "Frederick W. B. Li", "Xiaohui Liang"], "title": "MOST: Motion Diffusion Model for Rare Text via Temporal Clip Banzhaf Interaction", "categories": ["cs.CV"], "comment": null, "summary": "We introduce MOST, a novel motion diffusion model via temporal clip Banzhaf\ninteraction, aimed at addressing the persistent challenge of generating human\nmotion from rare language prompts. While previous approaches struggle with\ncoarse-grained matching and overlook important semantic cues due to motion\nredundancy, our key insight lies in leveraging fine-grained clip relationships\nto mitigate these issues. MOST's retrieval stage presents the first formulation\nof its kind - temporal clip Banzhaf interaction - which precisely quantifies\ntextual-motion coherence at the clip level. This facilitates direct,\nfine-grained text-to-motion clip matching and eliminates prevalent redundancy.\nIn the generation stage, a motion prompt module effectively utilizes retrieved\nmotion clips to produce semantically consistent movements. Extensive\nevaluations confirm that MOST achieves state-of-the-art text-to-motion\nretrieval and generation performance by comprehensively addressing previous\nchallenges, as demonstrated through quantitative and qualitative results\nhighlighting its effectiveness, especially for rare prompts."}
{"id": "2507.06592", "pdf": "https://arxiv.org/pdf/2507.06592", "abs": "https://arxiv.org/abs/2507.06592", "authors": ["Yang Chen", "Yueqi Duan", "Haowen Sun", "Jiwen Lu", "Yap-Peng Tan"], "title": "Ambiguity-aware Point Cloud Segmentation by Adaptive Margin Contrastive Learning", "categories": ["cs.CV"], "comment": "This article has been accepted for publication in IEEE Transactions\n  on Multimedia. arXiv admin note: text overlap with arXiv:2502.04111", "summary": "This paper proposes an adaptive margin contrastive learning method for 3D\nsemantic segmentation on point clouds. Most existing methods use equally\npenalized objectives, which ignore the per-point ambiguities and less\ndiscriminated features stemming from transition regions. However, as highly\nambiguous points may be indistinguishable even for humans, their manually\nannotated labels are less reliable, and hard constraints over these points\nwould lead to sub-optimal models. To address this, we first design\nAMContrast3D, a method comprising contrastive learning into an ambiguity\nestimation framework, tailored to adaptive objectives for individual points\nbased on ambiguity levels. As a result, our method promotes model training,\nwhich ensures the correctness of low-ambiguity points while allowing mistakes\nfor high-ambiguity points. As ambiguities are formulated based on position\ndiscrepancies across labels, optimization during inference is constrained by\nthe assumption that all unlabeled points are uniformly unambiguous, lacking\nambiguity awareness. Inspired by the insight of joint training, we further\npropose AMContrast3D++ integrating with two branches trained in parallel, where\na novel ambiguity prediction module concurrently learns point ambiguities from\ngenerated embeddings. To this end, we design a masked refinement mechanism that\nleverages predicted ambiguities to enable the ambiguous embeddings to be more\nreliable, thereby boosting segmentation performance and enhancing robustness.\nExperimental results on 3D indoor scene datasets, S3DIS and ScanNet,\ndemonstrate the effectiveness of the proposed method. Code is available at\nhttps://github.com/YangChenApril/AMContrast3D."}
{"id": "2507.06593", "pdf": "https://arxiv.org/pdf/2507.06593", "abs": "https://arxiv.org/abs/2507.06593", "authors": ["Qianyu Zhang", "Bolun Zheng", "Hangjia Pan", "Lingyu Zhu", "Zunjie Zhu", "Zongpeng Li", "Shiqi Wang"], "title": "Capturing Stable HDR Videos Using a Dual-Camera System", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "In HDR video reconstruction, exposure fluctuations in reference images from\nalternating exposure methods often result in flickering. To address this issue,\nwe propose a dual-camera system (DCS) for HDR video acquisition, where one\ncamera is assigned to capture consistent reference sequences, while the other\nis assigned to capture non-reference sequences for information supplementation.\nTo tackle the challenges posed by video data, we introduce an exposure-adaptive\nfusion network (EAFNet) to achieve more robust results. EAFNet introduced a\npre-alignment subnetwork to explore the influence of exposure, selectively\nemphasizing the valuable features across different exposure levels. Then, the\nenhanced features are fused by the asymmetric cross-feature fusion subnetwork,\nwhich explores reference-dominated attention maps to improve image fusion by\naligning cross-scale features and performing cross-feature fusion. Finally, the\nreconstruction subnetwork adopts a DWT-based multiscale architecture to reduce\nghosting artifacts and refine features at different resolutions. Extensive\nexperimental evaluations demonstrate that the proposed method achieves\nstate-of-the-art performance on different datasets, validating the great\npotential of the DCS in HDR video reconstruction. The codes and data captured\nby DCS will be available at https://github.com/zqqqyu/DCS."}
{"id": "2507.06603", "pdf": "https://arxiv.org/pdf/2507.06603", "abs": "https://arxiv.org/abs/2507.06603", "authors": ["Xu Shaowu", "Jia Xibin", "Gao Junyu", "Sun Qianmei", "Chang Jing", "Fan Chao"], "title": "Cross-Modal Dual-Causal Learning for Long-Term Action Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Long-term action recognition (LTAR) is challenging due to extended temporal\nspans with complex atomic action correlations and visual confounders. Although\nvision-language models (VLMs) have shown promise, they often rely on\nstatistical correlations instead of causal mechanisms. Moreover, existing\ncausality-based methods address modal-specific biases but lack cross-modal\ncausal modeling, limiting their utility in VLM-based LTAR. This paper proposes\n\\textbf{C}ross-\\textbf{M}odal \\textbf{D}ual-\\textbf{C}ausal \\textbf{L}earning\n(CMDCL), which introduces a structural causal model to uncover causal\nrelationships between videos and label texts.\n  CMDCL addresses cross-modal biases in text embeddings via textual causal\nintervention and removes confounders inherent in the visual modality through\nvisual causal intervention guided by the debiased text.\n  These dual-causal interventions enable robust action representations to\naddress LTAR challenges. Experimental results on three benchmarks including\nCharades, Breakfast and COIN, demonstrate the effectiveness of the proposed\nmodel. Our code is available at https://github.com/xushaowu/CMDCL."}
{"id": "2507.06606", "pdf": "https://arxiv.org/pdf/2507.06606", "abs": "https://arxiv.org/abs/2507.06606", "authors": ["Qing Zhang", "Guoquan Pei", "Yan Wang"], "title": "Omni-Fusion of Spatial and Spectral for Hyperspectral Image Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Medical Hyperspectral Imaging (MHSI) has emerged as a promising tool for\nenhanced disease diagnosis, particularly in computational pathology, offering\nrich spectral information that aids in identifying subtle biochemical\nproperties of tissues. Despite these advantages, effectively fusing both\nspatial-dimensional and spectral-dimensional information from MHSIs remains\nchallenging due to its high dimensionality and spectral redundancy inherent\ncharacteristics. To solve the above challenges, we propose a novel\nspatial-spectral omni-fusion network for hyperspectral image segmentation,\nnamed as Omni-Fuse. Here, we introduce abundant cross-dimensional feature\nfusion operations, including a cross-dimensional enhancement module that\nrefines both spatial and spectral features through bidirectional attention\nmechanisms, a spectral-guided spatial query selection to select the most\nspectral-related spatial feature as the query, and a two-stage\ncross-dimensional decoder which dynamically guide the model to focus on the\nselected spatial query. Despite of numerous attention blocks, Omni-Fuse remains\nefficient in execution. Experiments on two microscopic hyperspectral image\ndatasets show that our approach can significantly improve the segmentation\nperformance compared with the state-of-the-art methods, with over 5.73 percent\nimprovement in DSC. Code available at:\nhttps://github.com/DeepMed-Lab-ECNU/Omni-Fuse."}
{"id": "2507.06618", "pdf": "https://arxiv.org/pdf/2507.06618", "abs": "https://arxiv.org/abs/2507.06618", "authors": ["Yang Chen", "Yueqi Duan", "Haowen Sun", "Ziwei Wang", "Jiwen Lu", "Yap-Peng Tan"], "title": "PointVDP: Learning View-Dependent Projection by Fireworks Rays for 3D Point Cloud Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "In this paper, we propose view-dependent projection (VDP) to facilitate point\ncloud segmentation, designing efficient 3D-to-2D mapping that dynamically\nadapts to the spatial geometry from view variations. Existing projection-based\nmethods leverage view-independent projection in complex scenes, relying on\nstraight lines to generate direct rays or upward curves to reduce occlusions.\nHowever, their view independence provides projection rays that are limited to\npre-defined parameters by human settings, restricting point awareness and\nfailing to capture sufficient projection diversity across different view\nplanes. Although multiple projections per view plane are commonly used to\nenhance spatial variety, the projected redundancy leads to excessive\ncomputational overhead and inefficiency in image processing. To address these\nlimitations, we design a framework of VDP to generate data-driven projections\nfrom 3D point distributions, producing highly informative single-image inputs\nby predicting rays inspired by the adaptive behavior of fireworks. In addition,\nwe construct color regularization to optimize the framework, which emphasizes\nessential features within semantic pixels and suppresses the non-semantic\nfeatures within black pixels, thereby maximizing 2D space utilization in a\nprojected image. As a result, our approach, PointVDP, develops lightweight\nprojections in marginal computation costs. Experiments on S3DIS and ScanNet\nbenchmarks show that our approach achieves competitive results, offering a\nresource-efficient solution for semantic understanding."}
{"id": "2507.06639", "pdf": "https://arxiv.org/pdf/2507.06639", "abs": "https://arxiv.org/abs/2507.06639", "authors": ["Myungjang Pyeon", "Janghyeon Lee", "Minsoo Lee", "Juseung Yun", "Hwanil Choi", "Jonghyun Kim", "Jiwon Kim", "Yi Hu", "Jongseong Jang", "Soonyoung Lee"], "title": "EXAONE Path 2.0: Pathology Foundation Model with End-to-End Supervision", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "EXAONE Path 2.0 technical report", "summary": "In digital pathology, whole-slide images (WSIs) are often difficult to handle\ndue to their gigapixel scale, so most approaches train patch encoders via\nself-supervised learning (SSL) and then aggregate the patch-level embeddings\nvia multiple instance learning (MIL) or slide encoders for downstream tasks.\nHowever, patch-level SSL may overlook complex domain-specific features that are\nessential for biomarker prediction, such as mutation status and molecular\ncharacteristics, as SSL methods rely only on basic augmentations selected for\nnatural image domains on small patch-level area. Moreover, SSL methods remain\nless data efficient than fully supervised approaches, requiring extensive\ncomputational resources and datasets to achieve competitive performance. To\naddress these limitations, we present EXAONE Path 2.0, a pathology foundation\nmodel that learns patch-level representations under direct slide-level\nsupervision. Using only 37k WSIs for training, EXAONE Path 2.0 achieves\nstate-of-the-art average performance across 10 biomarker prediction tasks,\ndemonstrating remarkable data efficiency."}
{"id": "2507.06643", "pdf": "https://arxiv.org/pdf/2507.06643", "abs": "https://arxiv.org/abs/2507.06643", "authors": ["Farahdiba Zarin", "Riccardo Oliva", "Vinkle Srivastav", "Armine Vardazaryan", "Andrea Rosati", "Alice Zampolini Faustini", "Giovanni Scambia", "Anna Fagotti", "Pietro Mascagni", "Nicolas Padoy"], "title": "Learning from Sparse Point Labels for Dense Carcinosis Localization in Advanced Ovarian Cancer Assessment", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Learning from sparse labels is a challenge commonplace in the medical domain.\nThis is due to numerous factors, such as annotation cost, and is especially\ntrue for newly introduced tasks. When dense pixel-level annotations are needed,\nthis becomes even more unfeasible. However, being able to learn from just a few\nannotations at the pixel-level, while extremely difficult and underutilized,\ncan drive progress in studies where perfect annotations are not immediately\navailable. This work tackles the challenge of learning the dense prediction\ntask of keypoint localization from a few point annotations in the context of 2d\ncarcinosis keypoint localization from laparoscopic video frames for diagnostic\nplanning of advanced ovarian cancer patients. To enable this, we formulate the\nproblem as a sparse heatmap regression from a few point annotations per image\nand propose a new loss function, called Crag and Tail loss, for efficient\nlearning. Our proposed loss function effectively leverages positive sparse\nlabels while minimizing the impact of false negatives or missed annotations.\nThrough an extensive ablation study, we demonstrate the effectiveness of our\napproach in achieving accurate dense localization of carcinosis keypoints,\nhighlighting its potential to advance research in scenarios where dense\nannotations are challenging to obtain."}
{"id": "2507.06647", "pdf": "https://arxiv.org/pdf/2507.06647", "abs": "https://arxiv.org/abs/2507.06647", "authors": ["Chengkun Li", "Yuqi Tong", "Kai Chen", "Zhenya Yang", "Ruiyang Li", "Shi Qiu", "Jason Ying-Kuen Chan", "Pheng-Ann Heng", "Qi Dou"], "title": "ClipGS: Clippable Gaussian Splatting for Interactive Cinematic Visualization of Volumetric Medical Data", "categories": ["cs.CV"], "comment": "Early accepted by MICCAI 2025. Project is available at:\n  https://med-air.github.io/ClipGS", "summary": "The visualization of volumetric medical data is crucial for enhancing\ndiagnostic accuracy and improving surgical planning and education. Cinematic\nrendering techniques significantly enrich this process by providing\nhigh-quality visualizations that convey intricate anatomical details, thereby\nfacilitating better understanding and decision-making in medical contexts.\nHowever, the high computing cost and low rendering speed limit the requirement\nof interactive visualization in practical applications. In this paper, we\nintroduce ClipGS, an innovative Gaussian splatting framework with the clipping\nplane supported, for interactive cinematic visualization of volumetric medical\ndata. To address the challenges posed by dynamic interactions, we propose a\nlearnable truncation scheme that automatically adjusts the visibility of\nGaussian primitives in response to the clipping plane. Besides, we also design\nan adaptive adjustment model to dynamically adjust the deformation of Gaussians\nand refine the rendering performance. We validate our method on five volumetric\nmedical data (including CT and anatomical slice data), and reach an average\n36.635 PSNR rendering quality with 156 FPS and 16.1 MB model size,\noutperforming state-of-the-art methods in rendering quality and efficiency."}
{"id": "2507.06651", "pdf": "https://arxiv.org/pdf/2507.06651", "abs": "https://arxiv.org/abs/2507.06651", "authors": ["Juncheng Mu", "Chengwei Ren", "Weixiang Zhang", "Liang Pan", "Xiao-Ping Zhang", "Yue Gao"], "title": "Diff$^2$I2P: Differentiable Image-to-Point Cloud Registration with Diffusion Prior", "categories": ["cs.CV"], "comment": "ICCV 2025", "summary": "Learning cross-modal correspondences is essential for image-to-point cloud\n(I2P) registration. Existing methods achieve this mostly by utilizing metric\nlearning to enforce feature alignment across modalities, disregarding the\ninherent modality gap between image and point data. Consequently, this paradigm\nstruggles to ensure accurate cross-modal correspondences. To this end, inspired\nby the cross-modal generation success of recent large diffusion models, we\npropose Diff$^2$I2P, a fully Differentiable I2P registration framework,\nleveraging a novel and effective Diffusion prior for bridging the modality gap.\nSpecifically, we propose a Control-Side Score Distillation (CSD) technique to\ndistill knowledge from a depth-conditioned diffusion model to directly optimize\nthe predicted transformation. However, the gradients on the transformation fail\nto backpropagate onto the cross-modal features due to the non-differentiability\nof correspondence retrieval and PnP solver. To this end, we further propose a\nDeformable Correspondence Tuning (DCT) module to estimate the correspondences\nin a differentiable way, followed by the transformation estimation using a\ndifferentiable PnP solver. With these two designs, the Diffusion model serves\nas a strong prior to guide the cross-modal feature learning of image and point\ncloud for forming robust correspondences, which significantly improves the\nregistration. Extensive experimental results demonstrate that Diff$^2$I2P\nconsistently outperforms SoTA I2P registration methods, achieving over 7%\nimprovement in registration recall on the 7-Scenes benchmark."}
{"id": "2507.06654", "pdf": "https://arxiv.org/pdf/2507.06654", "abs": "https://arxiv.org/abs/2507.06654", "authors": ["Naoya Sogi", "Takashi Shibata", "Makoto Terao", "Masanori Suganuma", "Takayuki Okatani"], "title": "MS-DPPs: Multi-Source Determinantal Point Processes for Contextual Diversity Refinement of Composite Attributes in Text to Image Retrieval", "categories": ["cs.CV", "cs.AI", "cs.IR"], "comment": "IJCAI 2025. Code: https://github.com/NEC-N-SOGI/msdpp", "summary": "Result diversification (RD) is a crucial technique in Text-to-Image Retrieval\nfor enhancing the efficiency of a practical application. Conventional methods\nfocus solely on increasing the diversity metric of image appearances. However,\nthe diversity metric and its desired value vary depending on the application,\nwhich limits the applications of RD. This paper proposes a novel task called\nCDR-CA (Contextual Diversity Refinement of Composite Attributes). CDR-CA aims\nto refine the diversities of multiple attributes, according to the\napplication's context. To address this task, we propose Multi-Source DPPs, a\nsimple yet strong baseline that extends the Determinantal Point Process (DPP)\nto multi-sources. We model MS-DPP as a single DPP model with a unified\nsimilarity matrix based on a manifold representation. We also introduce Tangent\nNormalization to reflect contexts. Extensive experiments demonstrate the\neffectiveness of the proposed method. Our code is publicly available at\nhttps://github.com/NEC-N-SOGI/msdpp."}
{"id": "2507.06656", "pdf": "https://arxiv.org/pdf/2507.06656", "abs": "https://arxiv.org/abs/2507.06656", "authors": ["Hongjie Wu", "Mingqin Zhang", "Linchao He", "Ji-Zhe Zhou", "Jiancheng Lv"], "title": "Enhancing Diffusion Model Stability for Image Restoration via Gradient Management", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted to ACM Multimedia 2025. Preprint version", "summary": "Diffusion models have shown remarkable promise for image restoration by\nleveraging powerful priors. Prominent methods typically frame the restoration\nproblem within a Bayesian inference framework, which iteratively combines a\ndenoising step with a likelihood guidance step. However, the interactions\nbetween these two components in the generation process remain underexplored. In\nthis paper, we analyze the underlying gradient dynamics of these components and\nidentify significant instabilities. Specifically, we demonstrate conflicts\nbetween the prior and likelihood gradient directions, alongside temporal\nfluctuations in the likelihood gradient itself. We show that these\ninstabilities disrupt the generative process and compromise restoration\nperformance. To address these issues, we propose Stabilized Progressive\nGradient Diffusion (SPGD), a novel gradient management technique. SPGD\nintegrates two synergistic components: (1) a progressive likelihood warm-up\nstrategy to mitigate gradient conflicts; and (2) adaptive directional momentum\n(ADM) smoothing to reduce fluctuations in the likelihood gradient. Extensive\nexperiments across diverse restoration tasks demonstrate that SPGD\nsignificantly enhances generation stability, leading to state-of-the-art\nperformance in quantitative metrics and visually superior results. Code is\navailable at \\href{https://github.com/74587887/SPGD}{here}."}
{"id": "2507.06662", "pdf": "https://arxiv.org/pdf/2507.06662", "abs": "https://arxiv.org/abs/2507.06662", "authors": ["Yifan Yang", "Peili Song", "Enfan Lan", "Dong Liu", "Jingtai Liu"], "title": "MK-Pose: Category-Level Object Pose Estimation via Multimodal-Based Keypoint Learning", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Category-level object pose estimation, which predicts the pose of objects\nwithin a known category without prior knowledge of individual instances, is\nessential in applications like warehouse automation and manufacturing. Existing\nmethods relying on RGB images or point cloud data often struggle with object\nocclusion and generalization across different instances and categories. This\npaper proposes a multimodal-based keypoint learning framework (MK-Pose) that\nintegrates RGB images, point clouds, and category-level textual descriptions.\nThe model uses a self-supervised keypoint detection module enhanced with\nattention-based query generation, soft heatmap matching and graph-based\nrelational modeling. Additionally, a graph-enhanced feature fusion module is\ndesigned to integrate local geometric information and global context. MK-Pose\nis evaluated on CAMERA25 and REAL275 dataset, and is further tested for\ncross-dataset capability on HouseCat6D dataset. The results demonstrate that\nMK-Pose outperforms existing state-of-the-art methods in both IoU and average\nprecision without shape priors. Codes will be released at\n\\href{https://github.com/yangyifanYYF/MK-Pose}{https://github.com/yangyifanYYF/MK-Pose}."}
{"id": "2507.06671", "pdf": "https://arxiv.org/pdf/2507.06671", "abs": "https://arxiv.org/abs/2507.06671", "authors": ["Boyuan Tian", "Qizhe Gao", "Siran Xianyu", "Xiaotong Cui", "Minjia Zhang"], "title": "FlexGaussian: Flexible and Cost-Effective Training-Free Compression for 3D Gaussian Splatting", "categories": ["cs.CV"], "comment": "To appear at ACM MM 2025", "summary": "3D Gaussian splatting has become a prominent technique for representing and\nrendering complex 3D scenes, due to its high fidelity and speed advantages.\nHowever, the growing demand for large-scale models calls for effective\ncompression to reduce memory and computation costs, especially on mobile and\nedge devices with limited resources. Existing compression methods effectively\nreduce 3D Gaussian parameters but often require extensive retraining or\nfine-tuning, lacking flexibility under varying compression constraints.\n  In this paper, we introduce FlexGaussian, a flexible and cost-effective\nmethod that combines mixed-precision quantization with attribute-discriminative\npruning for training-free 3D Gaussian compression. FlexGaussian eliminates the\nneed for retraining and adapts easily to diverse compression targets.\nEvaluation results show that FlexGaussian achieves up to 96.4% compression\nwhile maintaining high rendering quality (<1 dB drop in PSNR), and is\ndeployable on mobile devices. FlexGaussian delivers high compression ratios\nwithin seconds, being 1.7-2.1x faster than state-of-the-art training-free\nmethods and 10-100x faster than training-involved approaches. The code is being\nprepared and will be released soon at:\nhttps://github.com/Supercomputing-System-AI-Lab/FlexGaussian"}
{"id": "2507.06679", "pdf": "https://arxiv.org/pdf/2507.06679", "abs": "https://arxiv.org/abs/2507.06679", "authors": ["Miaojing Shi", "Xiaowen Zhang", "Zijie Yue", "Yong Luo", "Cairong Zhao", "Li Li"], "title": "Text-promptable Object Counting via Quantity Awareness Enhancement", "categories": ["cs.CV"], "comment": "13 pages, 5 figures", "summary": "Recent advances in large vision-language models (VLMs) have shown remarkable\nprogress in solving the text-promptable object counting problem. Representative\nmethods typically specify text prompts with object category information in\nimages. This however is insufficient for training the model to accurately\ndistinguish the number of objects in the counting task. To this end, we propose\nQUANet, which introduces novel quantity-oriented text prompts with a\nvision-text quantity alignment loss to enhance the model's quantity awareness.\nMoreover, we propose a dual-stream adaptive counting decoder consisting of a\nTransformer stream, a CNN stream, and a number of Transformer-to-CNN\nenhancement adapters (T2C-adapters) for density map prediction. The\nT2C-adapters facilitate the effective knowledge communication and aggregation\nbetween the Transformer and CNN streams. A cross-stream quantity ranking loss\nis proposed in the end to optimize the ranking orders of predictions from the\ntwo streams. Extensive experiments on standard benchmarks such as FSC-147,\nCARPK, PUCPR+, and ShanghaiTech demonstrate our model's strong generalizability\nfor zero-shot class-agnostic counting. Code is available at\nhttps://github.com/viscom-tongji/QUANet"}
{"id": "2507.06687", "pdf": "https://arxiv.org/pdf/2507.06687", "abs": "https://arxiv.org/abs/2507.06687", "authors": ["Marcel Vosshans", "Omar Ait-Aider", "Youcef Mezouar", "Markus Enzweiler"], "title": "StixelNExT++: Lightweight Monocular Scene Segmentation and Representation for Collective Perception", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "This paper presents StixelNExT++, a novel approach to scene representation\nfor monocular perception systems. Building on the established Stixel\nrepresentation, our method infers 3D Stixels and enhances object segmentation\nby clustering smaller 3D Stixel units. The approach achieves high compression\nof scene information while remaining adaptable to point cloud and\nbird's-eye-view representations. Our lightweight neural network, trained on\nautomatically generated LiDAR-based ground truth, achieves real-time\nperformance with computation times as low as 10 ms per frame. Experimental\nresults on the Waymo dataset demonstrate competitive performance within a\n30-meter range, highlighting the potential of StixelNExT++ for collective\nperception in autonomous systems."}
{"id": "2507.06689", "pdf": "https://arxiv.org/pdf/2507.06689", "abs": "https://arxiv.org/abs/2507.06689", "authors": ["Hao Tang", "Ling Shao", "Zhenyu Zhang", "Luc Van Gool", "Nicu Sebe"], "title": "Spatial-Temporal Graph Mamba for Music-Guided Dance Video Synthesis", "categories": ["cs.CV"], "comment": "Accepted to TPAMI 2025", "summary": "We propose a novel spatial-temporal graph Mamba (STG-Mamba) for the\nmusic-guided dance video synthesis task, i.e., to translate the input music to\na dance video. STG-Mamba consists of two translation mappings:\nmusic-to-skeleton translation and skeleton-to-video translation. In the\nmusic-to-skeleton translation, we introduce a novel spatial-temporal graph\nMamba (STGM) block to effectively construct skeleton sequences from the input\nmusic, capturing dependencies between joints in both the spatial and temporal\ndimensions. For the skeleton-to-video translation, we propose a novel\nself-supervised regularization network to translate the generated skeletons,\nalong with a conditional image, into a dance video. Lastly, we collect a new\nskeleton-to-video translation dataset from the Internet, containing 54,944\nvideo clips. Extensive experiments demonstrate that STG-Mamba achieves\nsignificantly better results than existing methods."}
{"id": "2507.06719", "pdf": "https://arxiv.org/pdf/2507.06719", "abs": "https://arxiv.org/abs/2507.06719", "authors": ["Zhenyang Liu", "Sixiao Zheng", "Siyu Chen", "Cairong Zhao", "Longfei Liang", "Xiangyang Xue", "Yanwei Fu"], "title": "A Neural Representation Framework with LLM-Driven Spatial Reasoning for Open-Vocabulary 3D Visual Grounding", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Open-vocabulary 3D visual grounding aims to localize target objects based on\nfree-form language queries, which is crucial for embodied AI applications such\nas autonomous navigation, robotics, and augmented reality. Learning 3D language\nfields through neural representations enables accurate understanding of 3D\nscenes from limited viewpoints and facilitates the localization of target\nobjects in complex environments. However, existing language field methods\nstruggle to accurately localize instances using spatial relations in language\nqueries, such as ``the book on the chair.'' This limitation mainly arises from\ninadequate reasoning about spatial relations in both language queries and 3D\nscenes. In this work, we propose SpatialReasoner, a novel neural\nrepresentation-based framework with large language model (LLM)-driven spatial\nreasoning that constructs a visual properties-enhanced hierarchical feature\nfield for open-vocabulary 3D visual grounding. To enable spatial reasoning in\nlanguage queries, SpatialReasoner fine-tunes an LLM to capture spatial\nrelations and explicitly infer instructions for the target, anchor, and spatial\nrelation. To enable spatial reasoning in 3D scenes, SpatialReasoner\nincorporates visual properties (opacity and color) to construct a hierarchical\nfeature field. This field represents language and instance features using\ndistilled CLIP features and masks extracted via the Segment Anything Model\n(SAM). The field is then queried using the inferred instructions in a\nhierarchical manner to localize the target 3D instance based on the spatial\nrelation in the language query. Extensive experiments show that our framework\ncan be seamlessly integrated into different neural representations,\noutperforming baseline models in 3D visual grounding while empowering their\nspatial reasoning capability."}
{"id": "2507.06732", "pdf": "https://arxiv.org/pdf/2507.06732", "abs": "https://arxiv.org/abs/2507.06732", "authors": ["Sobhan Asasi", "Mohamed Ilyes Lakhal", "Richard Bowden"], "title": "Hierarchical Feature Alignment for Gloss-Free Sign Language Translation", "categories": ["cs.CV"], "comment": "Accepted in SLTAT", "summary": "Sign Language Translation (SLT) attempts to convert sign language videos into\nspoken sentences. However, many existing methods struggle with the disparity\nbetween visual and textual representations during end-to-end learning.\nGloss-based approaches help to bridge this gap by leveraging structured\nlinguistic information. While, gloss-free methods offer greater flexibility and\nremove the burden of annotation, they require effective alignment strategies.\nRecent advances in Large Language Models (LLMs) have enabled gloss-free SLT by\ngenerating text-like representations from sign videos. In this work, we\nintroduce a novel hierarchical pre-training strategy inspired by the structure\nof sign language, incorporating pseudo-glosses and contrastive video-language\nalignment. Our method hierarchically extracts features at frame, segment, and\nvideo levels, aligning them with pseudo-glosses and the spoken sentence to\nenhance translation quality. Experiments demonstrate that our approach improves\nBLEU-4 and ROUGE scores while maintaining efficiency."}
{"id": "2507.06733", "pdf": "https://arxiv.org/pdf/2507.06733", "abs": "https://arxiv.org/abs/2507.06733", "authors": ["Mahshid Shiri", "Cigdem Beyan", "Vittorio Murino"], "title": "MADPOT: Medical Anomaly Detection with CLIP Adaptation and Partial Optimal Transport", "categories": ["cs.CV"], "comment": "Accepted to ICIAP 2025 (this version is not peer-reviewed; it is the\n  submitted version). ICIAP 2025 proceedings DOI will appear here", "summary": "Medical anomaly detection (AD) is challenging due to diverse imaging\nmodalities, anatomical variations, and limited labeled data. We propose a novel\napproach combining visual adapters and prompt learning with Partial Optimal\nTransport (POT) and contrastive learning (CL) to improve CLIP's adaptability to\nmedical images, particularly for AD. Unlike standard prompt learning, which\noften yields a single representation, our method employs multiple prompts\naligned with local features via POT to capture subtle abnormalities. CL further\nenforces intra-class cohesion and inter-class separation. Our method achieves\nstate-of-the-art results in few-shot, zero-shot, and cross-dataset scenarios\nwithout synthetic data or memory banks. The code is available at\nhttps://github.com/mahshid1998/MADPOT."}
{"id": "2507.06735", "pdf": "https://arxiv.org/pdf/2507.06735", "abs": "https://arxiv.org/abs/2507.06735", "authors": ["Guan Zheng", "Xue Wang", "Wenhua Qian", "Peng Liu", "Runzhuo Ma"], "title": "Residual Prior-driven Frequency-aware Network for Image Fusion", "categories": ["cs.CV", "cs.LG", "cs.MM"], "comment": null, "summary": "Image fusion aims to integrate complementary information across modalities to\ngenerate high-quality fused images, thereby enhancing the performance of\nhigh-level vision tasks. While global spatial modeling mechanisms show\npromising results, constructing long-range feature dependencies in the spatial\ndomain incurs substantial computational costs. Additionally, the absence of\nground-truth exacerbates the difficulty of capturing complementary features\neffectively. To tackle these challenges, we propose a Residual Prior-driven\nFrequency-aware Network, termed as RPFNet. Specifically, RPFNet employs a\ndual-branch feature extraction framework: the Residual Prior Module (RPM)\nextracts modality-specific difference information from residual maps, thereby\nproviding complementary priors for fusion; the Frequency Domain Fusion Module\n(FDFM) achieves efficient global feature modeling and integration through\nfrequency-domain convolution. Additionally, the Cross Promotion Module (CPM)\nenhances the synergistic perception of local details and global structures\nthrough bidirectional feature interaction. During training, we incorporate an\nauxiliary decoder and saliency structure loss to strengthen the model's\nsensitivity to modality-specific differences. Furthermore, a combination of\nadaptive weight-based frequency contrastive loss and SSIM loss effectively\nconstrains the solution space, facilitating the joint capture of local details\nand global features while ensuring the retention of complementary information.\nExtensive experiments validate the fusion performance of RPFNet, which\neffectively integrates discriminative features, enhances texture details and\nsalient objects, and can effectively facilitate the deployment of the\nhigh-level vision task."}
{"id": "2507.06738", "pdf": "https://arxiv.org/pdf/2507.06738", "abs": "https://arxiv.org/abs/2507.06738", "authors": ["Xinyu Xie", "Weifeng Cao", "Jun Shi", "Yangyang Hu", "Hui Liang", "Wanyong Liang", "Xiaoliang Qian"], "title": "DIFFUMA: High-Fidelity Spatio-Temporal Video Prediction via Dual-Path Mamba and Diffusion Enhancement", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Spatio-temporal video prediction plays a pivotal role in critical domains,\nranging from weather forecasting to industrial automation. However, in\nhigh-precision industrial scenarios such as semiconductor manufacturing, the\nabsence of specialized benchmark datasets severely hampers research on modeling\nand predicting complex processes. To address this challenge, we make a twofold\ncontribution.First, we construct and release the Chip Dicing Lane Dataset\n(CHDL), the first public temporal image dataset dedicated to the semiconductor\nwafer dicing process. Captured via an industrial-grade vision system, CHDL\nprovides a much-needed and challenging benchmark for high-fidelity process\nmodeling, defect detection, and digital twin development.Second, we propose\nDIFFUMA, an innovative dual-path prediction architecture specifically designed\nfor such fine-grained dynamics. The model captures global long-range temporal\ncontext through a parallel Mamba module, while simultaneously leveraging a\ndiffusion module, guided by temporal features, to restore and enhance\nfine-grained spatial details, effectively combating feature degradation.\nExperiments demonstrate that on our CHDL benchmark, DIFFUMA significantly\noutperforms existing methods, reducing the Mean Squared Error (MSE) by 39% and\nimproving the Structural Similarity (SSIM) from 0.926 to a near-perfect 0.988.\nThis superior performance also generalizes to natural phenomena datasets. Our\nwork not only delivers a new state-of-the-art (SOTA) model but, more\nimportantly, provides the community with an invaluable data resource to drive\nfuture research in industrial AI."}
{"id": "2507.06739", "pdf": "https://arxiv.org/pdf/2507.06739", "abs": "https://arxiv.org/abs/2507.06739", "authors": ["Zishen Huang", "Chunyu Yang", "Mengyuan Ren"], "title": "PromptTea: Let Prompts Tell TeaCache the Optimal Threshold", "categories": ["cs.CV"], "comment": null, "summary": "Despite recent progress in video generation, inference speed remains a major\nbottleneck. A common acceleration strategy involves reusing model outputs via\ncaching mechanisms at fixed intervals. However, we find that such\nfixed-frequency reuse significantly degrades quality in complex scenes, while\nmanually tuning reuse thresholds is inefficient and lacks robustness. To\naddress this, we propose Prompt-Complexity-Aware (PCA) caching, a method that\nautomatically adjusts reuse thresholds based on scene complexity estimated\ndirectly from the input prompt. By incorporating prompt-derived semantic cues,\nPCA enables more adaptive and informed reuse decisions than conventional\ncaching methods. We also revisit the assumptions behind TeaCache and identify a\nkey limitation: it suffers from poor input-output relationship modeling due to\nan oversimplified prior. To overcome this, we decouple the noisy input, enhance\nthe contribution of meaningful textual information, and improve the model's\npredictive accuracy through multivariate polynomial feature expansion. To\nfurther reduce computational cost, we replace the static CFGCache with\nDynCFGCache, a dynamic mechanism that selectively reuses classifier-free\nguidance (CFG) outputs based on estimated output variations. This allows for\nmore flexible reuse without compromising output quality. Extensive experiments\ndemonstrate that our approach achieves significant acceleration-for example,\n2.79x speedup on the Wan2.1 model-while maintaining high visual fidelity across\na range of scenes."}
{"id": "2507.06744", "pdf": "https://arxiv.org/pdf/2507.06744", "abs": "https://arxiv.org/abs/2507.06744", "authors": ["Yafei Zhang", "Yongle Shang", "Huafeng Li"], "title": "Dual-Granularity Cross-Modal Identity Association for Weakly-Supervised Text-to-Person Image Matching", "categories": ["cs.CV", "cs.LG", "cs.MM"], "comment": null, "summary": "Weakly supervised text-to-person image matching, as a crucial approach to\nreducing models' reliance on large-scale manually labeled samples, holds\nsignificant research value. However, existing methods struggle to predict\ncomplex one-to-many identity relationships, severely limiting performance\nimprovements. To address this challenge, we propose a local-and-global\ndual-granularity identity association mechanism. Specifically, at the local\nlevel, we explicitly establish cross-modal identity relationships within a\nbatch, reinforcing identity constraints across different modalities and\nenabling the model to better capture subtle differences and correlations. At\nthe global level, we construct a dynamic cross-modal identity association\nnetwork with the visual modality as the anchor and introduce a confidence-based\ndynamic adjustment mechanism, effectively enhancing the model's ability to\nidentify weakly associated samples while improving overall sensitivity.\nAdditionally, we propose an information-asymmetric sample pair construction\nmethod combined with consistency learning to tackle hard sample mining and\nenhance model robustness. Experimental results demonstrate that the proposed\nmethod substantially boosts cross-modal matching accuracy, providing an\nefficient and practical solution for text-to-person image matching."}
{"id": "2507.06761", "pdf": "https://arxiv.org/pdf/2507.06761", "abs": "https://arxiv.org/abs/2507.06761", "authors": ["Yan Hon Michael Chung", "Donghyeok Choi"], "title": "Finetuning Vision-Language Models as OCR Systems for Low-Resource Languages: A Case Study of Manchu", "categories": ["cs.CV"], "comment": null, "summary": "Manchu, a critically endangered language essential for understanding early\nmodern Eastern Eurasian history, lacks effective OCR systems that can handle\nreal-world historical documents. This study develops high-performing OCR\nsystems by fine-tuning three open-source vision-language models (LLaMA-3.2-11B,\nQwen2.5-VL-7B, Qwen2.5-VL-3B) on 60,000 synthetic Manchu word images using\nparameter-efficient training. LLaMA-3.2-11B achieved exceptional performance\nwith 98.3\\% word accuracy and 0.0024 character error rate on synthetic data,\nwhile crucially maintaining 93.1\\% accuracy on real-world handwritten\ndocuments. Comparative evaluation reveals substantial advantages over\ntraditional approaches: while a CRNN baseline achieved 99.8\\% synthetic\naccuracy, it suffered severe degradation to 72.5\\% on real documents. Our\napproach demonstrates effective synthetic-to-real domain transfer, providing a\ncost-effective solution deployable on accessible infrastructure. This work\nestablishes a transferable framework for endangered language OCR that removes\ntechnical and financial barriers in digital humanities, enabling historians and\nlinguists to process historical archives without specialized computing\nresources. Code and model weights are available at\nhttps://github.com/mic7ch1/ManchuAI-OCR."}
{"id": "2507.06763", "pdf": "https://arxiv.org/pdf/2507.06763", "abs": "https://arxiv.org/abs/2507.06763", "authors": ["Saif Ur Rehman Khan", "Muhammad Nabeel Asim", "Sebastian Vollmer", "Andreas Dengel"], "title": "FOLC-Net: A Federated-Optimized Lightweight Architecture for Enhanced MRI Disease Diagnosis across Axial, Coronal, and Sagittal Views", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The framework is designed to improve performance in the analysis of combined\nas well as single anatomical perspectives for MRI disease diagnosis. It\nspecifically addresses the performance degradation observed in state-of-the-art\n(SOTA) models, particularly when processing axial, coronal, and sagittal\nanatomical planes. The paper introduces the FOLC-Net framework, which\nincorporates a novel federated-optimized lightweight architecture with\napproximately 1.217 million parameters and a storage requirement of only 0.9\nMB. FOLC-Net integrates Manta-ray foraging optimization (MRFO) mechanisms for\nefficient model structure generation, global model cloning for scalable\ntraining, and ConvNeXt for enhanced client adaptability. The model was\nevaluated on combined multi-view data as well as individual views, such as\naxial, coronal, and sagittal, to assess its robustness in various medical\nimaging scenarios. Moreover, FOLC-Net tests a ShallowFed model on different\ndata to evaluate its ability to generalize beyond the training dataset. The\nresults show that FOLC-Net outperforms existing models, particularly in the\nchallenging sagittal view. For instance, FOLC-Net achieved an accuracy of\n92.44% on the sagittal view, significantly higher than the 88.37% accuracy of\nstudy method (DL + Residual Learning) and 88.95% of DL models. Additionally,\nFOLC-Net demonstrated improved accuracy across all individual views, providing\na more reliable and robust solution for medical image analysis in decentralized\nenvironments. FOLC-Net addresses the limitations of existing SOTA models by\nproviding a framework that ensures better adaptability to individual views\nwhile maintaining strong performance in multi-view settings. The incorporation\nof MRFO, global model cloning, and ConvNeXt ensures that FOLC-Net performs\nbetter in real-world medical applications."}
{"id": "2507.06797", "pdf": "https://arxiv.org/pdf/2507.06797", "abs": "https://arxiv.org/abs/2507.06797", "authors": ["Antonella Barisic Kulas", "Andreja Jurasovic", "Stjepan Bogdan"], "title": "Unlocking Thermal Aerial Imaging: Synthetic Enhancement of UAV Datasets", "categories": ["cs.CV"], "comment": "Preprint. Accepted at ECMR 2025", "summary": "Thermal imaging from unmanned aerial vehicles (UAVs) holds significant\npotential for applications in search and rescue, wildlife monitoring, and\nemergency response, especially under low-light or obscured conditions. However,\nthe scarcity of large-scale, diverse thermal aerial datasets limits the\nadvancement of deep learning models in this domain, primarily due to the high\ncost and logistical challenges of collecting thermal data. In this work, we\nintroduce a novel procedural pipeline for generating synthetic thermal images\nfrom an aerial perspective. Our method integrates arbitrary object classes into\nexisting thermal backgrounds by providing control over the position, scale, and\norientation of the new objects, while aligning them with the viewpoints of the\nbackground. We enhance existing thermal datasets by introducing new object\ncategories, specifically adding a drone class in urban environments to the\nHIT-UAV dataset and an animal category to the MONET dataset. In evaluating\nthese datasets for object detection task, we showcase strong performance across\nboth new and existing classes, validating the successful expansion into new\napplications. Through comparative analysis, we show that thermal detectors\noutperform their visible-light-trained counterparts and highlight the\nimportance of replicating aerial viewing angles. Project page:\nhttps://github.com/larics/thermal_aerial_synthetic."}
{"id": "2507.06806", "pdf": "https://arxiv.org/pdf/2507.06806", "abs": "https://arxiv.org/abs/2507.06806", "authors": ["Eya Cherif", "Arthur Ouaknine", "Luke A. Brown", "Phuong D. Dao", "Kyle R. Kovach", "Bing Lu", "Daniel Mederer", "Hannes Feilhauer", "Teja Kattenborn", "David Rolnick"], "title": "GreenHyperSpectra: A multi-source hyperspectral dataset for global vegetation trait prediction", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Plant traits such as leaf carbon content and leaf mass are essential\nvariables in the study of biodiversity and climate change. However,\nconventional field sampling cannot feasibly cover trait variation at\necologically meaningful spatial scales. Machine learning represents a valuable\nsolution for plant trait prediction across ecosystems, leveraging hyperspectral\ndata from remote sensing. Nevertheless, trait prediction from hyperspectral\ndata is challenged by label scarcity and substantial domain shifts (\\eg across\nsensors, ecological distributions), requiring robust cross-domain methods.\nHere, we present GreenHyperSpectra, a pretraining dataset encompassing\nreal-world cross-sensor and cross-ecosystem samples designed to benchmark trait\nprediction with semi- and self-supervised methods. We adopt an evaluation\nframework encompassing in-distribution and out-of-distribution scenarios. We\nsuccessfully leverage GreenHyperSpectra to pretrain label-efficient\nmulti-output regression models that outperform the state-of-the-art supervised\nbaseline. Our empirical analyses demonstrate substantial improvements in\nlearning spectral representations for trait prediction, establishing a\ncomprehensive methodological framework to catalyze research at the intersection\nof representation learning and plant functional traits assessment. All code and\ndata are available at: https://github.com/echerif18/HyspectraSSL."}
{"id": "2507.06812", "pdf": "https://arxiv.org/pdf/2507.06812", "abs": "https://arxiv.org/abs/2507.06812", "authors": ["Xu Yang", "Shaoli Huang", "Shenbo Xie", "Xuelin Chen", "Yifei Liu", "Changxing Ding"], "title": "Democratizing High-Fidelity Co-Speech Gesture Video Generation", "categories": ["cs.CV", "cs.AI"], "comment": "ICCV 2025", "summary": "Co-speech gesture video generation aims to synthesize realistic,\naudio-aligned videos of speakers, complete with synchronized facial expressions\nand body gestures. This task presents challenges due to the significant\none-to-many mapping between audio and visual content, further complicated by\nthe scarcity of large-scale public datasets and high computational demands. We\npropose a lightweight framework that utilizes 2D full-body skeletons as an\nefficient auxiliary condition to bridge audio signals with visual outputs. Our\napproach introduces a diffusion model conditioned on fine-grained audio\nsegments and a skeleton extracted from the speaker's reference image,\npredicting skeletal motions through skeleton-audio feature fusion to ensure\nstrict audio coordination and body shape consistency. The generated skeletons\nare then fed into an off-the-shelf human video generation model with the\nspeaker's reference image to synthesize high-fidelity videos. To democratize\nresearch, we present CSG-405-the first public dataset with 405 hours of\nhigh-resolution videos across 71 speech types, annotated with 2D skeletons and\ndiverse speaker demographics. Experiments show that our method exceeds\nstate-of-the-art approaches in visual quality and synchronization while\ngeneralizing across speakers and contexts."}
{"id": "2507.06814", "pdf": "https://arxiv.org/pdf/2507.06814", "abs": "https://arxiv.org/abs/2507.06814", "authors": ["Qingsen Yan", "Kangbiao Shi", "Yixu Feng", "Tao Hu", "Peng Wu", "Guansong Pang", "Yanning Zhang"], "title": "HVI-CIDNet+: Beyond Extreme Darkness for Low-Light Image Enhancement", "categories": ["cs.CV"], "comment": "14 pages", "summary": "Low-Light Image Enhancement (LLIE) aims to restore vivid content and details\nfrom corrupted low-light images. However, existing standard RGB (sRGB) color\nspace-based LLIE methods often produce color bias and brightness artifacts due\nto the inherent high color sensitivity. While Hue, Saturation, and Value (HSV)\ncolor space can decouple brightness and color, it introduces significant red\nand black noise artifacts. To address this problem, we propose a new color\nspace for LLIE, namely Horizontal/Vertical-Intensity (HVI), defined by the HV\ncolor map and learnable intensity. The HV color map enforces small distances\nfor the red coordinates to remove red noise artifacts, while the learnable\nintensity compresses the low-light regions to remove black noise artifacts.\nAdditionally, we introduce the Color and Intensity Decoupling Network+\n(HVI-CIDNet+), built upon the HVI color space, to restore damaged content and\nmitigate color distortion in extremely dark regions. Specifically, HVI-CIDNet+\nleverages abundant contextual and degraded knowledge extracted from low-light\nimages using pre-trained vision-language models, integrated via a novel\nPrior-guided Attention Block (PAB). Within the PAB, latent semantic priors can\npromote content restoration, while degraded representations guide precise color\ncorrection, both particularly in extremely dark regions through the\nmeticulously designed cross-attention fusion mechanism. Furthermore, we\nconstruct a Region Refinement Block that employs convolution for\ninformation-rich regions and self-attention for information-scarce regions,\nensuring accurate brightness adjustments. Comprehensive results from benchmark\nexperiments demonstrate that the proposed HVI-CIDNet+ outperforms the\nstate-of-the-art methods on 10 datasets."}
{"id": "2507.06830", "pdf": "https://arxiv.org/pdf/2507.06830", "abs": "https://arxiv.org/abs/2507.06830", "authors": ["Tao Feng", "Xianbing Zhao", "Zhenhua Chen", "Tien Tsin Wong", "Hamid Rezatofighi", "Gholamreza Haffari", "Lizhen Qu"], "title": "Physics-Grounded Motion Forecasting via Equation Discovery for Trajectory-Guided Image-to-Video Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advances in diffusion-based and autoregressive video generation models\nhave achieved remarkable visual realism. However, these models typically lack\naccurate physical alignment, failing to replicate real-world dynamics in object\nmotion. This limitation arises primarily from their reliance on learned\nstatistical correlations rather than capturing mechanisms adhering to physical\nlaws. To address this issue, we introduce a novel framework that integrates\nsymbolic regression (SR) and trajectory-guided image-to-video (I2V) models for\nphysics-grounded video forecasting. Our approach extracts motion trajectories\nfrom input videos, uses a retrieval-based pre-training mechanism to enhance\nsymbolic regression, and discovers equations of motion to forecast physically\naccurate future trajectories. These trajectories then guide video generation\nwithout requiring fine-tuning of existing models. Evaluated on scenarios in\nClassical Mechanics, including spring-mass, pendulums, and projectile motions,\nour method successfully recovers ground-truth analytical equations and improves\nthe physical alignment of generated videos over baseline methods."}
{"id": "2507.06848", "pdf": "https://arxiv.org/pdf/2507.06848", "abs": "https://arxiv.org/abs/2507.06848", "authors": ["Joelle Hanna", "Damian Borth"], "title": "Know Your Attention Maps: Class-specific Token Masking for Weakly Supervised Semantic Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Weakly Supervised Semantic Segmentation (WSSS) is a challenging problem that\nhas been extensively studied in recent years. Traditional approaches often rely\non external modules like Class Activation Maps to highlight regions of interest\nand generate pseudo segmentation masks. In this work, we propose an end-to-end\nmethod that directly utilizes the attention maps learned by a Vision\nTransformer (ViT) for WSSS. We propose training a sparse ViT with multiple\n[CLS] tokens (one for each class), using a random masking strategy to promote\n[CLS] token - class assignment. At inference time, we aggregate the different\nself-attention maps of each [CLS] token corresponding to the predicted labels\nto generate pseudo segmentation masks. Our proposed approach enhances the\ninterpretability of self-attention maps and ensures accurate class assignments.\nExtensive experiments on two standard benchmarks and three specialized datasets\ndemonstrate that our method generates accurate pseudo-masks, outperforming\nrelated works. Those pseudo-masks can be used to train a segmentation model\nwhich achieves results comparable to fully-supervised models, significantly\nreducing the need for fine-grained labeled data."}
{"id": "2507.06856", "pdf": "https://arxiv.org/pdf/2507.06856", "abs": "https://arxiv.org/abs/2507.06856", "authors": ["Subrat Kishore Dutta", "Xiao Zhang"], "title": "IAP: Invisible Adversarial Patch Attack through Perceptibility-Aware Localization and Perturbation Optimization", "categories": ["cs.CV", "cs.AI"], "comment": "Published in ICCV 2025", "summary": "Despite modifying only a small localized input region, adversarial patches\ncan drastically change the prediction of computer vision models. However, prior\nmethods either cannot perform satisfactorily under targeted attack scenarios or\nfail to produce contextually coherent adversarial patches, causing them to be\neasily noticeable by human examiners and insufficiently stealthy against\nautomatic patch defenses. In this paper, we introduce IAP, a novel attack\nframework that generates highly invisible adversarial patches based on\nperceptibility-aware localization and perturbation optimization schemes.\nSpecifically, IAP first searches for a proper location to place the patch by\nleveraging classwise localization and sensitivity maps, balancing the\nsusceptibility of patch location to both victim model prediction and human\nvisual system, then employs a perceptibility-regularized adversarial loss and a\ngradient update rule that prioritizes color constancy for optimizing invisible\nperturbations. Comprehensive experiments across various image benchmarks and\nmodel architectures demonstrate that IAP consistently achieves competitive\nattack success rates in targeted settings with significantly improved patch\ninvisibility compared to existing baselines. In addition to being highly\nimperceptible to humans, IAP is shown to be stealthy enough to render several\nstate-of-the-art patch defenses ineffective."}
{"id": "2507.06858", "pdf": "https://arxiv.org/pdf/2507.06858", "abs": "https://arxiv.org/abs/2507.06858", "authors": ["Mathias Schulz", "Alexander Spenke", "Pia Funk", "Florian Blümel", "Markus Rohde", "Ralph Breithaupt", "Gerd Nolden", "Norbert Jung", "Robert Lange"], "title": "Longitudinal Study of Facial Biometrics at the BEZ: Temporal Variance Analysis", "categories": ["cs.CV"], "comment": "11 pages, 10 figures, 8 tables", "summary": "This study presents findings from long-term biometric evaluations conducted\nat the Biometric Evaluation Center (bez). Over the course of two and a half\nyears, our ongoing research with over 400 participants representing diverse\nethnicities, genders, and age groups were regularly assessed using a variety of\nbiometric tools and techniques at the controlled testing facilities. Our\nfindings are based on the General Data Protection Regulation-compliant local\nbez database with more than 238.000 biometric data sets categorized into\nmultiple biometric modalities such as face and finger. We used state-of-the-art\nface recognition algorithms to analyze long-term comparison scores. Our results\nshow that these scores fluctuate more significantly between individual days\nthan over the entire measurement period. These findings highlight the\nimportance of testing biometric characteristics of the same individuals over a\nlonger period of time in a controlled measurement environment and lays the\ngroundwork for future advancements in biometric data analysis."}
{"id": "2507.06906", "pdf": "https://arxiv.org/pdf/2507.06906", "abs": "https://arxiv.org/abs/2507.06906", "authors": ["Matthias Zeller", "Daniel Casado Herraez", "Bengisu Ayan", "Jens Behley", "Michael Heidingsfeld", "Cyrill Stachniss"], "title": "SemRaFiner: Panoptic Segmentation in Sparse and Noisy Radar Point Clouds", "categories": ["cs.CV"], "comment": "Accepted for publication in IEEE Robotics and Automation Letters\n  (RA-L)", "summary": "Semantic scene understanding, including the perception and classification of\nmoving agents, is essential to enabling safe and robust driving behaviours of\nautonomous vehicles. Cameras and LiDARs are commonly used for semantic scene\nunderstanding. However, both sensor modalities face limitations in adverse\nweather and usually do not provide motion information. Radar sensors overcome\nthese limitations and directly offer information about moving agents by\nmeasuring the Doppler velocity, but the measurements are comparably sparse and\nnoisy. In this paper, we address the problem of panoptic segmentation in sparse\nradar point clouds to enhance scene understanding. Our approach, called\nSemRaFiner, accounts for changing density in sparse radar point clouds and\noptimizes the feature extraction to improve accuracy. Furthermore, we propose\nan optimized training procedure to refine instance assignments by incorporating\na dedicated data augmentation. Our experiments suggest that our approach\noutperforms state-of-the-art methods for radar-based panoptic segmentation."}
{"id": "2507.06928", "pdf": "https://arxiv.org/pdf/2507.06928", "abs": "https://arxiv.org/abs/2507.06928", "authors": ["Qiyuan Dai", "Hanzhuo Huang", "Yu Wu", "Sibei Yang"], "title": "Adaptive Part Learning for Fine-Grained Generalized Category Discovery: A Plug-and-Play Enhancement", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Generalized Category Discovery (GCD) aims to recognize unlabeled images from\nknown and novel classes by distinguishing novel classes from known ones, while\nalso transferring knowledge from another set of labeled images with known\nclasses. Existing GCD methods rely on self-supervised vision transformers such\nas DINO for representation learning. However, focusing solely on the global\nrepresentation of the DINO CLS token introduces an inherent trade-off between\ndiscriminability and generalization. In this paper, we introduce an adaptive\npart discovery and learning method, called APL, which generates consistent\nobject parts and their correspondences across different similar images using a\nset of shared learnable part queries and DINO part priors, without requiring\nany additional annotations. More importantly, we propose a novel all-min\ncontrastive loss to learn discriminative yet generalizable part representation,\nwhich adaptively highlights discriminative object parts to distinguish similar\ncategories for enhanced discriminability while simultaneously sharing other\nparts to facilitate knowledge transfer for improved generalization. Our APL can\neasily be incorporated into different GCD frameworks by replacing their CLS\ntoken feature with our part representations, showing significant enhancements\non fine-grained datasets."}
{"id": "2507.06948", "pdf": "https://arxiv.org/pdf/2507.06948", "abs": "https://arxiv.org/abs/2507.06948", "authors": ["Yixin Zhao", "Yuyi Zhang", "Lianwen Jin"], "title": "MCCD: A Multi-Attribute Chinese Calligraphy Character Dataset Annotated with Script Styles, Dynasties, and Calligraphers", "categories": ["cs.CV"], "comment": "17 pages, 8 figures, 9 tables, accepted by the 19th International\n  Conference on Document Analysis and Recognition (ICDAR 2025)", "summary": "Research on the attribute information of calligraphy, such as styles,\ndynasties, and calligraphers, holds significant cultural and historical value.\nHowever, the styles of Chinese calligraphy characters have evolved dramatically\nthrough different dynasties and the unique touches of calligraphers, making it\nhighly challenging to accurately recognize these different characters and their\nattributes. Furthermore, existing calligraphic datasets are extremely scarce,\nand most provide only character-level annotations without additional attribute\ninformation. This limitation has significantly hindered the in-depth study of\nChinese calligraphy. To fill this gap, we present a novel Multi-Attribute\nChinese Calligraphy Character Dataset (MCCD). The dataset encompasses 7,765\ncategories with a total of 329,715 isolated image samples of Chinese\ncalligraphy characters, and three additional subsets were extracted based on\nthe attribute labeling of the three types of script styles (10 types),\ndynasties (15 periods) and calligraphers (142 individuals). The rich\nmulti-attribute annotations render MCCD well-suited diverse research tasks,\nincluding calligraphic character recognition, writer identification, and\nevolutionary studies of Chinese characters. We establish benchmark performance\nthrough single-task and multi-task recognition experiments across MCCD and all\nof its subsets. The experimental results demonstrate that the complexity of the\nstroke structure of the calligraphic characters, and the interplay between\ntheir different attributes, leading to a substantial increase in the difficulty\nof accurate recognition. MCCD not only fills a void in the availability of\ndetailed calligraphy datasets but also provides valuable resources for\nadvancing research in Chinese calligraphy and fostering advancements in\nmultiple fields. The dataset is available at\nhttps://github.com/SCUT-DLVCLab/MCCD."}
{"id": "2507.06949", "pdf": "https://arxiv.org/pdf/2507.06949", "abs": "https://arxiv.org/abs/2507.06949", "authors": ["Sebastian Fajardo", "Sina Mohammadi", "Jonas Gregorio de Souza", "César Ardila", "Alan Tapscott Baltar", "Shaddai Heidgen", "Maria Isabel Mayorga Hernández", "Sylvia Mota de Oliveira", "Fernando Montejo", "Marco Moderato", "Vinicius Peripato", "Katy Puche", "Carlos Reina", "Juan Carlos Vargas", "Frank W. Takes", "Marco Madella"], "title": "Pre-Columbian Settlements Shaped Palm Clusters in the Sierra Nevada de Santa Marta, Colombia", "categories": ["cs.CV"], "comment": null, "summary": "Ancient populations markedly transformed Neotropical forests, yet\nunderstanding the long-term effects of ancient human management, particularly\nat high-resolution scales, remains challenging. In this work we propose a new\napproach to investigate archaeological areas of influence based on vegetation\nsignatures. It consists of a deep learning model trained on satellite imagery\nto identify palm trees, followed by a clustering algorithm to identify palm\nclusters, which are then used to estimate ancient management areas. To assess\nthe palm distribution in relation to past human activity, we applied the\nproposed approach to unique high-resolution satellite imagery data covering 765\nkm2 of the Sierra Nevada de Santa Marta, Colombia. With this work, we also\nrelease a manually annotated palm tree dataset along with estimated locations\nof archaeological sites from ground-surveys and legacy records. Results\ndemonstrate how palms were significantly more abundant near archaeological\nsites showing large infrastructure investment. The extent of the largest palm\ncluster indicates that ancient human-managed areas linked to major\ninfrastructure sites may be up to two orders of magnitude bigger than indicated\nby archaeological evidence alone. Our findings suggest that pre-Columbian\npopulations influenced local vegetation fostering conditions conducive to palm\nproliferation, leaving a lasting ecological footprint. This may have lowered\nthe logistical costs of establishing infrastructure-heavy settlements in\notherwise less accessible locations. Overall, this study demonstrates the\npotential of integrating artificial intelligence approaches with new ecological\nand archaeological data to identify archaeological areas of interest through\nvegetation patterns, revealing fine-scale human-environment interactions."}
{"id": "2507.06959", "pdf": "https://arxiv.org/pdf/2507.06959", "abs": "https://arxiv.org/abs/2507.06959", "authors": ["Xiao Liang", "Jiawei Hu", "Di Wang", "Zhi Ma", "Lin Zhao", "Ronghan Li", "Bo Wan", "Quan Wang"], "title": "CheXPO: Preference Optimization for Chest X-ray VLMs with Counterfactual Rationale", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Vision-language models (VLMs) are prone to hallucinations that critically\ncompromise reliability in medical applications. While preference optimization\ncan mitigate these hallucinations through clinical feedback, its implementation\nfaces challenges such as clinically irrelevant training samples, imbalanced\ndata distributions, and prohibitive expert annotation costs. To address these\nchallenges, we introduce CheXPO, a Chest X-ray Preference Optimization strategy\nthat combines confidence-similarity joint mining with counterfactual rationale.\nOur approach begins by synthesizing a unified, fine-grained multi-task chest\nX-ray visual instruction dataset across different question types for supervised\nfine-tuning (SFT). We then identify hard examples through token-level\nconfidence analysis of SFT failures and use similarity-based retrieval to\nexpand hard examples for balancing preference sample distributions, while\nsynthetic counterfactual rationales provide fine-grained clinical preferences,\neliminating the need for additional expert input. Experiments show that CheXPO\nachieves 8.93% relative performance gain using only 5% of SFT samples, reaching\nstate-of-the-art performance across diverse clinical tasks and providing a\nscalable, interpretable solution for real-world radiology applications."}
{"id": "2507.06966", "pdf": "https://arxiv.org/pdf/2507.06966", "abs": "https://arxiv.org/abs/2507.06966", "authors": ["Sudharsan Madhavan", "Chengcheng Gui", "Lando Bosma", "Josiah Simeth", "Jue Jiang", "Nicolas Cote", "Nima Hassan Rezaeian", "Himanshu Nagar", "Victoria Brennan", "Neelam Tyagi", "Harini Veeraraghavan"], "title": "Segmentation Regularized Training for Multi-Domain Deep Learning Registration applied to MR-Guided Prostate Cancer Radiotherapy", "categories": ["cs.CV", "physics.med-ph"], "comment": "Preprint in preparation for submission", "summary": "Background: Accurate deformable image registration (DIR) is required for\ncontour propagation and dose accumulation in MR-guided adaptive radiotherapy\n(MRgART). This study trained and evaluated a deep learning DIR method for\ndomain invariant MR-MR registration. Methods: A progressively refined\nregistration and segmentation (ProRSeg) method was trained with 262 pairs of 3T\nMR simulation scans from prostate cancer patients using weighted segmentation\nconsistency loss. ProRSeg was tested on same- (58 pairs), cross- (72 1.5T MR\nLinac pairs), and mixed-domain (42 MRSim-MRL pairs) datasets for contour\npropagation accuracy of clinical target volume (CTV), bladder, and rectum. Dose\naccumulation was performed for 42 patients undergoing 5-fraction MRgART.\nResults: ProRSeg demonstrated generalization for bladder with similar Dice\nSimilarity Coefficients across domains (0.88, 0.87, 0.86). For rectum and CTV,\nperformance was domain-dependent with higher accuracy on cross-domain MRL\ndataset (DSCs 0.89) versus same-domain data. The model's strong cross-domain\nperformance prompted us to study the feasibility of using it for dose\naccumulation. Dose accumulation showed 83.3% of patients met CTV coverage (D95\n>= 40.0 Gy) and bladder sparing (D50 <= 20.0 Gy) constraints. All patients\nachieved minimum mean target dose (>40.4 Gy), but only 9.5% remained under\nupper limit (<42.0 Gy). Conclusions: ProRSeg showed reasonable multi-domain\nMR-MR registration performance for prostate cancer patients with preliminary\nfeasibility for evaluating treatment compliance to clinical constraints."}
{"id": "2507.06971", "pdf": "https://arxiv.org/pdf/2507.06971", "abs": "https://arxiv.org/abs/2507.06971", "authors": ["Fei Teng", "Kai Luo", "Sheng Wu", "Siyu Li", "Pujun Guo", "Jiale Wei", "Kunyu Peng", "Jiaming Zhang", "Kailun Yang"], "title": "Hallucinating 360°: Panoramic Street-View Generation via Local Scenes Diffusion and Probabilistic Prompting", "categories": ["cs.CV", "cs.RO", "eess.IV"], "comment": "The source code will be publicly available at\n  https://github.com/Bryant-Teng/Percep360", "summary": "Panoramic perception holds significant potential for autonomous driving,\nenabling vehicles to acquire a comprehensive 360{\\deg} surround view in a\nsingle shot. However, autonomous driving is a data-driven task. Complete\npanoramic data acquisition requires complex sampling systems and annotation\npipelines, which are time-consuming and labor-intensive. Although existing\nstreet view generation models have demonstrated strong data regeneration\ncapabilities, they can only learn from the fixed data distribution of existing\ndatasets and cannot achieve high-quality, controllable panoramic generation. In\nthis paper, we propose the first panoramic generation method Percep360 for\nautonomous driving. Percep360 enables coherent generation of panoramic data\nwith control signals based on the stitched panoramic data. Percep360 focuses on\ntwo key aspects: coherence and controllability. Specifically, to overcome the\ninherent information loss caused by the pinhole sampling process, we propose\nthe Local Scenes Diffusion Method (LSDM). LSDM reformulates the panorama\ngeneration as a spatially continuous diffusion process, bridging the gaps\nbetween different data distributions. Additionally, to achieve the controllable\ngeneration of panoramic images, we propose a Probabilistic Prompting Method\n(PPM). PPM dynamically selects the most relevant control cues, enabling\ncontrollable panoramic image generation. We evaluate the effectiveness of the\ngenerated images from three perspectives: image quality assessment (i.e.,\nno-reference and with reference), controllability, and their utility in\nreal-world Bird's Eye View (BEV) segmentation. Notably, the generated data\nconsistently outperforms the original stitched images in no-reference quality\nmetrics and enhances downstream perception models. The source code will be\npublicly available at https://github.com/Bryant-Teng/Percep360."}
{"id": "2507.06972", "pdf": "https://arxiv.org/pdf/2507.06972", "abs": "https://arxiv.org/abs/2507.06972", "authors": ["Johanna Orsholm", "John Quinto", "Hannu Autto", "Gaia Banelyte", "Nicolas Chazot", "Jeremy deWaard", "Stephanie deWaard", "Arielle Farrell", "Brendan Furneaux", "Bess Hardwick", "Nao Ito", "Amlan Kar", "Oula Kalttopää", "Deirdre Kerdraon", "Erik Kristensen", "Jaclyn McKeown", "Tommi Mononen", "Ellen Nein", "Hanna Rogers", "Tomas Roslin", "Paula Schmitz", "Jayme Sones", "Maija Sujala", "Amy Thompson", "Evgeny V. Zakharov", "Iuliia Zarubiieva", "Akshita Gupta", "Scott C. Lowe", "Graham W. Taylor"], "title": "A multi-modal dataset for insect biodiversity with imagery and DNA at the trap and individual level", "categories": ["cs.CV"], "comment": "13 pages, 6 figures, submitted to Scientific Data", "summary": "Insects comprise millions of species, many experiencing severe population\ndeclines under environmental and habitat changes. High-throughput approaches\nare crucial for accelerating our understanding of insect diversity, with DNA\nbarcoding and high-resolution imaging showing strong potential for automatic\ntaxonomic classification. However, most image-based approaches rely on\nindividual specimen data, unlike the unsorted bulk samples collected in\nlarge-scale ecological surveys. We present the Mixed Arthropod Sample\nSegmentation and Identification (MassID45) dataset for training automatic\nclassifiers of bulk insect samples. It uniquely combines molecular and imaging\ndata at both the unsorted sample level and the full set of individual\nspecimens. Human annotators, supported by an AI-assisted tool, performed two\ntasks on bulk images: creating segmentation masks around each individual\narthropod and assigning taxonomic labels to over 17 000 specimens. Combining\nthe taxonomic resolution of DNA barcodes with precise abundance estimates of\nbulk images holds great potential for rapid, large-scale characterization of\ninsect communities. This dataset pushes the boundaries of tiny object detection\nand instance segmentation, fostering innovation in both ecological and machine\nlearning research."}
{"id": "2507.06973", "pdf": "https://arxiv.org/pdf/2507.06973", "abs": "https://arxiv.org/abs/2507.06973", "authors": ["Qiyuan Dai", "Sibei Yang"], "title": "Free on the Fly: Enhancing Flexibility in Test-Time Adaptation with Online EM", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Vision-Language Models (VLMs) have become prominent in open-world image\nrecognition for their strong generalization abilities. Yet, their effectiveness\nin practical applications is compromised by domain shifts and distributional\nchanges, especially when test data distributions diverge from training data.\nTherefore, the paradigm of test-time adaptation (TTA) has emerged, enabling the\nuse of online off-the-shelf data at test time, supporting independent sample\npredictions, and eliminating reliance on test annotations. Traditional TTA\nmethods, however, often rely on costly training or optimization processes, or\nmake unrealistic assumptions about accessing or storing historical training and\ntest data. Instead, this study proposes FreeTTA, a training-free and\nuniversally available method that makes no assumptions, to enhance the\nflexibility of TTA. More importantly, FreeTTA is the first to explicitly model\nthe test data distribution, enabling the use of intrinsic relationships among\ntest samples to enhance predictions of individual samples without simultaneous\naccess--a direction not previously explored. FreeTTA achieves these advantages\nby introducing an online EM algorithm that utilizes zero-shot predictions from\nVLMs as priors to iteratively compute the posterior probabilities of each\nonline test sample and update parameters. Experiments demonstrate that FreeTTA\nachieves stable and significant improvements compared to state-of-the-art\nmethods across 15 datasets in both cross-domain and out-of-distribution\nsettings."}
{"id": "2507.06976", "pdf": "https://arxiv.org/pdf/2507.06976", "abs": "https://arxiv.org/abs/2507.06976", "authors": ["Sven Teufel", "Dominique Mayer", "Jörg Gamerdinger", "Oliver Bringmann"], "title": "DenoiseCP-Net: Efficient Collective Perception in Adverse Weather via Joint LiDAR-Based 3D Object Detection and Denoising", "categories": ["cs.CV"], "comment": null, "summary": "While automated vehicles hold the potential to significantly reduce traffic\naccidents, their perception systems remain vulnerable to sensor degradation\ncaused by adverse weather and environmental occlusions. Collective perception,\nwhich enables vehicles to share information, offers a promising approach to\novercoming these limitations. However, to this date collective perception in\nadverse weather is mostly unstudied. Therefore, we conduct the first study of\nLiDAR-based collective perception under diverse weather conditions and present\na novel multi-task architecture for LiDAR-based collective perception under\nadverse weather. Adverse weather conditions can not only degrade perception\ncapabilities, but also negatively affect bandwidth requirements and latency due\nto the introduced noise that is also transmitted and processed. Denoising prior\nto communication can effectively mitigate these issues. Therefore, we propose\nDenoiseCP-Net, a novel multi-task architecture for LiDAR-based collective\nperception under adverse weather conditions. DenoiseCP-Net integrates\nvoxel-level noise filtering and object detection into a unified sparse\nconvolution backbone, eliminating redundant computations associated with\ntwo-stage pipelines. This design not only reduces inference latency and\ncomputational cost but also minimizes communication overhead by removing\nnon-informative noise. We extended the well-known OPV2V dataset by simulating\nrain, snow, and fog using our realistic weather simulation models. We\ndemonstrate that DenoiseCP-Net achieves near-perfect denoising accuracy in\nadverse weather, reduces the bandwidth requirements by up to 23.6% while\nmaintaining the same detection accuracy and reducing the inference latency for\ncooperative vehicles."}
{"id": "2507.06992", "pdf": "https://arxiv.org/pdf/2507.06992", "abs": "https://arxiv.org/abs/2507.06992", "authors": ["Qilong Xing", "Zikai Song", "Youjia Zhang", "Na Feng", "Junqing Yu", "Wei Yang"], "title": "MCA-RG: Enhancing LLMs with Medical Concept Alignment for Radiology Report Generation", "categories": ["cs.CV", "cs.AI"], "comment": "MICCAI 2025", "summary": "Despite significant advancements in adapting Large Language Models (LLMs) for\nradiology report generation (RRG), clinical adoption remains challenging due to\ndifficulties in accurately mapping pathological and anatomical features to\ntheir corresponding text descriptions. Additionally, semantic agnostic feature\nextraction further hampers the generation of accurate diagnostic reports. To\naddress these challenges, we introduce Medical Concept Aligned Radiology Report\nGeneration (MCA-RG), a knowledge-driven framework that explicitly aligns visual\nfeatures with distinct medical concepts to enhance the report generation\nprocess. MCA-RG utilizes two curated concept banks: a pathology bank containing\nlesion-related knowledge, and an anatomy bank with anatomical descriptions. The\nvisual features are aligned with these medical concepts and undergo tailored\nenhancement. We further propose an anatomy-based contrastive learning procedure\nto improve the generalization of anatomical features, coupled with a matching\nloss for pathological features to prioritize clinically relevant regions.\nAdditionally, a feature gating mechanism is employed to filter out low-quality\nconcept features. Finally, the visual features are corresponding to individual\nmedical concepts, and are leveraged to guide the report generation process.\nExperiments on two public benchmarks (MIMIC-CXR and CheXpert Plus) demonstrate\nthat MCA-RG achieves superior performance, highlighting its effectiveness in\nradiology report generation."}
{"id": "2507.06994", "pdf": "https://arxiv.org/pdf/2507.06994", "abs": "https://arxiv.org/abs/2507.06994", "authors": ["Qilong Xing", "Zikai Song", "Bingxin Gong", "Lian Yang", "Junqing Yu", "Wei Yang"], "title": "Cross-Modality Masked Learning for Survival Prediction in ICI Treated NSCLC Patients", "categories": ["cs.CV", "cs.AI"], "comment": "MICCAI 2025", "summary": "Accurate prognosis of non-small cell lung cancer (NSCLC) patients undergoing\nimmunotherapy is essential for personalized treatment planning, enabling\ninformed patient decisions, and improving both treatment outcomes and quality\nof life. However, the lack of large, relevant datasets and effective\nmulti-modal feature fusion strategies pose significant challenges in this\ndomain. To address these challenges, we present a large-scale dataset and\nintroduce a novel framework for multi-modal feature fusion aimed at enhancing\nthe accuracy of survival prediction. The dataset comprises 3D CT images and\ncorresponding clinical records from NSCLC patients treated with immune\ncheckpoint inhibitors (ICI), along with progression-free survival (PFS) and\noverall survival (OS) data. We further propose a cross-modality masked learning\napproach for medical feature fusion, consisting of two distinct branches, each\ntailored to its respective modality: a Slice-Depth Transformer for extracting\n3D features from CT images and a graph-based Transformer for learning node\nfeatures and relationships among clinical variables in tabular data. The fusion\nprocess is guided by a masked modality learning strategy, wherein the model\nutilizes the intact modality to reconstruct missing components. This mechanism\nimproves the integration of modality-specific features, fostering more\neffective inter-modality relationships and feature interactions. Our approach\ndemonstrates superior performance in multi-modal integration for NSCLC survival\nprediction, surpassing existing methods and setting a new benchmark for\nprognostic models in this context."}
{"id": "2507.06999", "pdf": "https://arxiv.org/pdf/2507.06999", "abs": "https://arxiv.org/abs/2507.06999", "authors": ["Yahan Yu", "Yuyang Dong", "Masafumi Oyamada"], "title": "Learning Deliberately, Acting Intuitively: Unlocking Test-Time Reasoning in Multimodal LLMs", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": "Work in progress", "summary": "Reasoning is a key capability for large language models (LLMs), particularly\nwhen applied to complex tasks such as mathematical problem solving. However,\nmultimodal reasoning research still requires further exploration of modality\nalignment and training costs. Many of these approaches rely on additional data\nannotation and relevant rule-based rewards to enhance the understanding and\nreasoning ability, which significantly increases training costs and limits\nscalability. To address these challenges, we propose the\nDeliberate-to-Intuitive reasoning framework (D2I) that improves the\nunderstanding and reasoning ability of multimodal LLMs (MLLMs) without extra\nannotations and complex rewards. Specifically, our method sets deliberate\nreasoning strategies to enhance modality alignment only through the rule-based\nformat reward during training. While evaluating, the reasoning style shifts to\nintuitive, which removes deliberate reasoning strategies during training and\nimplicitly reflects the model's acquired abilities in the response. D2I\noutperforms baselines across both in-domain and out-of-domain benchmarks. Our\nfindings highlight the role of format reward in fostering transferable\nreasoning skills in MLLMs, and inspire directions for decoupling training-time\nreasoning depth from test-time response flexibility."}
{"id": "2507.07006", "pdf": "https://arxiv.org/pdf/2507.07006", "abs": "https://arxiv.org/abs/2507.07006", "authors": ["S M Taslim Uddin Raju", "Md. Milon Islam", "Md Rezwanul Haque", "Hamdi Altaheri", "Fakhri Karray"], "title": "GNN-ViTCap: GNN-Enhanced Multiple Instance Learning with Vision Transformers for Whole Slide Image Classification and Captioning", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Microscopic assessment of histopathology images is vital for accurate cancer\ndiagnosis and treatment. Whole Slide Image (WSI) classification and captioning\nhave become crucial tasks in computer-aided pathology. However, microscopic WSI\nface challenges such as redundant patches and unknown patch positions due to\nsubjective pathologist captures. Moreover, generating automatic pathology\ncaptions remains a significant challenge. To address these issues, we introduce\na novel GNN-ViTCap framework for classification and caption generation from\nhistopathological microscopic images. First, a visual feature extractor\ngenerates patch embeddings. Redundant patches are then removed by dynamically\nclustering these embeddings using deep embedded clustering and selecting\nrepresentative patches via a scalar dot attention mechanism. We build a graph\nby connecting each node to its nearest neighbors in the similarity matrix and\napply a graph neural network to capture both local and global context. The\naggregated image embeddings are projected into the language model's input space\nthrough a linear layer and combined with caption tokens to fine-tune a large\nlanguage model. We validate our method on the BreakHis and PatchGastric\ndatasets. GNN-ViTCap achieves an F1 score of 0.934 and an AUC of 0.963 for\nclassification, along with a BLEU-4 score of 0.811 and a METEOR score of 0.569\nfor captioning. Experimental results demonstrate that GNN-ViTCap outperforms\nstate of the art approaches, offering a reliable and efficient solution for\nmicroscopy based patient diagnosis."}
{"id": "2507.07013", "pdf": "https://arxiv.org/pdf/2507.07013", "abs": "https://arxiv.org/abs/2507.07013", "authors": ["Yutong Sun", "Sichen Zhu", "Peng Qiu"], "title": "Integrating Pathology Foundation Models and Spatial Transcriptomics for Cellular Decomposition from Histology Images", "categories": ["cs.CV"], "comment": null, "summary": "The rapid development of digital pathology and modern deep learning has\nfacilitated the emergence of pathology foundation models that are expected to\nsolve general pathology problems under various disease conditions in one\nunified model, with or without fine-tuning. In parallel, spatial\ntranscriptomics has emerged as a transformative technology that enables the\nprofiling of gene expression on hematoxylin and eosin (H&E) stained histology\nimages. Spatial transcriptomics unlocks the unprecedented opportunity to dive\ninto existing histology images at a more granular, cellular level. In this\nwork, we propose a lightweight and training-efficient approach to predict\ncellular composition directly from H&E-stained histology images by leveraging\ninformation-enriched feature embeddings extracted from pre-trained pathology\nfoundation models. By training a lightweight multi-layer perceptron (MLP)\nregressor on cell-type abundances derived via cell2location, our method\nefficiently distills knowledge from pathology foundation models and\ndemonstrates the ability to accurately predict cell-type compositions from\nhistology images, without physically performing the costly spatial\ntranscriptomics. Our method demonstrates competitive performance compared to\nexisting methods such as Hist2Cell, while significantly reducing computational\ncomplexity."}
{"id": "2507.07015", "pdf": "https://arxiv.org/pdf/2507.07015", "abs": "https://arxiv.org/abs/2507.07015", "authors": ["Hui Li", "Pengfei Yang", "Juanyang Chen", "Le Dong", "Yanxin Chen", "Quan Wang"], "title": "MST-Distill: Mixture of Specialized Teachers for Cross-Modal Knowledge Distillation", "categories": ["cs.CV", "cs.LG", "cs.MM"], "comment": "Accepted to ACM MM 2025 (The 33rd ACM International Conference on\n  Multimedia)", "summary": "Knowledge distillation as an efficient knowledge transfer technique, has\nachieved remarkable success in unimodal scenarios. However, in cross-modal\nsettings, conventional distillation methods encounter significant challenges\ndue to data and statistical heterogeneities, failing to leverage the\ncomplementary prior knowledge embedded in cross-modal teacher models. This\npaper empirically reveals two critical issues in existing approaches:\ndistillation path selection and knowledge drift. To address these limitations,\nwe propose MST-Distill, a novel cross-modal knowledge distillation framework\nfeaturing a mixture of specialized teachers. Our approach employs a diverse\nensemble of teacher models across both cross-modal and multimodal\nconfigurations, integrated with an instance-level routing network that\nfacilitates adaptive and dynamic distillation. This architecture effectively\ntranscends the constraints of traditional methods that rely on monotonous and\nstatic teacher models. Additionally, we introduce a plug-in masking module,\nindependently trained to suppress modality-specific discrepancies and\nreconstruct teacher representations, thereby mitigating knowledge drift and\nenhancing transfer effectiveness. Extensive experiments across five diverse\nmultimodal datasets, spanning visual, audio, and text, demonstrate that our\nmethod significantly outperforms existing state-of-the-art knowledge\ndistillation methods in cross-modal distillation tasks. The source code is\navailable at https://github.com/Gray-OREO/MST-Distill."}
{"id": "2507.07029", "pdf": "https://arxiv.org/pdf/2507.07029", "abs": "https://arxiv.org/abs/2507.07029", "authors": ["Parshva Dhilankumar Patel"], "title": "Design and Implementation of an OCR-Powered Pipeline for Table Extraction from Invoices", "categories": ["cs.CV", "cs.AI", "I.2.10; I.4.9; H.3.1"], "comment": "17 pages, 23 figures, submitted to arXiv in July 2025", "summary": "This paper presents the design and development of an OCR-powered pipeline for\nefficient table extraction from invoices. The system leverages Tesseract OCR\nfor text recognition and custom post-processing logic to detect, align, and\nextract structured tabular data from scanned invoice documents. Our approach\nincludes dynamic preprocessing, table boundary detection, and row-column\nmapping, optimized for noisy and non-standard invoice formats. The resulting\npipeline significantly improves data extraction accuracy and consistency,\nsupporting real-world use cases such as automated financial workflows and\ndigital archiving."}
{"id": "2507.07048", "pdf": "https://arxiv.org/pdf/2507.07048", "abs": "https://arxiv.org/abs/2507.07048", "authors": ["Bruce Coburn", "Jiangpeng He", "Megan E. Rollo", "Satvinder S. Dhaliwal", "Deborah A. Kerr", "Fengqing Zhu"], "title": "Evaluating Large Multimodal Models for Nutrition Analysis: A Benchmark Enriched with Contextual Metadata", "categories": ["cs.CV"], "comment": null, "summary": "Large Multimodal Models (LMMs) are increasingly applied to meal images for\nnutrition analysis. However, existing work primarily evaluates proprietary\nmodels, such as GPT-4. This leaves the broad range of LLMs underexplored.\nAdditionally, the influence of integrating contextual metadata and its\ninteraction with various reasoning modifiers remains largely uncharted. This\nwork investigates how interpreting contextual metadata derived from GPS\ncoordinates (converted to location/venue type), timestamps (transformed into\nmeal/day type), and the food items present can enhance LMM performance in\nestimating key nutritional values. These values include calories,\nmacronutrients (protein, carbohydrates, fat), and portion sizes. We also\nintroduce ACETADA, a new food-image dataset slated for public release. This\nopen dataset provides nutrition information verified by the dietitian and\nserves as the foundation for our analysis. Our evaluation across eight LMMs\n(four open-weight and four closed-weight) first establishes the benefit of\ncontextual metadata integration over straightforward prompting with images\nalone. We then demonstrate how this incorporation of contextual information\nenhances the efficacy of reasoning modifiers, such as Chain-of-Thought,\nMultimodal Chain-of-Thought, Scale Hint, Few-Shot, and Expert Persona.\nEmpirical results show that integrating metadata intelligently, when applied\nthrough straightforward prompting strategies, can significantly reduce the Mean\nAbsolute Error (MAE) and Mean Absolute Percentage Error (MAPE) in predicted\nnutritional values. This work highlights the potential of context-aware LMMs\nfor improved nutrition analysis."}
{"id": "2507.07073", "pdf": "https://arxiv.org/pdf/2507.07073", "abs": "https://arxiv.org/abs/2507.07073", "authors": ["Yulin An", "Enrique del Castillo"], "title": "An AI Approach for Learning the Spectrum of the Laplace-Beltrami Operator", "categories": ["cs.CV", "cs.AI"], "comment": "18 pages, 9 figures, submitted for publication", "summary": "The spectrum of the Laplace-Beltrami (LB) operator is central in geometric\ndeep learning tasks, capturing intrinsic properties of the shape of the object\nunder consideration. The best established method for its estimation, from a\ntriangulated mesh of the object, is based on the Finite Element Method (FEM),\nand computes the top k LB eigenvalues with a complexity of O(Nk), where N is\nthe number of points. This can render the FEM method inefficient when\nrepeatedly applied to databases of CAD mechanical parts, or in quality control\napplications where part metrology is acquired as large meshes and decisions\nabout the quality of each part are needed quickly and frequently. As a solution\nto this problem, we present a geometric deep learning framework to predict the\nLB spectrum efficiently given the CAD mesh of a part, achieving significant\ncomputational savings without sacrificing accuracy, demonstrating that the LB\nspectrum is learnable. The proposed Graph Neural Network architecture uses a\nrich set of part mesh features - including Gaussian curvature, mean curvature,\nand principal curvatures. In addition to our trained network, we make\navailable, for repeatability, a large curated dataset of real-world mechanical\nCAD models derived from the publicly available ABC dataset used for training\nand testing. Experimental results show that our method reduces computation time\nof the LB spectrum by approximately 5 times over linear FEM while delivering\ncompetitive accuracy."}
{"id": "2507.07077", "pdf": "https://arxiv.org/pdf/2507.07077", "abs": "https://arxiv.org/abs/2507.07077", "authors": ["Yimu Pan", "Manas Mehta", "Gwen Sincerbeaux", "Jeffery A. Goldstein", "Alison D. Gernand", "James Z. Wang"], "title": "Reading a Ruler in the Wild", "categories": ["cs.CV"], "comment": null, "summary": "Accurately converting pixel measurements into absolute real-world dimensions\nremains a fundamental challenge in computer vision and limits progress in key\napplications such as biomedicine, forensics, nutritional analysis, and\ne-commerce. We introduce RulerNet, a deep learning framework that robustly\ninfers scale \"in the wild\" by reformulating ruler reading as a unified\nkeypoint-detection problem and by representing the ruler with\ngeometric-progression parameters that are invariant to perspective\ntransformations. Unlike traditional methods that rely on handcrafted thresholds\nor rigid, ruler-specific pipelines, RulerNet directly localizes centimeter\nmarks using a distortion-invariant annotation and training strategy, enabling\nstrong generalization across diverse ruler types and imaging conditions while\nmitigating data scarcity. We also present a scalable synthetic-data pipeline\nthat combines graphics-based ruler generation with ControlNet to add\nphotorealistic context, greatly increasing training diversity and improving\nperformance. To further enhance robustness and efficiency, we propose DeepGP, a\nlightweight feed-forward network that regresses geometric-progression\nparameters from noisy marks and eliminates iterative optimization, enabling\nreal-time scale estimation on mobile or edge devices. Experiments show that\nRulerNet delivers accurate, consistent, and efficient scale estimates under\nchallenging real-world conditions. These results underscore its utility as a\ngeneralizable measurement tool and its potential for integration with other\nvision components for automated, scale-aware analysis in high-impact domains. A\nlive demo is available at https://huggingface.co/spaces/ymp5078/RulerNet-Demo."}
{"id": "2507.07079", "pdf": "https://arxiv.org/pdf/2507.07079", "abs": "https://arxiv.org/abs/2507.07079", "authors": ["Ziyue Liu", "Federico Girella", "Yiming Wang", "Davide Talon"], "title": "Evaluating Attribute Confusion in Fashion Text-to-Image Generation", "categories": ["cs.CV"], "comment": "Accepted to ICIAP25. Project page: site\n  [https://intelligolabs.github.io/L-VQAScore/\\", "summary": "Despite the rapid advances in Text-to-Image (T2I) generation models, their\nevaluation remains challenging in domains like fashion, involving complex\ncompositional generation. Recent automated T2I evaluation methods leverage\npre-trained vision-language models to measure cross-modal alignment. However,\nour preliminary study reveals that they are still limited in assessing rich\nentity-attribute semantics, facing challenges in attribute confusion, i.e.,\nwhen attributes are correctly depicted but associated to the wrong entities. To\naddress this, we build on a Visual Question Answering (VQA) localization\nstrategy targeting one single entity at a time across both visual and textual\nmodalities. We propose a localized human evaluation protocol and introduce a\nnovel automatic metric, Localized VQAScore (L-VQAScore), that combines visual\nlocalization with VQA probing both correct (reflection) and miss-localized\n(leakage) attribute generation. On a newly curated dataset featuring\nchallenging compositional alignment scenarios, L-VQAScore outperforms\nstate-of-the-art T2I evaluation methods in terms of correlation with human\njudgments, demonstrating its strength in capturing fine-grained\nentity-attribute associations. We believe L-VQAScore can be a reliable and\nscalable alternative to subjective evaluations."}
{"id": "2507.07095", "pdf": "https://arxiv.org/pdf/2507.07095", "abs": "https://arxiv.org/abs/2507.07095", "authors": ["Ke Fan", "Shunlin Lu", "Minyue Dai", "Runyi Yu", "Lixing Xiao", "Zhiyang Dou", "Junting Dong", "Lizhuang Ma", "Jingbo Wang"], "title": "Go to Zero: Towards Zero-shot Motion Generation with Million-scale Data", "categories": ["cs.CV"], "comment": "Project Page: https://vankouf.github.io/MotionMillion/", "summary": "Generating diverse and natural human motion sequences based on textual\ndescriptions constitutes a fundamental and challenging research area within the\ndomains of computer vision, graphics, and robotics. Despite significant\nadvancements in this field, current methodologies often face challenges\nregarding zero-shot generalization capabilities, largely attributable to the\nlimited size of training datasets. Moreover, the lack of a comprehensive\nevaluation framework impedes the advancement of this task by failing to\nidentify directions for improvement. In this work, we aim to push\ntext-to-motion into a new era, that is, to achieve the generalization ability\nof zero-shot. To this end, firstly, we develop an efficient annotation pipeline\nand introduce MotionMillion-the largest human motion dataset to date, featuring\nover 2,000 hours and 2 million high-quality motion sequences. Additionally, we\npropose MotionMillion-Eval, the most comprehensive benchmark for evaluating\nzero-shot motion generation. Leveraging a scalable architecture, we scale our\nmodel to 7B parameters and validate its performance on MotionMillion-Eval. Our\nresults demonstrate strong generalization to out-of-domain and complex\ncompositional motions, marking a significant step toward zero-shot human motion\ngeneration. The code is available at\nhttps://github.com/VankouF/MotionMillion-Codes."}
{"id": "2507.07104", "pdf": "https://arxiv.org/pdf/2507.07104", "abs": "https://arxiv.org/abs/2507.07104", "authors": ["Tiezheng Zhang", "Yitong Li", "Yu-cheng Chou", "Jieneng Chen", "Alan Yuille", "Chen Wei", "Junfei Xiao"], "title": "Vision-Language-Vision Auto-Encoder: Scalable Knowledge Distillation from Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "Building state-of-the-art Vision-Language Models (VLMs) with strong\ncaptioning capabilities typically necessitates training on billions of\nhigh-quality image-text pairs, requiring millions of GPU hours. This paper\nintroduces the Vision-Language-Vision (VLV) auto-encoder framework, which\nstrategically leverages key pretrained components: a vision encoder, the\ndecoder of a Text-to-Image (T2I) diffusion model, and subsequently, a Large\nLanguage Model (LLM). Specifically, we establish an information bottleneck by\nregularizing the language representation space, achieved through freezing the\npretrained T2I diffusion decoder. Our VLV pipeline effectively distills\nknowledge from the text-conditioned diffusion model using continuous\nembeddings, demonstrating comprehensive semantic understanding via high-quality\nreconstructions. Furthermore, by fine-tuning a pretrained LLM to decode the\nintermediate language representations into detailed descriptions, we construct\na state-of-the-art (SoTA) captioner comparable to leading models like GPT-4o\nand Gemini 2.0 Flash. Our method demonstrates exceptional cost-efficiency and\nsignificantly reduces data requirements; by primarily utilizing single-modal\nimages for training and maximizing the utility of existing pretrained models\n(image encoder, T2I diffusion model, and LLM), it circumvents the need for\nmassive paired image-text datasets, keeping the total training expenditure\nunder $1,000 USD."}
{"id": "2507.07105", "pdf": "https://arxiv.org/pdf/2507.07105", "abs": "https://arxiv.org/abs/2507.07105", "authors": ["Yushen Zuo", "Qi Zheng", "Mingyang Wu", "Xinrui Jiang", "Renjie Li", "Jian Wang", "Yide Zhang", "Gengchen Mai", "Lihong V. Wang", "James Zou", "Xiaoyu Wang", "Ming-Hsuan Yang", "Zhengzhong Tu"], "title": "4KAgent: Agentic Any Image to 4K Super-Resolution", "categories": ["cs.CV", "eess.IV"], "comment": "Project page: https://4kagent.github.io", "summary": "We present 4KAgent, a unified agentic super-resolution generalist system\ndesigned to universally upscale any image to 4K resolution (and even higher, if\napplied iteratively). Our system can transform images from extremely low\nresolutions with severe degradations, for example, highly distorted inputs at\n256x256, into crystal-clear, photorealistic 4K outputs. 4KAgent comprises three\ncore components: (1) Profiling, a module that customizes the 4KAgent pipeline\nbased on bespoke use cases; (2) A Perception Agent, which leverages\nvision-language models alongside image quality assessment experts to analyze\nthe input image and make a tailored restoration plan; and (3) A Restoration\nAgent, which executes the plan, following a recursive execution-reflection\nparadigm, guided by a quality-driven mixture-of-expert policy to select the\noptimal output for each step. Additionally, 4KAgent embeds a specialized face\nrestoration pipeline, significantly enhancing facial details in portrait and\nselfie photos. We rigorously evaluate our 4KAgent across 11 distinct task\ncategories encompassing a total of 26 diverse benchmarks, setting new\nstate-of-the-art on a broad spectrum of imaging domains. Our evaluations cover\nnatural images, portrait photos, AI-generated content, satellite imagery,\nfluorescence microscopy, and medical imaging like fundoscopy, ultrasound, and\nX-ray, demonstrating superior performance in terms of both perceptual (e.g.,\nNIQE, MUSIQ) and fidelity (e.g., PSNR) metrics. By establishing a novel agentic\nparadigm for low-level vision tasks, we aim to catalyze broader interest and\ninnovation within vision-centric autonomous agents across diverse research\ncommunities. We will release all the code, models, and results at:\nhttps://4kagent.github.io."}
{"id": "2507.07106", "pdf": "https://arxiv.org/pdf/2507.07106", "abs": "https://arxiv.org/abs/2507.07106", "authors": ["Vatsal Agarwal", "Matthew Gwilliam", "Gefen Kohavi", "Eshan Verma", "Daniel Ulbricht", "Abhinav Shrivastava"], "title": "Towards Multimodal Understanding via Stable Diffusion as a Task-Aware Feature Extractor", "categories": ["cs.CV", "cs.LG"], "comment": "Website: see https://vatsalag99.github.io/mustafar/", "summary": "Recent advances in multimodal large language models (MLLMs) have enabled\nimage-based question-answering capabilities. However, a key limitation is the\nuse of CLIP as the visual encoder; while it can capture coarse global\ninformation, it often can miss fine-grained details that are relevant to the\ninput query. To address these shortcomings, this work studies whether\npre-trained text-to-image diffusion models can serve as instruction-aware\nvisual encoders. Through an analysis of their internal representations, we find\ndiffusion features are both rich in semantics and can encode strong image-text\nalignment. Moreover, we find that we can leverage text conditioning to focus\nthe model on regions relevant to the input question. We then investigate how to\nalign these features with large language models and uncover a leakage\nphenomenon, where the LLM can inadvertently recover information from the\noriginal diffusion prompt. We analyze the causes of this leakage and propose a\nmitigation strategy. Based on these insights, we explore a simple fusion\nstrategy that utilizes both CLIP and conditional diffusion features. We\nevaluate our approach on both general VQA and specialized MLLM benchmarks,\ndemonstrating the promise of diffusion models for visual understanding,\nparticularly in vision-centric tasks that require spatial and compositional\nreasoning. Our project page can be found\nhttps://vatsalag99.github.io/mustafar/."}
{"id": "2507.06264", "pdf": "https://arxiv.org/pdf/2507.06264", "abs": "https://arxiv.org/abs/2507.06264", "authors": ["Weronika Hryniewska-Guzik", "Przemyslaw Biecek"], "title": "X-ray transferable polyrepresentation learning", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": "part of Weronika's PhD thesis", "summary": "The success of machine learning algorithms is inherently related to the\nextraction of meaningful features, as they play a pivotal role in the\nperformance of these algorithms. Central to this challenge is the quality of\ndata representation. However, the ability to generalize and extract these\nfeatures effectively from unseen datasets is also crucial. In light of this, we\nintroduce a novel concept: the polyrepresentation. Polyrepresentation\nintegrates multiple representations of the same modality extracted from\ndistinct sources, for example, vector embeddings from the Siamese Network,\nself-supervised models, and interpretable radiomic features. This approach\nyields better performance metrics compared to relying on a single\nrepresentation. Additionally, in the context of X-ray images, we demonstrate\nthe transferability of the created polyrepresentation to a smaller dataset,\nunderscoring its potential as a pragmatic and resource-efficient approach in\nvarious image-related solutions. It is worth noting that the concept of\npolyprepresentation on the example of medical data can also be applied to other\ndomains, showcasing its versatility and broad potential impact."}
{"id": "2507.06363", "pdf": "https://arxiv.org/pdf/2507.06363", "abs": "https://arxiv.org/abs/2507.06363", "authors": ["Szymon Płotka", "Maciej Chrabaszcz", "Gizem Mert", "Ewa Szczurek", "Arkadiusz Sitek"], "title": "Mamba Goes HoME: Hierarchical Soft Mixture-of-Experts for 3D Medical Image Segmentation", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "In recent years, artificial intelligence has significantly advanced medical\nimage segmentation. However, challenges remain, including efficient 3D medical\nimage processing across diverse modalities and handling data variability. In\nthis work, we introduce Hierarchical Soft Mixture-of-Experts (HoME), a\ntwo-level token-routing layer for efficient long-context modeling, specifically\ndesigned for 3D medical image segmentation. Built on the Mamba state-space\nmodel (SSM) backbone, HoME enhances sequential modeling through sparse,\nadaptive expert routing. The first stage employs a Soft Mixture-of-Experts\n(SMoE) layer to partition input sequences into local groups, routing tokens to\nspecialized per-group experts for localized feature extraction. The second\nstage aggregates these outputs via a global SMoE layer, enabling cross-group\ninformation fusion and global context refinement. This hierarchical design,\ncombining local expert routing with global expert refinement improves\ngeneralizability and segmentation performance, surpassing state-of-the-art\nresults across datasets from the three most commonly used 3D medical imaging\nmodalities and data quality."}
{"id": "2507.06380", "pdf": "https://arxiv.org/pdf/2507.06380", "abs": "https://arxiv.org/abs/2507.06380", "authors": ["Habibur Rahaman", "Atri Chatterjee", "Swarup Bhunia"], "title": "Secure and Storage-Efficient Deep Learning Models for Edge AI Using Automatic Weight Generation", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "7 pages, 7 figures", "summary": "Complex neural networks require substantial memory to store a large number of\nsynaptic weights. This work introduces WINGs (Automatic Weight Generator for\nSecure and Storage-Efficient Deep Learning Models), a novel framework that\ndynamically generates layer weights in a fully connected neural network (FC)\nand compresses the weights in convolutional neural networks (CNNs) during\ninference, significantly reducing memory requirements without sacrificing\naccuracy. WINGs framework uses principal component analysis (PCA) for\ndimensionality reduction and lightweight support vector regression (SVR) models\nto predict layer weights in the FC networks, removing the need for storing\nfull-weight matrices and achieving substantial memory savings. It also\npreferentially compresses the weights in low-sensitivity layers of CNNs using\nPCA and SVR with sensitivity analysis. The sensitivity-aware design also offers\nan added level of security, as any bit-flip attack with weights in compressed\nlayers has an amplified and readily detectable effect on accuracy. WINGs\nachieves 53x compression for the FC layers and 28x for AlexNet with MNIST\ndataset, and 18x for Alexnet with CIFAR-10 dataset with 1-2% accuracy loss.\nThis significant reduction in memory results in higher throughput and lower\nenergy for DNN inference, making it attractive for resource-constrained edge\napplications."}
{"id": "2507.06384", "pdf": "https://arxiv.org/pdf/2507.06384", "abs": "https://arxiv.org/abs/2507.06384", "authors": ["Emerson P. Grabke", "Babak Taati", "Masoom A. Haider"], "title": "Mitigating Multi-Sequence 3D Prostate MRI Data Scarcity through Domain Adaptation using Locally-Trained Latent Diffusion Models for Prostate Cancer Detection", "categories": ["eess.IV", "cs.CV"], "comment": "BT and MAH are co-senior authors on the work. This work has been\n  submitted to the IEEE for possible publication", "summary": "Objective: Latent diffusion models (LDMs) could mitigate data scarcity\nchallenges affecting machine learning development for medical image\ninterpretation. The recent CCELLA LDM improved prostate cancer detection\nperformance using synthetic MRI for classifier training but was limited to the\naxial T2-weighted (AxT2) sequence, did not investigate inter-institutional\ndomain shift, and prioritized radiology over histopathology outcomes. We\npropose CCELLA++ to address these limitations and improve clinical utility.\nMethods: CCELLA++ expands CCELLA for simultaneous biparametric prostate MRI\n(bpMRI) generation, including the AxT2, high b-value diffusion series (HighB)\nand apparent diffusion coefficient map (ADC). Domain adaptation was\ninvestigated by pretraining classifiers on real or LDM-generated synthetic data\nfrom an internal institution, followed with fine-tuning on progressively\nsmaller fractions of an out-of-distribution, external dataset. Results:\nCCELLA++ improved 3D FID for HighB and ADC but not AxT2 (0.013, 0.012, 0.063\nrespectively) sequences compared to CCELLA (0.060). Classifier pretraining with\nCCELLA++ bpMRI outperformed real bpMRI in AP and AUC for all domain adaptation\nscenarios. CCELLA++ pretraining achieved highest classifier performance below\n50% (n=665) external dataset volume. Conclusion: Synthetic bpMRI generated by\nour method can improve downstream classifier generalization and performance\nbeyond real bpMRI or CCELLA-generated AxT2-only images. Future work should seek\nto quantify medical image sample quality, balance multi-sequence LDM training,\nand condition the LDM with additional information. Significance: The proposed\nCCELLA++ LDM can generate synthetic bpMRI that outperforms real data for domain\nadaptation with a limited target institution dataset. Our code is available at\nhttps://github.com/grabkeem/CCELLA-plus-plus"}
{"id": "2507.06404", "pdf": "https://arxiv.org/pdf/2507.06404", "abs": "https://arxiv.org/abs/2507.06404", "authors": ["Matteo Tiezzi", "Tommaso Apicella", "Carlos Cardenas-Perez", "Giovanni Fregonese", "Stefano Dafarra", "Pietro Morerio", "Daniele Pucci", "Alessio Del Bue"], "title": "Learning to Evaluate Autonomous Behaviour in Human-Robot Interaction", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": null, "summary": "Evaluating and comparing the performance of autonomous Humanoid Robots is\nchallenging, as success rate metrics are difficult to reproduce and fail to\ncapture the complexity of robot movement trajectories, critical in Human-Robot\nInteraction and Collaboration (HRIC). To address these challenges, we propose a\ngeneral evaluation framework that measures the quality of Imitation Learning\n(IL) methods by focusing on trajectory performance. We devise the Neural Meta\nEvaluator (NeME), a deep learning model trained to classify actions from robot\njoint trajectories. NeME serves as a meta-evaluator to compare the performance\nof robot control policies, enabling policy evaluation without requiring human\ninvolvement in the loop. We validate our framework on ergoCub, a humanoid\nrobot, using teleoperation data and comparing IL methods tailored to the\navailable platform. The experimental results indicate that our method is more\naligned with the success rate obtained on the robot than baselines, offering a\nreproducible, systematic, and insightful means for comparing the performance of\nmultimodal imitation learning approaches in complex HRI tasks."}
{"id": "2507.06410", "pdf": "https://arxiv.org/pdf/2507.06410", "abs": "https://arxiv.org/abs/2507.06410", "authors": ["Peyman Sharifian", "Xiaotong Hong", "Alireza Karimian", "Mehdi Amini", "Hossein Arabi"], "title": "Attention-Enhanced Deep Learning Ensemble for Breast Density Classification in Mammography", "categories": ["eess.IV", "cs.CV"], "comment": "2025 IEEE Nuclear Science Symposium, Medical Imaging Conference and\n  Room Temperature Semiconductor Detector Conference", "summary": "Breast density assessment is a crucial component of mammographic\ninterpretation, with high breast density (BI-RADS categories C and D)\nrepresenting both a significant risk factor for developing breast cancer and a\ntechnical challenge for tumor detection. This study proposes an automated deep\nlearning system for robust binary classification of breast density (low: A/B\nvs. high: C/D) using the VinDr-Mammo dataset. We implemented and compared four\nadvanced convolutional neural networks: ResNet18, ResNet50, EfficientNet-B0,\nand DenseNet121, each enhanced with channel attention mechanisms. To address\nthe inherent class imbalance, we developed a novel Combined Focal Label\nSmoothing Loss function that integrates focal loss, label smoothing, and\nclass-balanced weighting. Our preprocessing pipeline incorporated advanced\ntechniques, including contrast-limited adaptive histogram equalization (CLAHE)\nand comprehensive data augmentation. The individual models were combined\nthrough an optimized ensemble voting approach, achieving superior performance\n(AUC: 0.963, F1-score: 0.952) compared to any single model. This system\ndemonstrates significant potential to standardize density assessments in\nclinical practice, potentially improving screening efficiency and early cancer\ndetection rates while reducing inter-observer variability among radiologists."}
{"id": "2507.06417", "pdf": "https://arxiv.org/pdf/2507.06417", "abs": "https://arxiv.org/abs/2507.06417", "authors": ["Laura Pituková", "Peter Sinčák", "László József Kovács"], "title": "Capsule-ConvKAN: A Hybrid Neural Approach to Medical Image Classification", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": "Preprint version. Accepted to IEEE SMC 2025", "summary": "This study conducts a comprehensive comparison of four neural network\narchitectures: Convolutional Neural Network, Capsule Network, Convolutional\nKolmogorov--Arnold Network, and the newly proposed Capsule--Convolutional\nKolmogorov--Arnold Network. The proposed Capsule-ConvKAN architecture combines\nthe dynamic routing and spatial hierarchy capabilities of Capsule Network with\nthe flexible and interpretable function approximation of Convolutional\nKolmogorov--Arnold Networks. This novel hybrid model was developed to improve\nfeature representation and classification accuracy, particularly in challenging\nreal-world biomedical image data. The architectures were evaluated on a\nhistopathological image dataset, where Capsule-ConvKAN achieved the highest\nclassification performance with an accuracy of 91.21\\%. The results demonstrate\nthe potential of the newly introduced Capsule-ConvKAN in capturing spatial\npatterns, managing complex features, and addressing the limitations of\ntraditional convolutional models in medical image classification."}
{"id": "2507.06418", "pdf": "https://arxiv.org/pdf/2507.06418", "abs": "https://arxiv.org/abs/2507.06418", "authors": ["Changchun Yang", "Haoyang Li", "Yushuai Wu", "Yilan Zhang", "Yifeng Jiao", "Yu Zhang", "Rihan Huang", "Yuan Cheng", "Yuan Qi", "Xin Guo", "Xin Gao"], "title": "PAST: A multimodal single-cell foundation model for histopathology and spatial transcriptomics in cancer", "categories": ["q-bio.QM", "cs.CV", "stat.AP"], "comment": null, "summary": "While pathology foundation models have transformed cancer image analysis,\nthey often lack integration with molecular data at single-cell resolution,\nlimiting their utility for precision oncology. Here, we present PAST, a\npan-cancer single-cell foundation model trained on 20 million paired\nhistopathology images and single-cell transcriptomes spanning multiple tumor\ntypes and tissue contexts. By jointly encoding cellular morphology and gene\nexpression, PAST learns unified cross-modal representations that capture both\nspatial and molecular heterogeneity at the cellular level. This approach\nenables accurate prediction of single-cell gene expression, virtual molecular\nstaining, and multimodal survival analysis directly from routine pathology\nslides. Across diverse cancers and downstream tasks, PAST consistently exceeds\nthe performance of existing approaches, demonstrating robust generalizability\nand scalability. Our work establishes a new paradigm for pathology foundation\nmodels, providing a versatile tool for high-resolution spatial omics,\nmechanistic discovery, and precision cancer research."}
{"id": "2507.06484", "pdf": "https://arxiv.org/pdf/2507.06484", "abs": "https://arxiv.org/abs/2507.06484", "authors": ["Fan-Yun Sun", "Shengguang Wu", "Christian Jacobsen", "Thomas Yim", "Haoming Zou", "Alex Zook", "Shangru Li", "Yu-Hsin Chou", "Ethem Can", "Xunlei Wu", "Clemens Eppner", "Valts Blukis", "Jonathan Tremblay", "Jiajun Wu", "Stan Birchfield", "Nick Haber"], "title": "3D-Generalist: Self-Improving Vision-Language-Action Models for Crafting 3D Worlds", "categories": ["cs.GR", "cs.CV"], "comment": "project website: https://ai.stanford.edu/~sunfanyun/3d-generalist/", "summary": "Despite large-scale pretraining endowing models with language and vision\nreasoning capabilities, improving their spatial reasoning capability remains\nchallenging due to the lack of data grounded in the 3D world. While it is\npossible for humans to manually create immersive and interactive worlds through\n3D graphics, as seen in applications such as VR, gaming, and robotics, this\nprocess remains highly labor-intensive. In this paper, we propose a scalable\nmethod for generating high-quality 3D environments that can serve as training\ndata for foundation models. We recast 3D environment building as a sequential\ndecision-making problem, employing Vision-Language-Models (VLMs) as policies\nthat output actions to jointly craft a 3D environment's layout, materials,\nlighting, and assets. Our proposed framework, 3D-Generalist, trains VLMs to\ngenerate more prompt-aligned 3D environments via self-improvement fine-tuning.\nWe demonstrate the effectiveness of 3D-Generalist and the proposed training\nstrategy in generating simulation-ready 3D environments. Furthermore, we\ndemonstrate its quality and scalability in synthetic data generation by\npretraining a vision foundation model on the generated data. After fine-tuning\nthe pre-trained model on downstream tasks, we show that it surpasses models\npre-trained on meticulously human-crafted synthetic data and approaches results\nachieved with real data orders of magnitude larger."}
{"id": "2507.06581", "pdf": "https://arxiv.org/pdf/2507.06581", "abs": "https://arxiv.org/abs/2507.06581", "authors": ["Qibiao Wu", "Yagang Wang", "Qian Zhang"], "title": "Airway Segmentation Network for Enhanced Tubular Feature Extraction", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Manual annotation of airway regions in computed tomography images is a\ntime-consuming and expertise-dependent task. Automatic airway segmentation is\ntherefore a prerequisite for enabling rapid bronchoscopic navigation and the\nclinical deployment of bronchoscopic robotic systems. Although convolutional\nneural network methods have gained considerable attention in airway\nsegmentation, the unique tree-like structure of airways poses challenges for\nconventional and deformable convolutions, which often fail to focus on fine\nairway structures, leading to missed segments and discontinuities. To address\nthis issue, this study proposes a novel tubular feature extraction network,\nnamed TfeNet. TfeNet introduces a novel direction-aware convolution operation\nthat first applies spatial rotation transformations to adjust the sampling\npositions of linear convolution kernels. The deformed kernels are then\nrepresented as line segments or polylines in 3D space. Furthermore, a tubular\nfeature fusion module (TFFM) is designed based on asymmetric convolution and\nresidual connection strategies, enhancing the network's focus on subtle airway\nstructures. Extensive experiments conducted on one public dataset and two\ndatasets used in airway segmentation challenges demonstrate that the proposed\nTfeNet achieves more accuracy and continuous airway structure predictions\ncompared with existing methods. In particular, TfeNet achieves the highest\noverall score of 94.95% on the current largest airway segmentation dataset,\nAirway Tree Modeling(ATM22), and demonstrates advanced performance on the lung\nfibrosis dataset(AIIB23). The code is available at\nhttps://github.com/QibiaoWu/TfeNet."}
{"id": "2507.06613", "pdf": "https://arxiv.org/pdf/2507.06613", "abs": "https://arxiv.org/abs/2507.06613", "authors": ["Anshuk Uppal", "Yuhta Takida", "Chieh-Hsin Lai", "Yuki Mitsufuji"], "title": "Denoising Multi-Beta VAE: Representation Learning for Disentanglement and Generation", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "24 pages, 8 figures and 7 tables", "summary": "Disentangled and interpretable latent representations in generative models\ntypically come at the cost of generation quality. The $\\beta$-VAE framework\nintroduces a hyperparameter $\\beta$ to balance disentanglement and\nreconstruction quality, where setting $\\beta > 1$ introduces an information\nbottleneck that favors disentanglement over sharp, accurate reconstructions. To\naddress this trade-off, we propose a novel generative modeling framework that\nleverages a range of $\\beta$ values to learn multiple corresponding latent\nrepresentations. First, we obtain a slew of representations by training a\nsingle variational autoencoder (VAE), with a new loss function that controls\nthe information retained in each latent representation such that the higher\n$\\beta$ value prioritize disentanglement over reconstruction fidelity. We then,\nintroduce a non-linear diffusion model that smoothly transitions latent\nrepresentations corresponding to different $\\beta$ values. This model denoises\ntowards less disentangled and more informative representations, ultimately\nleading to (almost) lossless representations, enabling sharp reconstructions.\nFurthermore, our model supports sample generation without input images,\nfunctioning as a standalone generative model. We evaluate our framework in\nterms of both disentanglement and generation quality. Additionally, we observe\nsmooth transitions in the latent spaces with respect to changes in $\\beta$,\nfacilitating consistent manipulation of generated outputs."}
{"id": "2507.06747", "pdf": "https://arxiv.org/pdf/2507.06747", "abs": "https://arxiv.org/abs/2507.06747", "authors": ["Daojie Peng", "Jiahang Cao", "Qiang Zhang", "Jun Ma"], "title": "LOVON: Legged Open-Vocabulary Object Navigator", "categories": ["cs.RO", "cs.CV"], "comment": "9 pages, 10 figures; Project Page:\n  https://daojiepeng.github.io/LOVON/", "summary": "Object navigation in open-world environments remains a formidable and\npervasive challenge for robotic systems, particularly when it comes to\nexecuting long-horizon tasks that require both open-world object detection and\nhigh-level task planning. Traditional methods often struggle to integrate these\ncomponents effectively, and this limits their capability to deal with complex,\nlong-range navigation missions. In this paper, we propose LOVON, a novel\nframework that integrates large language models (LLMs) for hierarchical task\nplanning with open-vocabulary visual detection models, tailored for effective\nlong-range object navigation in dynamic, unstructured environments. To tackle\nreal-world challenges including visual jittering, blind zones, and temporary\ntarget loss, we design dedicated solutions such as Laplacian Variance Filtering\nfor visual stabilization. We also develop a functional execution logic for the\nrobot that guarantees LOVON's capabilities in autonomous navigation, task\nadaptation, and robust task completion. Extensive evaluations demonstrate the\nsuccessful completion of long-sequence tasks involving real-time detection,\nsearch, and navigation toward open-vocabulary dynamic targets. Furthermore,\nreal-world experiments across different legged robots (Unitree Go2, B2, and\nH1-2) showcase the compatibility and appealing plug-and-play feature of LOVON."}
{"id": "2507.06764", "pdf": "https://arxiv.org/pdf/2507.06764", "abs": "https://arxiv.org/abs/2507.06764", "authors": ["Guixian Xu", "Jinglai Li", "Junqi Tang"], "title": "Fast Equivariant Imaging: Acceleration for Unsupervised Learning via Augmented Lagrangian and Auxiliary PnP Denoisers", "categories": ["eess.IV", "cs.CV", "cs.LG", "math.OC"], "comment": null, "summary": "We propose Fast Equivariant Imaging (FEI), a novel unsupervised learning\nframework to efficiently train deep imaging networks without ground-truth data.\nFrom the perspective of reformulating the Equivariant Imaging based\noptimization problem via the method of Lagrange multipliers and utilizing\nplug-and-play denoisers, this novel unsupervised scheme shows superior\nefficiency and performance compared to vanilla Equivariant Imaging paradigm. In\nparticular, our PnP-FEI scheme achieves an order-of-magnitude (10x)\nacceleration over standard EI on training U-Net with CT100 dataset for X-ray CT\nreconstruction, with improved generalization performance."}
{"id": "2507.06828", "pdf": "https://arxiv.org/pdf/2507.06828", "abs": "https://arxiv.org/abs/2507.06828", "authors": ["Xuesong Li", "Nassir Navab", "Zhongliang Jiang"], "title": "Speckle2Self: Self-Supervised Ultrasound Speckle Reduction Without Clean Data", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Image denoising is a fundamental task in computer vision, particularly in\nmedical ultrasound (US) imaging, where speckle noise significantly degrades\nimage quality. Although recent advancements in deep neural networks have led to\nsubstantial improvements in denoising for natural images, these methods cannot\nbe directly applied to US speckle noise, as it is not purely random. Instead,\nUS speckle arises from complex wave interference within the body\nmicrostructure, making it tissue-dependent. This dependency means that\nobtaining two independent noisy observations of the same scene, as required by\npioneering Noise2Noise, is not feasible. Additionally, blind-spot networks also\ncannot handle US speckle noise due to its high spatial dependency. To address\nthis challenge, we introduce Speckle2Self, a novel self-supervised algorithm\nfor speckle reduction using only single noisy observations. The key insight is\nthat applying a multi-scale perturbation (MSP) operation introduces\ntissue-dependent variations in the speckle pattern across different scales,\nwhile preserving the shared anatomical structure. This enables effective\nspeckle suppression by modeling the clean image as a low-rank signal and\nisolating the sparse noise component. To demonstrate its effectiveness,\nSpeckle2Self is comprehensively compared with conventional filter-based\ndenoising algorithms and SOTA learning-based methods, using both realistic\nsimulated US images and human carotid US images. Additionally, data from\nmultiple US machines are employed to evaluate model generalization and\nadaptability to images from unseen domains. \\textit{Code and datasets will be\nreleased upon acceptance."}
{"id": "2507.06867", "pdf": "https://arxiv.org/pdf/2507.06867", "abs": "https://arxiv.org/abs/2507.06867", "authors": ["Tiffany Ding", "Jean-Baptiste Fermanian", "Joseph Salmon"], "title": "Conformal Prediction for Long-Tailed Classification", "categories": ["stat.ML", "cs.CV", "cs.LG", "stat.ME"], "comment": null, "summary": "Many real-world classification problems, such as plant identification, have\nextremely long-tailed class distributions. In order for prediction sets to be\nuseful in such settings, they should (i) provide good class-conditional\ncoverage, ensuring that rare classes are not systematically omitted from the\nprediction sets, and (ii) be a reasonable size, allowing users to easily verify\ncandidate labels. Unfortunately, existing conformal prediction methods, when\napplied to the long-tailed setting, force practitioners to make a binary choice\nbetween small sets with poor class-conditional coverage or sets with very good\nclass-conditional coverage but that are extremely large. We propose methods\nwith guaranteed marginal coverage that smoothly trade off between set size and\nclass-conditional coverage. First, we propose a conformal score function,\nprevalence-adjusted softmax, that targets a relaxed notion of class-conditional\ncoverage called macro-coverage. Second, we propose a label-weighted conformal\nprediction method that allows us to interpolate between marginal and\nclass-conditional conformal prediction. We demonstrate our methods on Pl@ntNet\nand iNaturalist, two long-tailed image datasets with 1,081 and 8,142 classes,\nrespectively."}
{"id": "2507.06955", "pdf": "https://arxiv.org/pdf/2507.06955", "abs": "https://arxiv.org/abs/2507.06955", "authors": ["Kaveh Moradkhani", "R Jarrett Rushmore", "Sylvain Bouix"], "title": "SimCortex: Collision-free Simultaneous Cortical Surfaces Reconstruction", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Accurate cortical surface reconstruction from magnetic resonance imaging\n(MRI) data is crucial for reliable neuroanatomical analyses. Current methods\nhave to contend with complex cortical geometries, strict topological\nrequirements, and often produce surfaces with overlaps, self-intersections, and\ntopological defects. To overcome these shortcomings, we introduce SimCortex, a\ndeep learning framework that simultaneously reconstructs all brain surfaces\n(left/right white-matter and pial) from T1-weighted(T1w) MRI volumes while\npreserving topological properties. Our method first segments the T1w image into\na nine-class tissue label map. From these segmentations, we generate\nsubject-specific, collision-free initial surface meshes. These surfaces serve\nas precise initializations for subsequent multiscale diffeomorphic\ndeformations. Employing stationary velocity fields (SVFs) integrated via\nscaling-and-squaring, our approach ensures smooth, topology-preserving\ntransformations with significantly reduced surface collisions and\nself-intersections. Evaluations on standard datasets demonstrate that SimCortex\ndramatically reduces surface overlaps and self-intersections, surpassing\ncurrent methods while maintaining state-of-the-art geometric accuracy."}
{"id": "2507.06979", "pdf": "https://arxiv.org/pdf/2507.06979", "abs": "https://arxiv.org/abs/2507.06979", "authors": ["Panagiotis Koromilas", "Efthymios Georgiou", "Giorgos Bouritsas", "Theodoros Giannakopoulos", "Mihalis A. Nicolaou", "Yannis Panagakis"], "title": "A Principled Framework for Multi-View Contrastive Learning", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Contrastive Learning (CL), a leading paradigm in Self-Supervised Learning\n(SSL), typically relies on pairs of data views generated through augmentation.\nWhile multiple augmentations per instance (more than two) improve\ngeneralization in supervised learning, current CL methods handle additional\nviews suboptimally by simply aggregating different pairwise objectives. This\napproach suffers from four critical limitations: (L1) it utilizes multiple\noptimization terms per data point resulting to conflicting objectives, (L2) it\nfails to model all interactions across views and data points, (L3) it inherits\nfundamental limitations (e.g. alignment-uniformity coupling) from pairwise CL\nlosses, and (L4) it prevents fully realizing the benefits of increased view\nmultiplicity observed in supervised settings. We address these limitations\nthrough two novel loss functions: MV-InfoNCE, which extends InfoNCE to\nincorporate all possible view interactions simultaneously in one term per data\npoint, and MV-DHEL, which decouples alignment from uniformity across views\nwhile scaling interaction complexity with view multiplicity. Both approaches\nare theoretically grounded - we prove they asymptotically optimize for\nalignment of all views and uniformity, providing principled extensions to\nmulti-view contrastive learning. Our empirical results on ImageNet1K and three\nother datasets demonstrate that our methods consistently outperform existing\nmulti-view approaches and effectively scale with increasing view multiplicity.\nWe also apply our objectives to multimodal data and show that, in contrast to\nother contrastive objectives, they can scale beyond just two modalities. Most\nsignificantly, ablation studies reveal that MV-DHEL with five or more views\neffectively mitigates dimensionality collapse by fully utilizing the embedding\nspace, thereby delivering multi-view benefits observed in supervised learning."}
{"id": "2507.06993", "pdf": "https://arxiv.org/pdf/2507.06993", "abs": "https://arxiv.org/abs/2507.06993", "authors": ["Jieren Deng", "Aleksandar Cvetkovic", "Pak Kiu Chung", "Dragomir Yankov", "Chiqun Zhang"], "title": "The User-Centric Geo-Experience: An LLM-Powered Framework for Enhanced Planning, Navigation, and Dynamic Adaptation", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "Traditional travel-planning systems are often static and fragmented, leaving\nthem ill-equipped to handle real-world complexities such as evolving\nenvironmental conditions and unexpected itinerary disruptions. In this paper,\nwe identify three gaps between existing service providers causing frustrating\nuser experience: intelligent trip planning, precision \"last-100-meter\"\nnavigation, and dynamic itinerary adaptation. We propose three cooperative\nagents: a Travel Planning Agent that employs grid-based spatial grounding and\nmap analysis to help resolve complex multi-modal user queries; a Destination\nAssistant Agent that provides fine-grained guidance for the final navigation\nleg of each journey; and a Local Discovery Agent that leverages image\nembeddings and Retrieval-Augmented Generation (RAG) to detect and respond to\ntrip plan disruptions. With evaluations and experiments, our system\ndemonstrates substantial improvements in query interpretation, navigation\naccuracy, and disruption resilience, underscoring its promise for applications\nfrom urban exploration to emergency response."}
{"id": "2507.07000", "pdf": "https://arxiv.org/pdf/2507.07000", "abs": "https://arxiv.org/abs/2507.07000", "authors": ["Wijayathunga W. M. R. D. B"], "title": "Enhancing non-Rigid 3D Model Deformations Using Mesh-based Gaussian Splatting", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "We propose a novel framework that enhances non-rigid 3D model deformations by\nbridging mesh representations with 3D Gaussian splatting. While traditional\nGaussian splatting delivers fast, real-time radiance-field rendering, its\npost-editing capabilities and support for large-scale, non-rigid deformations\nremain limited. Our method addresses these challenges by embedding Gaussian\nkernels directly onto explicit mesh surfaces. This allows the mesh's inherent\ntopological and geometric priors to guide intuitive editing operations -- such\nas moving, scaling, and rotating individual 3D components -- and enables\ncomplex deformations like bending and stretching. This work paves the way for\nmore flexible 3D content-creation workflows in applications spanning virtual\nreality, character animation, and interactive design."}
{"id": "2507.07011", "pdf": "https://arxiv.org/pdf/2507.07011", "abs": "https://arxiv.org/abs/2507.07011", "authors": ["Daniel Onah", "Ravish Desai"], "title": "Deep Brain Net: An Optimized Deep Learning Model for Brain tumor Detection in MRI Images Using EfficientNetB0 and ResNet50 with Transfer Learning", "categories": ["eess.IV", "cs.CV"], "comment": "9 pages, 14 figures, 4 tables. To be submitted to a conference", "summary": "In recent years, deep learning has shown great promise in the automated\ndetection and classification of brain tumors from MRI images. However,\nachieving high accuracy and computational efficiency remains a challenge. In\nthis research, we propose Deep Brain Net, a novel deep learning system designed\nto optimize performance in the detection of brain tumors. The model integrates\nthe strengths of two advanced neural network architectures which are\nEfficientNetB0 and ResNet50, combined with transfer learning to improve\ngeneralization and reduce training time. The EfficientNetB0 architecture\nenhances model efficiency by utilizing mobile inverted bottleneck blocks, which\nincorporate depth wise separable convolutions. This design significantly\nreduces the number of parameters and computational cost while preserving the\nability of models to learn complex feature representations. The ResNet50\narchitecture, pre trained on large scale datasets like ImageNet, is fine tuned\nfor brain tumor classification. Its use of residual connections allows for\ntraining deeper networks by mitigating the vanishing gradient problem and\navoiding performance degradation. The integration of these components ensures\nthat the proposed system is both computationally efficient and highly accurate.\nExtensive experiments performed on publicly available MRI datasets demonstrate\nthat Deep Brain Net consistently outperforms existing state of the art methods\nin terms of classification accuracy, precision, recall, and computational\nefficiency. The result is an accuracy of 88 percent, a weighted F1 score of\n88.75 percent, and a macro AUC ROC score of 98.17 percent which demonstrates\nthe robustness and clinical potential of Deep Brain Net in assisting\nradiologists with brain tumor diagnosis."}
{"id": "2507.07100", "pdf": "https://arxiv.org/pdf/2507.07100", "abs": "https://arxiv.org/abs/2507.07100", "authors": ["Lan Li", "Da-Wei Zhou", "Han-Jia Ye", "De-Chuan Zhan"], "title": "Addressing Imbalanced Domain-Incremental Learning through Dual-Balance Collaborative Experts", "categories": ["cs.LG", "cs.CV"], "comment": "Accepted by ICML 2025", "summary": "Domain-Incremental Learning (DIL) focuses on continual learning in\nnon-stationary environments, requiring models to adjust to evolving domains\nwhile preserving historical knowledge. DIL faces two critical challenges in the\ncontext of imbalanced data: intra-domain class imbalance and cross-domain class\ndistribution shifts. These challenges significantly hinder model performance,\nas intra-domain imbalance leads to underfitting of few-shot classes, while\ncross-domain shifts require maintaining well-learned many-shot classes and\ntransferring knowledge to improve few-shot class performance in old domains. To\novercome these challenges, we introduce the Dual-Balance Collaborative Experts\n(DCE) framework. DCE employs a frequency-aware expert group, where each expert\nis guided by specialized loss functions to learn features for specific\nfrequency groups, effectively addressing intra-domain class imbalance.\nSubsequently, a dynamic expert selector is learned by synthesizing\npseudo-features through balanced Gaussian sampling from historical class\nstatistics. This mechanism navigates the trade-off between preserving many-shot\nknowledge of previous domains and leveraging new data to improve few-shot class\nperformance in earlier tasks. Extensive experimental results on four benchmark\ndatasets demonstrate DCE's state-of-the-art performance."}
